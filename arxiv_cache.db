{"_default": {"1": {"title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "authors": ["Alexander Okupnik, Johannes Schneider, Kyriakos Flouris"], "abstract": "arXiv:2511.00011v1 Announce Type: new \nRecent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also \"creatively\" enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.", "categories": ["cs.CV", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00011", "pdf_url": "https://arxiv.org/pdf/2511.00011.pdf", "is_interesting": false}, "2": {"title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "authors": ["Julio Jerison E. Macrohon, Gordon Hung"], "abstract": "arXiv:2511.00021v1 Announce Type: new \nCoral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00021", "pdf_url": "https://arxiv.org/pdf/2511.00021.pdf", "is_interesting": false}, "3": {"title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "authors": ["Jules Gerard, Leandro Di Bella, Filip Huyghe, Marc Kochzius"], "abstract": "arXiv:2511.00022v1 Announce Type: new \nCoral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00022", "pdf_url": "https://arxiv.org/pdf/2511.00022.pdf", "is_interesting": false}, "4": {"title": "Mutual Information guided Visual Contrastive Learning", "authors": ["Hanyang Chen, Yanchao Yang"], "abstract": "arXiv:2511.00028v1 Announce Type: new \nRepresentation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00028", "pdf_url": "https://arxiv.org/pdf/2511.00028.pdf", "is_interesting": false}, "5": {"title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "authors": ["Riya Gupta, Alexander Chowdhury, Sahil Nalawade"], "abstract": "arXiv:2511.00037v1 Announce Type: new \nFederated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.", "categories": ["cs.CV", "cs.DC"], "abs_url": "https://arxiv.org/abs/2511.00037", "pdf_url": "https://arxiv.org/pdf/2511.00037.pdf", "is_interesting": false}, "6": {"title": "Enhancing rice leaf images: An overview of image denoising techniques", "authors": ["Rupjyoti Chutia, Dibya Jyoti Bora"], "abstract": "arXiv:2511.00046v1 Announce Type: new \nDigital image processing involves the systematic handling of images using advanced computer algorithms, and has gained significant attention in both academic and practical fields. Image enhancement is a crucial preprocessing stage in the image-processing chain, improving image quality and emphasizing features. This makes subsequent tasks (segmentation, feature extraction, classification) more reliable. Image enhancement is essential for rice leaf analysis, aiding in disease detection, nutrient deficiency evaluation, and growth analysis. Denoising followed by contrast enhancement are the primary steps. Image filters, generally employed for denoising, transform or enhance visual characteristics like brightness, contrast, and sharpness, playing a crucial role in improving overall image quality and enabling the extraction of useful information. This work provides an extensive comparative study of well-known image-denoising methods combined with CLAHE (Contrast Limited Adaptive Histogram Equalization) for efficient denoising of rice leaf images. The experiments were performed on a rice leaf image dataset to ensure the data is relevant and representative. Results were examined using various metrics to comprehensively test enhancement methods. This approach provides a strong basis for assessing the effectiveness of methodologies in digital image processing and reveals insights useful for future adaptation in agricultural research and other domains.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00046", "pdf_url": "https://arxiv.org/pdf/2511.00046.pdf", "is_interesting": false}, "7": {"title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "authors": ["Zhiqi Qi, Runxin Zhao, Hanyang Zhuang, Chunxiang Wang, Ming Yang"], "abstract": "arXiv:2511.00060v1 Announce Type: new \nLiDAR-based roadside perception is a cornerstone of advanced Intelligent Transportation Systems (ITS). While considerable research has addressed optimal LiDAR placement for infrastructure, the profound impact of differing LiDAR scanning patterns on perceptual performance remains comparatively under-investigated. The inherent nature of various scanning modes - such as traditional repetitive (mechanical/solid-state) versus emerging non-repetitive (e.g. prism-based) systems - leads to distinct point cloud distributions at varying distances, critically dictating the efficacy of object detection and overall environmental understanding. To systematically investigate these differences in infrastructure-based contexts, we introduce the \"InfraLiDARs' Benchmark,\" a novel dataset meticulously collected in the CARLA simulation environment using concurrently operating infrastructure-based LiDARs exhibiting both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive statistical analysis of the respective LiDAR scanning abilities and evaluate the impact of these distinct patterns on the performance of various leading 3D object detection algorithms. Our findings reveal that non-repetitive scanning LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable detection performance across various scenarios. Despite non-repetitive LiDAR's limited perception range, it's a cost-effective option considering its low price. Ultimately, this study provides insights for setting up roadside perception system with optimal LiDAR scanning patterns and compatible algorithms for diverse roadside applications, and publicly releases the \"InfraLiDARs' Benchmark\" dataset to foster further research.", "categories": ["cs.CV", "cs.RO", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.00060", "pdf_url": "https://arxiv.org/pdf/2511.00060.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f66\u8def\u534f\u540c", "reason": "The paper investigates the impact of different LiDAR scanning patterns on 3D object detection performance in roadside perception systems using simulated infrastructure-based LiDARs. Although the focus is on roadside sensors rather than onboard vehicle sensors, the work directly contributes to perception capabilities crucial for autonomous driving and vehicle-infrastructure cooperation (V2I), making it strongly related to the autonomous driving field."}, "review": "```json\n{\n  \"score\": 8.8,\n  \"subfield\": \"\u57fa\u7840\u8bbe\u65bd\u611f\u77e5 / LiDAR\u626b\u63cf\u6a21\u5f0f\u5206\u6790 / 3D\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\",\n  \"sota_claim\": {\n    \"is_sota\": false,\n    \"dataset\": \"\u4e0d\u9002\u7528\uff08\u8bba\u6587\u672a\u58f0\u79f0\u5728\u7279\u5b9a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u800c\u662f\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff09\",\n    \"metrics\": \"\u4e0d\u9002\u7528\",\n    \"sota_details\": \"\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u5bf9\u4e0d\u540cLiDAR\u626b\u63cf\u6a21\u5f0f\u5728\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u4e2d\u7684\u5f71\u54cd\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u7684SOTA\u611f\u77e5\u7b97\u6cd5\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u4e00\u4e2a\u88ab\u4f4e\u4f30\u4f46\u81f3\u5173\u91cd\u8981\u7684\u65b9\u9762\u2014\u2014\u57fa\u7840\u8bbe\u65bdLiDAR\u7684\u626b\u63cf\u6a21\u5f0f\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76\u3002\u5176\u521b\u65b0\u6027\u4f53\u73b0\u5728\uff1a1) \u660e\u786e\u8bc6\u522b\u5e76\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LiDAR\u653e\u7f6e\u800c\u975e\u626b\u63cf\u6a21\u5f0f\u7684\u7a7a\u767d\uff1b2) \u5f15\u5165\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u65b0\u9896\u3001\u5168\u9762\u7684\u201cInfraLiDARs\u2019 Benchmark\u201d\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u7cbe\u5fc3\u6536\u96c6\uff0c\u6db5\u76d6\u4e86\u91cd\u590d\u6027\u548c\u975e\u91cd\u590d\u6027\u4e24\u79cd\u626b\u63cf\u8303\u5f0f\u4e0b\u7684\u591a\u79cdLiDAR\u548c\u4ea4\u901a\u573a\u666f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff1b3) \u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u626b\u63cf\u6a21\u5f0f\u7684LiDAR\u5bf9\u4e3b\u6d413D\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8bbe\u8ba1\u5168\u9762\uff0c\u6db5\u76d6\u4e86\u591a\u79cdLiDAR\u7c7b\u578b\u3001\u4ea4\u901a\u573a\u666f\u548c\u9886\u5148\u76843D\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u8fd9\u5c55\u793a\u4e86\u826f\u597d\u7684\u5b9e\u9a8c\u5b8c\u6574\u6027\u3002\u867d\u7136\u6570\u636e\u6765\u6e90\u4e8e\u4eff\u771f\u73af\u5883\uff0c\u4f46\u4f5c\u8005\u660e\u786e\u6307\u51fa\u4e86\u5176\u4f18\u70b9\uff08\u5982\u53ef\u63a7\u6027\u3001\u907f\u514d\u6807\u5b9a\u95ee\u9898\u3001\u5730\u9762\u771f\u503c\u6613\u5f97\uff09\uff0c\u4e14\u6570\u636e\u96c6\u7684\u516c\u5f00\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7814\u7a76\u7684\u53ef\u4fe1\u5ea6\u3002\u672c\u7814\u7a76\u7684\u6210\u679c\u5177\u6709\u6781\u9ad8\u7684\u884c\u4e1a\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u57fa\u7840\u8bbe\u65bd\u7aef\u8fdb\u884c\u4f20\u611f\u5668\u9009\u578b\u3001\u90e8\u7f72\u4f18\u5316\u548c\u7b97\u6cd5\u9002\u914d\u63d0\u4f9b\u76f4\u63a5\u3001\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u5c24\u5176\u662f\u5728V2I\uff08\u8f66\u8def\u534f\u540c\uff09\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u9886\u57df\u3002\"\n}\n```"}, "8": {"title": "World Simulation with Video Foundation Models for Physical AI", "authors": ["NVIDIA,  :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu"], "abstract": "arXiv:2511.00062v1 Announce Type: new \nWe introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.00062", "pdf_url": "https://arxiv.org/pdf/2511.00062.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u4eff\u771f\u8bc4\u4f30 / \u81ea\u4e3b\u7cfb\u7edf", "reason": "The paper introduces advanced simulation models like Cosmos-Predict2.5 and Cosmos-Transfer2.5, which are applicable to robotics and autonomous systems. These models are designed to improve synthetic data generation, policy evaluation, and simulation, which are relevant to autonomous driving for creating more reliable virtual environments for system training and evaluation."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f / \u5408\u6210\u6570\u636e\u751f\u6210 / \u4e16\u754c\u6a21\u578b\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"PAI-Bench-Predict-Image2World, RDS-HQ-HL\",\n    \"metrics\": \"PAI-Bench Overall Score, FVD, FID, TSE, CSE, LET-AP, F1, x-error (far), Category Acc.\",\n    \"sota_details\": \"\u5728PAI-Bench-Predict-Image2World\u4efb\u52a1\u4e2d\uff0c[Cosmos-Predict2.5-2B] (0.810 Overall Score) \u6027\u80fd\u8d85\u8d8a Wan2.1-14B \u548c Wan2.2-5B\uff0c\u5e76\u4e0e\u53c2\u6570\u91cf\u66f4\u5927\u7684 Wan2.2-27B-A14B (0.806 Overall Score) \u6301\u5e73\u3002\\n\u5728\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4efb\u52a1\u4e2d\uff0c[Cosmos-Transfer2.5-2B/auto/multiview] \u5728 RDS-HQ-HL \u6570\u636e\u96c6\u4e0a\uff0c\u89c6\u89c9\u8d28\u91cf\u6307\u6807 (FVD, FID) \u663e\u8457\u4f18\u4e8e\u5176\u524d\u8eab [Transfer1-7B-Sample-AV] (\u4f8b\u5982\uff0cFVD StyleGAN \u4ece 56.606 \u964d\u81f3 24.222)\uff0c\u5e76\u4e14\u611f\u77e5\u4efb\u52a1\u6307\u6807 (\u5982 LET-AP) \u63d0\u5347\u9ad8\u8fbe 60% (\u4ece 0.243 \u63d0\u5347\u81f3 0.394)\uff0c\u6027\u80fd\u975e\u5e38\u63a5\u8fd1\u771f\u5b9e\u89c6\u9891\uff08Reference\uff09\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 NVIDIA Cosmos \u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u4e00\u4ee3\uff0c[Cosmos-Predict2.5] \u548c [Cosmos-Transfer2.5]\uff0c\u4e13\u6ce8\u4e8e\u7269\u7406 AI \u9886\u57df\u7684\u89c6\u9891\u751f\u6210\u548c\u4e16\u754c\u4eff\u771f\u3002\u5176\u6838\u5fc3\u521b\u65b0\u6027\u5728\u4e8e\u5c06\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u5230\u4e16\u754c\u7684\u751f\u6210\u7edf\u4e00\u5230\u5355\u4e2a\u6d41\u5339\u914d\u6a21\u578b\u4e2d\uff0c\u5e76\u4e13\u95e8\u9488\u5bf9\u7269\u7406\u4e16\u754c\uff08\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\uff09\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u53ef\u63a7\u6027\u8fdb\u884c\u4f18\u5316\u3002\\n\\n**\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u6027\u53ca\u521b\u65b0\u6027**\uff1a\u8bba\u6587\u5c06\u81ea\u52a8\u9a7e\u9a76\u4f5c\u4e3a\u6838\u5fc3\u5e94\u7528\u4e4b\u4e00\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u5b9a\u5236\u7684\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u201c\u4e16\u754c\u573a\u666f\u56fe\u201d\uff08\u5305\u542b\u9ad8\u6e05\u5730\u56fe\u5143\u7d20\u548c\u52a8\u6001 3D \u8fb9\u754c\u6846\uff09\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u591a\u89c6\u89d2\u3001\u6761\u4ef6\u63a7\u5236\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u89c6\u9891\u751f\u6210\u3002\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u95ed\u73af\u4eff\u771f\u65b9\u9762\u5177\u6709\u6781\u9ad8\u7684\u521b\u65b0\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002\\n\\n**\u5b9e\u9a8c\u5b8c\u6574\u6027**\uff1a\u8bba\u6587\u5728\u8d85\u5927\u89c4\u6a21\uff082\u4ebf\u89c6\u9891\u7247\u6bb5\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\uff09\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002\u5728\u8bc4\u4f30\u65b9\u9762\uff0c\u9664\u4e86\u901a\u7528\u7684 PAI-Bench \u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u4e13\u95e8\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\uff08\u4f8b\u5982 3D \u969c\u788d\u7269\u548c\u8f66\u9053\u7ebf\u68c0\u6d4b\uff09\u8fdb\u884c\u4e86\u91cf\u5316\u8bc4\u4f30\uff0c\u5e76\u4e0e\u524d\u4ee3\u6a21\u578b\u53ca\u771f\u5b9e\u89c6\u9891\u8fdb\u884c\u5bf9\u6bd4\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u5c55\u793a\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u3002\u5f00\u653e\u6e90\u4ee3\u7801\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u6781\u5927\u5730\u63d0\u5347\u4e86\u5b9e\u9a8c\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002\\n\\n**\u53ef\u4fe1\u5ea6**\uff1a\u4f5c\u4e3a NVIDIA \u7684\u5de5\u4f5c\uff0c\u8be5\u7814\u7a76\u6295\u5165\u4e86\u5927\u91cf\u8d44\u6e90\uff0c\u4e14\u6a21\u578b\u548c\u57fa\u51c6\u7684\u5f00\u653e\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u53ef\u4fe1\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65b0\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u4ee3\uff0c\u5e76\u4e14\u5728\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u771f\u5b9e\u89c6\u9891\u7684\u611f\u77e5\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e7b\u89c9\u548c\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\\n\\n**\u884c\u4e1a\u6f5c\u529b**\uff1a\u8be5\u5de5\u4f5c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5177\u6709\u5de8\u5927\u7684\u884c\u4e1a\u6f5c\u529b\u3002\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u591a\u89c6\u89d2\u5408\u6210\u6570\u636e\u751f\u6210\u80fd\u529b\uff0c\u80fd\u591f\u5927\u5e45\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u91c7\u96c6\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u5b89\u5168\u6027\u3002\u5176\u5728 Sim2Real \u548c Real2Real \u8f6c\u6362\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u9e3f\u6c9f\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u89c4\u5212\u3001\u63a7\u5236\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002\"\n}\n```"}, "9": {"title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures", "authors": ["Harald Kristen, Daniel Kulmer, Manuela Hirschmugl"], "abstract": "arXiv:2511.00073v1 Announce Type: new \nRapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net's 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00073", "pdf_url": "https://arxiv.org/pdf/2511.00073.pdf", "is_interesting": false}, "10": {"title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation", "authors": ["Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian"], "abstract": "arXiv:2511.00090v1 Announce Type: new \nWe present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00090", "pdf_url": "https://arxiv.org/pdf/2511.00090.pdf", "is_interesting": false}, "11": {"title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL", "authors": ["Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi \"Jim\" Fan, Guanya Shi, Yuke Zhu"], "abstract": "arXiv:2511.00091v1 Announce Type: new \nSupervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist's deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.00091", "pdf_url": "https://arxiv.org/pdf/2511.00091.pdf", "is_interesting": false}, "12": {"title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation", "authors": ["Jiaming Liu, Dingwei Fan, Junyong Zhao, Chunlin Li, Haipeng Si, Liang Sun"], "abstract": "arXiv:2511.00095v1 Announce Type: new \nThe anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms response times. The software is released on https://github.com/6jm233333/spinalsam-r1.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00095", "pdf_url": "https://arxiv.org/pdf/2511.00095.pdf", "is_interesting": false}, "13": {"title": "A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning", "authors": ["Nils Porsche, Flurin M\\\"uller-Diesing, Sweta Banerjee, Miguel Goncalves, Marc Aubreville"], "abstract": "arXiv:2511.00098v1 Announce Type: new \nConfocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00098", "pdf_url": "https://arxiv.org/pdf/2511.00098.pdf", "is_interesting": false}, "14": {"title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video", "authors": ["Rotem Ezra, Hedi Zisling, Nimrod Berman, Ilan Naiman, Alexey Gorkor, Liran Nochumsohn, Eliya Nachmani, Omri Azencot"], "abstract": "arXiv:2511.00103v1 Announce Type: new \nDiffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: https://azencot-group.github.io/FreeSliders/", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00103", "pdf_url": "https://arxiv.org/pdf/2511.00103.pdf", "is_interesting": false}, "15": {"title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency", "authors": ["Piyushkumar Patel"], "abstract": "arXiv:2511.00107v1 Announce Type: new \nText to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.", "categories": ["cs.CV", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.00107", "pdf_url": "https://arxiv.org/pdf/2511.00107.pdf", "is_interesting": false}, "16": {"title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "authors": ["YingQiao Wang, Eric Bigelow, Boyi Li, Tomer Ullman"], "abstract": "arXiv:2511.00110v1 Announce Type: new \nWe propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time\" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00110", "pdf_url": "https://arxiv.org/pdf/2511.00110.pdf", "is_interesting": false}, "17": {"title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning", "authors": ["Hanae Elmekki, Amanda Spilkin, Ehsan Zakeri, Antonela Mariel Zanuttini, Ahmed Alagha, Hani Sami, Jamal Bentahar, Lyes Kadem, Wen-Fang Xie, Philippe Pibarot, Rabeb Mizouni, Hadi Otrok, Azzam Mourad, Sami Muhaidat"], "abstract": "arXiv:2511.00114v1 Announce Type: new \nCardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00114", "pdf_url": "https://arxiv.org/pdf/2511.00114.pdf", "is_interesting": false}, "18": {"title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images", "authors": ["Md Selim Sarowar, Sungho Kim"], "abstract": "arXiv:2511.00120v1 Announce Type: new \nThe primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00120", "pdf_url": "https://arxiv.org/pdf/2511.00120.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on 6D pose estimation using RGB-D images with an emphasis on improving robustness against occlusion and lighting variations. While this is a relevant task for object recognition and scene understanding in autonomous driving, the paper does not explicitly discuss autonomous driving systems or tasks like perception, prediction, or control."}}, "19": {"title": "Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation", "authors": ["Gaby Maroun, Salah Eddine Bekhouche, Fadi Dornaika"], "abstract": "arXiv:2511.00123v1 Announce Type: new \nAge estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00123", "pdf_url": "https://arxiv.org/pdf/2511.00123.pdf", "is_interesting": false}, "20": {"title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding", "authors": ["Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi"], "abstract": "arXiv:2511.00141v1 Announce Type: new \nRecent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00141", "pdf_url": "https://arxiv.org/pdf/2511.00141.pdf", "is_interesting": false}, "21": {"title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing", "authors": ["Jinsu Kim, Yunhun Nam, Minseon Kim, Sangpil Kim, Jongheon Jeong"], "abstract": "arXiv:2511.00143v1 Announce Type: new \nRecent advances in text-to-image models have increased the exposure of powerful image editing techniques as a tool, raising concerns about their potential for malicious use. An emerging line of research to address such threats focuses on implanting \"protective\" adversarial noise into images before their public release, so future attempts to edit them using text-to-image models can be impeded. However, subsequent works have shown that these adversarial noises are often easily \"reversed,\" e.g., with techniques as simple as JPEG compression, casting doubt on the practicality of the approach. In this paper, we argue that adversarial noise for image protection should not only be imperceptible, as has been a primary focus of prior work, but also irreversible, viz., it should be difficult to detect as noise provided that the original image is hidden. We propose a surprisingly simple method to enhance the robustness of image protection methods against noise reversal techniques. Specifically, it applies an adaptive per-region Gaussian blur on the noise to adjust the overall frequency spectrum. Through extensive experiments, we show that our method consistently improves the per-sample worst-case protection performance of existing methods against a wide range of reversal techniques on diverse image editing scenarios, while also reducing quality degradation due to noise in terms of perceptual metrics. Code is available at https://github.com/jsu-kim/BlurGuard.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00143", "pdf_url": "https://arxiv.org/pdf/2511.00143.pdf", "is_interesting": false}, "22": {"title": "CompAgent: An Agentic Framework for Visual Compliance Verification", "authors": ["Rahul Ghosh, Baishali Chaudhury, Hari Prasanna Das, Meghana Ashok, Ryan Razkenari, Sungmin Hong, Chun-Hao Liu"], "abstract": "arXiv:2511.00171v1 Announce Type: new \nVisual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00171", "pdf_url": "https://arxiv.org/pdf/2511.00171.pdf", "is_interesting": false}, "23": {"title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection", "authors": ["Mengfei Liang, Yiting Qu, Yukun Jiang, Michael Backes, Yang Zhang"], "abstract": "arXiv:2511.00181v1 Announce Type: new \nThe rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.", "categories": ["cs.CV", "cs.CR"], "abs_url": "https://arxiv.org/abs/2511.00181", "pdf_url": "https://arxiv.org/pdf/2511.00181.pdf", "is_interesting": false}, "24": {"title": "A Retrospect to Multi-prompt Learning across Vision and Language", "authors": ["Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo"], "abstract": "arXiv:2511.00191v1 Announce Type: new \nThe vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00191", "pdf_url": "https://arxiv.org/pdf/2511.00191.pdf", "is_interesting": false}, "25": {"title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "authors": ["Wenxuan Zhang, Peng Hu"], "abstract": "arXiv:2511.00211v1 Announce Type: new \nThe increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet's essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.00211", "pdf_url": "https://arxiv.org/pdf/2511.00211.pdf", "is_interesting": false}, "26": {"title": "DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy", "authors": ["Rajatsubhra Chakraborty, Ana Espinosa-Momox, Riley Haskin, Depeng Xu, Rosario Porras-Aguilar"], "abstract": "arXiv:2511.00218v1 Announce Type: new \nCell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM's simultaneous capture of complementary illumination and phase cues for robust cell segmentation.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00218", "pdf_url": "https://arxiv.org/pdf/2511.00218.pdf", "is_interesting": false}, "27": {"title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior", "authors": ["Fuming Yang, Yicong Li, Hanspeter Pfister, Jeff W. Lichtman, Yaron Meirovitch"], "abstract": "arXiv:2511.00231v1 Announce Type: new \nPetascale electron microscopy (EM) datasets push storage, transfer, and downstream analysis toward their current limits. We present a vector-quantized variational autoencoder-based (VQ-VAE) compression framework for EM that spans 16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme compression, with an optional Transformer prior that predicts bottom tokens (without changing the compression ratio) to restore texture via feature-wise linear modulation (FiLM) and concatenation; we further introduce an ROI-driven workflow that performs selective high-resolution reconstruction from 1024x-compressed latents only where needed.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00231", "pdf_url": "https://arxiv.org/pdf/2511.00231.pdf", "is_interesting": false}, "28": {"title": "Hyperbolic Optimal Transport", "authors": ["Yan Bin Ng, Xianfeng Gu"], "abstract": "arXiv:2511.00244v1 Announce Type: new \nThe optimal transport (OT) problem aims to find the most efficient mapping between two probability distributions under a given cost function, and has diverse applications in many fields such as machine learning, computer vision and computer graphics. However, existing methods for computing optimal transport maps are primarily developed for Euclidean spaces and the sphere. In this paper, we explore the problem of computing the optimal transport map in hyperbolic space, which naturally arises in contexts involving hierarchical data, networks, and multi-genus Riemann surfaces. We propose a novel and efficient algorithm for computing the optimal transport map in hyperbolic space using a geometric variational technique by extending methods for Euclidean and spherical geometry to the hyperbolic setting. We also perform experiments on synthetic data and multi-genus surface models to validate the efficacy of the proposed method.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00244", "pdf_url": "https://arxiv.org/pdf/2511.00244.pdf", "is_interesting": false}, "29": {"title": "Object-Aware 4D Human Motion Generation", "authors": ["Shurui Gui, Deep Anil Patel, Xiner Li, Martin Renqiang Min"], "abstract": "arXiv:2511.00248v1 Announce Type: new \nRecent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.00248", "pdf_url": "https://arxiv.org/pdf/2511.00248.pdf", "is_interesting": false}, "30": {"title": "Merlin L48 Spectrogram Dataset", "authors": ["Aaron Sun, Subhransu Maji, Grant Van Horn"], "abstract": "arXiv:2511.00252v1 Announce Type: new \nIn the single-positive multi-label (SPML) setting, each image in a dataset is labeled with the presence of a single class, while the true presence of other classes remains unknown. The challenge is to narrow the performance gap between this partially-labeled setting and fully-supervised learning, which often requires a significant annotation budget. Prior SPML methods were developed and benchmarked on synthetic datasets created by randomly sampling single positive labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and CUB200. However, this synthetic approach does not reflect real-world scenarios and fails to capture the fine-grained complexities that can lead to difficult misclassifications. In this work, we introduce the L48 dataset, a fine-grained, real-world multi-label dataset derived from recordings of bird sounds. L48 provides a natural SPML setting with single-positive annotations on a challenging, fine-grained domain, as well as two extended settings in which domain priors give access to additional negative labels. We benchmark existing SPML methods on L48 and observe significant performance differences compared to synthetic datasets and analyze method weaknesses, underscoring the need for more realistic and difficult benchmarks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00252", "pdf_url": "https://arxiv.org/pdf/2511.00252.pdf", "is_interesting": false}, "31": {"title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing", "authors": ["Fangxun Liu, S M Rayeed, Samuel Stevens, Alyson East, Cheng Hsuan Chiang, Colin Lee, Daniel Yi, Junke Yang, Tejas Naik, Ziyi Wang, Connor Kilrain, Elijah H Buckwalter, Jiacheng Hou, Saul Ibaven Bueno, Shuheng Wang, Xinyue Ma, Yifan Liu, Zhiyuan Tao, Ziheng Zhang, Eric Sokol, Michael Belitz, Sydne Record, Charles V. Stewart, Wei-Lun Chao"], "abstract": "arXiv:2511.00255v1 Announce Type: new \nIn entomology and ecology research, biologists often need to collect a large number of insects, among which beetles are the most common species. A common practice for biologists to organize beetles is to place them on trays and take a picture of each tray. Given the images of thousands of such trays, it is important to have an automated pipeline to process the large-scale data for further research. Therefore, we develop a 3-stage pipeline to detect all the beetles on each tray, sort and crop the image of each beetle, and do morphological segmentation on the cropped beetles. For detection, we design an iterative process utilizing a transformer-based open-vocabulary object detector and a vision-language model. For segmentation, we manually labeled 670 beetle images and fine-tuned two variants of a transformer-based segmentation model to achieve fine-grained segmentation of beetles with relatively high accuracy. The pipeline integrates multiple deep learning methods and is specialized for beetle image processing, which can greatly improve the efficiency to process large-scale beetle data and accelerate biological research.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00255", "pdf_url": "https://arxiv.org/pdf/2511.00255.pdf", "is_interesting": false}, "32": {"title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba", "authors": ["Linzhe Jiang, Jiayuan Huang, Sophia Bano, Matthew J. Clarkson, Zhehua Mao, Mobarak I. Hoque"], "abstract": "arXiv:2511.00260v1 Announce Type: new \nAccurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00260", "pdf_url": "https://arxiv.org/pdf/2511.00260.pdf", "is_interesting": false}, "33": {"title": "Spot The Ball: A Benchmark for Visual Social Inference", "authors": ["Neha Balamurugan, Sarah Wu, Adam Chun, Gabe Gaw, Cristobal Eyzaguirre, Tobias Gerstenberg"], "abstract": "arXiv:2511.00261v1 Announce Type: new \nHumans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people's gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics--such as guessing near the image center or nearby players--while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.", "categories": ["cs.CV", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00261", "pdf_url": "https://arxiv.org/pdf/2511.00261.pdf", "is_interesting": false}, "34": {"title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture", "authors": ["Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang"], "abstract": "arXiv:2511.00269v1 Announce Type: new \nAccurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00269", "pdf_url": "https://arxiv.org/pdf/2511.00269.pdf", "is_interesting": false}, "35": {"title": "Multi-View Consistent Human Image Customization via In-Context Learning", "authors": ["Hengjia Li, Jianjin Xu, Keli Cheng, Lei Wang, Ning Bi, Boxi Wu, Fernando De la Torre, Deng Cai"], "abstract": "arXiv:2511.00293v1 Announce Type: new \nRecent advances in personalized generative models demonstrate impressive results in creating identity-consistent images of the same person under diverse settings. Yet, we note that most methods cannot control the viewpoint of the generated image, nor generate consistent multiple views of the person. To address this problem, we propose a lightweight adaptation method, PersonalView, capable of enabling an existing model to acquire multi-view generation capability with as few as 100 training samples. PersonalView consists of two key components: First, we design a conditioning architecture to take advantage of the in-context learning ability of the pre-trained diffusion transformer. Second, we preserve the original generative ability of the pretrained model with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view consistency, text alignment, identity similarity, and visual quality of PersonalView and compare it to recent baselines with potential capability of multi-view customization. PersonalView significantly outperforms baselines trained on a large corpus of multi-view data with only 100 training samples.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00293", "pdf_url": "https://arxiv.org/pdf/2511.00293.pdf", "is_interesting": false}, "36": {"title": "Towards Automated Petrography", "authors": ["Isai Daniel Chac\\'on, Paola Ruiz Puentes, Jillian Pearse, Pablo Arbel\\'aez"], "abstract": "arXiv:2511.00328v1 Announce Type: new \nPetrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00328", "pdf_url": "https://arxiv.org/pdf/2511.00328.pdf", "is_interesting": false}, "37": {"title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models", "authors": ["Weidong Zhang, Pak Lun Kevin Ding, Huan Liu"], "abstract": "arXiv:2511.00335v1 Announce Type: new \nLightweight vision classification models such as MobileNet, ShuffleNet, and EfficientNet are increasingly deployed in mobile and embedded systems, yet their performance has been predominantly benchmarked on ImageNet. This raises critical questions: Do models that excel on ImageNet also generalize across other domains? How can cross-dataset robustness be systematically quantified? And which architectural elements consistently drive generalization under tight resource constraints? Here, we present the first systematic evaluation of 11 lightweight vision models (2.5M parameters), trained under a fixed 100-epoch schedule across 7 diverse datasets. We introduce the Cross-Dataset Score (xScore), a unified metric that quantifies the consistency and robustness of model performance across diverse visual domains. Our results show that (1) ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets, (2) xScore provides a scalable predictor of mobile model performance that can be estimated from just four datasets, and (3) certain architectural components--such as isotropic convolutions with higher spatial resolution and channel-wise attention--promote broader generalization, while Transformer-based blocks yield little additional benefit, despite incurring higher parameter overhead. This study provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides the development of future models that generalize robustly across diverse application domains.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00335", "pdf_url": "https://arxiv.org/pdf/2511.00335.pdf", "is_interesting": false}, "38": {"title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction", "authors": ["Yuhao Fang, Zijian Wang, Yao Lu, Ye Zhang, Chun Li"], "abstract": "arXiv:2511.00338v1 Announce Type: new \nThis work presents a novel hybrid approach that integrates Deep Operator Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex inverse problem. The method effectively addresses tasks such as source localization governed by the Navier-Stokes equations and image reconstruction, overcoming challenges related to nonlinearity, sparsity, and noisy data. By incorporating physics-informed constraints and task-specific regularization into the loss function, the framework ensures solutions that are both physically consistent and accurate. Validation on diverse synthetic and real datasets demonstrates its robustness, scalability, and precision, showcasing its broad potential applications in computational physics and imaging sciences.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00338", "pdf_url": "https://arxiv.org/pdf/2511.00338.pdf", "is_interesting": false}, "39": {"title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities", "authors": ["Xihang Qiu, Jiarong Cheng, Yuhao Fang, Wanpeng Zhang, Yao Lu, Ye Zhang, Chun Li"], "abstract": "arXiv:2511.00344v1 Announce Type: new \nMultimodal Emotion Recognition in Conversations (MERC) enhances emotional understanding through the fusion of multimodal signals. However, unpredictable modality absence in real-world scenarios significantly degrades the performance of existing methods. Conventional missing-modality recovery approaches, which depend on training with complete multimodal data, often suffer from semantic distortion under extreme data distributions, such as fixed-modality absence. To address this, we propose the Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) framework, pioneering the integration of federated learning into missing-modality recovery. By federated aggregation of modality-specific diffusion models trained on clients and broadcasting them to clients missing corresponding modalities, FedDISC overcomes single-client reliance on modality completeness. Additionally, the DISC-Diffusion module ensures consistency in context, speaker identity, and semantics between recovered and available modalities, using a Dialogue Graph Network to capture conversational dependencies and a Semantic Conditioning Network to enforce semantic alignment. We further introduce a novel Alternating Frozen Aggregation strategy, which cyclically freezes recovery and classifier modules to facilitate collaborative optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI datasets demonstrate that FedDISC achieves superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00344", "pdf_url": "https://arxiv.org/pdf/2511.00344.pdf", "is_interesting": false}, "40": {"title": "OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data", "authors": ["Amir Ziashahabi, Narges Ghasemi, Sajjad Shahabi, John Krumm, Salman Avestimehr, Cyrus Shahabi"], "abstract": "arXiv:2511.00345v1 Announce Type: new \nAccurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at https://github.com/amir-zsh/OSMGen.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00345", "pdf_url": "https://arxiv.org/pdf/2511.00345.pdf", "is_interesting": false}, "41": {"title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach", "authors": ["Mohd Ruhul Ameen, Akif Islam"], "abstract": "arXiv:2511.00352v1 Announce Type: new \nThe rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00352", "pdf_url": "https://arxiv.org/pdf/2511.00352.pdf", "is_interesting": false}, "42": {"title": "Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation", "authors": ["Niklas W\\\"olki, Lukas Kondmann, Christian Molli\\`ere, Martin Langer, Julia Gottfriedsen, Martin Werner"], "abstract": "arXiv:2511.00357v1 Announce Type: new \nOnboard cloud segmentation is a critical yet underexplored task in thermal Earth observation (EO), particularly for CubeSat missions constrained by limited hardware and spectral information. CubeSats often rely on a single thermal band and lack sufficient labeled data, making conventional cloud masking techniques infeasible. This work addresses these challenges by applying transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using a UNet with a lightweight MobileNet encoder. We pretrain the model on the public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small set of mission-specific samples in a joint-training setup, improving the macro F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a TensorRT engine and demonstrate full-image inference in under 5 seconds on an NVIDIA Jetson Nano. These results show that leveraging public datasets and lightweight architectures can enable accurate, efficient thermal-only cloud masking on-orbit, supporting real-time decision-making in data-limited EO missions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00357", "pdf_url": "https://arxiv.org/pdf/2511.00357.pdf", "is_interesting": false}, "43": {"title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "authors": ["Momen Khandoker Ope, Akif Islam, Mohd Ruhul Ameen, Abu Saleh Musa Miah, Md Rashedul Islam, Jungpil Shin"], "abstract": "arXiv:2511.00362v1 Announce Type: new \nCultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.", "categories": ["cs.CV", "cs.AI", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.00362", "pdf_url": "https://arxiv.org/pdf/2511.00362.pdf", "is_interesting": false}, "44": {"title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict", "authors": ["Chaochen Wu, Guan Luo, Meiyun Zuo, Zhitao Fan"], "abstract": "arXiv:2511.00370v1 Announce Type: new \nVideo moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment's boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents' localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00370", "pdf_url": "https://arxiv.org/pdf/2511.00370.pdf", "is_interesting": false}, "45": {"title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "authors": ["Jiaming Li, Junlei Wu, Sheng Wang, Honglin Xiong, Jiangdong Cai, Zihao Zhao, Yitao Zhu, Yuan Yin, Dinggang Shen, Qian Wang"], "abstract": "arXiv:2511.00381v1 Announce Type: new \nWidespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2\\% across classification tasks, while natural language generation metrics for automated reports remain within 1\\% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.", "categories": ["cs.CV", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00381", "pdf_url": "https://arxiv.org/pdf/2511.00381.pdf", "is_interesting": false}, "46": {"title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond", "authors": ["Fan Zhang, Haoxuan Li, Shengju Qian, Xin Wang, Zheng Lian, Hao Wu, Zhihong Zhu, Yuan Gao, Qiankun Li, Yefeng Zheng, Zhouchen Lin, Pheng-Ann Heng"], "abstract": "arXiv:2511.00389v1 Announce Type: new \nMultimodal Large Language Models (MLLMs) have revolutionized numerous research fields, including computer vision and affective computing. As a pivotal challenge in this interdisciplinary domain, facial expression recognition (FER) has evolved from separate, domain-specific models to more unified approaches. One promising avenue to unify FER tasks is converting conventional FER datasets into visual question-answering (VQA) formats, enabling the direct application of powerful generalist MLLMs for inference. However, despite the success of cutting-edge MLLMs in various tasks, their performance on FER tasks remains largely unexplored. To address this gap, we provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art MLLMs across four widely used FER datasets. Our results reveal that, while MLLMs exhibit good classification performance, they still face significant limitations in reasoning and interpretability. To this end, we introduce post-training strategies aimed at enhancing the facial expression reasoning capabilities of MLLMs. Specifically, we curate two high-quality and large-scale datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards (RLVR), respectively. Building upon them, we develop a unified and interpretable FER foundation model termed UniFER-7B, which outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00389", "pdf_url": "https://arxiv.org/pdf/2511.00389.pdf", "is_interesting": false}, "47": {"title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning", "authors": ["Xuanle Zhao, Deyang Jiang, Zhixiong Zeng, Lei Chen, Haibo Qiu, Jing Huang, Yufeng Zhong, Liming Zheng, Yilin Cao, Lin Ma"], "abstract": "arXiv:2511.00391v1 Announce Type: new \nMultimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like Chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode \\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on various multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, underscoring the effectiveness of our coarse-to-fine ViRL strategy. The code and model will be available at https://github.com/DocTron-hub/VinciCoder.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00391", "pdf_url": "https://arxiv.org/pdf/2511.00391.pdf", "is_interesting": false}, "48": {"title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks", "authors": ["Long Li, Shuichen Ji, Ziyang Luo, Nian Liu, Dingwen Zhang, Junwei Han"], "abstract": "arXiv:2511.00396v1 Announce Type: new \nWe present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO's key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00396", "pdf_url": "https://arxiv.org/pdf/2511.00396.pdf", "is_interesting": false}, "49": {"title": "LGCA: Enhancing Semantic Representation via Progressive Expansion", "authors": ["Thanh Hieu Cao, Trung Khang Tran, Gia Thinh Pham, Tuong Nghiem Diep, Thanh Binh Nguyen"], "abstract": "arXiv:2511.00419v1 Announce Type: new \nRecent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00419", "pdf_url": "https://arxiv.org/pdf/2511.00419.pdf", "is_interesting": false}, "50": {"title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "authors": ["Daichi Zhang, Tong Zhang, Jianmin Bao, Shiming Ge, Sabine S\\\"usstrunk"], "abstract": "arXiv:2511.00427v1 Announce Type: new \nWith the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00427", "pdf_url": "https://arxiv.org/pdf/2511.00427.pdf", "is_interesting": false}, "51": {"title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection", "authors": ["Daichi Zhang, Tong Zhang, Shiming Ge, Sabine S\\\"usstrunk"], "abstract": "arXiv:2511.00429v1 Announce Type: new \nDiffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00429", "pdf_url": "https://arxiv.org/pdf/2511.00429.pdf", "is_interesting": false}, "52": {"title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training", "authors": ["Xin Yao, Haiyang Zhao, Yimin Chen, Jiawei Guo, Kecheng Huang, Ming Zhao"], "abstract": "arXiv:2511.00446v1 Announce Type: new \nThe Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP's training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via https://github.com/xinyaocse/ToxicTextCLIP/.", "categories": ["cs.CV", "cs.CR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00446", "pdf_url": "https://arxiv.org/pdf/2511.00446.pdf", "is_interesting": false}, "53": {"title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "authors": ["Kiran Shahi, Anup Bagale"], "abstract": "arXiv:2511.00456v1 Announce Type: new \nThis study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach utilizes image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven ImageNet-pretrained architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and ViT-B16 under identical training conditions with focal loss and patient-wise splits to prevent data leakage. Experimental results on the Kermany CXR dataset demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test accuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides an optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised explainable models that enhance pneumonia screening transparency, and clinical trust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00456", "pdf_url": "https://arxiv.org/pdf/2511.00456.pdf", "is_interesting": false}, "54": {"title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation", "authors": ["Panwang Pan, Tingting Shen, Chenxin Li, Yunlong Lin, Kairun Wen, Jingjing Zhao, Yixuan Yuan"], "abstract": "arXiv:2511.00468v1 Announce Type: new \nRecent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00468", "pdf_url": "https://arxiv.org/pdf/2511.00468.pdf", "is_interesting": false}, "55": {"title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations", "authors": ["Navodini Wijethilake, Marina Ivory, Oscar MacCormac, Siddhant Kumar, Aaron Kujawa, Lorena Garcia-Foncillas Macias, Rebecca Burger, Amanda Hitchings, Suki Thomson, Sinan Barazi, Eleni Maratos, Rupert Obholzer, Dan Jiang, Fiona McClenaghan, Kazumi Chia, Omar Al-Salihi, Nick Thomas, Steve Connor, Tom Vercauteren, Jonathan Shapey"], "abstract": "arXiv:2511.00472v1 Announce Type: new \nAccurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00472", "pdf_url": "https://arxiv.org/pdf/2511.00472.pdf", "is_interesting": false}, "56": {"title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts", "authors": ["Weihao Bo, Yanpeng Sun, Yu Wang, Xinyu Zhang, Zechao Li"], "abstract": "arXiv:2511.00480v1 Announce Type: new \nIn this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on https://github.com/weihao-bo/FedMGP.git.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00480", "pdf_url": "https://arxiv.org/pdf/2511.00480.pdf", "is_interesting": false}, "57": {"title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models", "authors": ["Panwang Pan, Chenguo Lin, Jingjing Zhao, Chenxin Li, Yuchen Lin, Haopeng Li, Honglei Yan, Kairun Wen, Yunlong Lin, Yixuan Yuan, Yadong Mu"], "abstract": "arXiv:2511.00503v1 Announce Type: new \nWe introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00503", "pdf_url": "https://arxiv.org/pdf/2511.00503.pdf", "is_interesting": false}, "58": {"title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning", "authors": ["Hai-Dang Nguyen, Ha-Hieu Pham, Hao T. Nguyen, Huy-Hieu Pham"], "abstract": "arXiv:2511.00504v1 Announce Type: new \nWe present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset contains 17,597 question-answer pairs across 4,394 images, each annotated with radiologist-verified bounding boxes and clinical reasoning explanations. Our question taxonomy spans six diagnostic types-Where, What, Is there, How many, Which, and Yes/No-capturing diverse clinical intents. To improve reliability, we construct a balanced distribution of 41.7% positive and 58.3% negative samples, mitigating hallucinations in normal cases. Benchmarking with MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance reproducible and clinically grounded Med-VQA research. The dataset and evaluation tools are publicly available at huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00504", "pdf_url": "https://arxiv.org/pdf/2511.00504.pdf", "is_interesting": false}, "59": {"title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback", "authors": ["Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu, Kaiwei Wang, Kailun Yang"], "abstract": "arXiv:2511.00510v1 Announce Type: new \nThis paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack.", "categories": ["cs.CV", "cs.RO", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.00510", "pdf_url": "https://arxiv.org/pdf/2511.00510.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "None", "reason": "The paper focuses on multi-object tracking (MOT) in panoramic imagery, addressing challenges such as a 360-degree field of view, distortion, and identity ambiguity. While MOT could be used in autonomous driving systems, this paper does not specifically discuss autonomous driving systems or vehicles and is primarily focused on general tracking techniques rather than tasks directly related to AD such as perception, prediction, or control."}}, "60": {"title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "authors": ["Panwang Pan, Jingjing Zhao, Yuchen Lin, Chenguo Lin, Chenxin Li, Haopeng Li, Honglei Yan, Tingting Shen, Yadong Mu"], "abstract": "arXiv:2511.00511v1 Announce Type: new \nVideo generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. We introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. To faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer designs a \\textbf{hierarchical identity-preserving attention mechanism}, which effectively aggregates features within and across subjects and modalities. To effectively allow for the semantic following of user intention, we introduce \\textbf{semantic understanding via pretrained vision-language model (VLM)}, leveraging VLM's superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. Considering that standard diffusion loss often fails in aligning the critical concepts like subject ID, we employ an \\textbf{online reinforcement learning phase} to drive the overall training objective of ID-Composer into RLVR. Extensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00511", "pdf_url": "https://arxiv.org/pdf/2511.00511.pdf", "is_interesting": false}, "61": {"title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation", "authors": ["Fangyu Wu, Yujun Cai"], "abstract": "arXiv:2511.00523v1 Announce Type: new \nVision language models such as CLIP have shown remarkable performance in zero shot classification, but remain susceptible to spurious correlations, where irrelevant visual features influence predictions. Existing debiasing methods often require access to training data and explicit group labels to perform fine-tuning or adjust embeddings, which limits their practicality in real-world settings. Test-time methods attempt to avoid this constraint, but many still depend on prior knowledge of dataset specific biases, limiting their generalizability in open set settings. In this work, we propose a test-time debiasing method for ViT based CLIP models that requires no additional training or assumptions of bias annotations. Our approach uses a pretrained segmentation model to isolate the target visual attribute, then adjusts the non target regions so that their embeddings are uniformly similar to all class specific text prompts. This procedure removes unintended bias signals from confounding visual regions while preserving the target attribute. Experiments on Waterbirds and CelebA show that our method outperforms existing test-time debiasing approaches in both group robustness metrics and Attention IoU. These results demonstrate the effectiveness of segmentation guided interventions for scalable and annotation free bias mitigation in vision language models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00523", "pdf_url": "https://arxiv.org/pdf/2511.00523.pdf", "is_interesting": false}, "62": {"title": "Text-guided Fine-Grained Video Anomaly Detection", "authors": ["Jihao Gu, Kun Li, He Wang, Kaan Ak\\c{s}it"], "abstract": "arXiv:2511.00524v1 Announce Type: new \nVideo Anomaly Detection (VAD) aims to identify anomalous events within video segments. In scenarios such as surveillance or industrial process monitoring, anomaly detection is of critical importance. While existing approaches are semi-automated, requiring human assessment for anomaly detection, traditional VADs offer limited output as either normal or anomalous. We propose Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD) that performs pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly Encoder (RAE) that transforms the heatmaps into learnable textual embeddings, guiding the LVLM to accurately identify and localize anomalous events in videos. This significantly enhances both the granularity and interactivity of anomaly detection. The proposed method achieving SOTA performance by demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and 67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset, and subjectively verified more preferable textual description on the ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories; Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00524", "pdf_url": "https://arxiv.org/pdf/2511.00524.pdf", "is_interesting": false}, "63": {"title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era", "authors": ["Wenbing Zhu, Chengjie Wang, Bin-Bin Gao, Jiangning Zhang, Guannan Jiang, Jie Hu, Zhenye Gan, Lidong Wang, Ziqing Zhou, Linjie Cheng, Yurui Pan, Bo Peng, Mingmin Chi, Lizhuang Ma"], "abstract": "arXiv:2511.00540v1 Announce Type: new \nIndustrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark's substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-/few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00540", "pdf_url": "https://arxiv.org/pdf/2511.00540.pdf", "is_interesting": false}, "64": {"title": "MIFO: Learning and Synthesizing Multi-Instance from One Image", "authors": ["Kailun Su, Ziqi He, Xi Wang, Yang Zhou"], "abstract": "arXiv:2511.00542v1 Announce Type: new \nThis paper proposes a method for precise learning and synthesizing multi-instance semantics from a single image. The difficulty of this problem lies in the limited training data, and it becomes even more challenging when the instances to be learned have similar semantics or appearance. To address this, we propose a penalty-based attention optimization to disentangle similar semantics during the learning stage. Then, in the synthesis, we introduce and optimize box control in attention layers to further mitigate semantic leakage while precisely controlling the output layout. Experimental results demonstrate that our method achieves disentangled and high-quality semantic learning and synthesis, strikingly balancing editability and instance consistency. Our method remains robust when dealing with semantically or visually similar instances or rare-seen objects. The code is publicly available at https://github.com/Kareneveve/MIFO", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00542", "pdf_url": "https://arxiv.org/pdf/2511.00542.pdf", "is_interesting": false}, "65": {"title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting", "authors": ["Chun-Tin Wu, Jun-Cheng Chen"], "abstract": "arXiv:2511.00560v1 Announce Type: new \nAlthough 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00560", "pdf_url": "https://arxiv.org/pdf/2511.00560.pdf", "is_interesting": false}, "66": {"title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective", "authors": ["Wei Feng, Zongyuan Ge"], "abstract": "arXiv:2511.00573v1 Announce Type: new \nGeneralized Category Discovery (GCD) aims to leverage labeled samples from known categories to cluster unlabeled data that may include both known and unknown categories. While existing methods have achieved impressive results under standard conditions, their performance often deteriorates in the presence of distribution shifts. In this paper, we explore a more realistic task: Domain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled data includes not only unknown categories but also samples from unknown domains. To tackle this challenge, we propose a \\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized Cat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE) that enhances the model's ability to discover categories under distributional shift by leveraging frequency-domain information. Specifically, we first propose a frequency-based domain separation strategy that partitions samples into known and unknown domains by measuring their amplitude differences. We then propose two types of frequency-domain perturbation strategies: a cross-domain strategy, which adapts to new distributions by exchanging amplitude components across domains, and an intra-domain strategy, which enhances robustness to intra-domain variations within the unknown domain. Furthermore, we extend the self-supervised contrastive objective and semantic clustering loss to better guide the training process. Finally, we introduce a clustering-difficulty-aware resampling technique to adaptively focus on harder-to-cluster categories, further enhancing model performance. Extensive experiments demonstrate that our method effectively mitigates the impact of distributional shifts across various benchmark datasets and achieves superior performance in discovering both known and unknown categories.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00573", "pdf_url": "https://arxiv.org/pdf/2511.00573.pdf", "is_interesting": false}, "67": {"title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "authors": ["Yousuf Ahmed Siddiqui, Sufiyaan Usmani, Umer Tariq, Jawwad Ahmed Shamsi, Muhammad Burhan Khan"], "abstract": "arXiv:2511.00580v1 Announce Type: new \nVideo anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00580", "pdf_url": "https://arxiv.org/pdf/2511.00580.pdf", "is_interesting": false}, "68": {"title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "authors": ["Yating Yu, Congqi Cao, Zhaoying Wang, Weihua Meng, Jie Li, Yuxin Li, Zihao Wei, Zhongpei Shen, Jiajun Zhang"], "abstract": "arXiv:2511.00613v1 Announce Type: new \nHow far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00613", "pdf_url": "https://arxiv.org/pdf/2511.00613.pdf", "is_interesting": false}, "69": {"title": "Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach", "authors": ["Oluwatosin Alabi, Meng Wei, Charlie Budd, Tom Vercauteren, Miaojing Shi"], "abstract": "arXiv:2511.00643v1 Announce Type: new \nUnderstanding surgical instrument-tissue interactions requires not only identifying which instrument performs which action on which anatomical target, but also grounding these interactions spatially within the surgical scene. Existing surgical action triplet recognition methods are limited to learning from frame-level classification, failing to reliably link actions to specific instrument instances.Previous attempts at spatial grounding have primarily relied on class activation maps, which lack the precision and robustness required for detailed instrument-tissue interaction analysis.To address this gap, we propose grounding surgical action triplets with instrument instance segmentation, or triplet segmentation for short, a new unified task which produces spatially grounded  outputs.We start by presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000 annotated frames, linking instrument instance masks with action verb and anatomical target annotations, and establishing the first benchmark for strongly supervised, instance-level triplet grounding and evaluation.To learn triplet segmentation, we propose TargetFusionNet, a novel architecture that extends Mask2Former with a target-aware fusion mechanism to address the challenge of accurate anatomical target prediction by fusing weak anatomy priors with instrument instance queries.Evaluated across recognition, detection, and triplet segmentation metrics, TargetFusionNet consistently improves performance over existing baselines, demonstrating that strong instance supervision combined with weak target priors significantly enhances the accuracy and robustness of surgical action understanding.Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets. The proposed benchmark and architecture pave the way for more interpretable, surgical scene understanding.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00643", "pdf_url": "https://arxiv.org/pdf/2511.00643.pdf", "is_interesting": false}, "70": {"title": "Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset", "authors": ["Lassi Ruoppa, Tarmo Hietala, Verneri Sepp\\\"anen, Josef Taher, Teemu Hakala, Xiaowei Yu, Antero Kukko, Harri Kaartinen, Juha Hyypp\\\"a"], "abstract": "arXiv:2511.00653v1 Announce Type: new \nIndividual tree segmentation (ITS) from LiDAR point clouds is fundamental for applications such as forest inventory, carbon monitoring and biodiversity assessment. Traditionally, ITS has been achieved with unsupervised geometry-based algorithms, while more recent advances have shifted toward supervised deep learning (DL). In the past, progress in method development was hindered by the lack of large-scale benchmark datasets, and the availability of novel data formats, particularly multispectral (MS) LiDAR, remains limited to this day, despite evidence that MS reflectance can improve the accuracy of ITS. This study introduces FGI-EMIT, the first large-scale MS airborne laser scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550 nm, the dataset consists of 1,561 manually annotated trees, with a particular focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked four conventional unsupervised algorithms and four supervised DL approaches. Hyperparameters of unsupervised methods were optimized using a Bayesian approach, while DL models were trained from scratch. Among the unsupervised methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL approaches performed significantly better overall, with the best model, ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. An ablation study demonstrated that current DL-based approaches generally fail to leverage MS reflectance information when it is provided as additional input features, although single channel reflectance can improve accuracy marginally, especially for understory trees. A performance analysis across point densities further showed that DL methods consistently remain superior to unsupervised algorithms, even at densities as low as 10 points/m$^2$.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00653", "pdf_url": "https://arxiv.org/pdf/2511.00653.pdf", "is_interesting": false}, "71": {"title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control", "authors": ["Mehmet Yigit Avci, Pedro Borges, Virginia Fernandez, Paul Wright, Mehmet Yigitsoy, Sebastien Ourselin, Jorge Cardoso"], "abstract": "arXiv:2511.00681v1 Announce Type: new \nMagnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00681", "pdf_url": "https://arxiv.org/pdf/2511.00681.pdf", "is_interesting": false}, "72": {"title": "Outlier-Aware Post-Training Quantization for Image Super-Resolution", "authors": ["Hailing Wang, jianglin Lu, Yitian Zhang, Yun Fu"], "abstract": "arXiv:2511.00682v1 Announce Type: new \nQuantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75 speedup.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00682", "pdf_url": "https://arxiv.org/pdf/2511.00682.pdf", "is_interesting": false}, "73": {"title": "Evolve to Inspire: Novelty Search for Diverse Image Generation", "authors": ["Alex Inch, Passawis Chaiyapattanaporn, Yuchen Zhu, Yuan Lu, Ting-Wen Ko, Davide Paglieri"], "abstract": "arXiv:2511.00686v1 Announce Type: new \nText-to-image diffusion models, while proficient at generating high-fidelity im- ages, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00686", "pdf_url": "https://arxiv.org/pdf/2511.00686.pdf", "is_interesting": false}, "74": {"title": "Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics", "authors": ["Taifour Yousra, Beghdadi Azeddine, Marie Luong, Zuheng Ming"], "abstract": "arXiv:2511.00698v1 Announce Type: new \nLow-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00698", "pdf_url": "https://arxiv.org/pdf/2511.00698.pdf", "is_interesting": false}, "75": {"title": "Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data", "authors": ["Hugo Massaroli, Hernan Chaves, Pilar Anania, Mauricio Farez, Emmanuel Iarussi, Viviana Siless"], "abstract": "arXiv:2511.00728v1 Announce Type: new \nDeep learning models have shown strong performance in diagnosing Alzheimer's disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with training datasets largely composed of North American cohorts such as those in the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their generalization to underrepresented populations remains underexplored. In this study, we benchmark convolutional and Transformer-based models on the ADNI dataset and assess their generalization performance on a novel Latin American clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show that while all models achieve high AUCs on ADNI (up to .96, .97), their performance drops substantially on FLENI (down to .82, .80, respectively), revealing a significant domain shift. The tested architectures demonstrated similar performance, calling into question the supposed advantages of transformers for this specific task. Through ablation studies, we identify per-image normalization and a correct sampling selection as key factors for generalization. Occlusion sensitivity analysis further reveals that models trained on ADNI, generally attend to canonical hypometabolic regions for the AD class, but focus becomes unclear for the other classes and for FLENI scans. These findings highlight the need for population-aware validation of diagnostic AI models and motivate future work on domain adaptation and cohort diversification.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00728", "pdf_url": "https://arxiv.org/pdf/2511.00728.pdf", "is_interesting": false}, "76": {"title": "Towards classification-based representation learning for place recognition on LiDAR scans", "authors": ["Dmitrii Khizbullin, Maksim Konoplia"], "abstract": "arXiv:2511.00738v1 Announce Type: new \nPlace recognition is a crucial task in autonomous driving, allowing vehicles to determine their position using sensor data. While most existing methods rely on contrastive learning, we explore an alternative approach by framing place recognition as a multi-class classification problem. Our method assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to classify each scan's position directly. We evaluate this approach on the NuScenes dataset and show that it achieves competitive performance compared to contrastive learning-based methods while offering advantages in training efficiency and stability.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00738", "pdf_url": "https://arxiv.org/pdf/2511.00738.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5 / \u89c6\u89c9\u5b9a\u4f4d", "reason": "The paper discusses place recognition for autonomous driving, specifically using LiDAR scans for determining vehicle position. This task is crucial for autonomous vehicles to understand their environment and position, which directly contributes to the overall perception system, making it highly relevant to autonomous driving."}, "review": "```json\n{\n  \"score\": 6.0,\n  \"subfield\": \"LiDAR-based Place Recognition / \u5168\u5c40\u5b9a\u4f4d\",\n  \"sota_claim\": {\n    \"is_sota\": false,\n    \"dataset\": \"NuScenes\",\n    \"metrics\": \"Recall@1, Recall@1%\",\n    \"sota_details\": \"\u8bba\u6587\u660e\u786e\u6307\u51fa\uff1a\u201c\u5f53\u524d\u65b9\u6cd5\u5728\u4e0e\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u65f6\uff0c\u6027\u80fd\u4f3c\u4e4e\u672a\u80fd\u8ddf\u4e0a\u3002\u201d\u5728Boston\u5730\u56fe\u4e0a\uff0c\u672c\u6587\u65b9\u6cd5\u7684Recall@1\u4e3a39.13%\uff0c\u800c\u57fa\u7ebfOverlapTransformer\u4e3a82.05%\uff1b\u5728SG-one\u5730\u56fe\u4e0a\uff0c\u672c\u6587\u65b9\u6cd5\u7684Recall@1\u4e3a56.34%\uff0c\u800c\u57fa\u7ebfOverlapTransformer\u4e3a98.73%\u3002\u6027\u80fd\u8fdc\u4f4e\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002\"\n  },\n  \"reason\": \"\u672c\u6587\u63d0\u51fa\u5c06LiDAR\u70b9\u4e91\u7684\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002\u8fd9\u4e00\u65b9\u6cd5\u8bba\u4e0a\u7684\u521b\u65b0\u6027\u8f83\u5f3a\uff0c\u5e76\u5f15\u5165\u4e86\u63a9\u853d\u4ea4\u53c9\u71b5\u635f\u5931\u6765\u5904\u7406\u7a7a\u95f4\u7c7b\u522b\u95f4\u7684\u8f6f\u8fb9\u754c\uff0c\u65e8\u5728\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002\u5730\u70b9\u8bc6\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5168\u5c40\u5b9a\u4f4d\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u56e0\u6b64\u7814\u7a76\u65b9\u5411\u4e0e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u9ad8\u5ea6\u76f8\u5173\u3002\\n\\n\u7136\u800c\uff0c\u5c3d\u7ba1\u65b9\u6cd5\u5177\u6709\u65b0\u9896\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u8fdc\u4f4e\u4e8e\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60SOTA\u65b9\u6cd5\uff0c\u5982OverlapTransformer\u3002\u4f5c\u8005\u4e5f\u5766\u8bda\u5730\u627f\u8ba4\u4e86\u8fd9\u4e00\u70b9\uff0c\u5e76\u6307\u51fa\u6a21\u578b\u7279\u5f81\u63d0\u53d6\u5668\u53ef\u80fd\u8fc7\u4e8e\u8f7b\u91cf\u7ea7\uff0c\u5b66\u4e60\u80fd\u529b\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u8bba\u6587\u7684\u8bc4\u4f30\u4ec5\u9650\u4e8e\u57df\u5185\uff08in-domain\uff09\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u7684\u57df\u5916\uff08out-of-domain\uff09\u8bc4\u4f30\u3002\u5bf9\u4e8e\u5927\u89c4\u6a21\u5730\u56fe\uff0c\u5206\u7c7b\u65b9\u6cd5\u5728\u7c7b\u522b\u6570\u91cf\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u4e5f\u88ab\u4f5c\u8005\u6307\u51fa\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002\\n\\n\u603b\u4f53\u800c\u8a00\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u66ff\u4ee3\u7814\u7a76\u65b9\u5411\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u5e76\u5728\u6982\u5ff5\u4e0a\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u3002\u4f46\u7531\u4e8e\u76ee\u524d\u6027\u80fd\u663e\u8457\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027\uff08\u5982\u53ef\u6269\u5c55\u6027\u548c\u57df\u5916\u6cdb\u5316\uff09\uff0c\u5176\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u7684\u76f4\u63a5\u4ef7\u503c\u548c\u884c\u4e1a\u6f5c\u529b\u8f83\u4f4e\u3002\u8be5\u8bba\u6587\u66f4\u50cf\u662f\u4e00\u4e2a\u7406\u5ff5\u7684\u9a8c\u8bc1\uff0c\u800c\u975e\u4e00\u4e2a\u5e26\u6765\u6027\u80fd\u7a81\u7834\u7684\u6210\u679c\u3002\"\n}\n```"}, "77": {"title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models", "authors": ["Tanvi Dinkar, Aiqi Jiang, Gavin Abercrombie, Ioannis Konstas"], "abstract": "arXiv:2511.00749v1 Announce Type: new \nSocial media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode 'beauty' and erase 'ugliness', and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such as \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models -- biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00749", "pdf_url": "https://arxiv.org/pdf/2511.00749.pdf", "is_interesting": false}, "78": {"title": "A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection", "authors": ["Anis Suttan Shahrir, Zakiah Ayop, Syarulnaziah Anawar, Norulzahrah Mohd Zainudin"], "abstract": "arXiv:2511.00777v1 Announce Type: new \nDurian plantation suffers from animal intrusions that cause crop damage and financial loss. The traditional farming practices prove ineffective due to the unavailability of monitoring without human intervention. The fast growth of machine learning and Internet of Things (IoT) technology has led to new ways to detect animals. However, current systems are limited by dependence on single object detection algorithms, less accessible notification platforms, and limited deterrent mechanisms. This research suggests an IoT-enabled animal detection system for durian crops. The system integrates YOLOv5 and SSD object detection algorithms to improve detection accuracy. The system provides real-time monitoring, with detected intrusions automatically reported to farmers via Telegram notifications for rapid response. An automated sound mechanism (e.g., tiger roar) is triggered once the animal is detected. The YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%, 85% and 70%, respectively. The system shows the highest accuracy in daytime and decreases at night, regardless of whether the image is still or a video. Overall, this study contributes a comprehensive and practical framework that combines detection, notification, and deterrence, paving the way for future innovations in automated farming solutions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00777", "pdf_url": "https://arxiv.org/pdf/2511.00777.pdf", "is_interesting": false}, "79": {"title": "Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking", "authors": ["Juan Wang, Yasutomo Kawanishi, Tomo Miyazaki, Zhijie Wang, Shinichiro Omachi"], "abstract": "arXiv:2511.00785v1 Announce Type: new \n3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00785", "pdf_url": "https://arxiv.org/pdf/2511.00785.pdf", "is_interesting": false}, "80": {"title": "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data", "authors": ["Viswa Chaitanya Marella, Suhasnadh Reddy Veluru, Sai Teja Erukude"], "abstract": "arXiv:2511.00795v1 Announce Type: new \nFederated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00795", "pdf_url": "https://arxiv.org/pdf/2511.00795.pdf", "is_interesting": false}, "81": {"title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing", "authors": ["Zhihui Chen, Mengling Feng"], "abstract": "arXiv:2511.00801v1 Announce Type: new \nRecent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [https://github.com/richardChenzhihui/med-banana-50k].", "categories": ["cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.00801", "pdf_url": "https://arxiv.org/pdf/2511.00801.pdf", "is_interesting": false}, "82": {"title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding", "authors": ["Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu, Changyou Chen, Ruiyi Zhang"], "abstract": "arXiv:2511.00810v1 Announce Type: new \nGraphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00810", "pdf_url": "https://arxiv.org/pdf/2511.00810.pdf", "is_interesting": false}, "83": {"title": "TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation", "authors": ["Yue Gou, Fanghui Song, Yuming Xing, Shengzhu Shi, Zhichang Guo, Boying Wu"], "abstract": "arXiv:2511.00815v1 Announce Type: new \nPancreas segmentation in medical image processing is a persistent challenge due to its small size, low contrast against adjacent tissues, and significant topological variations. Traditional level set methods drive boundary evolution using gradient flows, often ignoring pointwise topological effects. Conversely, deep learning-based segmentation networks extract rich semantic features but frequently sacrifice structural details. To bridge this gap, we propose a novel model named TA-LSDiff, which combined topology-aware diffusion probabilistic model and level set energy, achieving segmentation without explicit geometric evolution. This energy function guides implicit curve evolution by integrating the input image and deep features through four complementary terms. To further enhance boundary precision, we introduce a pixel-adaptive refinement module that locally modulates the energy function using affinity weighting from neighboring evidence. Ablation studies systematically quantify the contribution of each proposed component. Evaluations on four public pancreas datasets demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming existing methods. These results establish TA-LSDiff as a practical and accurate solution for pancreas segmentation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00815", "pdf_url": "https://arxiv.org/pdf/2511.00815.pdf", "is_interesting": false}, "84": {"title": "OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models", "authors": ["Ruoxiang Huang, Xindian Ma, Rundong Kong, Zhen Yuan, Peng Zhang"], "abstract": "arXiv:2511.00821v1 Announce Type: new \nVision-Language Models (VLMs) have demonstrated strong performance across various multimodal tasks, where position encoding plays a vital role in modeling both the sequential structure of textual information and the spatial structure of visual information. However, current VLMs commonly adopt modality-unified 1D or 2D positional indexing strategies, which treat textual and visual tokens uniformly without accounting for their distinct structural properties and sequential continuity for text and spatial coherence for vision. To address this limitation, we propose OMEGA, a novel position encoding framework that employs Modality-Specific Position Encoding (MSPE) to assign positional indices while preserving the inherent structures of each modality across separate coordinate dimensions. Additionally, to align the information density of multimodal data in the positional index space, OMEGA introduces Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the position encoding step size of visual tokens based on the embedding entropy of both modalities. Experimental results demonstrate that OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks. On visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00821", "pdf_url": "https://arxiv.org/pdf/2511.00821.pdf", "is_interesting": false}, "85": {"title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack", "authors": ["Xin Liu, Aoyang Zhou, Aoyang Zhou"], "abstract": "arXiv:2511.00831v1 Announce Type: new \nVisual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00831", "pdf_url": "https://arxiv.org/pdf/2511.00831.pdf", "is_interesting": false}, "86": {"title": "Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials", "authors": ["Yifan Pu, Jixuan Ying, Qixiu Li, Tianzhu Ye, Dongchen Han, Xiaochen Wang, Ziyi Wang, Xinyu Shao, Gao Huang, Xiu Li"], "abstract": "arXiv:2511.00833v1 Announce Type: new \nVision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at https://github.com/LeapLabTHU/LinearDiff.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00833", "pdf_url": "https://arxiv.org/pdf/2511.00833.pdf", "is_interesting": false}, "87": {"title": "Parameter Interpolation Adversarial Training for Robust Image Classification", "authors": ["Xin Liu, Yichen Yang, Kun He, John E. Hopcroft"], "abstract": "arXiv:2511.00836v1 Announce Type: new \nThough deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00836", "pdf_url": "https://arxiv.org/pdf/2511.00836.pdf", "is_interesting": false}, "88": {"title": "OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks", "authors": ["Zhihao Peng, Cheng Wang, Shengyuan Liu, Zhiying Liang, Yixuan Yuan"], "abstract": "arXiv:2511.00846v1 Announce Type: new \nBrain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark \\& code.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00846", "pdf_url": "https://arxiv.org/pdf/2511.00846.pdf", "is_interesting": false}, "89": {"title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction", "authors": ["Yu Liu, Zhijie Liu, Zedong Yang, You-Fu Li, He Kong"], "abstract": "arXiv:2511.00858v1 Announce Type: new \nPredicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00858", "pdf_url": "https://arxiv.org/pdf/2511.00858.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u8f68\u8ff9\u9884\u6d4b / \u884c\u4eba\u610f\u56fe\u9884\u6d4b", "reason": "The paper focuses on predicting pedestrian crossing intentions, a crucial task for autonomous vehicles to navigate safely in urban environments. It discusses handling occlusion scenarios, which directly relates to improving prediction accuracy for autonomous driving systems in real-world traffic conditions."}, "review": "```json\n{\n  \"score\": 9.0,\n  \"subfield\": \"\u884c\u4eba\u610f\u56fe\u9884\u6d4b / \u8f68\u8ff9\u9884\u6d4b / \u591a\u4f20\u611f\u5668\u878d\u5408\u611f\u77e5\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"PIE (Pedestrian Intention Estimation), JAAD (Joint Attention for Autonomous Driving)\",\n    \"metrics\": \"Accuracy (Acc), Area Under the Curve (AUC), F1-score (F1)\",\n    \"sota_details\": \"\u5728PIE\u548cJAAD\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u6709\u906e\u6321\u573a\u666f\uff08Element Occlusion (EO) \u548c Partial Occlusion (PO)\uff0c\u906e\u6321\u957f\u5ea61-5\u5e27\uff09\u4e0b\u7684Acc\u3001AUC\u3001F1\u4e09\u9879\u6307\u6807\u5747\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728PIE\u6570\u636e\u96c6\u7684EO5\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u5728Acc\u3001AUC\u3001F1\u4e0a\u5206\u522b\u6bd4TrEP\u63d0\u5347\u4e865%\u30014%\u548c5%\uff1b\u5728JAAD\u6570\u636e\u96c6\u7684EO5\u573a\u666f\u4e0b\uff0cAcc\u3001AUC\u3001F1\u5206\u522b\u63d0\u5347\u4e864%\u30013%\u548c3%\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u5728\u906e\u6321\u573a\u666f\u4e0b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6027\u7684 Occlusion-Aware Diffusion Model (ODM)\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u5904\u7406\u906e\u6321\u4e0b\u7684\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u906e\u6321\u611f\u77e5\u6269\u6563Transformer\u548c\u906e\u6321\u63a9\u7801\u5f15\u5bfc\u9006\u8fc7\u7a0b\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd9\u4e00\u65b9\u5411\u5728\u73b0\u6709\u5de5\u4f5c\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5177\u6709\u663e\u8457\u7684\u521b\u65b0\u6027\u3002\\n\\n\u5b9e\u9a8c\u90e8\u5206\u975e\u5e38\u5b8c\u6574\u548c\u4e25\u8c28\u3002\u4f5c\u8005\u5728\u4e24\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6570\u636e\u96c6\uff08PIE\u548cJAAD\uff09\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u7c7b\u578b\uff08Element Occlusion\u548cPartial Occlusion\uff09\u53ca\u4e0d\u540c\u7a0b\u5ea6\u7684\u906e\u6321\u8bbe\u7f6e\uff0c\u6a21\u62df\u4e86\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002\u4e0e\u591a\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5982TrEP\u7b49Transformer-based\u65b9\u6cd5\uff09\u7684\u5bf9\u6bd4\u7ed3\u679c\uff0c\u6e05\u6670\u5730\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u5404\u79cd\u906e\u6321\u573a\u666f\u4e0b\u7684\u6301\u7eed\u6027\u6027\u80fd\u63d0\u5347\uff0c\u6709\u529b\u5730\u652f\u6301\u4e86\u5176SOTA\u7684\u58f0\u660e\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u5bf9\u6269\u6563\u63a9\u7801\u3001Transformer\u63a9\u7801\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u6269\u6563\u6b65\u6570\u3001\u7279\u5f81\u91cd\u5efa\u3001\u591a\u6a21\u6001\u8f93\u5165\u548c\u95e8\u63a7\u878d\u5408\u673a\u5236\u7b49\u51e0\u4e4e\u6240\u6709\u6838\u5fc3\u7ec4\u4ef6\u90fd\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002\u5b9a\u6027\u5206\u6790\uff08\u5982\u53bb\u566a\u53ef\u89c6\u5316\u548c\u610f\u56fe\u9884\u6d4b\u6848\u4f8b\uff09\u548c\u5bf9\u5931\u8d25\u6848\u4f8b\u7684\u8ba8\u8bba\u4e5f\u589e\u5f3a\u4e86\u8bba\u6587\u7684\u53ef\u4fe1\u5ea6\u3002\\n\\n\u8be5\u7814\u7a76\u76f4\u63a5\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u5b89\u5168\u6538\u5173\u7684\u95ee\u9898\u2014\u2014\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u884c\u4eba\u610f\u56fe\u9884\u6d4b\uff0c\u5177\u6709\u5de8\u5927\u7684\u884c\u4e1a\u5e94\u7528\u6f5c\u529b\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u5f00\u9500\u8f83\u5927\uff0c\u4f46\u8bba\u6587\u660e\u786e\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u5c06\u5305\u62ec\u8f7b\u91cf\u5316\u67b6\u6784\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u8868\u660e\u4e86\u5bf9\u5b9e\u9645\u90e8\u7f72\u7684\u8003\u91cf\u3002\u9274\u4e8e\u5176\u9ad8\u5ea6\u7684\u521b\u65b0\u6027\u3001\u624e\u5b9e\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u7684\u5173\u952e\u5f71\u54cd\uff0c\u8be5\u8bba\u6587\u5728\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u5177\u6709\u975e\u5e38\u9ad8\u7684\u4ef7\u503c\u3002\"\n}\n```"}, "90": {"title": "Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion", "authors": ["Jaehyun Park, Konyul Park, Daehun Kim, Junseo Park, Jun Won Choi"], "abstract": "arXiv:2511.00859v1 Announce Type: new \nIn autonomous driving, transparency in the decision-making of perception models is critical, as even a single misperception can be catastrophic. Yet with multi-sensor inputs, it is difficult to determine how each modality contributes to a prediction because sensor information becomes entangled within the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. To our knowledge, LMD is the first approach to attribute the predictions of a perception model to individual input modalities in a sensor-fusion system for autonomous driving. We evaluate LMD on pretrained fusion models under camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous driving. Its effectiveness is validated using structured perturbation-based metrics and modality-wise visual decompositions, demonstrating practical applicability to interpreting high-capacity multimodal architectures. Code is available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00859", "pdf_url": "https://arxiv.org/pdf/2511.00859.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u591a\u4f20\u611f\u5668\u878d\u5408 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper introduces a method for interpreting the contributions of individual modalities in sensor fusion systems, specifically in the context of autonomous driving. The proposed approach is directly applicable to understanding and improving multimodal sensor fusion in autonomous vehicles, which is a key task for perception in autonomous driving systems."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"\u53ef\u89e3\u91caAI (XAI) / \u591a\u4f20\u611f\u5668\u878d\u5408\u611f\u77e5\u89e3\u91ca\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"nuScenes\",\n    \"metrics\": \"Pearson Correlation Coefficient (PCC), Mean Squared Error (MSE) ( perturbation-based metrics: Rp/R, Rp/C, Cp/R, Cp/C)\",\n    \"sota_details\": \"\u8bba\u6587\u63d0\u51faLMD\u4f5c\u4e3a\u9996\u4e2a\u5728\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u4e2d\u5c06\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u5f52\u56e0\u4e8e\u5355\u4e2a\u8f93\u5165\u6a21\u6001\u7684\u65b9\u6cd5\u3002\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cLMD\uff08\u91c7\u7528Ratio Rule\uff09\u5728\u591a\u6a21\u6001\u5206\u79bb\u6027\uff08\u7531\u65b0\u5f15\u5165\u7684PCC\u548cMSE\u6307\u6807\u8861\u91cf\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728\u96f7\u8fbe+\u76f8\u673a\u878d\u5408\u6a21\u578b\u4e2d\uff0c\u5f53\u96f7\u8fbe\u8f93\u5165\u53d7\u6270\u52a8\u65f6\uff0cLMD\u7684\u96f7\u8fbe\u9884\u6d4b\u53d8\u5316\uff08Rp/R\uff09\u4ec5\u4e3a0.05\uff0c\u800c\u76f8\u673a\u9884\u6d4b\u53d8\u5316\uff08Rp/C\uff09\u4e3a1.00\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u9694\u79bb\u6a21\u6001\u5f71\u54cd\uff1b\u4e0e\u6b64\u76f8\u5bf9\uff0c\u57fa\u7ebf\u6a21\u578b\u663e\u793a\u51fa\u8de8\u6a21\u6001\u7684\u6b8b\u4f59\u5173\u8054\u3002LMD+SHAP\u5728\u6a21\u6001\u7ea7\u5206\u79bb\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684SHAP\uff0c\u4f8b\u5982\u5728\u96f7\u8fbe+\u76f8\u673a\u4e0a\uff0cLMD+SHAP\u7684Rp/C\u4e3a0.9385\uff0cSHAP\u4e3a0.6909\uff0cCp/R\u4e3a0.8942\uff0cSHAP\u4e3a0.6746\uff08\u66f4\u9ad8\u8868\u793a\u4e0d\u53d8\u6027\u66f4\u597d\uff09\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLMD\uff08Layer-Wise Modality Decomposition\uff09\u7684\u540e\u9a8c\u3001\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u65e8\u5728\u5206\u89e3\u81ea\u52a8\u9a7e\u9a76\u591a\u4f20\u611f\u5668\u878d\u5408\u6a21\u578b\u4e2d\u5404\u5c42\u7ea7\u7684\u6a21\u6001\u7279\u5b9a\u4fe1\u606f\u3002\u8fd9\u9879\u5de5\u4f5c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5177\u6709\u6781\u9ad8\u7684\u4ef7\u503c\u548c\u91cd\u8981\u6027\uff0c\u76f4\u63a5\u89e3\u51b3\u4e86\u9ed1\u76d2\u591a\u6a21\u6001\u611f\u77e5\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6838\u5fc3\u6311\u6218\u3002\\n\\n**\u521b\u65b0\u6027 (9.5/10):** \u8bba\u6587\u63d0\u51fa\u7684LMD\u662f\u9996\u6b21\u5c1d\u8bd5\u5728\u81ea\u52a8\u9a7e\u9a76\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u4e2d\u5c06\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u5f52\u56e0\u4e8e\u5355\u4e2a\u8f93\u5165\u6a21\u6001\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u5ea6\u521b\u65b0\u6027\u3002\u901a\u8fc7\u5bf9\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u8fdb\u884c\u5c40\u90e8\u7ebf\u6027\u5316\uff0c\u5e76\u57fa\u4e8eLayer-Wise Relevance Propagation (LRP)\u548cDeep Taylor Decomposition (DTD)\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5c42\u7ea7\u6a21\u6001\u5206\u89e3\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u4e00\u5957\u65b0\u9896\u7684\u57fa\u4e8e\u6270\u52a8\u7684\u6307\u6807\uff08Pearson Correlation Coefficient\u548cMean Squared Error\uff09\u6765\u91cf\u5316\u8bc4\u4f30\u6a21\u6001\u5206\u79bb\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002\\n\\n**\u5b9e\u9a8c\u5b8c\u6574\u6027 (9.0/10):** \u5b9e\u9a8c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u9a8c\u8bc1\u4e86LMD\u5728\u76f8\u673a-\u96f7\u8fbe\u3001\u76f8\u673a-LiDAR\u4ee5\u53ca\u76f8\u673a-\u96f7\u8fbe-LiDAR\u7b49\u591a\u79cd\u4f20\u611f\u5668\u878d\u5408\u914d\u7f6e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4f7f\u7528\u4e86SimpleBEV\u548cCRN\u7b49\u5177\u4f53\u878d\u5408\u6a21\u578b\u3002\u8bba\u6587\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5b9a\u6027\u5206\u6790\uff08\u901a\u8fc7\u53ef\u89c6\u5316\u5c55\u793a\u6a21\u6001\u8d21\u732e\uff09\uff0c\u8fd8\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5305\u62ec\u5bf9\u4e0d\u540c\u504f\u5dee\u62c6\u5206\u7b56\u7565\uff08\u5982Ratio Rule\u3001Identity Rule\u7b49\uff09\u7684\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u6700\u4f18\u914d\u7f6e\u3002\u8ba1\u7b97\u6548\u7387\u5206\u6790\u4e5f\u6bd4\u8f83\u4e86LMD\u4e0eLRP\u548cShapley-based\u65b9\u6cd5\u7684\u590d\u6742\u5ea6\u3002\\n\\n**\u53ef\u4fe1\u5ea6 (9.0/10):** \u65b9\u6cd5\u5efa\u7acb\u5728\u624e\u5b9e\u7684\u6570\u5b66\u548cXAI\u7406\u8bba\u57fa\u7840\u4e4b\u4e0a\uff08Taylor\u5c55\u5f00\u3001LRP\u3001DTD\uff09\uff0c\u9644\u5f55\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6570\u5b66\u63a8\u5bfc\u3002\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u4e14\u652f\u6301\u8bba\u6587\u7684\u6838\u5fc3\u4e3b\u5f20\uff0c\u5373LMD\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u6a21\u6001\u5206\u79bb\u3002\u5f00\u653e\u6e90\u4ee3\u7801\u7684\u627f\u8bfa\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u5bf9\u6f5c\u5728\u98ce\u9669\uff08\u5982\u8bef\u5bfc\u6027\u4fdd\u8bc1\u3001\u504f\u5dee\u653e\u5927\uff09\u7684\u8ba8\u8bba\u4e5f\u663e\u793a\u4e86\u7814\u7a76\u56e2\u961f\u7684\u4e25\u8c28\u6027\u3002\\n\\n**\u884c\u4e1a\u6f5c\u529b (10/10):** \u53ef\u89e3\u91ca\u6027\u662f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b9e\u73b0L3\u53ca\u4ee5\u4e0a\u7ea7\u522b\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\u4e4b\u4e00\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u6709\u52a9\u4e8e\u7406\u89e3\u590d\u6742\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u5355\u4f20\u611f\u5668\u6545\u969c\uff0c\u52a0\u901f\u7cfb\u7edf\u5ba1\u8ba1\u548c\u8ba4\u8bc1\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002\u5176\u6a21\u578b\u65e0\u5173\u6027\u4e5f\u4f7f\u5176\u80fd\u591f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u3002\"\n}\n```"}, "91": {"title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks", "authors": ["Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang"], "abstract": "arXiv:2511.00908v1 Announce Type: new \nVisual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.00908", "pdf_url": "https://arxiv.org/pdf/2511.00908.pdf", "is_interesting": false}, "92": {"title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs", "authors": ["Yan Shu, Chi Liu, Robin Chen, Derek Li, Bryan Dai"], "abstract": "arXiv:2511.00916v1 Announce Type: new \nMultimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature -- encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00916", "pdf_url": "https://arxiv.org/pdf/2511.00916.pdf", "is_interesting": false}, "93": {"title": "Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval", "authors": ["Hanwen Su, Ge Song, Jiyan Wang, Yuanbo Zhu"], "abstract": "arXiv:2511.00925v1 Announce Type: new \nThe problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g. e-commerce. Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance. Therefore, in this paper, we introduce an approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It consists of three components: (i) a Uni-modal Feature Extraction Module that includes a CLIP text encoder and a ViT for extracting textual and visual tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an alignment weight list by the local and global aggregation blocks to measure the aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss Module aiming to improve the balance of domains in the triplet loss. Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, show our method delivers superior performances over the state-of-the-art ZS-SBIR methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00925", "pdf_url": "https://arxiv.org/pdf/2511.00925.pdf", "is_interesting": false}, "94": {"title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference", "authors": ["Liuzhuozheng Li, Yue Gong, Shanyuan Liu, Bo Cheng, Yuhang Ma, Liebucha Wu, Dengyang Jiang, Zanyi Wang, Dawei Leng, Yuhui Yin"], "abstract": "arXiv:2511.00956v1 Announce Type: new \nWe propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00956", "pdf_url": "https://arxiv.org/pdf/2511.00956.pdf", "is_interesting": false}, "95": {"title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis", "authors": ["Dongheng Lin, Mengxue Qu, Kunyang Han, Jianbo Jiao, Xiaojie Jin, Yunchao Wei"], "abstract": "arXiv:2511.00962v1 Announce Type: new \nMost video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: https://rathgrith.github.io/Unified_Frame_VAA/.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00962", "pdf_url": "https://arxiv.org/pdf/2511.00962.pdf", "is_interesting": false}, "96": {"title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel", "authors": ["Suzhong Fu, Rui Sun, Xuan Ding, Jingqi Dong, Yiming Yang, Yao Zhu, Min Chang Jordan Ren, Delin Deng, Angelica Aviles-Rivero, Shuguang Cui, Zhen Li"], "abstract": "arXiv:2511.00981v1 Announce Type: new \nAccurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00981", "pdf_url": "https://arxiv.org/pdf/2511.00981.pdf", "is_interesting": false}, "97": {"title": "MID: A Self-supervised Multimodal Iterative Denoising Framework", "authors": ["Chang Nie, Tianchen Deng, Zhe Liu, Hesheng Wang"], "abstract": "arXiv:2511.00997v1 Announce Type: new \nData denoising is a persistent challenge across scientific and engineering domains. Real-world data is frequently corrupted by complex, non-linear noise, rendering traditional rule-based denoising methods inadequate. To overcome these obstacles, we propose a novel self-supervised multimodal iterative denoising (MID) framework. MID models the collected noisy data as a state within a continuous process of non-linear noise accumulation. By iteratively introducing further noise, MID learns two neural networks: one to estimate the current noise step and another to predict and subtract the corresponding noise increment. For complex non-linear contamination, MID employs a first-order Taylor expansion to locally linearize the noise process, enabling effective iterative removal. Crucially, MID does not require paired clean-noisy datasets, as it learns noise characteristics directly from the noisy inputs. Experiments across four classic computer vision tasks demonstrate MID's robustness, adaptability, and consistent state-of-the-art performance. Moreover, MID exhibits strong performance and adaptability in tasks within the biomedical and bioinformatics domains.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00997", "pdf_url": "https://arxiv.org/pdf/2511.00997.pdf", "is_interesting": false}, "98": {"title": "Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya", "authors": ["Hassan Ugail, Ismail Lujain Jaleel"], "abstract": "arXiv:2511.01000v1 Announce Type: new \nArt authentication of Francisco Goya's works presents complex computational challenges due to his heterogeneous stylistic evolution and extensive historical patterns of forgery. We introduce a novel multimodal machine learning framework that applies identical feature extraction techniques to both visual and X-ray radiographic images of Goya paintings. The unified feature extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy measures, energy calculations, and colour distribution analysis applied consistently across both imaging modalities. The extracted features from both visual and X-ray images are processed through an optimised One-Class Support Vector Machine with hyperparameter tuning. Using a dataset of 24 authenticated Goya paintings with corresponding X-ray images, split into an 80/20 train-test configuration with 10-fold cross-validation, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy of our pipeline, achieving 92.3% authentication confidence through unified multimodal feature analysis. Our results indicate substantial performance improvement over single-modal approaches, establishing the effectiveness of applying identical computational methods to both visual and radiographic imagery in art authentication applications.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01000", "pdf_url": "https://arxiv.org/pdf/2511.01000.pdf", "is_interesting": false}, "99": {"title": "HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images", "authors": ["Mohammad Amanour Rahman"], "abstract": "arXiv:2511.01013v1 Announce Type: new \nB-mode ultrasound for breast cancer diagnosis faces challenges: speckle, operator dependency, and indistinct boundaries. Existing deep learning suffers from single-task learning, architectural constraints (CNNs lack global context, Transformers local features), and black-box decision-making. These gaps hinder clinical adoption.\n  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous segmentation and classification with intrinsic interpretability. Its dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks. An attention-gated decoder provides precision and explainability. We introduce dual-pipeline interpretability: (1) intrinsic attention validation with quantitative IoU verification (mean: 0.86), and (2) Grad-CAM for classification reasoning.\n  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant Recall, eliminating false negatives. Ablation studies confirm multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%.\n  Crucially, we conduct the first cross-dataset generalization study for hybrid CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058), confirming domain shift. However, progressive fine-tuning with only 10% target-domain data (68 images) recovers 92.5% performance. With 50% data, our model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and demonstrating true generalization.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01013", "pdf_url": "https://arxiv.org/pdf/2511.01013.pdf", "is_interesting": false}, "100": {"title": "FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning", "authors": ["JunXi Yuan"], "abstract": "arXiv:2511.01026v1 Announce Type: new \nWe present FastBoost, a parameter-efficient neural architecture that achieves state-of-the-art performance on CIFAR benchmarks through a novel Dynamically Scaled Progressive Attention (DSPA) mechanism. Our design establishes new efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and 93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and 74.85% (0.44M parameters) The breakthrough stems from three fundamental innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention blending with dynamic weights. (2) Phase Scaling: Training-stage-aware intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The architecture features dual attention pathways with real-time weight adjustment, cascaded refinement layers (increasing gradient flow by 12.7%), and a hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic attention and efficient convolution operations demonstrates unprecedented parameter-accuracy trade-offs, enabling deployment in resource-constrained edge devices without accuracy degradation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01026", "pdf_url": "https://arxiv.org/pdf/2511.01026.pdf", "is_interesting": false}, "101": {"title": "T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression", "authors": ["Nikolay I. Kalmykov, Razan Dibo, Kaiyu Shen, Xu Zhonghan, Anh-Huy Phan, Yipeng Liu, Ivan Oseledets"], "abstract": "arXiv:2511.01079v1 Announce Type: new \nNeural image compression (NIC) has become the state-of-the-art for rate-distortion performance, yet its security vulnerabilities remain significantly less understood than those of classifiers. Existing adversarial attacks on NICs are often naive adaptations of pixel-space methods, overlooking the unique, structured nature of the compression pipeline. In this work, we propose a more advanced class of vulnerabilities by introducing T-MLA, the first targeted multiscale log--exponential attack framework. Our approach crafts adversarial perturbations in the wavelet domain by directly targeting the quality of the attacked and reconstructed images. This allows for a principled, offline attack where perturbations are strategically confined to specific wavelet subbands, maximizing distortion while ensuring perceptual stealth. Extensive evaluation across multiple state-of-the-art NIC architectures on standard image compression benchmarks reveals a large drop in reconstruction quality while the perturbations remain visually imperceptible. Our findings reveal a critical security flaw at the core of generative and content delivery pipelines.", "categories": ["cs.CV", "cs.NA", "math.NA"], "abs_url": "https://arxiv.org/abs/2511.01079", "pdf_url": "https://arxiv.org/pdf/2511.01079.pdf", "is_interesting": false}, "102": {"title": "GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction", "authors": ["Narges Ghasemi, Amir Ziashahabi, Salman Avestimehr, Cyrus Shahabi"], "abstract": "arXiv:2511.01082v1 Announce Type: new \nImage geolocalization, the task of determining an image's geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at https://github.com/NNargesNN/GeoToken.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01082", "pdf_url": "https://arxiv.org/pdf/2511.01082.pdf", "is_interesting": false}, "103": {"title": "SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices", "authors": ["Md. Abid Hasan Rafi, Mst. Fatematuj Johora, Pankaj Bhowmik"], "abstract": "arXiv:2511.01087v1 Announce Type: new \nThe emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01087", "pdf_url": "https://arxiv.org/pdf/2511.01087.pdf", "is_interesting": false}, "104": {"title": "Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images", "authors": ["Veronica Marsico, Antonio Quintero-Rincon, Hadj Batatia"], "abstract": "arXiv:2511.01098v1 Announce Type: new \nThis study presents a novel method for diagnosing respiratory diseases using image data. It combines Epanechnikov's non-parametric kernel density estimation (EKDE) with a bimodal logistic regression classifier in a statistical-model-based learning scheme. EKDE's flexibility in modeling data distributions without assuming specific shapes and its adaptability to pixel intensity variations make it valuable for extracting key features from medical images. The method was tested on 13808 randomly selected chest X-rays from the COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of 59.26%, and a specificity of 74.18%, demonstrating moderate performance in detecting respiratory disease while showing room for improvement in sensitivity. While clinical expertise remains essential for further refining the model, this study highlights the potential of EKDE-based approaches to enhance diagnostic accuracy and reliability in medical imaging.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01098", "pdf_url": "https://arxiv.org/pdf/2511.01098.pdf", "is_interesting": false}, "105": {"title": "Anatomically Constrained Transformers for Echocardiogram Analysis", "authors": ["Alexander Thorley, Agis Chartsias, Jordan Strom, Jeremy Slivnick, Dipak Kotecha, Alberto Gomez, Jinming Duan"], "abstract": "arXiv:2511.01109v1 Announce Type: new \nVideo transformers have recently demonstrated strong potential for echocardiogram (echo) analysis, leveraging self-supervised pre-training and flexible adaptation across diverse tasks. However, like other models operating on videos, they are prone to learning spurious correlations from non-diagnostic regions such as image backgrounds. To overcome this limitation, we propose the Video Anatomically Constrained Transformer (ViACT), a novel framework that integrates anatomical priors directly into the transformer architecture. ViACT represents a deforming anatomical structure as a point set and encodes both its spatial geometry and corresponding image patches into transformer tokens. During pre-training, ViACT follows a masked autoencoding strategy that masks and reconstructs only anatomical patches, enforcing that representation learning is focused on the anatomical region. The pre-trained model can then be fine-tuned for tasks localized to this region. In this work we focus on the myocardium, demonstrating the framework on echo analysis tasks such as left ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA) detection. The anatomical constraint focuses transformer attention within the myocardium, yielding interpretable attention maps aligned with regions of known CA pathology. Moreover, ViACT generalizes to myocardium point tracking without requiring task-specific components such as correlation volumes used in specialized tracking networks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01109", "pdf_url": "https://arxiv.org/pdf/2511.01109.pdf", "is_interesting": false}, "106": {"title": "Boosting performance of computer vision applications through embedded GPUs on the edge", "authors": ["Fabio Diniz Rossi"], "abstract": "arXiv:2511.01129v1 Announce Type: new \nComputer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.", "categories": ["cs.CV", "cs.DC"], "abs_url": "https://arxiv.org/abs/2511.01129", "pdf_url": "https://arxiv.org/pdf/2511.01129.pdf", "is_interesting": false}, "107": {"title": "Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis", "authors": ["Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon"], "abstract": "arXiv:2511.01131v1 Announce Type: new \nHuman-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01131", "pdf_url": "https://arxiv.org/pdf/2511.01131.pdf", "is_interesting": false}, "108": {"title": "Learning with Category-Equivariant Architectures for Human Activity Recognition", "authors": ["Yoshihiro Maruyama"], "abstract": "arXiv:2511.01139v1 Announce Type: new \nWe propose CatEquiv, a category-equivariant neural network for Human Activity Recognition (HAR) from inertial sensors that systematically encodes temporal, amplitude, and structural symmetries. In particular, we introduce the categorical symmetry product where cyclic time shifts, positive gains and the sensor-hierarchy poset together capture the categorical symmetry structure of the data. CatEquiv achieves equivariance with respect to the categorical symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv attains markedly higher robustness compared with circularly padded CNNs and plain CNNs. These results demonstrate that enforcing categorical symmetries yields strong invariance and generalization without additional model capacity.", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01139", "pdf_url": "https://arxiv.org/pdf/2511.01139.pdf", "is_interesting": false}, "109": {"title": "MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation", "authors": ["Ziyi Wang, Yuanmei Zhang, Dorna Esrafilzadeh, Ali R. Jalili, Suncheng Xiang"], "abstract": "arXiv:2511.01143v1 Announce Type: new \nEarly and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at https://github.com/JeremyXSC/MicroAUNet.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01143", "pdf_url": "https://arxiv.org/pdf/2511.01143.pdf", "is_interesting": false}, "110": {"title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation", "authors": ["Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang, Jiageng Mao, Jiuhai Chen, Jiatao Gu, Yue Wang, Furong Huang"], "abstract": "arXiv:2511.01163v1 Announce Type: new \nUnified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01163", "pdf_url": "https://arxiv.org/pdf/2511.01163.pdf", "is_interesting": false}, "111": {"title": "Web-Scale Collection of Video Data for 4D Animal Reconstruction", "authors": ["Brian Nlong Zhao, Jiajun Wu, Shangzhe Wu"], "abstract": "arXiv:2511.01169v1 Announce Type: new \nComputer vision for animals holds great promise for wildlife research but often depends on large-scale data, while existing collection methods rely on controlled capture setups. Recent data-driven approaches show the potential of single-view, non-invasive analysis, yet current animal video datasets are limited--offering as few as 2.4K 15-frame clips and lacking key processing for animal-centric 3D/4D tasks. We introduce an automated pipeline that mines YouTube videos and processes them into object-centric clips, along with auxiliary annotations valuable for downstream tasks like pose estimation, tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos (2M frames)--an order of magnitude more than prior works. To demonstrate its utility, we focus on the 4D quadruped animal reconstruction task. To support this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually filtered sequences with 11K frames showcasing clean, diverse animal motions. We evaluate state-of-the-art model-based and model-free methods on Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic 3D shapes, while the latter yields more natural reconstructions but scores lower--revealing a gap in current evaluation. To address this, we enhance a recent model-free approach with sequence-level optimization, establishing the first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and baseline aim to advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos. Code and datasets are available at https://github.com/briannlongzhao/Animal-in-Motion.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01169", "pdf_url": "https://arxiv.org/pdf/2511.01169.pdf", "is_interesting": false}, "112": {"title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution", "authors": ["Peng Du, Hui Li, Han Xu, Paul Barom Jeon, Dongwook Lee, Daehyun Ji, Ran Yang, Feng Zhu"], "abstract": "arXiv:2511.01175v1 Announce Type: new \nDiscrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01175", "pdf_url": "https://arxiv.org/pdf/2511.01175.pdf", "is_interesting": false}, "113": {"title": "A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment", "authors": ["Minmin Zeng"], "abstract": "arXiv:2511.01194v1 Announce Type: new \nAction Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01194", "pdf_url": "https://arxiv.org/pdf/2511.01194.pdf", "is_interesting": false}, "114": {"title": "MoSa: Motion Generation with Scalable Autoregressive Modeling", "authors": ["Mengyuan Liu, Sheng Yan, Yong Wang, Yingjie Li, Gui-Bin Bian, Hong Liu"], "abstract": "arXiv:2511.01200v1 Announce Type: new \nWe introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at https://mosa-web.github.io/MoSa-web", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01200", "pdf_url": "https://arxiv.org/pdf/2511.01200.pdf", "is_interesting": false}, "115": {"title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA", "authors": ["Heyu Guo, Shanmu Wang, Ruichun Ma, Shiqi Jiang, Yasaman Ghasempour, Omid Abari, Baining Guo, Lili Qi"], "abstract": "arXiv:2511.01210v1 Announce Type: new \nVision-language-action (VLA) models have shown strong generalization for action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception is needed to guide the manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01210", "pdf_url": "https://arxiv.org/pdf/2511.01210.pdf", "is_interesting": false}, "116": {"title": "Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering", "authors": ["Riddhi Jain, Manasi Patwardhan, Parijat Deshpande, Venkataramana Runkana"], "abstract": "arXiv:2511.01213v1 Announce Type: new \nThe immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.\n  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01213", "pdf_url": "https://arxiv.org/pdf/2511.01213.pdf", "is_interesting": false}, "117": {"title": "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering", "authors": ["Zahra Mehraban, Sebastien Glaser, Michael Milford, Ronald Schroeter"], "abstract": "arXiv:2511.01223v1 Announce Type: new \nDomain adaptation is required for automated driving models to generalize well across diverse road conditions. This paper explores a training method for domain adaptation to adapt PilotNet, an end-to-end deep learning-based model, for left-hand driving conditions using real-world Australian highway data. Four training methods were evaluated: (1) a baseline model trained on U.S. right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a model pretrained on flipped U.S. data and then finetuned on Australian highways. This setup examines whether incorporating flipped data enhances the model adaptation by providing an initial left-hand driving alignment. The paper compares model performance regarding steering prediction accuracy and attention, using saliency-based analysis to measure attention shifts across significant road regions. Results show that pretraining on flipped data alone worsens prediction stability due to misaligned feature representations, but significantly improves adaptation when followed by fine-tuning, leading to lower prediction error and stronger focus on left-side cues. To validate this approach across different architectures, the same experiments were done on ResNet, which confirmed similar adaptation trends. These findings emphasize the importance of preprocessing techniques, such as flipped-data pretraining, followed by fine-tuning to improve model adaptation with minimal retraining requirements.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01223", "pdf_url": "https://arxiv.org/pdf/2511.01223.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u7aef\u5230\u7aef\u63a7\u5236 / \u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u9002\u5e94", "reason": "The paper discusses domain adaptation for an end-to-end deep learning model (PilotNet) in the context of autonomous driving, specifically for adapting steering control to left-hand driving conditions. This is a key task for autonomous vehicles, making it highly relevant to the autonomous driving field."}, "review": "```json\n{\n  \"score\": 8.3,\n  \"subfield\": \"\u81ea\u52a8\u9a7e\u9a76\u57df\u9002\u5e94 / \u7aef\u5230\u7aef\u8f6c\u5411\u63a7\u5236\",\n  \"sota_claim\": {\n    \"is_sota\": false,\n    \"dataset\": \"NVIDIA Dataset (\u7f8e\u56fd\u53f3\u8235\u9a7e\u9a76\u6570\u636e), \u81ea\u5b9a\u4e49\u6fb3\u5927\u5229\u4e9a\u9ad8\u901f\u516c\u8def\u6570\u636e\u96c6 (\u5de6\u8235\u9a7e\u9a76\u6570\u636e)\",\n    \"metrics\": \"\u5747\u65b9\u8bef\u5dee (MSE), \u9884\u6d4b\u76f8\u5173\u7cfb\u6570 (Correlation Coefficient)\",\n    \"sota_details\": \"\u8bba\u6587\u58f0\u79f0\u5176\u63d0\u51fa\u7684\u201c\u7ffb\u8f6c\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3\u5e76\u5fae\u8c03\u201d\u7b56\u7565\u5728PilotNet\u548cResNet\u67b6\u6784\u4e0b\uff0c\u76f8\u5bf9\u4e8e\u5176\u4ed6\u4e09\u79cd\u5185\u90e8\u5bf9\u6bd4\u7b56\u7565\uff08\u7f8e\u56fd\u6570\u636e\u76f4\u63a5\u8bad\u7ec3\u3001\u7ffb\u8f6c\u7f8e\u56fd\u6570\u636e\u76f4\u63a5\u8bad\u7ec3\u3001\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\uff09\uff0c\u5728\u6fb3\u5927\u5229\u4e9a\u5de6\u8235\u9a7e\u9a76\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u6700\u4f4e\u7684MSE\u548c\u6700\u9ad8\u7684\u9884\u6d4b\u76f8\u5173\u6027\uff0c\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728PilotNet\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06MSE\u4ece\u57fa\u7ebf\u6a21\u578b\u7684343.59\u663e\u8457\u964d\u4f4e\u52303.51\uff0c\u4f18\u4e8e\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3\u540e\u76f4\u63a5\u5fae\u8c03\u76844.17\uff0c\u66f4\u662f\u8fdc\u4f4e\u4e8e\u4ec5\u7ffb\u8f6c\u7f8e\u56fd\u6570\u636e\u76f4\u63a5\u8bad\u7ec3\u7684930.03\u3002\"\n  },\n  \"reason\": \"\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7aef\u5230\u7aef\u8f6c\u5411\u6a21\u578b\u7684\u7b80\u5355\u800c\u9ad8\u6548\u7684\u57df\u9002\u5e94\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3\u4ece\u53f3\u8235\u9a7e\u9a76\u73af\u5883\uff08\u7f8e\u56fd\uff09\u5230\u5de6\u8235\u9a7e\u9a76\u73af\u5883\uff08\u6fb3\u5927\u5229\u4e9a\uff09\u7684\u6cdb\u5316\u96be\u9898\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u7ed3\u5408\u56fe\u50cf\u6c34\u5e73\u7ffb\u8f6c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5de6\u8235\u9a7e\u9a76\u7684\u521d\u6b65\u5bf9\u9f50\u548c\u540e\u7eed\u9002\u5e94\u3002 \\n\\n\u521b\u65b0\u6027\uff1a\u5c3d\u7ba1\u56fe\u50cf\u7ffb\u8f6c\u672c\u8eab\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u4f46\u8bba\u6587\u9996\u6b21\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5de6\u8235\u8f6c\u5411\u8fd9\u4e00\u7279\u5b9a\u4e14\u5173\u952e\u7684\u57df\u9002\u5e94\u95ee\u9898\uff0c\u7cfb\u7edf\u5730\u9a8c\u8bc1\u4e86\u5176\u4e0e\u5fae\u8c03\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u663e\u8457\u56fe\u7684\u6ce8\u610f\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u89e3\u91ca\u3002\u8fd9\u79cd\u5bf9\u7b80\u5355\u7b56\u7565\u7684\u6df1\u5165\u5256\u6790\u53ca\u5176\u5728\u89e3\u51b3\u5173\u952e\u5b9e\u9645\u573a\u666f\u95ee\u9898\u4e0a\u7684\u5e94\u7528\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5de5\u7a0b\u548c\u5b9e\u8df5\u521b\u65b0\u4ef7\u503c\u3002 \\n\\n\u5b9e\u9a8c\u5b8c\u6574\u6027\uff1a\u5b9e\u9a8c\u8bbe\u8ba1\u4e25\u8c28\uff0c\u6e05\u6670\u5bf9\u6bd4\u4e86\u56db\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728PilotNet\u548cResNet\u4e24\u79cd\u4e0d\u540c\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u666e\u9002\u6027\u3002\u4f7f\u7528\u4e86\u5b9a\u5236\u7684\u6fb3\u5927\u5229\u4e9a\u9ad8\u901f\u516c\u8def\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u6570\u636e\u9884\u5904\u7406\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u8f6c\u5411\u9884\u6d4b\u7cbe\u5ea6\uff08MSE\u3001\u76f8\u5173\u7cfb\u6570\uff09\u548c\u57fa\u4e8e\u663e\u8457\u56fe\u7684\u6ce8\u610f\u529b\u5206\u5e03\u5206\u6790\uff0c\u7279\u522b\u662f\u901a\u8fc7\u52a8\u6001ROI\u548cCanny\u8fb9\u7f18\u68c0\u6d4b\u6765\u786e\u4fdd\u6ce8\u610f\u529b\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5bf9\u9053\u8def\u76f8\u5173\u7279\u5f81\u7684\u5173\u6ce8\u3002 \\n\\n\u53ef\u4fe1\u5ea6\uff1a\u5b9e\u9a8c\u7ed3\u679c\u6e05\u6670\u4e00\u81f4\uff0c\u8de8\u4e0d\u540c\u67b6\u6784\u663e\u793a\u51fa\u76f8\u540c\u8d8b\u52bf\uff0c\u589e\u5f3a\u4e86\u7ed3\u8bba\u7684\u53ef\u9760\u6027\u3002\u5bf9\u201c\u4ec5\u7ffb\u8f6c\u6570\u636e\u9884\u8bad\u7ec3\u201d\u4e3a\u4f55\u8868\u73b0\u4e0d\u4f73\u7684\u89e3\u91ca\uff08\u8bed\u4e49\u4e0d\u4e00\u81f4\uff09\u662f\u5408\u7406\u7684\u3002\u663e\u8457\u56fe\u5206\u6790\u6709\u529b\u5730\u652f\u6491\u4e86\u7ed3\u8bba\uff0c\u8868\u660e\u6a21\u578b\u786e\u5b9e\u5b66\u4f1a\u4e86\u5173\u6ce8\u5de6\u4fa7\u8def\u6807\uff0c\u4e14\u5728\u4fdd\u6301\u4e2d\u5fc3\u548c\u53f3\u4fa7\u6ce8\u610f\u529b\u7684\u5e73\u8861\u65b9\u9762\u505a\u5f97\u66f4\u597d\uff0c\u6709\u52a9\u4e8e\u5b89\u5168\u9a7e\u9a76\u3002 \\n\\n\u884c\u4e1a\u6f5c\u529b\uff1a\u8be5\u65b9\u6cd5\u6781\u5177\u5b9e\u7528\u4ef7\u503c\u3002\u5de6\u53f3\u8235\u9a7e\u9a76\u8f6c\u6362\u662f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5168\u7403\u90e8\u7f72\u9762\u4e34\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5355\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u4e14\u6570\u636e\u9700\u6c42\u5c11\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u7684\u9002\u5e94\u548c\u90e8\u7f72\u3002\u5176\u7ed3\u679c\u76f4\u63a5\u5e94\u7528\u4e8e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u8f6c\u5411\u63a7\u5236\u6a21\u5757\uff0c\u56e0\u6b64\u4e0e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u76f4\u63a5\u76f8\u5173\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u843d\u5730\u6f5c\u529b\u3002 \\n\\n\u4e0d\u8db3\u4e4b\u5904\u5728\u4e8e\uff0c\u8bba\u6587\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5185\u90e8\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u672a\u80fd\u4e0e\u66f4\u5e7f\u6cdb\u7684\u3001\u5df2\u6709\u7684\u5148\u8fdb\u57df\u9002\u5e94\u7b97\u6cd5\uff08\u5982\u5bf9\u6297\u6027\u57df\u9002\u5e94\u3001\u7279\u5f81\u5bf9\u9f50\u7b49\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u9650\u5236\u4e86\u5176\u5728\u5b66\u672f\u521b\u65b0\u524d\u6cbf\u7684\u8d21\u732e\u3002\u7136\u800c\uff0c\u8003\u8651\u5230\u5176\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9ad8\u5b9e\u7528\u4ef7\u503c\u548c\u5206\u6790\u6df1\u5ea6\uff0c\u6574\u4f53\u8bc4\u4ef7\u4ecd\u8f83\u9ad8\u3002\"\n}\n```"}, "118": {"title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark", "authors": ["Rajmund Nagy (KTH Royal Institute of Technology), Hendric Voss (Bielefeld University), Thanh Hoang-Minh (University of Science -- VNUHCM), Mihail Tsakov (Independent Researcher), Teodor Nikolov (Motorica AB), Zeyi Zhang (Peking University), Tenglong Ao (Peking University), Sicheng Yang (Huawei Technologies Ltd), Shaoli Huang (Astribot), Yongkang Cheng (Astribot), M. Hamza Mughal (Max-Planck Institute for Informatics, SIC), Rishabh Dabral (Max-Planck Institute for Informatics, SIC), Kiran Chhatre (KTH Royal Institute of Technology), Christian Theobalt (Max-Planck Institute for Informatics, SIC), Libin Liu (Peking University), Stefan Kopp (Bielefeld University), Rachel McDonnell (Trinity College Dublin), Michael Neff (University of California, Davis), Taras Kucherenko (SEED -- Electronic Arts), Youngwoo Yoon (Electronics and Telecommunications Research Institute), Gustav Eje Henter (KTH Royal Institute of Technology, Motorica AB)"], "abstract": "arXiv:2511.01233v1 Announce Type: new \nWe review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models -- each trained by its original authors -- across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies -- enabling new evaluations without model reimplementation required -- alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.", "categories": ["cs.CV", "cs.GR", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.01233", "pdf_url": "https://arxiv.org/pdf/2511.01233.pdf", "is_interesting": false}, "119": {"title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video", "authors": ["Vishakha Lall, Yisi Liu"], "abstract": "arXiv:2511.01237v1 Announce Type: new \nHuman gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01237", "pdf_url": "https://arxiv.org/pdf/2511.01237.pdf", "is_interesting": false}, "120": {"title": "Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability", "authors": ["Zhixuan Zhang, Pingyu Wang, Xingjian Zheng, Linbo Qing, Qi Liu"], "abstract": "arXiv:2511.01240v1 Announce Type: new \nTransferable attacks generate adversarial examples on surrogate models to fool unknown victim models, posing real-world threats and growing research interest. Despite focusing on flat losses for transferable adversarial examples, recent studies still fall into suboptimal regions, especially the flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce a novel black-box gradient-based transferable attack from a perspective of dual-order information. Specifically, we feasibly propose Adversarial Flatness (AF) to the deceptive flatness problem and a theoretical assurance for adversarial transferability. Based on this, using an efficient approximation of our objective, we instantiate our attack as Adversarial Flatness Attack (AFA), addressing the altered gradient sign issue. Additionally, to further improve the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by enhancing the inner-loop sampling efficiency. The comprehensive results on ImageNet-compatible dataset demonstrate superiority over six baselines, generating adversarial examples in flatter regions and boosting transferability across model architectures. When tested on input transformation attacks or the Baidu Cloud API, our method outperforms baselines.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01240", "pdf_url": "https://arxiv.org/pdf/2511.01240.pdf", "is_interesting": false}, "121": {"title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "authors": ["Yu Tian, Zhongheng Yang, Chenshi Liu, Yiyun Su, Ziwei Hong, Zexi Gong, Jingyuan Xu"], "abstract": "arXiv:2511.01243v1 Announce Type: new \nBrain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01243", "pdf_url": "https://arxiv.org/pdf/2511.01243.pdf", "is_interesting": false}, "122": {"title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop", "authors": ["YoungJae Cheong, Jhonghyun An"], "abstract": "arXiv:2511.01250v1 Announce Type: new \nLiDAR semantic segmentation degrades in adverse weather because refraction, scattering, and point dropouts corrupt geometry. Prior work in weather simulation, mixing-based augmentation, domain randomization, and uncertainty or boundary regularization improves robustness but still overlooks structural vulnerabilities near boundaries, corners, and sparse regions. We present a Light Geometry-aware adapter. The module aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across the 0~360 degree wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points and computes simple local statistics, which are compressed into compact geometry-aware cues. During training, these cues drive region-aware regularization that stabilizes predictions in structurally fragile areas. The adapter is plug and play, complements augmentation, and can be enabled only during training with negligible inference cost. We adopt a source-only cross-weather setup where models train on SemanticKITTI and are evaluated on SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by 7.9 percentage points over the data-centric augmentation baseline and by 0.6 points over the class-centric regularization baseline. These results indicate that geometry-driven regularization is a key direction for all-weather LiDAR segmentation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01250", "pdf_url": "https://arxiv.org/pdf/2511.01250.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses improving LiDAR semantic segmentation under adverse weather conditions, which directly relates to autonomous driving tasks that rely on LiDAR for perception. While it focuses on segmentation performance, this is highly relevant for autonomous vehicles' perception systems, especially in weather-affected environments."}, "review": "```json\n{\n  \"score\": 8.9,\n  \"subfield\": \"LiDAR\u611f\u77e5 / \u8bed\u4e49\u5206\u5272 / \u6076\u52a3\u5929\u6c14\u9c81\u68d2\u6027 / \u9886\u57df\u6cdb\u5316\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"SemanticKITTI (\u8bad\u7ec3\u96c6) -> SemanticSTF (\u6d4b\u8bd5\u96c6)\",\n    \"metrics\": \"mIoU\",\n    \"sota_details\": \"\u5728Source-Only Cross-Weather LiDAR\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0cmIoU\u76f8\u6bd4\u6570\u636e\u4e2d\u5fc3\u589e\u5f3a\u57fa\u7ebf\uff08LiDARWeather [2]\uff09\u63d0\u5347+7.9%\uff0c\u76f8\u6bd4\u7c7b\u522b\u4e2d\u5fc3\u6b63\u5219\u5316\u57fa\u7ebf\uff08No Thing, Nothing [1]\uff09\u63d0\u5347+0.6%\uff0c\u5237\u65b0\u4e86\u8be5\u4efb\u52a1\u7684SOTA\u3002\"\n  },\n  \"reason\": \"\u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d LiDAR \u8bed\u4e49\u5206\u5272\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u51e0\u4f55\u5931\u771f\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002\u5176\u4e3b\u8981\u521b\u65b0\u70b9\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u8f7b\u91cf\u7ea7\u51e0\u4f55\u611f\u77e5\u9002\u914d\u5668 (Light Geometry-aware adapter)\u201d\u3002\u8be5\u9002\u914d\u5668\u901a\u8fc7\u5c40\u90e8\u7a97\u53e3 KNN \u548c\u73af\u5f62\u586b\u5145\uff08\u5904\u7406 0\u00b0-360\u00b0 \u65b9\u4f4d\u89d2\u7f1d\u9699\uff09\u6765\u663e\u5f0f\u5730\u611f\u77e5\u5e76\u6ce8\u5165\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff08\u5982\u8fb9\u754c\u3001\u89d2\u843d\u548c\u7a00\u758f\u533a\u57df\u7684\u7ed3\u6784\u8106\u5f31\u6027\uff09\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u5b66\u4e60\u70b9\u4e22\u5f03\uff08LPD\uff09\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u533a\u57df\u611f\u77e5\u7684\u6b63\u5219\u5316\u3002\\n\\n\u521b\u65b0\u6027\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u9488\u5bf9 LiDAR \u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u7279\u5b9a\u51e0\u4f55\u5931\u771f\u63d0\u4f9b\u4e86\u6709\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u901a\u7528\u6570\u636e\u589e\u5f3a\u6216\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u3002\u5176\u63d2\u62d4\u5373\u7528\u3001\u6a21\u578b\u65e0\u5173\u7684\u7279\u6027\u4ee5\u53ca\u8bad\u7ec3\u65f6\u51e0\u4e4e\u65e0\u989d\u5916\u63a8\u7406\u5f00\u9500\u7684\u7279\u70b9\uff0c\u4f7f\u5176\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u6f5c\u529b\u3002\\n\\n\u5b9e\u9a8c\u5b8c\u6574\u6027\u65b9\u9762\uff0c\u8bba\u6587\u5728\u6311\u6218\u6027\u7684 'source-only cross-weather' \u8bbe\u7f6e\u4e0b\uff0c\u4f7f\u7528 SemanticKITTI \u8bad\u7ec3\u5e76\u5728 SemanticSTF \u4e0a\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u79cd\u6570\u636e\u589e\u5f3a\u3001\u9886\u57df\u6cdb\u5316/\u9002\u5e94\u3001\u9886\u57df\u968f\u673a\u5316\u4ee5\u53ca\u7279\u5b9a\u6b63\u5219\u5316\u65b9\u6cd5\u3002\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u5747\u652f\u6301\u5176\u4e3b\u8981\u89c2\u70b9\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u7c7b\u522b\u548c\u8fb9\u754c\u654f\u611f\u7c7b\u522b\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u8d85\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\\n\\n\u7ed3\u679c\u53ef\u4fe1\u5ea6\u9ad8\uff0cSOTA \u58f0\u660e\u5728\u7279\u5b9a\u4efb\u52a1\u8bbe\u7f6e\u4e0b\u5f97\u5230\u4e86\u6e05\u6670\u7684\u6570\u636e\u652f\u6301\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8be5\u7814\u7a76\u5177\u6709\u5f88\u9ad8\u7684\u4ef7\u503c\uff0c\u5bf9\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u771f\u5b9e\u6076\u52a3\u73af\u5883\u4e0b\u7684\u611f\u77e5\u9c81\u68d2\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\"\n}\n```"}, "123": {"title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls", "authors": ["Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang"], "abstract": "arXiv:2511.01266v1 Announce Type: new \nCurrent motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01266", "pdf_url": "https://arxiv.org/pdf/2511.01266.pdf", "is_interesting": false}, "124": {"title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers", "authors": ["Tan Tang, Yanhong Wu, Junming Gao, Yingcai Wu"], "abstract": "arXiv:2511.01274v1 Announce Type: new \nAncient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01274", "pdf_url": "https://arxiv.org/pdf/2511.01274.pdf", "is_interesting": false}, "125": {"title": "Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions", "authors": ["Karma Phuntsho,  Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn"], "abstract": "arXiv:2511.01284v1 Announce Type: new \nFoundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01284", "pdf_url": "https://arxiv.org/pdf/2511.01284.pdf", "is_interesting": false}, "126": {"title": "Detecting Generated Images by Fitting Natural Image Distributions", "authors": ["Yonggang Zhang, Jun Nie, Xinmei Tian, Mingming Gong, Kun Zhang, Bo Han"], "abstract": "arXiv:2511.01293v1 Announce Type: new \nThe increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method. Code is available at https://github.com/tmlr-group/ConV.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01293", "pdf_url": "https://arxiv.org/pdf/2511.01293.pdf", "is_interesting": false}, "127": {"title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark", "authors": ["Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, Jiaqi Wang"], "abstract": "arXiv:2511.01295v1 Announce Type: new \nRecent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01295", "pdf_url": "https://arxiv.org/pdf/2511.01295.pdf", "is_interesting": false}, "128": {"title": "REASON: Probability map-guided dual-branch fusion framework for gastric content assessment", "authors": ["Nu-Fnag Xiao, De-Xing Huang, Le-Tian Wang, Mei-Jiang Gui, Qi Fu, Xiao-Liang Xie, Shi-Qi Liu, Shuangyi Wang, Zeng-Guang Hou, Ying-Wei Wang, Xiao-Hu Zhou"], "abstract": "arXiv:2511.01302v1 Announce Type: new \nAccurate assessment of gastric content from ultrasound is critical for stratifying aspiration risk at induction of general anesthesia. However, traditional methods rely on manual tracing of gastric antra and empirical formulas, which face significant limitations in both efficiency and accuracy. To address these challenges, a novel two-stage probability map-guided dual-branch fusion framework (REASON) for gastric content assessment is proposed. In stage 1, a segmentation model generates probability maps that suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch classifier fuses information from two standard views, right lateral decubitus (RLD) and supine (SUP), to improve the discrimination of learned features. Experimental results on a self-collected dataset demonstrate that the proposed framework outperforms current state-of-the-art approaches by a significant margin. This framework shows great promise for automated preoperative aspiration risk assessment, offering a more robust, efficient, and accurate solution for clinical practice.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01302", "pdf_url": "https://arxiv.org/pdf/2511.01302.pdf", "is_interesting": false}, "129": {"title": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation", "authors": ["Chentao Li, Behzad Bozorgtabar, Yifang Ping, Pan Huang, Jing Qin"], "abstract": "arXiv:2511.01304v1 Announce Type: new \nMultiple instance learning (MIL) has been widely used for representing whole-slide pathology images. However, spatial, semantic, and decision entanglements among instances limit its representation and interpretability. To address these challenges, we propose a latent factor grouping-boosted cluster-reasoning instance disentangled learning framework for whole-slide image (WSI) interpretable representation in three phases. First, we introduce a novel positive semi-definite latent factor grouping that maps instances into a latent subspace, effectively mitigating spatial entanglement in MIL. To alleviate semantic entanglement, we employs instance probability counterfactual inference and optimization via cluster-reasoning instance disentangling. Finally, we employ a generalized linear weighted decision via instance effect re-weighting to address decision entanglement. Extensive experiments on multicentre datasets demonstrate that our model outperforms all state-of-the-art models. Moreover, it attains pathologist-aligned interpretability through disentangled representations and a transparent decision-making process.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01304", "pdf_url": "https://arxiv.org/pdf/2511.01304.pdf", "is_interesting": false}, "130": {"title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models", "authors": ["Tae-Young Lee, Juwon Seo, Jong Hwan Ko, Gyeong-Moon Park"], "abstract": "arXiv:2511.01307v1 Announce Type: new \nRecent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at https://github.com/KU-VGI/APDM.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01307", "pdf_url": "https://arxiv.org/pdf/2511.01307.pdf", "is_interesting": false}, "131": {"title": "MVSMamba: Multi-View Stereo with State Space Model", "authors": ["Jianfei Jiang, Qiankun Liu, Hongyuan Liu, Haochen Yu, Liyong Wang, Jiansheng Chen, Huimin Ma"], "abstract": "arXiv:2511.01315v1 Announce Type: new \nRobust feature representations are essential for learning-based Multi-View Stereo (MVS), which relies on accurate feature matching. Recent MVS methods leverage Transformers to capture long-range dependencies based on local features extracted by conventional feature pyramid networks. However, the quadratic complexity of Transformer-based MVS methods poses challenges to balance performance and efficiency. Motivated by the global modeling capability and linear complexity of the Mamba architecture, we propose MVSMamba, the first Mamba-based MVS network. MVSMamba enables efficient global feature aggregation with minimal computational overhead. To fully exploit Mamba's potential in MVS, we propose a Dynamic Mamba module (DM-module) based on a novel reference-centered dynamic scanning strategy, which enables: (1) Efficient intra- and inter-view feature interaction from the reference to source views, (2) Omnidirectional multi-view feature representations, and (3) Multi-scale global feature aggregation. Extensive experimental results demonstrate MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and the Tanks-and-Temples benchmark with both superior performance and efficiency. The source code is available at https://github.com/JianfeiJ/MVSMamba.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01315", "pdf_url": "https://arxiv.org/pdf/2511.01315.pdf", "is_interesting": false}, "132": {"title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model", "authors": ["Sampriti Soor, Alik Pramanick, Jothiprakash K, Arijit Sur"], "abstract": "arXiv:2511.01317v1 Announce Type: new \nThe rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model's ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01317", "pdf_url": "https://arxiv.org/pdf/2511.01317.pdf", "is_interesting": false}, "133": {"title": "RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation", "authors": ["Jierui Qu, Jianchun Zhao"], "abstract": "arXiv:2511.01328v1 Announce Type: new \nMedical image segmentation is essential for computer-assisted diagnosis and treatment planning, yet substantial anatomical variability and boundary ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet, a segmentation network that unifies local modeling with global context to strengthen boundary delineation and detail preservation. RDTE-UNet employs a hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for adaptive boundary enhancement, HVDA for fine-grained feature modeling, and EulerFF for fusion weighting guided by Euler's formula. Together, these components improve structural consistency and boundary accuracy across morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has achieved a comparable level in terms of segmentation accuracy and boundary quality.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01328", "pdf_url": "https://arxiv.org/pdf/2511.01328.pdf", "is_interesting": false}, "134": {"title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles", "authors": ["Trishanu Das, Abhilash Nandy, Khush Bajaj, Deepiha S"], "abstract": "arXiv:2511.01340v1 Announce Type: new \nUnderstanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and $20-30\\%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.01340", "pdf_url": "https://arxiv.org/pdf/2511.01340.pdf", "is_interesting": false}, "135": {"title": "MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement", "authors": ["Jierui Qu, Jianchun Zhao"], "abstract": "arXiv:2511.01345v1 Announce Type: new \nAccurate segmentation of medical images is fundamental to tumor diagnosis and treatment planning. SAM-based interactive segmentation has gained attention for its strong generalization, but most methods follow a single-point-to-single-object paradigm, which limits multi-lesion segmentation. Moreover, ViT backbones capture global context but often miss high-fidelity local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework with a competitive query optimization strategy that shifts from single-point-to-single-mask to single-point-to-multi-instance. A prompt-conditioned instance-query generator transforms a single point prompt into multiple specialized queries, enabling retrieval of all semantically similar lesions across the 3D volume from a single exemplar. A hybrid CNN-Transformer encoder injects CNN-derived boundary saliency into ViT self-attention via spatial gating. A competitively optimized query decoder then enables end-to-end, parallel, multi-instance prediction through inter-query competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels and exhibits strong robustness to prompts, providing a practical solution for efficient annotation of clinically relevant multi-lesion cases.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01345", "pdf_url": "https://arxiv.org/pdf/2511.01345.pdf", "is_interesting": false}, "136": {"title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion", "authors": ["Linhao Huang"], "abstract": "arXiv:2511.01355v1 Announce Type: new \nRecent advancements in text-to-image diffusion models have significantly improved the personalization and stylization of generated images. However, previous studies have only assessed content similarity under a single style intensity. In our experiments, we observe that increasing style intensity leads to a significant loss of content features, resulting in a suboptimal content-style frontier. To address this, we propose a novel approach to expand the content-style frontier by leveraging Content-Style Subspace Blending and a Content-Style Balance loss. Our method improves content similarity across varying style intensities, significantly broadening the content-style frontier. Extensive experiments demonstrate that our approach outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower Inverted Generational Distance (IGD) and Generational Distance (GD) scores compared to current methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01355", "pdf_url": "https://arxiv.org/pdf/2511.01355.pdf", "is_interesting": false}, "137": {"title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering", "authors": ["Qiangguo Jin, Xianyao Zheng, Hui Cui, Changming Sun, Yuqi Fang, Cong Cong, Ran Su, Leyi Wei, Ping Xuan, Junbo Wang"], "abstract": "arXiv:2511.01357v1 Announce Type: new \nMedical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model's capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at https://github.com/BioMedIA-repo/CMI-MTL.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01357", "pdf_url": "https://arxiv.org/pdf/2511.01357.pdf", "is_interesting": false}, "138": {"title": "EREBUS: End-to-end Robust Event Based Underwater Simulation", "authors": ["Hitesh Kyatham, Arjun Suresh, Aadi Palnitkar, Yiannis Aloimonos"], "abstract": "arXiv:2511.01381v1 Announce Type: new \nThe underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01381", "pdf_url": "https://arxiv.org/pdf/2511.01381.pdf", "is_interesting": false}, "139": {"title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "authors": ["Xinyu Mao, Junsi Li, Haoji Zhang, Yu Liang, Ming Sun"], "abstract": "arXiv:2511.01390v1 Announce Type: new \nFine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at https://github.com/Sweet4tars/seps.git.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.01390", "pdf_url": "https://arxiv.org/pdf/2511.01390.pdf", "is_interesting": false}, "140": {"title": "Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction", "authors": ["Ya Wen, Yutong Qiao, Chi Chiu Lam, Ioannis Brilakis, Sanghoon Lee, Mun On Wong"], "abstract": "arXiv:2511.01399v1 Announce Type: new \nInventory management of firefighting assets is crucial for emergency preparedness, risk assessment, and on-site fire response. However, conventional methods are inefficient due to limited capabilities in automated asset recognition and reconstruction. To address the challenge, this research introduces the Fire-ART dataset and develops a panoramic image-based reconstruction approach for semantic enrichment of firefighting assets into BIM models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626 images and 6,627 instances, making it an extensive and publicly accessible dataset for asset recognition. In addition, the reconstruction approach integrates modified cube-map conversion and radius-based spherical camera projection to enhance recognition and localization accuracy. Through validations with two real-world case studies, the proposed approach achieves F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters, respectively. The Fire-ART dataset and the reconstruction approach offer valuable resources and robust technical solutions to enhance the accurate digital management of fire safety equipment.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01399", "pdf_url": "https://arxiv.org/pdf/2511.01399.pdf", "is_interesting": false}, "141": {"title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "authors": ["Reza Karimzadeh, Albert Alonso, Frans Zdyb, Julius B. Kirkegaard, Bulat Ibragimov"], "abstract": "arXiv:2511.01411v1 Announce Type: new \nFaithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve/delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01411", "pdf_url": "https://arxiv.org/pdf/2511.01411.pdf", "is_interesting": false}, "142": {"title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation", "authors": ["Yongqi Yang, Huayang Huang, Xu Peng, Xiaobin Hu, Donghao Luo, Jiangning Zhang, Chengjie Wang, Yu Wu"], "abstract": "arXiv:2511.01419v1 Announce Type: new \nRecent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model's n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01419", "pdf_url": "https://arxiv.org/pdf/2511.01419.pdf", "is_interesting": false}, "143": {"title": "UniSOT: A Unified Framework for Multi-Modality Single Object Tracking", "authors": ["Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Xu Zhou, Feng Wu"], "abstract": "arXiv:2511.01427v1 Announce Type: new \nSingle object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0\\% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0\\% main metric across all three RGB+X video modalities.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01427", "pdf_url": "https://arxiv.org/pdf/2511.01427.pdf", "is_interesting": false}, "144": {"title": "Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation", "authors": ["Seongkyu Choi, Jhonghyun An"], "abstract": "arXiv:2511.01434v1 Announce Type: new \nOff-road semantic segmentation suffers from thick, inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Designs that fuse only at low resolution blur edges and propagate local errors, whereas maintaining high-resolution pathways or repeating high-resolution fusions is costly and fragile to noise. We introduce a resolutionaware token decoder that balances global semantics, local consistency, and boundary fidelity under imperfect supervision. Most computation occurs at a low-resolution bottleneck; a gated cross-attention injects fine-scale detail, and only a sparse, uncertainty-selected set of pixels is refined. The components are co-designed and tightly integrated: global self-attention with lightweight dilated depthwise refinement restores local coherence; a gated cross-attention integrates fine-scale features from a standard high-resolution encoder stream without amplifying noise; and a class-aware point refinement corrects residual ambiguities with negligible overhead. During training, we add a boundary-band consistency regularizer that encourages coherent predictions in a thin neighborhood around annotated edges, with no inference-time cost. Overall, the results indicate competitive performance and improved stability across transitions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01434", "pdf_url": "https://arxiv.org/pdf/2511.01434.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on off-road semantic segmentation and the challenges of boundary refinement, which is a general computer vision task. While this could be relevant to autonomous driving in terms of scene understanding, the paper does not specifically address autonomous driving systems, vehicle perception, or related tasks such as trajectory prediction or control."}}, "145": {"title": "Contrast-Guided Cross-Modal Distillation for Thermal Object Detection", "authors": ["SiWoo Kim, JhongHyun An"], "abstract": "arXiv:2511.01435v1 Announce Type: new \nRobust perception at night remains challenging for thermal-infrared detection: low contrast and weak high-frequency cues lead to duplicate, overlapping boxes, missed small objects, and class confusion. Prior remedies either translate TIR to RGB and hope pixel fidelity transfers to detection -- making performance fragile to color or structure artifacts -- or fuse RGB and TIR at test time, which requires extra sensors, precise calibration, and higher runtime cost. Both lines can help in favorable conditions, but do not directly shape the thermal representation used by the detector. We keep mono-modality inference and tackle the root causes during training. Specifically, we introduce training-only objectives that sharpen instance-level decision boundaries by pulling together features of the same class and pushing apart those of different classes -- suppressing duplicate and confusing detections -- and that inject cross-modal semantic priors by aligning the student's multi-level pyramid features with an RGB-trained teacher, thereby strengthening texture-poor thermal features without visible input at test time. In experiments, our method outperformed prior approaches and achieved state-of-the-art performance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01435", "pdf_url": "https://arxiv.org/pdf/2511.01435.pdf", "is_interesting": false}, "146": {"title": "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction", "authors": ["Riddhi Jain, Manasi Patwardhan, Aayush Mishra, Parijat Deshpande, Beena Rai"], "abstract": "arXiv:2511.01449v1 Announce Type: new \nTo effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01449", "pdf_url": "https://arxiv.org/pdf/2511.01449.pdf", "is_interesting": false}, "147": {"title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation", "authors": ["Jie Du, Xinyu Gong, Qingshan Tan, Wen Li, Yangming Cheng, Weitao Wang, Chenlu Zhan, Suhui Wu, Hao Zhang, Jun Zhang"], "abstract": "arXiv:2511.01450v1 Announce Type: new \nRecent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01450", "pdf_url": "https://arxiv.org/pdf/2511.01450.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "148": {"title": "When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA", "authors": ["Dennis Pierantozzi, Luca Carlini, Mauro Orazio Drago, Chiara Lena, Cesare Hassan, Elena De Momi, Danail Stoyanov, Sophia Bano, Mobarak I. Hoque"], "abstract": "arXiv:2511.01458v1 Announce Type: new \nSafety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at https://github.com/DennisPierantozzi/QASNNE", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01458", "pdf_url": "https://arxiv.org/pdf/2511.01458.pdf", "is_interesting": false}, "149": {"title": "Efficiently Training A Flat Neural Network Before It has been Quantizated", "authors": ["Peng Xia, Junbiao Pang, Tianyang Cai"], "abstract": "arXiv:2511.01462v1 Announce Type: new \nPost-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01462", "pdf_url": "https://arxiv.org/pdf/2511.01462.pdf", "is_interesting": false}, "150": {"title": "HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA", "authors": ["Lei Hu, Yongjing Ye, Shihong Xia"], "abstract": "arXiv:2511.01463v1 Announce Type: new \nThe expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.", "categories": ["cs.CV", "cs.AI", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.01463", "pdf_url": "https://arxiv.org/pdf/2511.01463.pdf", "is_interesting": false}, "151": {"title": "SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks", "authors": ["Changyuan Zhao, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Hongyang Du, Zehui Xiong, Dong In Kim, Ping Zhang"], "abstract": "arXiv:2511.01466v1 Announce Type: new \nDeep joint source-channel coding (JSCC) has emerged as a promising paradigm for semantic communication, delivering significant performance gains over conventional separate coding schemes. However, existing JSCC frameworks remain vulnerable to physical-layer adversarial threats, such as pilot spoofing and subcarrier jamming, compromising semantic fidelity. In this paper, we propose SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly enhances the security and robustness of deep JSCC under adversarial wireless environments. Different from prior diffusion-guided JSCC methods that suffer from high inference latency, SecDiff employs pseudoinverse-guided sampling and adaptive guidance weighting, enabling flexible step-size control and efficient semantic reconstruction. To counter jamming attacks, we introduce a power-based subcarrier masking strategy and recast recovery as a masked inpainting problem, solved via diffusion guidance. For pilot spoofing, we formulate channel estimation as a blind inverse problem and develop an expectation-minimization (EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and a channel operator. Notably, our method alternates between pilot recovery and channel estimation, enabling joint refinement of both variables throughout the diffusion process. Extensive experiments over orthogonal frequency-division multiplexing (OFDM) channels under adversarial conditions show that SecDiff outperforms existing secure and generative JSCC baselines by achieving a favorable trade-off between reconstruction quality and computational cost. This balance makes SecDiff a promising step toward practical, low-latency, and attack-resilient semantic communications.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01466", "pdf_url": "https://arxiv.org/pdf/2511.01466.pdf", "is_interesting": false}, "152": {"title": "EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance", "authors": ["Zhiyang Jia, Hongyan Cui, Ge Gao, Bo Li, Minjie Zhang, Zishuo Gao, Huiwen Huang, Caisheng Zhuo"], "abstract": "arXiv:2511.01498v1 Announce Type: new \nPerson re-identification (ReID) plays a pivotal role in computer vision, particularly in surveillance and security applications within IoT-enabled smart environments. This study introduces the Enhanced Pedestrian Alignment Network (EPAN), tailored for robust ReID across diverse IoT surveillance conditions. EPAN employs a dual-branch architecture to mitigate the impact of perspective and environmental changes, extracting alignment information under varying scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction capabilities, achieving outstanding performance on the Inspection-Personnel dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of 78.82%. This highlights EPAN's potential for real-world IoT applications, enabling effective and reliable person ReID across diverse cameras in surveillance and security systems. The code and data are available at: https://github.com/ggboy2580/EPAN", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01498", "pdf_url": "https://arxiv.org/pdf/2511.01498.pdf", "is_interesting": false}, "153": {"title": "SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation", "authors": ["Yufeng Jin, Niklas Funk, Vignesh Prasad, Zechu Li, Mathias Franzius, Jan Peters, Georgia Chalvatzaki"], "abstract": "arXiv:2511.01501v1 Announce Type: new \nObject pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01501", "pdf_url": "https://arxiv.org/pdf/2511.01501.pdf", "is_interesting": false}, "154": {"title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning", "authors": ["Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan"], "abstract": "arXiv:2511.01502v1 Announce Type: new \nUnsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01502", "pdf_url": "https://arxiv.org/pdf/2511.01502.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "3D\u611f\u77e5 / \u89c6\u89c9\u91cc\u7a0b\u8ba1", "reason": "The paper focuses on unsupervised joint learning of depth and ego-motion estimation from consecutive video frames, which are core components of visual odometry and 3D perception systems often used in autonomous driving. While the work is general-purpose and not explicitly targeted at autonomous vehicles, its methods directly apply to onboard perception modules for self-localization and scene understanding in autonomous driving."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"3D\u611f\u77e5 / \u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1 / \u89c6\u89c9\u91cc\u7a0b\u8ba1\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"KITTI Odometry, MIAS-Odom, DDAD, nuScenes\",\n    \"metrics\": \"\u89c6\u89c9\u91cc\u7a0b\u8ba1: ATE, et, er; \u6df1\u5ea6\u4f30\u8ba1: Abs Rel, Sq Rel, RMSE, RMSE log, \u03b4 < 1.25^i (i=1,2,3)\",\n    \"sota_details\": \"\u5728KITTI Odometry (Seqs 09-10, 11-21) \u548c\u65b0\u6536\u96c6\u7684MIAS-Odom\u6570\u636e\u96c6\u4e0a\uff0cDiMoDE\u5728\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6df7\u5408SOTA\u65b9\u6cd5\u3002\u5728DDAD\u548cnuScenes\u6570\u636e\u96c6\u4e0a\uff0cDiMoDE\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u4e5f\u53d6\u5f97SOTA\u6027\u80fd\u3002\u5728KITTI\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e0a\uff0cDiMoDE\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u4e0a\u5c55\u73b0\u4f18\u52bf\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiMoDE\u7684\u65e0\u76d1\u7763\u8054\u5408\u6df1\u5ea6\u548c\u81ea\u8fd0\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u70b9\u5728\u4e8e\u5bf9\u8fd0\u52a8\u5206\u91cf\u8fdb\u884c\u533a\u5206\u6027\u5904\u7406\u3002\u8bba\u6587\u901a\u8fc7\u6df1\u5165\u5206\u6790\u65cb\u8f6c\u3001\u5207\u5411\u5e73\u79fb\u548c\u5f84\u5411\u5e73\u79fb\u5bf9\u521a\u6027\u5149\u6d41\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u6df7\u6dc6\u6216\u5ffd\u7565\u4e86\u8fd9\u4e9b\u5206\u91cf\u7684\u72ec\u7279\u51e0\u4f55\u7279\u6027\u3002DiMoDE\u5f15\u5165\u4e86\u663e\u5f0f\u51e0\u4f55\u7ea6\u675f\uff0c\u901a\u8fc7\u5149\u5b66\u8f74\u548c\u6210\u50cf\u5e73\u9762\u5bf9\u9f50\u6765\u5206\u522b\u5f15\u5bfc\u548c\u4f18\u5316\u4e0d\u540c\u7684\u8fd0\u52a8\u5206\u91cf\uff0c\u5e76\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u540c\u8f74\u548c\u5171\u9762\u5f62\u5f0f\uff0c\u4ece\u800c\u901a\u8fc7\u5c01\u95ed\u5f62\u5f0f\u7684\u51e0\u4f55\u5173\u7cfb\u76f8\u4e92\u63a8\u5bfc\u6df1\u5ea6\u548c\u7ffb\u8bd1\u5206\u91cf\uff0c\u8fd9\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u5177\u6709\u5f88\u5f3a\u7684\u521b\u65b0\u6027\u3002\\n\\n\u5b9e\u9a8c\u90e8\u5206\u975e\u5e38\u5b8c\u6574\u548c\u4e25\u8c28\u3002\u4f5c\u8005\u5728KITTI Odometry\u3001MIAS-Odom\uff08\u65b0\u6536\u96c6\u7684\u6311\u6218\u6027\u6570\u636e\u96c6\uff09\u3001DDAD\u3001nuScenes\u7b49\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5e76\u4e0e\u591a\u79cdSOTA\u65b9\u6cd5\uff08\u5305\u62ec\u51e0\u4f55\u3001\u5b66\u4e60\u548c\u6df7\u5408\u65b9\u6cd5\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002\u7279\u522b\u662f\u5728MIAS-Odom\u548cnuScenes\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cDiMoDE\u5728\u89c6\u89c9\u91cc\u7a0b\u8ba1\u548c\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8bbe\u8ba1\u5468\u5bc6\uff0c\u6e05\u6670\u5730\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7ea6\u675f\uff08\u5305\u62ec\u5bf9PoseNet\u548cDepthNet\u7684\u7ea6\u675f\uff09\u7684\u6709\u6548\u6027\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u7684\u5fc5\u8981\u6027\u4ee5\u53ca\u6a21\u578b\u5bf9\u8d85\u53c2\u6570\u7684\u9c81\u68d2\u6027\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002\\n\\n\u8be5\u7814\u7a76\u76f4\u63a5\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d3D\u611f\u77e5\uff08\u7279\u522b\u662f\u590d\u6742\u73af\u5883\u4e2d\u9c81\u68d2\u7684\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\uff09\u7684\u5173\u952e\u6311\u6218\u3002\u65e0\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u964d\u4f4e\u4e86\u5bf9\u6602\u8d35\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u800c\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\u5219\u6709\u671b\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002DiMoDE\u5728\u6076\u52a3\u6761\u4ef6\uff08\u5982\u5927\u65cb\u8f6c\u3001\u76f8\u673a\u6296\u52a8\u3001\u66dd\u5149\u4e0d\u8db3\u3001\u8fd0\u52a8\u6a21\u7cca\uff09\u4e0b\u7684\u51fa\u8272\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5177\u6709\u5de8\u5927\u7684\u884c\u4e1a\u5e94\u7528\u6f5c\u529b\u3002\u8bba\u6587\u4e25\u8c28\u7684\u5206\u6790\u3001\u8be6\u5c3d\u7684\u5b9e\u9a8c\u4ee5\u53ca\u5bf9SOTA\u58f0\u660e\u7684\u8c28\u614e\u63aa\u8f9e\uff08\u4f8b\u5982\uff0c\u627f\u8ba4\u5728KITTI\u6df1\u5ea6\u4e0a\u4ec5\u5177\u7ade\u4e89\u529b\u800c\u975e\u7edd\u5bf9SOTA\uff09\uff0c\u90fd\u4f53\u73b0\u4e86\u7814\u7a76\u7684\u9ad8\u5ea6\u53ef\u4fe1\u5ea6\u3002\"\n}\n```"}, "155": {"title": "Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement", "authors": ["Derong Kong, Zhixiong Yang, Shengxi Li, Shuaifeng Zhi, Li Liu, Zhen Liu, Jingyuan Xia"], "abstract": "arXiv:2511.01510v1 Announce Type: new \nLow-light image enhancement (LLIE) faces persistent challenges in balancing reconstruction fidelity with cross-scenario generalization. While existing methods predominantly focus on deterministic pixel-level mappings between paired low/normal-light images, they often neglect the continuous physical process of luminance transitions in real-world environments, leading to performance drop when normal-light references are unavailable. Inspired by empirical analysis of natural luminance dynamics revealing power-law distributed intensity transitions, this paper introduces Luminance-Aware Statistical Quantification (LASQ), a novel framework that reformulates LLIE as a statistical sampling process over hierarchical luminance distributions. Our LASQ re-conceptualizes luminance transition as a power-law distribution in intensity coordinate space that can be approximated by stratified power functions, therefore, replacing deterministic mappings with probabilistic sampling over continuous luminance layers. A diffusion forward process is designed to autonomously discover optimal transition paths between luminance layers, achieving unsupervised distribution emulation without normal-light references. In this way, it considerably improves the performance in practical situations, enabling more adaptable and versatile light restoration. This framework is also readily applicable to cases with normal-light references, where it achieves superior performance on domain-specific datasets alongside better generalization-ability across non-reference datasets.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01510", "pdf_url": "https://arxiv.org/pdf/2511.01510.pdf", "is_interesting": false}, "156": {"title": "Example-Based Feature Painting on Textures", "authors": ["Andrei-Timotei Ardelean, Tim Weyrich"], "abstract": "arXiv:2511.01513v1 Announce Type: new \nIn this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.01513", "pdf_url": "https://arxiv.org/pdf/2511.01513.pdf", "is_interesting": false}, "157": {"title": "NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation", "authors": ["Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu"], "abstract": "arXiv:2511.01517v1 Announce Type: new \nCurrent text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at https://github.com/giddyyupp/NSYNC.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01517", "pdf_url": "https://arxiv.org/pdf/2511.01517.pdf", "is_interesting": false}, "158": {"title": "Driving scenario generation and evaluation using a structured layer representation and foundational models", "authors": ["Arthur Hubert, Gamal Elghazaly, Rapha\\\"el Frank"], "abstract": "arXiv:2511.01541v1 Announce Type: new \nRare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at https://github.com/Valgiz/5LMSG.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01541", "pdf_url": "https://arxiv.org/pdf/2511.01541.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u4eff\u771f\u8bc4\u4f30 / \u9a7e\u9a76\u573a\u666f\u751f\u6210", "reason": "The paper focuses on generating and evaluating rare driving scenarios for autonomous vehicle development using a structured five-layer model and large foundational models. Scenario generation and evaluation are essential for autonomous driving, especially for testing and improving vehicle behavior in diverse and challenging environments. The proposed approach directly contributes to the simulation and assessment tasks in autonomous driving, making it highly relevant."}, "review": "```json\n{\n  \"score\": 7.8,\n  \"subfield\": \"\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u751f\u6210\u4e0e\u8bc4\u4f30 / \u5927\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\",\n  \"sota_claim\": {\n    \"is_sota\": false,\n    \"dataset\": \"\",\n    \"metrics\": \"\",\n    \"sota_details\": \"\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u5206\u5c42\u8868\u793a\u548c\u57fa\u7840\u6a21\u578b\uff08LLM/MLLM\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u751f\u6210\u4e0e\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u548c\u4ef7\u503c\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a\\n\\n1.  **\u521b\u65b0\u6027 (8/10)**: \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e94\u5c42\u7ed3\u6784\u5316\u6a21\u578b\u6765\u8868\u793a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u8fd9\u79cd\u6a21\u578b\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9053\u8def\u7ed3\u6784\u3001\u8def\u8fb9\u7ed3\u6784\u3001\u4e34\u65f6\u53d8\u5316\u3001\u52a8\u6001\u7269\u4f53\u548c\u73af\u5883\u6761\u4ef6\u7b49\u4e0d\u540c\u5c42\u7ea7\uff0c\u6781\u5927\u5730\u63d0\u5347\u4e86\u573a\u666f\u63cf\u8ff0\u7684\u7cbe\u7ec6\u5ea6\u548c\u53ef\u63a7\u6027\u3002\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4ece\u771f\u5b9e\u6570\u636e\uff08nuScenes\uff09\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfcLLM\u5bf9\u7279\u5b9a\u5c42\u7ea7\u8fdb\u884c\u7f16\u8f91\u6765\u751f\u6210\u7a00\u6709\u6216\u8fb9\u7f18\u573a\u666f\uff0c\u8fd9\u79cd\u7ed3\u5408LLM\u751f\u6210\u80fd\u529b\u4e0e\u7ed3\u6784\u5316\u6307\u5bfc\u7684\u65b9\u6cd5\u5177\u6709\u8f83\u5f3a\u7684\u521b\u65b0\u6027\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5bf9\u73b0\u6709\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5165\u7684\u573a\u666f\u8bc4\u4f30\u6307\u6807\uff08\u591a\u6837\u6027\u3001\u539f\u521b\u6027/OODness\uff09\u8fdb\u884c\u4e86\u6539\u8fdb\u548c\u6269\u5c55\uff0c\u4f7f\u5176\u80fd\u9002\u7528\u4e8e\u5206\u5c42\u7ed3\u6784\u5316\u573a\u666f\u7684\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u5c42\u7ea7\u5206\u6790\u3002\\n\\n2.  **\u5b9e\u9a8c\u5b8c\u6574\u6027 (7.5/10)**: \u5b9e\u9a8c\u57fa\u4e8enuScenes-mini\u6570\u636e\u96c6\uff08\u5305\u542b10\u4e2a20\u79d2\u7684\u573a\u666f\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u975e\u7ed3\u6784\u5316\u3001\u8f6f\u7ed3\u6784\u5316\u548c\u786c\u7ed3\u6784\u5316\u4e94\u5c42\u6a21\u578b\u5728\u751f\u6210\u573a\u666f\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u7684LLM\u63d0\u793a\u7b56\u7565\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5f15\u5165\u6539\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5bf9\u751f\u6210\u7684\u573a\u666f\u5728\u4e0d\u540c\u5c42\u7ea7\u4e0a\u7684\u591a\u6837\u6027\u548c\u539f\u521b\u6027\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\u3002\u867d\u7136\u5b9e\u9a8c\u6570\u636e\u96c6\u89c4\u6a21\u76f8\u5bf9\u8f83\u5c0f\uff0c\u4f46\u5176\u8bbe\u8ba1\u8db3\u4ee5\u9a8c\u8bc1\u6240\u63d0\u51fa\u6846\u67b6\u548c\u6838\u5fc3\u601d\u60f3\u7684\u6709\u6548\u6027\u53ca\u4e0d\u540c\u7ec4\u4ef6\u7684\u5f71\u54cd\u3002\u8bba\u6587\u8fd8\u4f7f\u7528\u4e16\u754c\u57fa\u5ea7\u6a21\u578b\uff08Veo3\uff09\u5bf9\u751f\u6210\u7684\u6587\u672c\u573a\u666f\u8fdb\u884c\u4e86\u89c6\u9891\u751f\u6210\u6f14\u793a\uff0c\u5c3d\u7ba1\u89c6\u9891\u8d28\u91cf\u5b58\u5728\u4e00\u4e9b\u4e0d\u8fde\u8d2f\u6027\uff0c\u4f46\u4f5c\u8005\u5766\u8bda\u5730\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u9650\u5236\u3002\\n\\n3.  **\u53ef\u4fe1\u5ea6 (8/10)**: \u8bba\u6587\u4f7f\u7528\u4e86Google Gemini 2.5 Pro\u4f5c\u4e3aMLLM\u548cLLM\uff0c\u4ee5\u53caGemini-embedding-001\u4f5c\u4e3a\u8bed\u4e49\u5d4c\u5165\u6a21\u578b\uff0c\u8fd9\u4e9b\u90fd\u662f\u4e1a\u754c\u9886\u5148\u4e14\u6027\u80fd\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u7814\u7a76\u7684\u53ef\u9760\u6027\u3002\u4f5c\u8005\u5bf9\u751f\u6210\u89c6\u9891\u4e2d\u5b58\u5728\u7684\u5c40\u9650\u6027\uff08\u4f8b\u5982\uff0cLLM\u53ef\u80fd\u53bb\u9664\u6216\u5206\u7ec4\u539f\u59cb\u573a\u666f\u4e2d\u7684\u7269\u4f53\uff0c\u4ee5\u53ca\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e0d\u8fde\u8d2f\u548c\u5e7b\u89c9\uff09\u8fdb\u884c\u4e86\u516c\u5f00\u8ba8\u8bba\uff0c\u5c55\u73b0\u4e86\u4e25\u8c28\u7684\u79d1\u5b66\u6001\u5ea6\u3002\\n\\n4.  **\u884c\u4e1a\u6f5c\u529b (8.5/10)**: \u7a00\u6709\u548c\u8fb9\u7f18\u573a\u666f\u7684\u751f\u6210\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6d4b\u8bd5\u3001\u9a8c\u8bc1\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u573a\u666f\u751f\u6210\u4e0e\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u3001\u4e14\u80fd\u591f\u9488\u5bf9\u6027\u751f\u6210\u5173\u952e\u573a\u666f\u7684\u5f3a\u5927\u5de5\u5177\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u8fb9\u7f18\u6848\u4f8b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u80fd\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002\u7ed3\u6784\u5316\u8868\u793a\u4e5f\u4e3a\u672a\u6765\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u573a\u666f\u7f16\u8f91\u548c\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u57fa\u7840\u3002\\n\\n**\u603b\u8bc4**: \u5c3d\u7ba1\u8bba\u6587\u4e3b\u8981\u805a\u7126\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u201c\u573a\u666f\u751f\u6210\u4e0e\u8bc4\u4f30\u201d\u8fd9\u4e00\u8f85\u52a9\u6027\u4f46\u6781\u5176\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\uff0c\u800c\u975e\u8f66\u7aef\u76f4\u63a5\u8fd0\u884c\u7684\u611f\u77e5\u3001\u51b3\u7b56\u6216\u63a7\u5236\u6a21\u5757\uff0c\u4f46\u5176\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u9a8c\u8bc1\u548c\u6570\u636e\u589e\u5f3a\u7684\u63a8\u52a8\u4f5c\u7528\u662f\u57fa\u7840\u4e14\u6218\u7565\u6027\u7684\u3002\u8bba\u6587\u5728\u521b\u65b0\u6027\u3001\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\u548c\u672a\u6765\u6f5c\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u76ee\u524d\u4ece\u6587\u672c\u63cf\u8ff0\u5230\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\u4f9d\u7136\u5b58\u5728\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5176\u7ed3\u6784\u5316\u6587\u672c\u573a\u666f\u7684\u751f\u6210\u4e0e\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u503c\u5f97\u4e00\u4e2a\u8f83\u9ad8\u7684\u8bc4\u5206\u3002\"\n}\n```"}, "159": {"title": "PCD-ReID: Occluded Person Re-Identification for Base Station Inspection", "authors": ["Ge Gao, Zishuo Gao, Hongyan Cui, Zhiyang Jia, Zhuang Luo, ChaoPeng Liu"], "abstract": "arXiv:2511.01546v1 Announce Type: new \nOccluded pedestrian re-identification (ReID) in base station environments is a critical task in computer vision, particularly for surveillance and security applications. This task faces numerous challenges, as occlusions often obscure key body features, increasing the complexity of identification. Traditional ResNet-based ReID algorithms often fail to address occlusions effectively, necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component Discrepancy) algorithm to address these issues. The contributions of this work are as follows: To tackle the occlusion problem, we design a Transformer-based PCD network capable of extracting shared component features, such as helmets and uniforms. To mitigate overfitting on public datasets, we collected new real-world patrol surveillance images for model training, covering six months, 10,000 individuals, and over 50,000 images. Comparative experiments with existing ReID algorithms demonstrate that our model achieves a mean Average Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1 improvement over ResNet50-based methods. Experimental evaluations indicate that PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in tower inspection scenarios, highlighting its potential for practical deployment in surveillance and security applications.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01546", "pdf_url": "https://arxiv.org/pdf/2511.01546.pdf", "is_interesting": false}, "160": {"title": "NOA: a versatile, extensible tool for AI-based organoid analysis", "authors": ["Mikhail Konov, Lion J. Gleiter, Khoa Co, Monica Yabal, Tingying Peng"], "abstract": "arXiv:2511.01549v1 Announce Type: new \nAI tools can greatly enhance the analysis of organoid microscopy images, from detection and segmentation to feature extraction and classification. However, their limited accessibility to biologists without programming experience remains a major barrier, resulting in labor-intensive and largely manual workflows. Although a few AI models for organoid analysis have been developed, most existing tools remain narrowly focused on specific tasks. In this work, we introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user interface to simplify AI-based organoid analysis. NOA integrates modules for detection, segmentation, tracking, feature extraction, custom feature annotation and ML-based feature prediction. It interfaces multiple state-of-the-art algorithms and is implemented as an open-source napari plugin for maximal flexibility and extensibility. We demonstrate the versatility of NOA through three case studies, involving the quantification of morphological changes during organoid differentiation, assessment of phototoxicity effects, and prediction of organoid viability and differentiation state. Together, these examples illustrate how NOA enables comprehensive, AI-driven organoid image analysis within an accessible and extensible framework.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01549", "pdf_url": "https://arxiv.org/pdf/2511.01549.pdf", "is_interesting": false}, "161": {"title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model", "authors": ["Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong"], "abstract": "arXiv:2511.01571v1 Announce Type: new \nVision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01571", "pdf_url": "https://arxiv.org/pdf/2511.01571.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on Vision-Language-Action models for visuomotor control in robotics, specifically in pixel-level scene understanding and multimodal prompting. While the methods and models could potentially be adapted to autonomous driving systems, the paper itself does not explicitly discuss autonomous driving or its core tasks such as vehicle control, perception, or decision-making."}}, "162": {"title": "Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images", "authors": ["Md Sumon Ali, Muzammil Behzad"], "abstract": "arXiv:2511.01574v1 Announce Type: new \nCompared to traditional methods, Deep Learning (DL) becomes a key technology for computer vision tasks. Synthetic data generation is an interesting use case for DL, especially in the field of medical imaging such as Magnetic Resonance Imaging (MRI). The need for this task since the original MRI data is limited. The generation of realistic medical images is completely difficult and challenging. Generative Adversarial Networks (GANs) are useful for creating synthetic medical images. In this paper, we propose a DL based methodology for creating synthetic MRI data using the Deep Convolutional Generative Adversarial Network (DC-GAN) to address the problem of limited data. We also employ a Convolutional Neural Network (CNN) classifier to classify the brain tumor using synthetic data and real MRI data. CNN is used to evaluate the quality and utility of the synthetic images. The classification result demonstrates comparable performance on real and synthetic images, which validates the effectiveness of GAN-generated images for downstream tasks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01574", "pdf_url": "https://arxiv.org/pdf/2511.01574.pdf", "is_interesting": false}, "163": {"title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation", "authors": ["Yizhu Chen, Chen Ju, Zhicheng Wang, Shuai Xiao, Xu Chen, Jinsong Lan, Xiaoyong Zhu, Ying Chen"], "abstract": "arXiv:2511.01593v1 Announce Type: new \nThe unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01593", "pdf_url": "https://arxiv.org/pdf/2511.01593.pdf", "is_interesting": false}, "164": {"title": "Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography", "authors": ["Agnar Martin Bj{\\o}rnstad, Elias Stenhede, Arian Ranjbar"], "abstract": "arXiv:2511.01600v1 Announce Type: new \nAccurate tumor size measurement is a cornerstone of evaluating cancer treatment response. The most widely adopted standard for this purpose is the Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on measuring the longest tumor diameter in a single plane. However, volumetric measurements have been shown to provide a more reliable assessment of treatment effect. Their clinical adoption has been limited, though, due to the labor-intensive nature of manual volumetric annotation. In this paper, we present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed for efficient volumetric tumor segmentation from CT scans annotated with RECIST annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1: Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of 63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an average inference time of 14.4 s on CPU on the public validation dataset.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01600", "pdf_url": "https://arxiv.org/pdf/2511.01600.pdf", "is_interesting": false}, "165": {"title": "DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning", "authors": ["Mahmut Selman Gokmen, Cody Bumgardner"], "abstract": "arXiv:2511.01610v1 Announce Type: new \nVision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01610", "pdf_url": "https://arxiv.org/pdf/2511.01610.pdf", "is_interesting": false}, "166": {"title": "Benchmark-Ready 3D Anatomical Shape Classification", "authors": ["Tom\\'a\\v{s} Krsi\\v{c}ka, Tibor Kub\\'ik"], "abstract": "arXiv:2511.01613v1 Announce Type: new \nProgress in anatomical 3D shape classification is limited by the complexity of mesh data and the lack of standardized benchmarks, highlighting the need for robust learning methods and reproducible evaluation. We introduce two key steps toward clinically and benchmark-ready anatomical shape classification via self-supervised graph autoencoding. We propose Precomputed Structural Pooling (PSPooling), a non-learnable mesh pooling operator designed for efficient and structure-preserving graph coarsening in 3D anatomical shape analysis. PSPooling precomputes node correspondence sets based on geometric proximity, enabling parallelizable and reversible pooling and unpooling operations with guaranteed support structure. This design avoids the sparsity and reconstruction issues of selection-based methods and the sequential overhead of edge contraction approaches, making it particularly suitable for high-resolution medical meshes. To demonstrate its effectiveness, we integrate PSPooling into a self-supervised graph autoencoder that learns anatomy-aware representations from unlabeled surface meshes. We evaluate the downstream benefits on MedShapeNet19, a new curated benchmark dataset we derive from MedShapeNet, consisting of 19 anatomical classes with standardized training, validation, and test splits. Experiments show that PSPooling significantly improves reconstruction fidelity and classification accuracy in low-label regimes, establishing a strong baseline for medical 3D shape learning. We hope that MedShapeNet19 will serve as a widely adopted benchmark for anatomical shape classification and further research in medical 3D shape analysis. Access the complete codebase, model weights, and dataset information here: https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01613", "pdf_url": "https://arxiv.org/pdf/2511.01613.pdf", "is_interesting": false}, "167": {"title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers", "authors": ["Mohamed Eltahir, Ali Habibullah, Lama Ayash, Tanveer Hussain, Naeemullah Khan"], "abstract": "arXiv:2511.01617v1 Announce Type: new \nIn the retrieval domain, candidates' fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates' representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM's prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https://github.com/mohammad2012191/ViC", "categories": ["cs.CV", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.01617", "pdf_url": "https://arxiv.org/pdf/2511.01617.pdf", "is_interesting": false}, "168": {"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "authors": ["Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo"], "abstract": "arXiv:2511.01618v1 Announce Type: new \nRecent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.01618", "pdf_url": "https://arxiv.org/pdf/2511.01618.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.4, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses the improvement of spatial reasoning abilities in Multimodal Large Language Models (MLLMs), which is relevant for autonomous driving, especially for 3D scene understanding and reasoning tasks. While not directly about autonomous driving systems, the techniques and improvements outlined can be applied to robotics and autonomous systems, including tasks like 3D perception and reasoning in autonomous driving."}, "review": "```json\n{\n  \"score\": 7.8,\n  \"subfield\": \"\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528 / \u7a7a\u95f4\u63a8\u7406\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"Viewpoint-100K, CV-Bench, BLINK\",\n    \"metrics\": \"Accuracy (various spatial reasoning tasks)\",\n    \"sota_details\": \"\u5728Viewpoint-100K\u4efb\u52a1\uff08BLINK Multi-View\u7ec4\u6210\u90e8\u5206\uff09\u4e0a\u8fbe\u523099.2%\u7684\u51c6\u786e\u7387\uff0c\u8fdc\u8d85\u968f\u673a\u731c\u6d4b\u57fa\u7ebf\u3002\u5728CV-Bench\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e13\u6709\u6a21\u578b\u3002\u57283DSRBench\u4e0a\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u6709\u63d0\u5347\uff0c\u4f46\u7565\u4f4e\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u3002\"\n  },\n  \"reason\": \"\u8fd9\u7bc7\u8bba\u6587\u805a\u7126\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u57283D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2dMLLMs\u8fdb\u884c\u9ad8\u7ea7\u611f\u77e5\u548c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u521b\u65b0\u6027\u5f3a\uff0c\u63d0\u51fa\u4e86Viewpoint Learning\u4efb\u52a1\u548cViewpoint-100K\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\uff08SFT+RL\uff09\u5fae\u8c03\u7b56\u7565\u53ca\u6df7\u5408\u51b7\u542f\u52a8\u521d\u59cb\u5316\u65b9\u6cd5\u6765\u6ce8\u5165\u7a7a\u95f4\u57fa\u7840\u77e5\u8bc6\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u4f9d\u8d562D\u7ebf\u7d22\u3001\u7f3a\u4e4f3D\u4e00\u81f4\u6027\u7406\u89e3\u7684\u6838\u5fc3\u75db\u70b9\u3002\u5b9e\u9a8c\u5728\u591a\u4e2a\u4e3b\u6d41\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4e0e\u591a\u6b3e\u5148\u8fdbMLLMs\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5176\u63d0\u51fa\u7684Viewpoint\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u8868\u73b0\uff0c\u5e76\u5728CV-Bench\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u4e13\u6709\u6a21\u578b\u3002\u867d\u7136\u5b83\u4e0d\u662f\u76f4\u63a5\u7684BEV\u611f\u77e5\u6216\u89c4\u5212\u63a7\u5236\u6a21\u5757\uff0c\u4f46\u5176\u76ee\u6807\u662f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u7684MLLM\u201c\u5927\u8111\u201d\u57fa\u7840\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u8fdb\u884c\u9c81\u68d2\u76843D\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\uff0c\u8fd9\u4e0e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u9ad8\u7ea7\u667a\u80fd\u9700\u6c42\u76f4\u63a5\u76f8\u5173\uff0c\u5177\u6709\u91cd\u8981\u7684\u884c\u4e1a\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u5176\u4ef7\u503c\u8fdc\u8d85\u4e00\u822c\u6027AI\u7814\u7a76\uff0c\u7b26\u5408\u81ea\u52a8\u9a7e\u9a76\u5927\u6a21\u578b\u65b9\u5411\u7684\u6df1\u5165\u63a2\u8ba8\u3002\"\n}\n```"}, "169": {"title": "Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward", "authors": ["Xiaogang Xu, Ruihang Chu, Jian Wang, Kun Zhou, Wenjie Shu, Harry Yang, Ser-Nam Lim, Hao Chen, Liang Lin"], "abstract": "arXiv:2511.01645v1 Announce Type: new \nReinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth's distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01645", "pdf_url": "https://arxiv.org/pdf/2511.01645.pdf", "is_interesting": false}, "170": {"title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback", "authors": ["Ropeway Liu, Hangjie Yuan, Bo Dong, Jiazheng Xing, Jinwang Wang, Rui Zhao, Yan Xing, Weihua Chen, Fan Wang"], "abstract": "arXiv:2511.01678v1 Announce Type: new \nRelighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01678", "pdf_url": "https://arxiv.org/pdf/2511.01678.pdf", "is_interesting": false}, "171": {"title": "Progressive Translation of H&E to IHC with Enhanced Structural Fidelity", "authors": ["Yuhang Kang, Ziyu Su, Tianyang Wang, Zaibo Li, Wei Chen, Muhammad Khalid Khan Niazi"], "abstract": "arXiv:2511.01698v1 Announce Type: new \nCompared to hematoxylin-eosin (H&amp;E) staining, immunohistochemistry (IHC) not only maintains the structural features of tissue samples, but also provides high-resolution protein localization, which is essential for aiding in pathology diagnosis. Despite its diagnostic value, IHC remains a costly and labor-intensive technique. Its limited scalability and constraints in multiplexing further hinder widespread adoption, especially in resource-limited settings. Consequently, researchers are increasingly exploring computational stain translation techniques to synthesize IHC-equivalent images from H&amp;E-stained slides, aiming to extract protein-level information more efficiently and cost-effectively. However, most existing stain translation techniques rely on a linearly weighted summation of multiple loss terms within a single objective function, strategy that often overlooks the interdepedence among these components-resulting in suboptimal image quality and an inability to simultaneously preserve structural authenticity and color fidelity. To address this limitation, we propose a novel network architecture that follows a progressive structure, incorporating color and cell border generation logic, which enables each visual aspect to be optimized in a stage-wise and decoupled manner. To validate the effectiveness of our proposed network architecture, we build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We introduce additional loss functions based on 3,3'-diaminobenzidine (DAB) chromogen concentration and image gradient, enhancing color fidelity and cell boundary clarity in the generated IHC images. By reconstructing the generation pipeline using our structure-color-cell boundary progressive mechanism, experiments on HER2 and ER datasets demonstrated that the model significantly improved visual quality and achieved finer structural details.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01698", "pdf_url": "https://arxiv.org/pdf/2511.01698.pdf", "is_interesting": false}, "172": {"title": "Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond", "authors": ["Xin Qiao, Matteo Poggi, Xing Wei, Pengchao Deng, Yanhui Zhou, Stefano Mattoccia"], "abstract": "arXiv:2511.01704v1 Announce Type: new \nUnder-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations-such as signal attenuation, multi-path interference (MPI), and temporal noise-that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01704", "pdf_url": "https://arxiv.org/pdf/2511.01704.pdf", "is_interesting": false}, "173": {"title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark", "authors": ["Yi Zhang, Zheng Wang, Chen Zhen, Wenjie Ruan, Qing Guo, Siddartha Khastgir, Carsten Maple, Xingyu Zhao"], "abstract": "arXiv:2511.01724v1 Announce Type: new \nDeep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at https://tmpspace.github.io/PRBenchLeaderboard/.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01724", "pdf_url": "https://arxiv.org/pdf/2511.01724.pdf", "is_interesting": false}, "174": {"title": "Toward Strategy Identification and Subtask Decomposition In Task Exploration", "authors": ["Tom Odem"], "abstract": "arXiv:2511.01728v1 Announce Type: new \nThis research builds on work in anticipatory human-machine interaction, a subfield of human-machine interaction where machines can facilitate advantageous interactions by anticipating a user's future state. The aim of this research is to further a machine's understanding of user knowledge, skill, and behavior in pursuit of implicit coordination. A task explorer pipeline was developed that uses clustering techniques, paired with factor analysis and string edit distance, to automatically identify key global and local strategies that are used to complete tasks. Global strategies identify generalized sets of actions used to complete tasks, while local strategies identify sequences that used those sets of actions in a similar composition. Additionally, meaningful subtasks of various lengths are identified within the tasks. The task explorer pipeline was able to automatically identify key strategies used to complete tasks and encode user runs with hierarchical subtask structures. In addition, a Task Explorer application was developed to easily review pipeline results. The task explorer pipeline can be easily modified to any action-based time-series data and the identified strategies and subtasks help to inform humans and machines on user knowledge, skill, and behavior.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01728", "pdf_url": "https://arxiv.org/pdf/2511.01728.pdf", "is_interesting": false}, "175": {"title": "CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays", "authors": ["Yefeng Wu, Yucheng Song, Ling Wu, Shan Wan, Yecheng Zhao"], "abstract": "arXiv:2511.01730v1 Announce Type: new \nPneumonia remains a leading cause of morbidity and mortality worldwide, necessitating accurate and efficient automated detection systems. While recent transformer-based detectors like RT-DETR have shown promise in object detection tasks, their application to medical imaging, particularly pneumonia detection in chest X-rays, remains underexplored. This paper presents CGF-DETR, an enhanced real-time detection transformer specifically designed for pneumonia detection. We introduce XFABlock in the backbone to improve multi-scale feature extraction through convolutional attention mechanisms integrated with CSP architecture. To achieve efficient feature aggregation, we propose SPGA module that replaces standard multi-head attention with dynamic gating mechanisms and single-head self-attention. Additionally, GCFC3 is designed for the neck to enhance feature representation through multi-path convolution fusion while maintaining real-time performance via structural re-parameterization. Extensive experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR achieves 82.2\\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\\% while maintaining comparable inference speed at 48.1 FPS. Our ablation studies confirm that each proposed module contributes meaningfully to the overall performance improvement, with the complete model achieving 50.4\\% mAP@[0.5:0.95]", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01730", "pdf_url": "https://arxiv.org/pdf/2511.01730.pdf", "is_interesting": false}, "176": {"title": "3EED: Ground Everything Everywhere in 3D", "authors": ["Rong Li, Yuhao Dong, Tianshuai Hu, Ao Liang, Youquan Liu, Dongyue Lu, Liang Pan, Lingdong Kong, Junwei Liang, Ziwei Liu"], "abstract": "arXiv:2511.01755v1 Announce Type: new \nVisual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes -- 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01755", "pdf_url": "https://arxiv.org/pdf/2511.01755.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "3D\u611f\u77e5 / \u591a\u6a21\u6001\u878d\u5408", "reason": "The paper discusses a 3D grounding benchmark using RGB and LiDAR data across multiple platforms, which is relevant to autonomous driving systems that rely on 3D perception and multi-modal sensor fusion for object localization and scene understanding. While the primary focus is on embodied agents and language-driven 3D perception, the methods and data presented have strong applicability to autonomous driving tasks involving multi-sensor integration and 3D object detection."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"3D\u89c6\u89c9\u5b9a\u4f4d / \u591a\u5e73\u53f0\u591a\u6a21\u6001\u611f\u77e5\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"3EED (\u5305\u542bVehicle, Drone, Quadruped\u4e09\u4e2a\u5e73\u53f0\u7684\u6570\u636e)\",\n    \"metrics\": \"Acc@25, Acc@50, mIoU\",\n    \"sota_details\": \"\u8bba\u6587\u63d0\u51fa\u4e863EED\u6570\u636e\u96c6\u4ee5\u53ca\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u7684\u7edf\u4e00\u57fa\u7ebf\u65b9\u6cd5\uff08Ours\uff09\u3002\u8be5\u57fa\u7ebf\u65b9\u6cd5\u57283EED\u6240\u6709\u5355\u5e73\u53f0\u3001\u8de8\u5e73\u53f0\u8fc1\u79fb\u548c\u591a\u5e73\u53f0\u8054\u5408\u8bad\u7ec3\u76843D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\uff0c\u5747\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u4e3b\u6d41\u76843D\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff08\u5982BUTD-DETR, EDA, WildRefer\uff09\u7684\u9002\u5e94\u6027\u8868\u73b0\u3002\u4f8b\u5982\uff0c\u5728Vehicle\u5355\u5e73\u53f0\u5355\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0cOurs\u7684Acc@25\u8fbe\u523078.37%\uff0c\u8fdc\u9ad8\u4e8eBUTD-DETR\u768452.38%\uff1b\u5728\u8de8\u5e73\u53f0\u8fc1\u79fb\u4efb\u52a1\u4e2d\uff08\u5728Vehicle\u8bad\u7ec3\uff0c\u5728Drone\u6d4b\u8bd5\uff09\uff0cOurs\u7684Acc@25\u4e3a18.16%\uff0c\u800cBUTD-DETR\u4ec5\u4e3a1.54%\u3002\u5728\u591a\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0cOurs\u7684Acc@25\u548cmIoU\u5206\u522b\u4e3a32.32%\u548c56.40%\uff0c\u540c\u6837\u4f18\u4e8eBUTD-DETR\u768425.40%\u548c47.88%\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e863EED\uff0c\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u3001\u5927\u89c4\u6a21\u3001\u591a\u5e73\u53f0\u3001\u591a\u6a21\u6001\u7684\u6237\u59163D\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\u3002\u5176\u6838\u5fc3\u4ef7\u503c\u5728\u4e8e\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u5ba4\u5185\u73af\u5883\u3001\u5355\u4e00\u5e73\u53f0\u548c\u89c4\u6a21\u53d7\u9650\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u66f4\u5e7f\u6cdb\u7684\u5177\u8eab\u667a\u80fd\u9886\u57df\u63d0\u4f9b\u4e86\u6781\u5177\u6311\u6218\u6027\u548c\u771f\u5b9e\u6027\u7684\u7814\u7a76\u5e73\u53f0\u3002\\n\\n**\u521b\u65b0\u6027 (9.5/10)**:\\n1.  **\u6570\u636e\u96c6\u521b\u65b0\u6027\u6781\u9ad8**\uff1a3EED\u662f\u9996\u4e2a\u6db5\u76d6\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e09\u79cd\u5f02\u6784\u5e73\u53f0\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86RGB\u548cLiDAR\u6570\u636e\u3002\u5176\u89c4\u6a21\uff08\u8d85\u8fc7128,000\u4e2a\u5bf9\u8c61\uff0c22,000\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u6307\u4ee3\u8868\u8fbe\uff09\u6bd4\u73b0\u6709\u6237\u5916\u6570\u636e\u96c6\u592710\u500d\uff0c\u5e76\u4e14\u6db5\u76d6\u4e86\u66f4\u5e7f\u9614\u7684\u573a\u666f\u8303\u56f4\u548c\u9ad8\u5ea6\u53d8\u5316\uff0c\u8fd9\u662f\u5f53\u524d3D\u89c6\u89c9\u5b9a\u4f4d\u9886\u57df\u6025\u9700\u7684\u3002\\n2.  **\u6807\u6ce8\u6d41\u7a0b\u521b\u65b0**\uff1a\u7ed3\u5408\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u793a\u4e0e\u4eba\u5de5\u9a8c\u8bc1\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u6570\u636e\u6807\u6ce8\uff0c\u8fd9\u5bf9\u4e8e\u6784\u5efa\u5982\u6b64\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\u3002\\n3.  **\u65b9\u6cd5\u521b\u65b0**\uff1a\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u878d\u5408\u4e86\u5e73\u53f0\u611f\u77e5\u5f52\u4e00\u5316 (CPA)\u3001\u591a\u5c3a\u5ea6\u91c7\u6837 (MSS) \u548c\u5c3a\u5ea6\u611f\u77e5\u878d\u5408 (SAF) \u7b49\u6280\u672f\u3002CPA\u901a\u8fc7\u5bf9\u91cd\u529b\u8f74\u7684\u5bf9\u9f50\u548c\u9ad8\u5ea6\u5f52\u4e00\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5e73\u53f0\u95f4\u7684\u51e0\u4f55\u5dee\u5f02\uff1bMSS\u548cSAF\u5219\u80fd\u5e94\u5bf9LiDAR\u7a00\u758f\u6027\u3001\u7269\u4f53\u5c3a\u5ea6\u5de8\u5927\u53d8\u5316\u4ee5\u53ca\u8de8\u5e73\u53f0\u89c6\u89d2/\u5bc6\u5ea6\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u90fd\u662f\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u771f\u5b9e\u5b58\u5728\u7684\u95ee\u9898\u3002\\n\\n**\u5b9e\u9a8c\u5b8c\u6574\u6027 (9.5/10)**:\\n1.  **\u5168\u9762\u8bc4\u4f30**\uff1a\u8bbe\u8ba1\u4e86\u56db\u79cd\u57fa\u51c6\u8bc4\u4f30\u534f\u8bae\uff1a\u5355\u5e73\u53f0\u5355\u7269\u4f53\u5b9a\u4f4d\u3001\u8de8\u5e73\u53f0\u8fc1\u79fb\u3001\u591a\u7269\u4f53\u5b9a\u4f4d\u3001\u591a\u5e73\u53f0\u8054\u5408\u8bad\u7ec3\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\\n2.  **\u8be6\u7ec6\u6d88\u878d\u7814\u7a76**\uff1a\u5bf9CPA\u3001MSS\u3001SAF\u7b49\u6838\u5fc3\u6a21\u5757\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u6e05\u6670\u5730\u5c55\u793a\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u8d21\u732e\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7a33\u5065\u6027\u3002\\n3.  **\u591a\u89d2\u5ea6\u5206\u6790**\uff1a\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u5305\u62ec\u89c6\u70b9\u51e0\u4f55\u3001\u7269\u4f53\u5bc6\u5ea6\u3001\u70b9\u4e91\u51e0\u4f55\u5206\u5e03\u7b49\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u5b9a\u4f4d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\\n4.  **\u5f3a\u5927\u57fa\u7ebf\u5bf9\u6bd4**\uff1a\u4e0e\u591a\u4e2aSOTA\u76843D\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\uff08\u5982BUTD-DETR\u3001EDA\u3001WildRefer\uff09\u8fdb\u884c\u4e86\u5145\u5206\u5bf9\u6bd4\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u57fa\u7ebf\u65b9\u6cd5\u57283EED\u4e0a\u7684\u663e\u8457\u4f18\u52bf\u3002\\n\\n**\u53ef\u4fe1\u5ea6 (9.0/10)**:\\n1.  **\u6570\u636e\u6765\u6e90\u53ef\u9760**\uff1a\u6570\u636e\u96c6\u6784\u5efa\u5728Waymo Open Dataset\u548cM3ED\u7b49\u516c\u8ba4\u7684\u9ad8\u8d28\u91cf\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e4b\u4e0a\uff0c\u786e\u4fdd\u4e86\u539f\u59cb\u6570\u636e\u7684\u53ef\u9760\u6027\u3002\\n2.  **\u65b9\u6cd5\u9610\u8ff0\u6e05\u6670**\uff1a\u5bf9\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u7ed3\u6784\u548c\u8bc4\u4f30\u6307\u6807\u7684\u63cf\u8ff0\u8be6\u7ec6\u800c\u900f\u660e\u3002\\n3.  **\u7ed3\u679c\u652f\u6301\u5ea6\u9ad8**\uff1a\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u5730\u652f\u6301\u4e86\u8bba\u6587\u7684\u6838\u5fc3\u4e3b\u5f20\uff0c\u53733EED\u80fd\u591f\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a8\u52a8\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u76843D\u89c6\u89c9\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\u3002\\n4.  **\u8d44\u6e90\u5f00\u653e\u6027**\uff1a\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\u7684\u53d1\u5e03\uff0c\u786e\u4fdd\u4e86\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u548c\u793e\u533a\u534f\u4f5c\u7684\u53ef\u80fd\u6027\u3002\\n\\n**\u884c\u4e1a\u6f5c\u529b (9.5/10)**:\\n1.  **\u76f4\u63a5\u76f8\u5173\u81ea\u52a8\u9a7e\u9a76**\uff1a\u8bba\u6587\u7684Vehicle\u5e73\u53f0\u6570\u636e\u76f4\u63a5\u6765\u6e90\u4e8eWaymo\uff0c\u5e76\u4e14\u63d0\u51fa\u76843D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7406\u89e3\u9ad8\u5c42\u4eba\u7c7b\u6307\u4ee4\u3001\u5b9e\u73b0\u7cbe\u786e\u611f\u77e5\u7684\u6838\u5fc3\u80fd\u529b\u4e4b\u4e00\u3002\\n2.  **\u62d3\u5c55\u5177\u8eab\u667a\u80fd\u8fb9\u754c**\uff1a\u591a\u5e73\u53f0\uff08\u65e0\u4eba\u673a\u3001\u56db\u8db3\u673a\u5668\u4eba\uff09\u7684\u5f15\u5165\uff0c\u5c063D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5177\u8eab\u667a\u80fd\u9886\u57df\uff0c\u5bf9\u4e8e\u672a\u6765\u667a\u80fd\u7269\u6d41\u3001\u5de1\u68c0\u3001\u6551\u63f4\u7b49\u573a\u666f\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u8fd9\u4e9b\u90fd\u4e0e\u81ea\u52a8\u9a7e\u9a76\u751f\u6001\u7cfb\u7edf\u7d27\u5bc6\u76f8\u5173\u3002\\n3.  **\u6311\u6218\u524d\u6cbf\u95ee\u9898**\uff1a\u660e\u786e\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u70b9\u4e91\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u7b49\u65b9\u9762\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u90fd\u662f\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u9886\u57df\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u96be\u9898\u3002\\n4.  **\u4fc3\u8fdb\u901a\u7528\u6a21\u578b\u53d1\u5c55**\uff1a\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5c06\u6781\u5927\u5730\u63a8\u52a8\u5f00\u53d1\u80fd\u591f\u8de8\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u3001\u89c6\u70b9\u548c\u73af\u5883\u6761\u4ef6\u8fdb\u884c\u9c81\u68d2\u6cdb\u5316\u7684\u8bed\u8a00\u9a71\u52a83D\u611f\u77e5\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0L4/L5\u7ea7\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\\n\\n\u7efc\u5408\u6765\u770b\uff0c\u8fd9\u7bc7\u8bba\u6587\u5728\u81ea\u52a8\u9a7e\u9a76\u53ca\u5177\u8eab\u667a\u80fd\u9886\u57df\u505a\u51fa\u4e86\u975e\u5e38\u624e\u5b9e\u548c\u5177\u6709\u524d\u77bb\u6027\u7684\u5de5\u4f5c\u3002\u5176\u53d1\u5e03\u7684\u5927\u89c4\u6a21\u591a\u5e73\u53f0\u591a\u6a21\u6001\u6570\u636e\u96c6\u4ee5\u53ca\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u7684\u5f3a\u5927\u57fa\u7ebf\uff0c\u5c06\u5bf9\u672a\u6765\u7684\u7814\u7a76\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002\u9274\u4e8e\u5176\u6781\u9ad8\u7684\u521b\u65b0\u6027\u3001\u8be6\u5c3d\u7684\u5b9e\u9a8c\u4ee5\u53ca\u5bf9\u884c\u4e1a\u672a\u6765\u53d1\u5c55\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7ed9\u4e88\u9ad8\u5206\u3002\"\n}\n```"}, "177": {"title": "HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain", "authors": ["Kai Zhai, Ziyan Huang, Qiang Nie, Xiang Li, Bo Ouyang"], "abstract": "arXiv:2511.01756v1 Announce Type: new \n2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01756", "pdf_url": "https://arxiv.org/pdf/2511.01756.pdf", "is_interesting": false}, "178": {"title": "Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image", "authors": ["Yuxiao Yang, Xiao-Xiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, Wei Yin"], "abstract": "arXiv:2511.01767v1 Announce Type: new \nIn this work, we introduce \\textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01767", "pdf_url": "https://arxiv.org/pdf/2511.01767.pdf", "is_interesting": false}, "179": {"title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs", "authors": ["Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai"], "abstract": "arXiv:2511.01768v1 Announce Type: new \nAlthough transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01768", "pdf_url": "https://arxiv.org/pdf/2511.01768.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "3D\u611f\u77e5 / \u8f68\u8ff9\u9884\u6d4b / \u89c4\u5212\u63a7\u5236", "reason": "The paper discusses a unified model for autonomous driving, UniLION, that efficiently handles multi-modal inputs (LiDAR, images, and temporal data) for key tasks such as 3D perception (3D object detection, tracking, occupancy prediction), motion prediction, and end-to-end planning, all of which are directly related to core autonomous driving tasks."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"\u7edf\u4e00\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b / 3D\u57fa\u7840\u6a21\u578b\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"nuScenes (validation set)\",\n    \"metrics\": \"NDS, mAP, AMOTA, mIoU, RayIoU, minADE, L2, Collision Rate\",\n    \"sota_details\": \"UniLION\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u5176LCT\uff08LiDAR-Camera-Temporal\uff09\u914d\u7f6e\u57283D\u591a\u76ee\u6807\u8ddf\u8e2a\uff08AMOTA 76.5%\uff09\u3001BEV\u5730\u56fe\u5206\u5272\uff08mIoU 73.3%\uff09\u30013D\u5360\u7528\u9884\u6d4b\uff08RayIoU 51.3%\uff09\u548c\u7aef\u5230\u7aef\u89c4\u5212\uff08L2 0.65m\uff0c\u90e8\u5206\u89c4\u5212\u6307\u6807\uff09\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002\u540c\u65f6\uff0c\u5728\u6240\u6709\u56db\u79cd\u914d\u7f6e\uff08L, LT, LC, LCT\uff09\u4e0b\uff0cUniLION\u57283D\u76ee\u6807\u68c0\u6d4b\u3001\u591a\u76ee\u6807\u8ddf\u8e2a\u3001BEV\u5730\u56fe\u5206\u5272\u30013D\u5360\u7528\u9884\u6d4b\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u7b49\u6838\u5fc3\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86SOTA\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\"\n  },\n  \"reason\": \"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7edf\u4e00\u81ea\u52a8\u9a7e\u9a76\u6a21\u578bUniLION\uff0c\u5176\u6838\u5fc3\u5728\u4e8e\u4f7f\u7528\u7ebf\u6027\u7fa4RNN\u4f5c\u4e3a3D\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5927\u89c4\u6a21\u3001\u5f02\u6784\u3001\u65f6\u5e8f\u6570\u636e\u65f6\u9762\u4e34\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002\u901a\u8fc7\u5c06LiDAR\u70b9\u4e91\u3001\u591a\u89c6\u89d2\u56fe\u50cf\u751a\u81f3\u65f6\u5e8f\u4fe1\u606f\u76f4\u63a5\u62fc\u63a5\u4e3atoken\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027RNN\u8fdb\u884c\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u3001\u591a\u65f6\u5e8f\u7684\u9690\u5f0f\u878d\u5408\uff0c\u800c\u975e\u4f9d\u8d56\u663e\u5f0f\u7684\u3001\u624b\u8bbe\u8ba1\u7684\u878d\u5408\u6a21\u5757\uff0c\u8fd9\u5728\u67b6\u6784\u4e0a\u5177\u6709\u5f88\u5f3a\u7684\u521b\u65b0\u6027\uff0c\u7b26\u5408\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u6784\u5efa\u57fa\u7840\u6a21\u578b\u7684\u8d8b\u52bf\u3002\\n\\n\u5b9e\u9a8c\u90e8\u5206\u975e\u5e38\u5b8c\u6574\u548c\u5168\u9762\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5bf93D\u611f\u77e5\uff08\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u5360\u7528\u9884\u6d4b\u3001BEV\u5730\u56fe\u5206\u5272\uff09\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u4e03\u9879\u6838\u5fc3\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u6d4b\u8bd5\u4e86LiDAR-only\u3001LiDAR-temporal\u3001multi-modal\u548cmulti-modal temporal\u56db\u79cd\u4e0d\u540c\u7684\u8f93\u5165\u914d\u7f6e\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u3002\u5927\u91cf\u7684\u6d88\u878d\u5b9e\u9a8c\u6e05\u6670\u5730\u8bc1\u660e\u4e86\u5404\u4e2a\u7ec4\u4ef6\uff08\u59823D\u7a7a\u95f4\u7279\u5f81\u63cf\u8ff0\u7b26\u3001\u81ea\u52a8\u56de\u5f52\u4f53\u7d20\u751f\u6210\u3001\u52a8\u6001\u635f\u5931\u5e73\u8861\uff09\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u6a21\u578b\u5728\u4e0d\u540c\u56fe\u50cf\u9aa8\u5e72\u7f51\u7edc\u3001\u4e0d\u540c\u7ebf\u6027RNN\u7b97\u5b50\u3001\u7a97\u53e3\u5927\u5c0f\u548c\u7ec4\u5927\u5c0f\u65b9\u9762\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u503c\u5f97\u79f0\u8d5e\u7684\u662f\u5176\u5728\u4f20\u611f\u5668\u9519\u4f4d\u4e0b\u7684\u9c81\u68d2\u6027\u5206\u6790\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u8bba\u6587\u6210\u529f\u5c55\u793a\u4e86\u201c\u4e00\u4e2a\u6a21\u578b\u652f\u6301\u6240\u6709\u201d\uff08One Model for All\uff09\u7684\u80fd\u529b\uff0c\u5373\u5728\u591a\u6a21\u6001\u65f6\u5e8f\u6570\u636e\u4e0a\u8bad\u7ec3\u4e00\u6b21\u540e\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728LiDAR-only\u3001temporal LiDAR\u6216multi-modal\u7b49\u4e0d\u540c\u914d\u7f6e\u4e0b\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u5927\u5927\u7b80\u5316\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u548c\u90e8\u7f72\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u4e86\u884c\u4e1a\u6f5c\u529b\u3002\\n\\n\u5728\u7ed3\u679c\u53ef\u4fe1\u5ea6\u65b9\u9762\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9a\u91cf\u6bd4\u8f83\uff0cUniLION\u5728\u591a\u9879\u4efb\u52a1\u548c\u914d\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86SOTA\u6216\u9886\u5148\u6c34\u5e73\uff0c\u5c3d\u7ba1\u5c11\u6570\u7279\u5b9a\u6307\u6807\uff08\u5982\u90e8\u5206\u89c4\u5212\u4efb\u52a1\u7684\u78b0\u649e\u7387\uff09\u672a\u80fd\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u8868\u73b0\u4f9d\u7136\u5353\u8d8a\u4e14\u4e00\u81f4\u3002\u5176\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u7edf\u4e00\u67b6\u6784\u7684\u4f18\u52bf\uff0c\u4f7f\u5176\u5728\u6548\u7387\u548c\u7cfb\u7edf\u6574\u5408\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\\n\\n\u7efc\u4e0a\u6240\u8ff0\uff0cUniLION\u4ee3\u8868\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bbe\u8ba1\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u6b65\u65b9\u5411\uff0c\u6709\u671b\u4e3a\u672a\u6765\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u7075\u6d3b\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u56e0\u6b64\u7ed9\u4e88\u9ad8\u5206\u3002\"\n}\n```"}, "180": {"title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment", "authors": ["Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai, Geng Guo, Jing Zhang, Yinlu Ding, Nassir Navab, Jiebo Luo"], "abstract": "arXiv:2511.01775v1 Announce Type: new \nFoundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.01775", "pdf_url": "https://arxiv.org/pdf/2511.01775.pdf", "is_interesting": false}, "181": {"title": "PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution", "authors": ["Tejas Sarnaik, Manan Shah, Ravi Hegde"], "abstract": "arXiv:2511.01802v1 Announce Type: new \nRetrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG framework that underscores the significance of prompt formulation in facilitating entity extraction, fact selection, and passage reranking for multi-hop question answering. Our approach creates a symbolic knowledge graph from text data by encoding entities and factual relationships as structured facts triples. We use LLMs selectively during online retrieval to perform semantic filtering and answer generation. We also use entity-guided graph traversal through Personalized PageRank (PPR) to support efficient, scalable retrieval based on the knowledge graph we built. Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA, with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%, respectively. These results show that prompt design is an important part of improving retrieval accuracy and response quality. This research lays the groundwork for more efficient and comprehensible multi-hop question-answering systems, highlighting the importance of prompt-aware graph reasoning.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01802", "pdf_url": "https://arxiv.org/pdf/2511.01802.pdf", "is_interesting": false}, "182": {"title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art", "authors": ["Sagi Eppel, Alona Strugatski"], "abstract": "arXiv:2511.01817v1 Announce Type: new \nThe ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the Scitextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,200 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the connection between the visual patterns that shape our world and the mechanisms that produce them. Created by an agentic AI pipeline that autonomously collects and implements models in standardized form, we use SciTextures to evaluate the ability of leading AI models to link visual patterns to the models and code that generate them, and to identify different patterns that emerged from the same process. We also test AIs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world pattern and asking the AI to identify, model, and code the mechanism that formed the pattern, then run this code to generate a simulated image that is compared to the real image. These benchmarks show that vision-language models (VLMs) can understand and simulate the physical system beyond a visual pattern. The dataset and code are available at: https://zenodo.org/records/17485502", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01817", "pdf_url": "https://arxiv.org/pdf/2511.01817.pdf", "is_interesting": false}, "183": {"title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning", "authors": ["Ming Li, Jike Zhong, Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Yuxiang Lai, Wei Chen, Konstantinos Psounis, Kaipeng Zhang"], "abstract": "arXiv:2511.01833v1 Announce Type: new \nThe frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01833", "pdf_url": "https://arxiv.org/pdf/2511.01833.pdf", "is_interesting": false}, "184": {"title": "VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games", "authors": ["Yurun Wu, Yousong Sun, Burkhard Wunsche, Jia Wang, Elliott Wen"], "abstract": "arXiv:2511.00002v1 Announce Type: cross \nVirtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry's rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent's temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00002", "pdf_url": "https://arxiv.org/pdf/2511.00002.pdf", "is_interesting": false}, "185": {"title": "Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment", "authors": ["Adrian-Dinu Urse, Dumitru-Clementin Cercel, Florin Pop"], "abstract": "arXiv:2511.00004v1 Announce Type: cross \nNatural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00004", "pdf_url": "https://arxiv.org/pdf/2511.00004.pdf", "is_interesting": false}, "186": {"title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "authors": ["Suhasnadh Reddy Veluru, Sai Teja Erukude, Viswa Chaitanya Marella"], "abstract": "arXiv:2511.00020v1 Announce Type: cross \nIn the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00020", "pdf_url": "https://arxiv.org/pdf/2511.00020.pdf", "is_interesting": false}, "187": {"title": "LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks", "authors": ["Pradeep M, Ritesh Pallod, Satyen Abrol, Muthu Raman, Ian Anderson"], "abstract": "arXiv:2511.00072v1 Announce Type: cross \nGenerative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3--7\\% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00072", "pdf_url": "https://arxiv.org/pdf/2511.00072.pdf", "is_interesting": false}, "188": {"title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation", "authors": ["Marios Impraimakis, Evangelia Nektaria Palkanoglou"], "abstract": "arXiv:2511.00099v1 Announce Type: cross \nThe optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SY", "eess.SP", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00099", "pdf_url": "https://arxiv.org/pdf/2511.00099.pdf", "is_interesting": false}, "189": {"title": "Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification", "authors": ["Marios Impraimakis"], "abstract": "arXiv:2511.00100v1 Announce Type: cross \nThe dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SP", "eess.SY", "stat.AP"], "abs_url": "https://arxiv.org/abs/2511.00100", "pdf_url": "https://arxiv.org/pdf/2511.00100.pdf", "is_interesting": false}, "190": {"title": "GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow", "authors": ["Mengbo Wang, Shourya Verma, Aditya Malusare, Luopin Wang, Yiyang Lu, Vaneet Aggarwal, Mario Sola, Ananth Grama, Nadia Atallah Lanman"], "abstract": "arXiv:2511.00119v1 Announce Type: cross \nSpatial transcriptomics (ST) technologies can be used to align transcriptomes with histopathological morphology, presenting exciting new opportunities for biomolecular discovery. Using ST data, we construct a novel framework, GeneFlow, to map transcriptomics onto paired cellular images. By combining an attention-based RNA encoder with a conditional UNet guided by rectified flow, we generate high-resolution images with different staining methods (e.g. H&amp;E, DAPI) to highlight various cellular/tissue structures. Rectified flow with high-order ODE solvers creates a continuous, bijective mapping between transcriptomics and image manifolds, addressing the many-to-one relationship inherent in this problem. Our method enables the generation of realistic cellular morphology features and spatially resolved intercellular interactions from observational gene expression profiles, provides potential to incorporate genetic/chemical perturbations, and enables disease diagnosis by revealing dysregulated patterns in imaging phenotypes. Our rectified flow-based method outperforms diffusion-based baseline method in all experiments. Code can be found at https://github.com/wangmengbo/GeneFlow.", "categories": ["q-bio.QM", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00119", "pdf_url": "https://arxiv.org/pdf/2511.00119.pdf", "is_interesting": false}, "191": {"title": "Melanoma Classification Through Deep Ensemble Learning and Explainable AI", "authors": ["Wadduwage Shanika Perera, ABM Islam, Van Vung Pham, Min Kyung An"], "abstract": "arXiv:2511.00246v1 Announce Type: cross \nMelanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation's shortcomings in DL models' decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00246", "pdf_url": "https://arxiv.org/pdf/2511.00246.pdf", "is_interesting": false}, "192": {"title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "authors": ["Abhinav Joshi, Vaibhav Sharma, Sanjeet Singh, Ashutosh Modi"], "abstract": "arXiv:2511.00270v1 Announce Type: cross \nSign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00270", "pdf_url": "https://arxiv.org/pdf/2511.00270.pdf", "is_interesting": false}, "193": {"title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping", "authors": ["Lingpeng Chen, Jiakun Tang, Apple Pui-Yi Chui, Ziyang Hong, Junfeng Wu"], "abstract": "arXiv:2511.00392v1 Announce Type: cross \nAccurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00392", "pdf_url": "https://arxiv.org/pdf/2511.00392.pdf", "is_interesting": false}, "194": {"title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling", "authors": ["Zenghao Niu, Weicheng Xie, Siyang Song, Zitong Yu, Feng Liu, Linlin Shen"], "abstract": "arXiv:2511.00411v1 Announce Type: cross \nAdversarial attacks present a critical challenge to deep neural networks' robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling's magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00411", "pdf_url": "https://arxiv.org/pdf/2511.00411.pdf", "is_interesting": false}, "195": {"title": "Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model", "authors": ["Ruthwik Reddy Doodipala, Pankaj Pandey, Carolina Torres Rojas, Manob Jyoti Saikia, Ranganatha Sitaram"], "abstract": "arXiv:2511.00443v1 Announce Type: cross \nThe emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00443", "pdf_url": "https://arxiv.org/pdf/2511.00443.pdf", "is_interesting": false}, "196": {"title": "Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements", "authors": ["Xiaolong Li, Zhi-Qin John Xu, Yan Ren, Tianming Qiu, Xiaowen Wang"], "abstract": "arXiv:2511.00449v1 Announce Type: cross \nAccurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).", "categories": ["eess.IV", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00449", "pdf_url": "https://arxiv.org/pdf/2511.00449.pdf", "is_interesting": false}, "197": {"title": "Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation", "authors": ["Aditya Parikh, Sneha Das, Aasa Feragen"], "abstract": "arXiv:2511.00477v1 Announce Type: cross \nAlgorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model's actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00477", "pdf_url": "https://arxiv.org/pdf/2511.00477.pdf", "is_interesting": false}, "198": {"title": "Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach", "authors": ["Renjun Gao, Xiangjie Kong, Dongting Cai, Boyi Fu, Junxiang Yang"], "abstract": "arXiv:2511.00508v1 Announce Type: cross \nReconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen--Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank--Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \\textit{Star Wars}, validate the algorithm's accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in https://github.com/cfdyang521/C-3PO/tree/main.", "categories": ["math.NA", "cs.CG", "cs.CV", "cs.NA"], "abs_url": "https://arxiv.org/abs/2511.00508", "pdf_url": "https://arxiv.org/pdf/2511.00508.pdf", "is_interesting": false}, "199": {"title": "Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance", "authors": ["Yunchuan Guan, Yu Liu, Ke Zhou, Hui Li, Sen Jia, Zhiqi Shen, Ziyang Wang, Xinglin Zhang, Tao Chen, Jenq-Neng Hwang, Lei Li"], "abstract": "arXiv:2511.00543v1 Announce Type: cross \nRecent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp's superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.", "categories": ["cs.LG", "cs.CV", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00543", "pdf_url": "https://arxiv.org/pdf/2511.00543.pdf", "is_interesting": false}, "200": {"title": "Image-based ground distance detection for crop-residue-covered soil", "authors": ["Baochao Wang, Xingyu Zhang, Qingtao Zong, Alim Pulatov, Shuqi Shang, Dongwei Wang"], "abstract": "arXiv:2511.00548v1 Announce Type: cross \nConservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.", "categories": ["eess.IV", "cs.CV", "cs.GR", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00548", "pdf_url": "https://arxiv.org/pdf/2511.00548.pdf", "is_interesting": false}, "201": {"title": "GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations", "authors": ["Zixuan Sun, Shuaifeng Zhi, Ruize Li, Jingyuan Xia, Yongxiang Liu, Weidong Jiang"], "abstract": "arXiv:2511.00598v1 Announce Type: cross \nRegistration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: https://github.com/Zi-Xuan-Sun/GDROS.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00598", "pdf_url": "https://arxiv.org/pdf/2511.00598.pdf", "is_interesting": false}, "202": {"title": "Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars", "authors": ["Ali Khalid, Jaiaid Mobin, Sumanth Rao Appala, Avinash Maurya, Stephany Berrio Perez, M. Mustafa Rafique, Fawad Ahmad"], "abstract": "arXiv:2511.00652v1 Announce Type: cross \nAn autonomous vehicle can generate several terabytes of sensor data per day. A significant portion of this data consists of 3D point clouds produced by depth sensors such as LiDARs. This data must be transferred to cloud storage, where it is utilized for training machine learning models or conducting analyses, such as forensic investigations in the event of an accident. To reduce network and storage costs, this paper introduces DejaView. Although prior work uses interframe redundancies to compress data, DejaView searches for and uses redundancies on larger temporal scales (days and months) for more effective compression. We designed DejaView with the insight that the operating area of autonomous vehicles is limited and that vehicles mostly traverse the same routes daily. Consequently, the 3D data they collect daily is likely similar to the data they have captured in the past. To capture this, the core of DejaView is a diff operation that compactly represents point clouds as delta w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end implementation of DejaView can compress point clouds by a factor of 210 at a reconstruction error of only 15 cm.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00652", "pdf_url": "https://arxiv.org/pdf/2511.00652.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "3D\u611f\u77e5 / \u6570\u636e\u538b\u7f29 / \u81ea\u4e3b\u9a7e\u9a76\u6570\u636e\u7ba1\u7406", "reason": "The paper discusses a LiDAR data compression method specifically for autonomous vehicles to manage large-scale sensor data more effectively. While it primarily focuses on optimizing data storage and transfer, it is directly related to autonomous driving systems, particularly in managing the 3D data generated by sensors like LiDAR, which is crucial for autonomous vehicle operation."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u7ba1\u7406 / LiDAR\u70b9\u4e91\u538b\u7f29\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"\u81ea\u5efa\u771f\u5b9e\u4e16\u754cLiDAR\u6570\u636e\u96c6 (Ouster OS1-128, 2\u4e2a\u6708297K\u70b9\u4e91), CARLA\u5408\u6210\u6570\u636e\u96c6\",\n    \"metrics\": \"\u538b\u7f29\u6bd4 (Compression Ratio, CR), Chamfer\u8ddd\u79bb (Chamfer Distance, CD), \u70b9\u9762\u5cf0\u503c\u4fe1\u566a\u6bd4 (Point-to-Plane PSNR), \u5b9a\u4f4d\u76f8\u5bf9\u5e73\u79fb\u8bef\u5dee (RTE), \u76ee\u6807\u68c0\u6d4b\u5e73\u5747\u7cbe\u5ea6 (AP@70), \u8bed\u4e49\u5206\u5272\u5e73\u5747IoU (mIoU)\",\n    \"sota_details\": \"\u5728\u81ea\u5efa\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cDejaView\u572814cm Chamfer\u8ddd\u79bb\u4e0b\u5b9e\u73b0\u4e86220\u500d\u538b\u7f29\u6bd4\uff0c\u8fdc\u8d85GPCC (122x), Octree (112x), Draco (80x) \u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u6574\u4f53\u538b\u7f29\u6bd4\u6bd4\u73b0\u6709SOTA\u65b9\u6cd5\u9ad82.5\u500d\u3002\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u4e0b\uff0cDejaView\u5728\u5b9a\u4f4d\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5747\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5bf9\u6bd4\u57fa\u7ebf\uff0c\u4e14\u8ba1\u7b97\u65f6\u95f4\u8fdc\u4f4e\u4e8e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5 (OctAttention\u5feb3990\u500d)\u3002\"\n  },\n  \"reason\": \"\u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e00\u4e2a\u6781\u5176\u91cd\u8981\u4e14\u5b9e\u9645\u7684\u95ee\u9898\uff1aLiDAR\u6570\u636e\u91cf\u5de8\u5927\u5e26\u6765\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u6000\u65e7\u9a71\u52a8\uff08Nostalgia-Driven\uff09\u201d\u7684\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5DejaView\uff0c\u5229\u7528\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7ecf\u5e38\u884c\u9a76\u76f8\u540c\u8def\u7ebf\u3001\u5de5\u4f5c\u533a\u57df\u53d7\u9650\u7684\u7279\u6027\uff0c\u6316\u6398LiDAR\u6570\u636e\u5728\u66f4\u957f\u7684\u65f6\u95f4\u5c3a\u5ea6\uff08\u5929\u3001\u6708\uff09\u4e0a\u7684\u7a7a\u95f4\u5197\u4f59\u6027\u3002\\n\\n**\u521b\u65b0\u6027 (9/10):**\\n1.  **\u72ec\u7279\u89c6\u89d2**: \u533a\u522b\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5173\u6ce8\u5e27\u95f4\u6216\u5c40\u90e8\u7a7a\u95f4\u5197\u4f59\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8f66\u8f86\u8fd0\u8425\u533a\u57df\u548c\u8def\u7ebf\u7684\u91cd\u590d\u6027\uff0c\u5728\u957f\u671f\u5386\u53f2\u6570\u636e\u4e2d\u5bfb\u627e\u5197\u4f59\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u65b0\u9896\u4e14\u7b26\u5408\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u6d1e\u5bdf\u3002\\n2.  **\u7ea7\u8054\u5dee\u5206\u64cd\u4f5c**: \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5386\u53f2\u53c2\u8003\u70b9\u4e91\u548c3D\u9ad8\u7cbe\u5730\u56fe\u7684\u7ea7\u8054\u5dee\u5206\uff08cascaded diff\uff09\u64cd\u4f5c\uff0c\u6709\u6548\u5730\u51cf\u5c11\u4e86\u9700\u8981\u5b58\u50a8\u7684\u72ec\u5360\u70b9\uff0c\u540c\u65f6\u907f\u514d\u4e86\u591a\u53c2\u8003\u70b9\u4e91\u5e26\u6765\u7684\u9ad8\u5ef6\u8fdf\u548c\u5b58\u50a8\u5f00\u9500\u3002\\n3.  **\u6df7\u5408\u641c\u7d22\u7b97\u6cd5**: \u9488\u5bf9\u70b9\u4e91\u5dee\u5206\u8ba1\u7b97\u4e2d\u7684\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u6743\u8861\uff0c\u8bbe\u8ba1\u4e86\u7c97\u7c92\u5ea6\uff08Octree\uff09\u548c\u7ec6\u7c92\u5ea6\uff08KD-tree\uff09\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6700\u8fd1\u90bb\u641c\u7d22\u7b97\u6cd5\uff0c\u4fdd\u8bc1\u4e86\u9ad8\u538b\u7f29\u6bd4\u548c\u4f4e\u5ef6\u8fdf\u3002\\n4.  **\u5de5\u7a0b\u5b9e\u8df5**: \u7ed3\u5408\u4e86\u591a\u79cd\u73b0\u6709\u538b\u7f29\u6280\u672f\uff08Draco\u3001LZMA\uff09\u4ee5\u53ca\u7d22\u5f15\u7684Delta\u7f16\u7801\uff0c\u5f62\u6210\u4e86\u4e00\u5957\u7aef\u5230\u7aef\u7684\u5b9e\u7528\u7cfb\u7edf\u3002\\n\\n**\u5b9e\u9a8c\u5b8c\u6574\u6027 (9.5/10):**\\n1.  **\u5168\u9762\u7684\u6570\u636e\u96c6**: \u5b9e\u9a8c\u5728\u81ea\u5efa\u7684\u771f\u5b9e\u4e16\u754cLiDAR\u6570\u636e\u96c6\uff08Ouster OS1-128\uff0c\u4e24\u6708\u6570\u636e\uff09\u548cCARLA\u4eff\u771f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u666e\u9002\u6027\u3002\\n2.  **\u5e7f\u6cdb\u7684\u57fa\u7ebf\u5bf9\u6bd4**: \u5bf9\u6bd4\u4e86Draco, Octree, MPEG GPCC\u7b49\u4f20\u7edf\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5b66\u4e60\u578b\u65b9\u6cd5OctAttention\uff0c\u8986\u76d6\u4e86\u4e3b\u6d41\u6280\u672f\u3002\\n3.  **\u591a\u7ef4\u5ea6\u8bc4\u4f30**: \u4e0d\u4ec5\u8bc4\u4f30\u4e86\u538b\u7f29\u6bd4\u3001Chamfer\u8ddd\u79bb\u3001PSNR\u7b49\u57fa\u672c\u538b\u7f29\u6307\u6807\uff0c\u66f4\u5173\u952e\u7684\u662f\u6df1\u5165\u8bc4\u4f30\u4e86\u538b\u7f29\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e0b\u6e38\u4efb\u52a1\uff08\u5b9a\u4f4d\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002\\n4.  **\u8be6\u7ec6\u7684\u6d88\u878d\u548c\u654f\u611f\u6027\u5206\u6790**: \u5206\u6790\u4e86\u8ddd\u79bb\u9608\u503c\u3001\u4f20\u611f\u5668\u566a\u58f0\u3001LiDAR\u901a\u9053\u6570\u3001\u52a8\u6001\u73af\u5883\u7b49\u56e0\u7d20\u5bf9\u538b\u7f29\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u53ca\u5176\u53c2\u6570\u8c03\u8282\u80fd\u529b\u3002\\n5.  **\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u5206\u6790**: \u8be6\u7ec6\u62c6\u89e3\u4e86DejaView\u5404\u6a21\u5757\u7684\u8ba1\u7b97\u5ef6\u8fdf\uff0c\u5e76\u4e0e\u57fa\u7ebf\u8fdb\u884c\u4e86\u7aef\u5230\u7aef\u541e\u5410\u91cf\u5bf9\u6bd4\uff0c\u8003\u8651\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002\\n\\n**\u53ef\u4fe1\u5ea6 (9/10):**\\n1.  **\u91cf\u5316\u7ed3\u679c\u5145\u5206**: \u8bba\u6587\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u56fe\u8868\u548c\u6570\u636e\u6765\u652f\u6301\u5176SOTA\u58f0\u660e\uff0c\u5c24\u5176\u662f\u5728\u76f8\u540c\u91cd\u5efa\u8bef\u5dee\u4e0b\u7684\u538b\u7f29\u6bd4\u4f18\u52bf\u4ee5\u53ca\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002\\n2.  **\u65b9\u6cd5\u8bba\u6e05\u6670**: DejaView\u7684\u8bbe\u8ba1\u601d\u8def\u3001\u7b97\u6cd5\u7ec6\u8282\u548c\u5b9e\u73b0\u6b65\u9aa4\u63cf\u8ff0\u6e05\u6670\uff0c\u6613\u4e8e\u7406\u89e3\u548c\u590d\u73b0\u3002\\n3.  **\u5f00\u653e\u4ee3\u7801**: \u4f5c\u8005\u627f\u8bfa\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\uff0c\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u9a8c\u8bc1\u6027\u3002\\n4.  **\u5bf9\u6743\u8861\u7684\u8ba4\u8bc6**: \u8bba\u6587\u6e05\u695a\u5730\u8ba8\u8bba\u4e86\u538b\u7f29\u6bd4\u4e0e\u91cd\u5efa\u8bef\u5dee\u3001\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5c55\u793a\u4e86DejaView\u5982\u4f55\u5728\u8fd9\u79cd\u6743\u8861\u4e2d\u53d6\u5f97\u6700\u4f73\u8868\u73b0\u3002\\n\\n**\u884c\u4e1a\u6f5c\u529b (9.5/10):**\\n1.  **\u89e3\u51b3\u6838\u5fc3\u75db\u70b9**: \u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u7206\u70b8\u6027\u589e\u957f\u5e26\u6765\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8e\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\\n2.  **\u8d4b\u80fdML\u5f00\u53d1**: \u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u80fd\u663e\u8457\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u7684\u6570\u636e\u7ba1\u7406\u6210\u672c\uff0c\u52a0\u901fAV\u7684\u8fed\u4ee3\u5f00\u53d1\u548c\u90e8\u7f72\u3002\\n3.  **\u63d0\u5347\u5b89\u5168\u9a8c\u8bc1**: \u538b\u7f29\u540e\u7684\u6570\u636e\u4ecd\u80fd\u4fdd\u6301\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u4e8b\u6545\u53d6\u8bc1\u3001\u5b89\u5168\u9a8c\u8bc1\u7b49\u5173\u952e\u5e94\u7528\u3002\\n4.  **\u5b9e\u7528\u6027\u9ad8**: \u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff08\u8fdc\u8d85\u5b66\u4e60\u578b\u65b9\u6cd5\uff09\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u7ba1\u7ebf\u4e2d\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u843d\u5730\u6f5c\u529b\u3002\\n\\n\u5c3d\u7ba1\u538b\u7f29\u53d1\u751f\u5728\u8f66\u8f86\u79bb\u7ebf\u540e\uff0c\u4f46\u5176\u76ee\u7684\u548c\u5f71\u54cd\u76f4\u63a5\u670d\u52a1\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u3001\u9a8c\u8bc1\u548c\u90e8\u7f72\uff0c\u5bf9\u6574\u4e2a\u81ea\u52a8\u9a7e\u9a76\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u5176\u4ef7\u503c\u8fdc\u8d85\u201c\u4e0d\u76f4\u63a5\u548c\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u201d\u7684\u9650\u5236\u3002\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u5546\u4e1a\u5316\u89c4\u6a21\u6269\u5c55\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u74f6\u9888\uff0c\u5177\u6709\u6781\u9ad8\u7684\u5de5\u7a0b\u548c\u7814\u7a76\u4ef7\u503c\u3002\"\n}\n```"}, "203": {"title": "Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images", "authors": ["Alberto Di Biase"], "abstract": "arXiv:2511.00702v1 Announce Type: cross \nDoctors and researchers routinely use diffusion tensor imaging (DTI) and tractography to visualize the fibrous structure of tissues in the human body. This paper explores the connection of these techniques to the painterly rendering of images. Using a tractography algorithm the presented method can place brush strokes that mimic the painting process of human artists, analogously to how fibres are tracked in DTI. The analogue to the diffusion tensor for image orientation is the structural tensor, which can provide better local orientation information than the gradient alone. I demonstrate this technique in portraits and general images, and discuss the parallels between fibre tracking and brush stroke placement, and frame it in the language of tractography. This work presents an exploratory investigation into the cross-domain application of diffusion tensor imaging techniques to painterly rendering of images. All the code is available at https://github.com/tito21/st-python", "categories": ["cs.GR", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00702", "pdf_url": "https://arxiv.org/pdf/2511.00702.pdf", "is_interesting": false}, "204": {"title": "EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment", "authors": ["Abhiram Kusumba, Maitreya Patel, Kyle Min, Changhoon Kim, Chitta Baral, Yezhou Yang"], "abstract": "arXiv:2511.00804v1 Announce Type: cross \nErasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current \"concept erasure\" techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model's prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00804", "pdf_url": "https://arxiv.org/pdf/2511.00804.pdf", "is_interesting": false}, "205": {"title": "LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons", "authors": ["Shashank Nag, Alan T. L. Bacellar, Zachary Susskind, Anshul Jha, Logan Liberty, Aishwarya Sivakumar, Eugene B. John, Krishnan Kailas, Priscila M. V. Lima, Neeraja J. Yadwadkar, Felipe M. G. Franca, Lizy K. John"], "abstract": "arXiv:2511.00812v1 Announce Type: cross \nVision Transformers have been tremendously successful in computer vision tasks. However, their large computational, memory, and energy demands are a challenge for edge inference on FPGAs -- a field that has seen a recent surge in demand. We recognize the benefits of recent works on logic and Look Up Table (LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in offering models that simultaneously reduce both the memory and compute footprints. However, these models natively do not perform well on common vision tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge optimized vision transformer design that integrates layers of LUT neurons within the transformer architecture. Based on our characterization that reveals that a majority of model weights and computations are from the channel mixer (MLP layer), we design an alternate LUT-based channel mixer, and simultaneously develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to replace each multiplication with a table lookup, our architecture utilizes a neural learning approach which natively learns the LUT functions. This approach allows for reduced model sizes, and a computational and energy-efficient inference solution for vision transformer models. Evaluating on edge-suitable workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT eliminates over 60% of the model weights and 50% of the multiplications in the model, and achieves 1.9x energy efficiency and 1.3x lower latency over an integer quantized ViT accelerator, while also offering superior throughput against prior works at a 10.9W power budget.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00812", "pdf_url": "https://arxiv.org/pdf/2511.00812.pdf", "is_interesting": false}, "206": {"title": "Learning with Category-Equivariant Representations for Human Activity Recognition", "authors": ["Yoshihiro Maruyama"], "abstract": "arXiv:2511.00900v1 Announce Type: cross \nHuman activity recognition is challenging because sensor signals shift with context, motion, and environment; effective models must therefore remain stable as the world around them changes. We introduce a categorical symmetry-aware learning framework that captures how signals vary over time, scale, and sensor hierarchy. We build these factors into the structure of feature representations, yielding models that automatically preserve the relationships between sensors and remain stable under realistic distortions such as time shifts, amplitude drift, and device orientation changes. On the UCI Human Activity Recognition benchmark, this categorical symmetry-driven design improves out-of-distribution accuracy by approx. 46 percentage points (approx. 3.6x over the baseline), demonstrating that abstract symmetry principles can translate into concrete performance gains in everyday sensing tasks via category-equivariant representation theory.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00900", "pdf_url": "https://arxiv.org/pdf/2511.00900.pdf", "is_interesting": false}, "207": {"title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "authors": ["Xiangyu Shi, Zerui Li, Yanyuan Qiao, Qi Wu"], "abstract": "arXiv:2511.00933v1 Announce Type: cross \nRecent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.00933", "pdf_url": "https://arxiv.org/pdf/2511.00933.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses vision-and-language navigation for robotic systems, focusing on zero-shot navigation in continuous environments. While the methods could be applicable in autonomous driving contexts, the paper itself does not specifically address autonomous vehicles, their perception, prediction, or decision-making tasks, but rather focuses on robotic navigation, which has indirect relevance to AD."}}, "208": {"title": "Few-Shot Multimodal Medical Imaging: A Theoretical Framework", "authors": ["Md Talha Mohsin, Ismail Abdulrashid"], "abstract": "arXiv:2511.01140v1 Announce Type: cross \nMedical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.01140", "pdf_url": "https://arxiv.org/pdf/2511.01140.pdf", "is_interesting": false}, "209": {"title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping", "authors": ["Lijie Wang, Lianjie Guo, Ziyi Xu, Qianhao Wang, Fei Gao, Xieyuanli Chen"], "abstract": "arXiv:2511.01186v1 Announce Type: cross \nReconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01186", "pdf_url": "https://arxiv.org/pdf/2511.01186.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5 / 3D\u611f\u77e5", "reason": "The paper discusses LiDAR inertial visual odometry (LIVO) and its integration with a 3D vision model (VGGT), which are important for large-scale mapping, perception, and navigation in robotics. Although it focuses more on LiDAR-based scene understanding and mapping, these methods are highly applicable to autonomous driving, where accurate 3D perception and sensor fusion are key tasks for vehicle navigation and environment understanding."}, "review": "```json\n{\n  \"score\": 8.9,\n  \"subfield\": \"\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa / \u591a\u4f20\u611f\u5668\u878d\u5408 / 3D\u91cd\u5efa\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"MARS-LVIG, FAST-LIVO2, MUN_FRL, Self-Collect datasets\",\n    \"metrics\": \"Chamfer Distance (CD), ICP Overlap Ratio, Average Wasserstein Distance (AWD), Fitness (\u51e0\u4f55\u7cbe\u5ea6); Color Distance (CD), Color Fidelity (CF), Local Color Recall (LCR), Color Consistency Score (CCS) (\u8272\u5f69\u8d28\u91cf)\",\n    \"sota_details\": \"\u5728\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\uff0cLiDAR-VGGT \u5728\u6240\u6709\u56db\u4e2a\u6307\u6807\uff08CD\u3001ICP Overlap Ratio\u3001AWD\u3001Fitness\uff09\u4e0a\u5747\u4f18\u4e8eVGGT-Long\u3001VGGT-SLAM (Sim3/SL4) \u548c SLAM3R \u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728MARS-LVIG\u6570\u636e\u96c6\u4e0a\uff0c\u9664\u4e86\u6700\u5177\u6311\u6218\u6027\u7684AMvalley01\u5e8f\u5217\u5916\uff0c\u5728\u5176\u4f59\u56db\u4e2a\u5e8f\u5217\u4e0a\u7684ICP Overlap Ratio\u5747\u8d85\u8fc750%\u3002\u5728\u8272\u5f69\u8d28\u91cf\u65b9\u9762\uff0c\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u9c81\u68d2\u6027\u5206\u6790\u4e2d\uff0cLiDAR-VGGT \u5bf9\u5916\u90e8\u6821\u51c6\u8bef\u5dee\uff08\u5982\u65f6\u95f4\u540c\u6b65\u3001\u5916\u90e8\u53c2\u6570\u6821\u51c6\u4e0d\u51c6\u786e\uff09\u7684\u97e7\u6027\u4f18\u4e8e FAST-LIVO2\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 LiDAR-VGGT\uff0c\u4e00\u4e2a\u521b\u65b0\u6027\u7684\u7c97\u5230\u7ec6\u4e24\u9636\u6bb5\u878d\u5408\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe\uff08LiDAR\uff09\u7a00\u758f\u6027\u4e0eVGGT\u6a21\u578b\uff08Visual Geometry Grounded Transformer\uff09\u7f3a\u4e4f\u5ea6\u91cf\u5c3a\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u5bc6\u96c6\u3001\u5ea6\u91cf\u51c6\u786e\u4e14\u5168\u5c40\u4e00\u81f4\u7684\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\u3002\u5176\u521b\u65b0\u6027\u4f53\u73b0\u5728\uff1a1) \u9996\u6b21\u5c06LiDAR\u4e0eVGGT\u7d27\u5bc6\u8026\u5408\uff0c\u5b9e\u73b0\u4e86\u4e24\u8005\u4f18\u52bf\u4e92\u8865\uff1b2) \u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u9884\u878d\u5408\u6a21\u5757\uff0c\u5305\u542b\u7ebf\u6027\u5ea6\u9a8c\u8bc1\u548c\u5c3a\u5ea6\u611f\u77e5RANSAC\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u521d\u59cb\u5316\u548c\u5c3a\u5ea6\u7c97\u6821\u51c6\uff1b3) \u5f15\u5165\u4e86\u5e26\u8fb9\u754c\u6846\u6b63\u5219\u5316\u7684\u8de8\u6a21\u6001Sim(3)\u914d\u51c6\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u76f8\u673a\u548cLiDAR\u89c6\u573a\uff08FOV\uff09\u5dee\u5f02\u5bfc\u81f4\u7684\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u914d\u51c6\u7a33\u5b9a\u6027\uff1b4) \u63d0\u51fa\u4e86\u65b0\u7684\u5f69\u8272\u70b9\u4e91\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u7684\u7a7a\u767d\u3002\u5728\u5b9e\u9a8c\u5b8c\u6574\u6027\u65b9\u9762\uff0c\u8bba\u6587\u5728MARS-LVIG\u3001FAST-LIVO2\u3001MUN_FRL\u7b49\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u53ca\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u800c\u5f7b\u5e95\u7684\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u51e0\u4f55\u7cbe\u5ea6\u548c\u8272\u5f69\u8d28\u91cf\uff0c\u5e76\u4e0eVGGT-Long\u3001VGGT-SLAM\u3001SLAM3R\u548cFAST-LIVO2\u7b49\u591a\u4e2a\u5f3a\u52b2\u57fa\u7ebf\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002\u6d88\u878d\u5b9e\u9a8c\u548c\u9c81\u68d2\u6027\u5206\u6790\u4e5f\u5145\u5206\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5bf9\u5916\u90e8\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u53ef\u4fe1\u5ea6\u9ad8\uff0c\u6570\u636e\u548c\u53ef\u89c6\u5316\u8bc1\u636e\u652f\u6301\u4e86\u5176\u4f18\u5f02\u6027\u80fd\u7684\u4e3b\u5f20\u3002\u4ece\u884c\u4e1a\u6f5c\u529b\u6765\u770b\uff0c\u751f\u6210\u5927\u89c4\u6a21\u3001\u9ad8\u5bc6\u5ea6\u3001\u5ea6\u91cf\u51c6\u786e\u4e14\u5168\u5c40\u4e00\u81f4\u7684\u5f69\u8272\u70b9\u4e91\u662f\u81ea\u52a8\u9a7e\u9a76\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u3001\u5b9a\u4f4d\u548c\u73af\u5883\u7406\u89e3\u7684\u5173\u952e\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u75db\u70b9\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u4f20\u611f\u5668\u6821\u51c6\u548c\u540c\u6b65\u8981\u6c42\u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u9274\u4e8e\u5176\u9ad8\u5ea6\u7684\u521b\u65b0\u6027\u3001\u624e\u5b9e\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4ee5\u53ca\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6838\u5fc3\u9700\u6c42\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\uff0c\u7ed9\u4e88\u9ad8\u5206\u3002\"\n}\n```"}, "210": {"title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects", "authors": ["Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu, Lan Xu"], "abstract": "arXiv:2511.01294v1 Announce Type: cross \nA deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for complex systems like robots or objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or text prompts. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01294", "pdf_url": "https://arxiv.org/pdf/2511.01294.pdf", "is_interesting": false}, "211": {"title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "authors": ["Yuhang Huang, Zekai Lin, Fan Zhong, Lei Liu"], "abstract": "arXiv:2511.01425v1 Announce Type: cross \nExplanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18\\% compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\\Delta$Brier=+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.", "categories": ["cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01425", "pdf_url": "https://arxiv.org/pdf/2511.01425.pdf", "is_interesting": false}, "212": {"title": "Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization", "authors": ["Zhicheng Wang, Chen Ju, Xu Chen, Shuai Xiao, Jinsong Lan, Xiaoyong Zhu, Ying Chen, Zhiguo Cao"], "abstract": "arXiv:2511.01588v1 Announce Type: cross \nEmbedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01588", "pdf_url": "https://arxiv.org/pdf/2511.01588.pdf", "is_interesting": false}, "213": {"title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "authors": ["Renjun Gao, Peiyan Zhong"], "abstract": "arXiv:2511.01594v1 Announce Type: cross \nMultimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01594", "pdf_url": "https://arxiv.org/pdf/2511.01594.pdf", "is_interesting": false}, "214": {"title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process", "authors": ["Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao, Feilong Tang, Donglin Wang, Haoang Li"], "abstract": "arXiv:2511.01718v1 Announce Type: cross \nVision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01718", "pdf_url": "https://arxiv.org/pdf/2511.01718.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u7aef\u5230\u7aef\u5b66\u4e60 / \u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper presents a unified vision-language-action diffusion model designed for embodied agents to understand visual and language inputs and generate corresponding actions. While it is not explicitly focused on autonomous driving, such VLA frameworks and diffusion-based joint perception-action modeling can be applied to end-to-end autonomous driving systems that learn to perceive and act directly from multi-modal inputs. Therefore, the work is indirectly relevant to autonomous driving research through its potential applicability to end-to-end decision-making models."}, "review": "```json\n{\n  \"score\": 5.0,\n  \"subfield\": \"VLA (Vision-Language-Action) \u6a21\u578b / \u673a\u5668\u4eba\u64cd\u4f5c (Robotic Manipulation) / Embodied AI\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"CALVIN, LIBERO, SimplerEnv\",\n    \"metrics\": \"Average Length (CALVIN), Average Success Rate (LIBERO, SimplerEnv), Inference Speed (tokens/s)\",\n    \"sota_details\": \"\u5728CALVIN ABCD\u2192D\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5e73\u5747\u6210\u529f\u957f\u5ea6\u8fbe\u52304.64\uff0c\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u3002\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6574\u4f53\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523092.7%\uff0c\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u3002\u5728SimplerEnv-WidowX\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6574\u4f53\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523062.5%\uff0c\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u3002\u63a8\u7406\u901f\u5ea6\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u5feb4\u500d\u3002\"\n  },\n  \"reason\": \"\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6269\u6563VLA\uff08UD-VLA\uff09\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u79bb\u6563\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff08JD3P\uff09\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u534f\u540c\u4f18\u5316\u3002JD3P\u5728\u5355\u6b21\u53bb\u566a\u8f68\u8ff9\u4e2d\u6574\u5408\u4e86\u591a\u79cd\u6a21\u6001\uff0c\u4f7f\u7406\u89e3\u3001\u751f\u6210\u548c\u884c\u52a8\u80fd\u591f\u5185\u5728\u534f\u540c\u3002\u8bba\u6587\u5728\u7edf\u4e00\u7684\u591a\u6a21\u6001token\u7a7a\u95f4\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u57fa\u7840\u4e0a\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u591a\u79cd\u63a8\u7406\u65f6\u6280\u672f\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\u3002\\n\\n**\u81ea\u52a8\u9a7e\u9a76\u5173\u8054\u6027\u5224\u65ad\uff1a** \u672c\u7814\u7a76\u7684\u6838\u5fc3\u65b9\u5411\u662f\u9762\u5411\u5177\u8eab\u667a\u80fd\u4f53\uff08Embodied Agent\uff09\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6765\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5e76\u6267\u884c\u76f8\u5e94\u52a8\u4f5c\u3002\u5176\u5e94\u7528\u573a\u666f\u4e3b\u8981\u662f\u673a\u68b0\u81c2\u64cd\u4f5c\uff0c\u800c\u975e\u76f4\u63a5\u7684\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u3002\u867d\u7136VLA\u6a21\u578b\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u9884\u6d4b\u3001\u89c4\u5212\u7b49\u6982\u5ff5\u4e0e\u81ea\u52a8\u9a7e\u9a76\u6709\u9ad8\u5c42\u6b21\u7684\u5171\u901a\u4e4b\u5904\uff0c\u4f46\u5177\u4f53\u4efb\u52a1\u3001\u6570\u636e\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u76ee\u6807\u573a\u666f\u4e0e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u6839\u636e\u8bc4\u5ba1\u8981\u6c42\uff0c\u5982\u679c\u4e0d\u76f4\u63a5\u548c\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\uff0c\u6700\u9ad8\u8bc4\u5206\u4e3a5\u5206\u3002\\n\\n**\u57fa\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u8bc4\u4ef7\uff08\u4f46\u5206\u6570\u53d7\u9650\u4e8e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u6027\uff09\uff1a**\\n*   **\u521b\u65b0\u6027\uff08\u9ad8\uff09\uff1a** \u8bba\u6587\u63d0\u51fa\u7684JD3P\u901a\u8fc7\u540c\u6b65\u53bb\u566a\u8fc7\u7a0b\u8054\u5408\u4f18\u5316\u89c6\u89c9\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ee5\u53ca\u5728\u7edf\u4e00token\u7a7a\u95f4\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u7684\u8bbe\u8ba1\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u521b\u65b0\u6027\u3002\u8fd9\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u4e2d\u6a21\u6001\u5206\u79bb\u6216\u751f\u6210\u4e0e\u884c\u52a8\u89e3\u8026\u7684\u5c40\u9650\u6027\u3002\u672a\u6765\u56fe\u50cf\u4f5c\u4e3a\u52a8\u4f5c\u9884\u6d4b\u7684\u6307\u5bfc\uff0c\u4ee5\u53ca\u79bb\u6563\u6269\u6563\u6a21\u578b\u5e26\u6765\u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\uff0c\u90fd\u662f\u91cd\u8981\u7684\u4eae\u70b9\u3002\\n*   **\u5b9e\u9a8c\u5b8c\u6574\u6027\uff08\u9ad8\uff09\uff1a** \u8bba\u6587\u5728CALVIN\u3001LIBERO\u3001SimplerEnv\u4e09\u4e2a\u4e3b\u6d41\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5927\u91cf\u73b0\u6709SOTA\u65b9\u6cd5\u8fdb\u884c\u4e86\u8be6\u7ec6\u6bd4\u8f83\u3002\u6d88\u878d\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u3001\u672a\u6765\u56fe\u50cf\u751f\u6210\u3001\u4ee5\u53caJD3P\u76f8\u5bf9\u4e8e\u5176\u4ed6\u89e3\u7801\u673a\u5236\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5b9e\u9a8c\uff0c\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u8bf4\u670d\u529b\u3002\\n*   **\u53ef\u4fe1\u5ea6\uff08\u9ad8\uff09\uff1a** SOTA\u58f0\u79f0\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5f97\u5230\u6570\u636e\u652f\u6301\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u5c24\u5176\u662f\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u5b9e\u73b0\u4e864\u500d\u52a0\u901f\u3002\u5b9e\u9a8c\u7ed3\u679c\u548c\u5206\u6790\u6e05\u6670\u660e\u4e86\uff0c\u4e14\u5bf9\u65b9\u6cd5\u672c\u8eab\u7684\u5c40\u9650\u6027\u4e5f\u6709\u6240\u8ba8\u8bba\uff08\u4f8b\u5982\u56fe\u50cf\u751f\u6210\u4fdd\u771f\u5ea6\uff09\u3002\\n*   **\u884c\u4e1a\u6f5c\u529b\uff08\u9ad8\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\uff09\uff1a** UD-VLA\u5728\u7edf\u4e00\u7406\u89e3\u3001\u751f\u6210\u548c\u884c\u52a8\u65b9\u9762\u5c55\u73b0\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4ee5\u53ca\u663e\u8457\u63d0\u5347\u7684\u63a8\u7406\u6548\u7387\uff0c\u5bf9\u4e8e\u5f00\u53d1\u66f4\u901a\u7528\u3001\u66f4\u9c81\u68d2\u7684\u5177\u8eab\u667a\u80fd\u4f53\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u72b6\u6001\u6765\u6307\u5bfc\u52a8\u4f5c\u89c4\u5212\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u590d\u6742\u3001\u957f\u65f6\u7a0b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002\\n\\n\u7efc\u5408\u6765\u770b\uff0c\u82e5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u8fdb\u884c\u8bc4\u5ba1\uff0c\u8fd9\u4f1a\u662f\u4e00\u7bc7\u9ad8\u8d28\u91cf\u4e14\u5177\u6709\u9ad8\u521b\u65b0\u6027\u7684\u8bba\u6587\u3002\u4f46\u7531\u4e8e\u5176\u4e0e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u76f4\u63a5\u5173\u8054\u6027\u8f83\u4f4e\uff0c\u56e0\u6b64\u4e25\u683c\u6309\u7167\u8981\u6c42\u7ed9\u51fa5\u5206\u7684\u6700\u9ad8\u5206\u3002\"\n}\n```"}, "215": {"title": "Fractional Diffusion Bridge Models", "authors": ["Gabriel Nobis, Maximilian Springenberg, Arina Belova, Rembert Daems, Christoph Knochenhauer, Manfred Opper, Tolga Birdal, Wojciech Samek"], "abstract": "arXiv:2511.01795v1 Announce Type: cross \nWe present Fractional Diffusion Bridge Models (FDBM), a novel generative diffusion bridge framework driven by an approximation of the rich and non-Markovian fractional Brownian motion (fBM). Real stochastic processes exhibit a degree of memory effects (correlations in time), long-range dependencies, roughness and anomalous diffusion phenomena that are not captured in standard diffusion or bridge modeling due to the use of Brownian motion (BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM), we construct FDBM that enable tractable inference while preserving the non-Markovian nature of fBM. We prove the existence of a coupling-preserving generative diffusion bridge and leverage it for future state prediction from paired training data. We then extend our formulation to the Schr\\\"{o}dinger bridge problem and derive a principled loss function to learn the unpaired data translation. We evaluate FDBM on both tasks: predicting future protein conformations from aligned data, and unpaired image translation. In both settings, FDBM achieves superior performance compared to the Brownian baselines, yielding lower root mean squared deviation (RMSD) of C$_\\alpha$ atomic positions in protein structure prediction and lower Fr\\'echet Inception Distance (FID) in unpaired image translation.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.01795", "pdf_url": "https://arxiv.org/pdf/2511.01795.pdf", "is_interesting": false}, "216": {"title": "Coupled quasi-harmonic bases", "authors": ["A. Kovnatsky, M. M. Bronstein, A. M. Bronstein, K. Glashoff, R. Kimmel"], "abstract": "arXiv:1210.0026v2 Announce Type: replace \nThe use of Laplacian eigenbases has been shown to be fruitful in many computer graphics applications. Today, state-of-the-art approaches to shape analysis, synthesis, and correspondence rely on these natural harmonic bases that allow using classical tools from harmonic analysis on manifolds. However, many applications involving multiple shapes are obstacled by the fact that Laplacian eigenbases computed independently on different shapes are often incompatible with each other. In this paper, we propose the construction of common approximate eigenbases for multiple shapes using approximate joint diagonalization algorithms. We illustrate the benefits of the proposed approach on tasks from shape editing, pose transfer, correspondence, and similarity.", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/1210.0026", "pdf_url": "https://arxiv.org/pdf/1210.0026.pdf", "is_interesting": false}, "217": {"title": "Exploring Effective Factors for Improving Visual In-Context Learning", "authors": ["Yanpeng Sun, Qiang Chen, Xiaofan Li, Jian Wang, Jingdong Wang, Zechao Li"], "abstract": "arXiv:2304.04748v2 Announce Type: replace \nThe In-Context Learning (ICL) is to understand a new task via a few demonstrations (aka. prompt) and predict new inputs without tuning the models. While it has been widely studied in NLP, it is still a relatively new area of research in computer vision. To reveal the factors influencing the performance of visual in-context learning, this paper shows that prompt selection and prompt fusion are two major factors that have a direct impact on the inference performance of visual context learning. Prompt selection is the process of identifying the most appropriate prompt or example to help the model understand new tasks. This is important because providing the model with relevant prompts can help it learn more effectively and efficiently. Prompt fusion involves combining knowledge from different positions within the large-scale visual model. By doing this, the model can leverage the diverse knowledge stored in different parts of the model to improve its performance on new tasks. Based these findings, we propose a simple framework prompt-SelF for visual in-context learning. Specifically, we first use the pixel-level retrieval method to select a suitable prompt, and then use different prompt fusion methods to activate all the knowledge stored in the large-scale model, and finally ensemble the prediction results obtained from different prompt fusion methods to obtain the final prediction results. And we conduct extensive experiments on single-object segmentation and detection tasks to demonstrate the effectiveness of prompt-SelF. Remarkably, the prompt-SelF has outperformed OSLSM based meta-learning in 1-shot segmentation for the first time. This indicated the great potential of visual in-context learning. The source code and models will be available at https://github.com/syp2ysy/prompt-SelF.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2304.04748", "pdf_url": "https://arxiv.org/pdf/2304.04748.pdf", "is_interesting": false}, "218": {"title": "DeGMix: Efficient Multi-Task Dense Prediction with Deformable and Gating Mixer", "authors": ["Yangyang Xu, Yibo Yang, Bernard Ghanem, Lefei Zhang, Bo Du, Jun Zhu"], "abstract": "arXiv:2308.05721v5 Announce Type: replace \nConvolution neural networks and Transformers have their own advantages and both have been widely used for dense prediction in multi-task learning (MTL). Existing studies typically employ either CNNs (effectively capture local spatial patterns) or Transformers (capturing long-range dependencies) independently, but integrating their strengths may yield more robust models. In this work, we present an efficient MTL model that combines the adaptive capabilities of deformable CNN and query-based Transformer with shared gating for MTL of dense prediction. This combination may offer a simple and efficient solution owing to its powerful and flexible task-specific learning and the advantages of lower cost, less complexity, and smaller parameters than traditional MTL methods. We introduce an efficient multi-task dense prediction with deformable and gating mixer (DeGMix). First, the deformable mixer encoder contains two types of operators: the channel-aware mixing operator leveraged to allow communication among different channels, and the spatial-aware deformable operator with deformable convolution applied to efficiently sample more informative spatial locations. Second, the task-aware gating transformer decoder is used to perform task-specific predictions, in which task interaction block integrated with self-attention is applied to capture task interaction features, and the task query block integrated with gating attention is leveraged to dynamically select the corresponding task-specific features. Furthermore, the results of the experiment demonstrate that the proposed DeGMix uses fewer GFLOPs and significantly outperforms current Transformer-based and CNN-based competitive models on a variety of metrics on three dense prediction datasets. Our code and models are available at https://github.com/yangyangxu0/DeMTG.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2308.05721", "pdf_url": "https://arxiv.org/pdf/2308.05721.pdf", "is_interesting": false}, "219": {"title": "HAT: Hybrid Attention Transformer for Image Restoration", "authors": ["Xiangyu Chen, Xintao Wang, Wenlong Zhang, Xiangtao Kong, Yu Qiao, Jiantao Zhou, Chao Dong"], "abstract": "arXiv:2309.05239v3 Announce Type: replace \nTransformer-based methods have shown impressive performance in image restoration tasks, such as image super-resolution and denoising. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better restoration, we propose a new Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to further exploit the potential of the model for further improvement. Extensive experiments have demonstrated the effectiveness of the proposed modules. We further scale up the model to show that the performance of the SR task can be greatly improved. Besides, we extend HAT to more image restoration applications, including real-world image super-resolution, Gaussian image denoising and image compression artifacts reduction. Experiments on benchmark and real-world datasets demonstrate that our HAT achieves state-of-the-art performance both quantitatively and qualitatively. Codes and models are publicly available at https://github.com/XPixelGroup/HAT.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2309.05239", "pdf_url": "https://arxiv.org/pdf/2309.05239.pdf", "is_interesting": false}, "220": {"title": "Targeted Attack Improves Protection against Unauthorized Diffusion Customization", "authors": ["Boyang Zheng, Chumeng Liang, Xiaoyu Wu"], "abstract": "arXiv:2310.04687v5 Announce Type: replace \nDiffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization. Our code is available on GitHub: https://github.com/psyker-team/mist-v2.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2310.04687", "pdf_url": "https://arxiv.org/pdf/2310.04687.pdf", "is_interesting": false}, "221": {"title": "Balancing Efficiency and Quality: MoEISR for Arbitrary-Scale Image Super-Resolution", "authors": ["Young Jae Oh, Jihun Kim, Jihoon Nam, Tae Hyun Kim"], "abstract": "arXiv:2311.12077v2 Announce Type: replace \nArbitrary-scale image super-resolution employing implicit neural functions has gained significant attention lately due to its capability to upscale images across diverse scales utilizing only a single model. Nevertheless, these methodologies have imposed substantial computational demands as they involve querying every target pixel to a single resource-intensive decoder. In this paper, we introduce a novel and efficient framework, the Mixture-of-Experts Implicit Super-Resolution (MoEISR), which enables super-resolution at arbitrary scales with significantly increased computational efficiency without sacrificing reconstruction quality. MoEISR dynamically allocates the most suitable decoding expert to each pixel using a lightweight mapper module, allowing experts with varying capacities to reconstruct pixels across regions with diverse complexities. Our experiments demonstrate that MoEISR successfully reduces significant amount of floating point operations (FLOPs) while delivering comparable or superior peak signal-to-noise ratio (PSNR).", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2311.12077", "pdf_url": "https://arxiv.org/pdf/2311.12077.pdf", "is_interesting": false}, "222": {"title": "VRP-SAM: SAM with Visual Reference Prompt", "authors": ["Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Xiaofan Li, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li"], "abstract": "arXiv:2402.17726v4 Announce Type: replace \nIn this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation. The source code and models will be available at https://github.com/syp2ysy/VRP-SAM", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2402.17726", "pdf_url": "https://arxiv.org/pdf/2402.17726.pdf", "is_interesting": false}, "223": {"title": "OpenMaterial: A Large-scale Dataset of Complex Materials for 3D Reconstruction", "authors": ["Zheng Dang, Jialu Huang, Fei Wang, Mathieu Salzmann"], "abstract": "arXiv:2406.08894v2 Announce Type: replace \nRecent advances in deep learning, such as neural radiance fields and implicit neural representations, have significantly advanced 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals, glass, and plastics, remains challenging due to the breakdown of multi-view color consistency in the presence of specular reflections, refractions, and transparency. This limitation is further exacerbated by the lack of benchmark datasets that explicitly model material-dependent light transport. To address this, we introduce OpenMaterial, a large-scale semi-synthetic dataset for benchmarking material-aware 3D reconstruction. It comprises 1,001 objects spanning 295 distinct materials, including conductors, dielectrics, plastics, and their roughened variants, captured under 714 diverse lighting conditions. By integrating lab-measured Index of Refraction (IOR) spectra, OpenMaterial enables the generation of high-fidelity multi-view images that accurately simulate complex light-matter interactions. It provides multi-view images, 3D shape models, camera poses, depth maps, and object masks, establishing the first extensive benchmark for evaluating 3D reconstruction on challenging materials. We evaluate 11 state-of-the-art methods for 3D reconstruction and novel view synthesis, conducting ablation studies to assess the impact of material type, shape complexity, and illumination on reconstruction performance. Our results indicate that OpenMaterial provides a strong and fair basis for developing more robust, physically-informed 3D reconstruction techniques to better handle real-world optical complexities.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2406.08894", "pdf_url": "https://arxiv.org/pdf/2406.08894.pdf", "is_interesting": false}, "224": {"title": "Bidirectional Regression for Monocular 6DoF Head Pose Estimation and Reference System Alignment", "authors": ["Sungho Chun, Boeun Kim, Hyung Jin Chang, Ju Yong Chang"], "abstract": "arXiv:2407.14136v2 Announce Type: replace \nPrecise six-degree-of-freedom (6DoF) head pose estimation is crucial for safety-critical applications and human-computer interaction scenarios, yet existing monocular methods still struggle with robust pose estimation. We revisit this problem by introducing TRGv2, a lightweight extension of our previous Translation, Rotation, and Geometry (TRG) network, which explicitly models the bidirectional interaction between facial geometry and head pose. TRGv2 jointly infers facial landmarks and 6DoF pose through an iterative refinement loop with landmark-to-image projection, ensuring metric consistency among face size, rotation, and depth. To further improve generalization to out-of-distribution data, TRGv2 regresses correction parameters instead of directly predicting translation, combining them with a pinhole camera model for analytic depth estimation. In addition, we identify a previously overlooked source of bias in cross-dataset evaluations due to inconsistent head center definitions across different datasets. To address this, we propose a reference system alignment strategy that quantifies and corrects translation bias, enabling fair comparisons across datasets. Extensive experiments on ARKitFace, BIWI, and the challenging DD-Pose benchmarks demonstrate that TRGv2 outperforms state-of-the-art methods in both accuracy and efficiency. Code and newly annotated landmarks for DD-Pose will be publicly available.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2407.14136", "pdf_url": "https://arxiv.org/pdf/2407.14136.pdf", "is_interesting": false}, "225": {"title": "Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets", "authors": ["Aleksander Ogonowski, Micha{\\l} \\.Zebrowski, Arkadiusz \\'Cwiek, Tobiasz Jarosiewicz, Konrad Klimaszewski, Adam Padee, Piotr Wasiuk, Micha{\\l} W\\'ojcik"], "abstract": "arXiv:2407.17339v2 Announce Type: replace \nMost of the intrusion detection methods in computer networks are based on traffic flow characteristics. However, this approach may not fully exploit the potential of deep learning algorithms to directly extract features and patterns from raw packets. Moreover, it impedes real-time monitoring due to the necessity of waiting for the processing pipeline to complete and introduces dependencies on additional software components.\n  In this paper, we investigate deep learning methodologies capable of detecting attacks in real-time directly from raw packet data within network traffic. We propose a novel approach where packets are stacked into windows and separately recognised, with a 2D image representation suitable for processing with computer vision models. Our investigation utilizes the CIC IDS-2017 dataset, which includes both benign traffic and prevalent real-world attacks, providing a comprehensive foundation for our research.", "categories": ["cs.CV", "cs.AI", "cs.CR"], "abs_url": "https://arxiv.org/abs/2407.17339", "pdf_url": "https://arxiv.org/pdf/2407.17339.pdf", "is_interesting": false}, "226": {"title": "Scalable Autoregressive Image Generation with Mamba", "authors": ["Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, Guoqi Li"], "abstract": "arXiv:2408.12245v5 Announce Type: replace \nWe introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mamba's core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at https://github.com/hp-l33/AiM", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2408.12245", "pdf_url": "https://arxiv.org/pdf/2408.12245.pdf", "is_interesting": false}, "227": {"title": "ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions", "authors": ["Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao"], "abstract": "arXiv:2409.18932v4 Announce Type: replace \nImages captured in challenging environments--such as nighttime, smoke, rainy weather, and underwater--often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed ``ReviveDiff'', which can address various degradations and bring images back to life by enhancing and restoring their quality. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2409.18932", "pdf_url": "https://arxiv.org/pdf/2409.18932.pdf", "is_interesting": false}, "228": {"title": "Enhancing Action Recognition by Leveraging the Hierarchical Structure of Actions and Textual Context", "authors": ["Manuel Benavent-Lledo, David Mulero-P\\'erez, David Ortiz-Perez, Jose Garcia-Rodriguez, Antonis Argyros"], "abstract": "arXiv:2410.21275v2 Announce Type: replace \nWe propose a novel approach to improve action recognition by exploiting the hierarchical organization of actions and by incorporating contextualized textual information, including location and previous actions, to reflect the action's temporal context. To achieve this, we introduce a transformer architecture tailored for action recognition that employs both visual and textual features. Visual features are obtained from RGB and optical flow data, while text embeddings represent contextual information. Furthermore, we define a joint loss function to simultaneously train the model for both coarse- and fine-grained action recognition, effectively exploiting the hierarchical nature of actions. To demonstrate the effectiveness of our method, we extend the Toyota Smarthome Untrimmed (TSU) dataset by incorporating action hierarchies, resulting in the Hierarchical TSU dataset, a hierarchical dataset designed for monitoring activities of the elderly in home environments. An ablation study assesses the performance impact of different strategies for integrating contextual and hierarchical data. Experimental results demonstrate that the proposed method consistently outperforms SOTA methods on the Hierarchical TSU dataset, Assembly101 and IkeaASM, achieving over a 17% improvement in top-1 accuracy.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.21275", "pdf_url": "https://arxiv.org/pdf/2410.21275.pdf", "is_interesting": false}, "229": {"title": "A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image Super-Resolution with Transformers and TaylorShift", "authors": ["Sanath Budakegowdanadoddi Nagaraju, Brian Bernhard Moser, Tobias Christian Nauen, Stanislav Frolov, Federico Raue, Andreas Dengel"], "abstract": "arXiv:2411.10231v2 Announce Type: replace \nTransformer-based architectures have recently advanced the image reconstruction quality of super-resolution (SR) models. Yet, their scalability remains limited by quadratic attention costs and coarse patch embeddings that weaken pixel-level fidelity. We propose TaylorIR, a plug-and-play framework that enforces 1x1 patch embeddings for true pixel-wise reasoning and replaces conventional self-attention with TaylorShift, a Taylor-series-based attention mechanism enabling full token interactions with near-linear complexity. Across multiple SR benchmarks, TaylorIR delivers state-of-the-art performance while reducing memory consumption by up to 60%, effectively bridging the gap between fine-grained detail restoration and efficient transformer scaling.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "abs_url": "https://arxiv.org/abs/2411.10231", "pdf_url": "https://arxiv.org/pdf/2411.10231.pdf", "is_interesting": false}, "230": {"title": "TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient and Robust Multi-View 3D Scene Understanding", "authors": ["Quang P. M. Pham, Khoi T. N. Nguyen, Lan C. Ngo, Truong Do, Dezhen Song, Truong-Son Hy"], "abstract": "arXiv:2411.10509v3 Announce Type: replace \nScene graphs have proven to be highly effective for various scene understanding tasks due to their compact and explicit representation of relational information. However, current methods often overlook the critical importance of preserving symmetry when generating scene graphs from 3D point clouds, which can lead to reduced accuracy and robustness, particularly when dealing with noisy, multi-view data. Furthermore, a major limitation of prior approaches is the lack of temporal modeling to capture time-dependent relationships among dynamically evolving entities in a scene. To address these challenges, we propose Temporal Equivariant Scene Graph Neural Network (TESGNN), consisting of two key components: (1) an Equivariant Scene Graph Neural Network (ESGNN), which extracts information from 3D point clouds to generate scene graph while preserving crucial symmetry properties, and (2) a Temporal Graph Matching Network, which fuses scene graphs generated by ESGNN across multiple time sequences into a unified global representation using an approximate graph-matching algorithm. Our combined architecture TESGNN shown to be effective compared to existing methods in scene graph generation, achieving higher accuracy and faster training convergence. Moreover, we show that leveraging the symmetry-preserving property produces a more stable and accurate global scene representation compared to existing approaches. Finally, it is computationally efficient and easily implementable using existing frameworks, making it well-suited for real-time applications in robotics and computer vision. This approach paves the way for more robust and scalable solutions to complex multi-view scene understanding challenges. Our source code is publicly available at: https://github.com/HySonLab/TESGraph", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2411.10509", "pdf_url": "https://arxiv.org/pdf/2411.10509.pdf", "is_interesting": false}, "231": {"title": "V2X-Radar: A Multi-modal Dataset with 4D Radar for Cooperative Perception", "authors": ["Lei Yang, Xinyu Zhang, Jun Li, Chen Wang, Jiaqi Ma, Zhiying Song, Tong Zhao, Ziying Song, Li Wang, Mo Zhou, Yang Shen, Kai Wu, Chen Lv"], "abstract": "arXiv:2411.10962v4 Announce Type: replace \nModern autonomous vehicle perception systems often struggle with occlusions and limited perception range. Previous studies have demonstrated the effectiveness of cooperative perception in extending the perception range and overcoming occlusions, thereby enhancing the safety of autonomous driving. In recent years, a series of cooperative perception datasets have emerged; however, these datasets primarily focus on cameras and LiDAR, neglecting 4D Radar, a sensor used in single-vehicle autonomous driving to provide robust perception in adverse weather conditions. In this paper, to bridge the gap created by the absence of 4D Radar datasets in cooperative perception, we present V2X-Radar, the first large-scale, real-world multi-modal dataset featuring 4D Radar. V2X-Radar dataset is collected using a connected vehicle platform and an intelligent roadside unit equipped with 4D Radar, LiDAR, and multi-view cameras. The collected data encompasses sunny and rainy weather conditions, spanning daytime, dusk, and nighttime, as well as various typical challenging scenarios. The dataset consists of 20K LiDAR frames, 40K camera images, and 20K 4D Radar data, including 350K annotated boxes across five categories. To support various research domains, we have established V2X-Radar-C for cooperative perception, V2X-Radar-I for roadside perception, and V2X-Radar-V for single-vehicle perception. Furthermore, we provide comprehensive benchmarks across these three sub-datasets. We will release all datasets and benchmark codebase at https://huggingface.co/datasets/yanglei18/V2X-Radar and https://github.com/yanglei18/V2X-Radar.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2411.10962", "pdf_url": "https://arxiv.org/pdf/2411.10962.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u591a\u4f20\u611f\u5668\u878d\u5408 / \u8f66\u8def\u534f\u540c", "reason": "The paper presents the V2X-Radar dataset, which includes data from 4D Radar, LiDAR, and cameras for cooperative and roadside perception, directly addressing key tasks in autonomous driving such as sensor fusion and vehicle cooperation in challenging conditions. This is highly relevant to enhancing autonomous vehicle perception and safety, thus strongly related to autonomous driving."}, "review": "```json\n{\n  \"score\": 9.3,\n  \"subfield\": \"\u5408\u4f5c\u611f\u77e5 / \u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b / \u6570\u636e\u96c6\",\n  \"sota_claim\": {\n    \"is_sota\": false,\n    \"dataset\": \"\",\n    \"metrics\": \"\",\n    \"sota_details\": \"\u8bba\u6587\u63d0\u51fa\u4e86V2X-Radar\u6570\u636e\u96c6\uff0c\u5e76\u58f0\u79f0\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b4D\u96f7\u8fbe\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u534f\u540c\u611f\u77e5\u6570\u636e\u96c6\u3002\u8bba\u6587\u6ca1\u6709\u5728\u73b0\u6709\u6570\u636e\u96c6\u4e0a\u5ba3\u79f0SOTA\uff0c\u800c\u662f\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u4e3a\u73b0\u6709\u7b97\u6cd5\u5efa\u7acb\u4e86\u57fa\u51c6\uff08benchmark\uff09\uff0c\u5e76\u5f97\u51fa\u4e86\u5173\u4e8e4D\u96f7\u8fbe\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u534f\u540c\u611f\u77e5\u6027\u80fd\u5f71\u54cd\u7684\u91cd\u8981\u53d1\u73b0\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86V2X-Radar\uff0c\u8fd9\u662f\u9996\u4e2a\u5c064D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5f15\u5165\u534f\u540c\u611f\u77e5\u9886\u57df\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u8fd9\u586b\u8865\u4e86\u73b0\u6709\u534f\u540c\u611f\u77e5\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\u800c\u5ffd\u89c64D\u96f7\u8fbe\u7684\u7a7a\u767d\uff0c\u800c4D\u96f7\u8fbe\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u5177\u6709\u5353\u8d8a\u7684\u611f\u77e5\u9c81\u68d2\u6027\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u5b9e\u9645\u843d\u5730\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u521b\u65b0\u6027\u6781\u5f3a\u3002\\n\\n\u6570\u636e\u96c6\u7684\u6536\u96c6\u8fc7\u7a0b\u975e\u5e38\u4e25\u8c28\uff0c\u901a\u8fc7\u7f51\u8054\u8f66\u8f86\u5e73\u53f0\u548c\u667a\u80fd\u8def\u4fa7\u5355\u5143\uff0c\u914d\u5907\u4e864D\u96f7\u8fbe\u3001\u6fc0\u5149\u96f7\u8fbe\u548c\u591a\u89c6\u89d2\u76f8\u673a\u3002\u6570\u636e\u6db5\u76d6\u4e86\u6674\u5929\u3001\u96e8\u5929\u3001\u96fe\u5929\u3001\u96ea\u5929\u7b49\u591a\u79cd\u5929\u6c14\u6761\u4ef6\uff0c\u4ee5\u53ca\u767d\u5929\u3001\u9ec4\u660f\u3001\u591c\u665a\u7b49\u4e0d\u540c\u65f6\u95f4\u6bb5\uff0c\u573a\u666f\u591a\u6837\u6027\u9ad8\uff0c\u80fd\u591f\u652f\u6301\u5bf9\u9c81\u68d2\u611f\u77e5\u7b97\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u3002\u4f20\u611f\u5668\u540c\u6b65\u548c\u6807\u5b9a\u5de5\u4f5c\u63cf\u8ff0\u8be6\u7ec6\u4e14\u7cbe\u786e\uff0c\u4fdd\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u3002\\n\\n\u5b9e\u9a8c\u90e8\u5206\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6570\u636e\u5206\u6790\uff0c\u8fd8\u4e3a\u5355\u8f66\u7aef\uff08\u8def\u4fa7\u548c\u8f66\u8f7d\uff09\u4ee5\u53ca\u534f\u540c\u611f\u77e5\u4efb\u52a1\u76843D\u76ee\u6807\u68c0\u6d4b\u5efa\u7acb\u4e86\u5168\u9762\u7684\u57fa\u51c6\u3002\u8fd9\u4e9b\u57fa\u51c6\u6db5\u76d6\u4e86\u591a\u79cd\u4e3b\u6d41\u7684\u611f\u77e5\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5173\u952e\u7684\u6d88\u878d\u7814\u7a76\uff1a\u4f8b\u5982\uff0c\u5206\u6790\u4e86\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u534f\u540c\u611f\u77e5\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4ee5\u53ca4D\u96f7\u8fbe\uff08\u7279\u522b\u662f\u591a\u666e\u52d2\u4fe1\u606f\uff09\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u5bf9\u611f\u77e5\u7684\u63d0\u5347\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u7684\u4e92\u8865\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u672a\u6765\u534f\u540c\u611f\u77e5\u7b97\u6cd5\u7684\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u7684\u6307\u5bfc\u610f\u4e49\u3002\\n\\n\u4ece\u884c\u4e1a\u6f5c\u529b\u6765\u770b\uff0c4D\u96f7\u8fbe\u5728\u91cf\u4ea7\u8f66\u4e0a\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u800c\u534f\u540c\u611f\u77e5\u662f\u7a81\u7834\u5355\u8f66\u611f\u77e5\u5c40\u9650\u6027\u7684\u5173\u952e\u3002V2X-Radar\u6570\u636e\u96c6\u5c06\u8fd9\u4e24\u8005\u7ed3\u5408\uff0c\u76f4\u63a5\u670d\u52a1\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5177\u6709\u5de8\u5927\u7684\u5de5\u4e1a\u4ef7\u503c\u3002\u8bba\u6587\u627f\u8bfa\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5e93\uff0c\u5c06\u6781\u5927\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002\u7efc\u5408\u6765\u770b\uff0c\u8fd9\u662f\u4e00\u9879\u9ad8\u8d28\u91cf\u3001\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u529b\u7684\u5de5\u4f5c\u3002\"\n}\n```"}, "232": {"title": "Phys4DGen: Physics-Compliant 4D Generation with Multi-Material Composition Perception", "authors": ["Jiajing Lin, Zhenzhong Wang, Dejun Xu, Shu Jiang, YunPeng Gong, Min Jiang"], "abstract": "arXiv:2411.16800v5 Announce Type: replace \n4D content generation aims to create dynamically evolving 3D content that responds to specific input objects such as images or 3D representations. Current approaches typically incorporate physical priors to animate 3D representations, but these methods suffer from significant limitations: they not only require users lacking physics expertise to manually specify material properties but also struggle to effectively handle the generation of multi-material composite objects. To address these challenges, we propose Phys4DGen, a novel 4D generation framework that integrates multi-material composition perception with physical simulation. The framework achieves automated, physically plausible 4D generation through three innovative modules: first, the 3D Material Grouping module partitions heterogeneous material regions on 3D representations' surfaces via semantic segmentation; second, the Internal Physical Structure Discovery module constructs the mechanical structure of object interiors; finally, we distill physical prior knowledge from multimodal large language models to enable rapid and automatic material properties identification for both objects' surfaces and interiors. Experiments on both synthetic and real-world datasets demonstrate that Phys4DGen can generate high-fidelity 4D content with physical realism in open-world scenarios, significantly outperforming state-of-the-art methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2411.16800", "pdf_url": "https://arxiv.org/pdf/2411.16800.pdf", "is_interesting": false}, "233": {"title": "Gaussian Splashing: Direct Volumetric Rendering Underwater", "authors": ["Nir Mualem, Roy Amoyal, Oren Freifeld, Derya Akkaynak"], "abstract": "arXiv:2411.19588v2 Announce Type: replace \nIn underwater images, most useful features are occluded by water. The extent of the occlusion depends on imaging geometry and can vary even across a sequence of burst images. As a result, 3D reconstruction methods robust on in-air scenes, like Neural Radiance Field methods (NeRFs) or 3D Gaussian Splatting (3DGS), fail on underwater scenes. While a recent underwater adaptation of NeRFs achieved state-of-the-art results, it is impractically slow: reconstruction takes hours and its rendering rate, in frames per second (FPS), is less than 1. Here, we present a new method that takes only a few minutes for reconstruction and renders novel underwater scenes at 140 FPS. Named Gaussian Splashing, our method unifies the strengths and speed of 3DGS with an image formation model for capturing scattering, introducing innovations in the rendering and depth estimation procedures and in the 3DGS loss function. Despite the complexities of underwater adaptation, our method produces images at unparalleled speeds with superior details. Moreover, it reveals distant scene details with far greater clarity than other methods, dramatically improving reconstructed and rendered images. We demonstrate results on existing datasets and a new dataset we have collected.\n  Additional visual results are available at: https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/ .", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2411.19588", "pdf_url": "https://arxiv.org/pdf/2411.19588.pdf", "is_interesting": false}, "234": {"title": "Epistemic Uncertainty for Generated Image Detection", "authors": ["Jun Nie, Yonggang Zhang, Tongliang Liu, Yiu-ming Cheung, Bo Han, Xinmei Tian"], "abstract": "arXiv:2412.05897v2 Announce Type: replace \nWe introduce a novel framework for AI-generated image detection through epistemic uncertainty, aiming to address critical security concerns in the era of generative models. Our key insight stems from the observation that distributional discrepancies between training and testing data manifest distinctively in the epistemic uncertainty space of machine learning models. In this context, the distribution shift between natural and generated images leads to elevated epistemic uncertainty in models trained on natural images when evaluating generated ones. Hence, we exploit this phenomenon by using epistemic uncertainty as a proxy for detecting generated images. This converts the challenge of generated image detection into the problem of uncertainty estimation, underscoring the generalization performance of the model used for uncertainty estimation. Fortunately, advanced large-scale vision models pre-trained on extensive natural images have shown excellent generalization performance for various scenarios. Thus, we utilize these pre-trained models to estimate the epistemic uncertainty of images and flag those with high uncertainty as generated. Extensive experiments demonstrate the efficacy of our method. Code is available at https://github.com/tmlr-group/WePe.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2412.05897", "pdf_url": "https://arxiv.org/pdf/2412.05897.pdf", "is_interesting": false}, "235": {"title": "FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies", "authors": ["Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, Wei Tsang Ooi"], "abstract": "arXiv:2412.06708v3 Announce Type: replace \nEvent cameras offer unparalleled advantages for real-time perception in dynamic environments, thanks to the microsecond-level temporal resolution and asynchronous operation. Existing event detectors, however, are limited by fixed-frequency paradigms and fail to fully exploit the high-temporal resolution and adaptability of event data. To address these limitations, we propose FlexEvent, a novel framework that enables detection at varying frequencies. Our approach consists of two key components: FlexFuse, an adaptive event-frame fusion module that integrates high-frequency event data with rich semantic information from RGB frames, and FlexTune, a frequency-adaptive fine-tuning mechanism that generates frequency-adjusted labels to enhance model generalization across varying operational frequencies. This combination allows our method to detect objects with high accuracy in both fast-moving and static scenarios, while adapting to dynamic environments. Extensive experiments on large-scale event camera datasets demonstrate that our approach surpasses state-of-the-art methods, achieving significant improvements in both standard and high-frequency settings. Notably, our method maintains robust performance when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving its effectiveness in extreme conditions. Our framework sets a new benchmark for event-based object detection and paves the way for more adaptable, real-time vision systems.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2412.06708", "pdf_url": "https://arxiv.org/pdf/2412.06708.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses event-based object detection, which is relevant to real-time perception systems in autonomous driving. The flexibility of adapting detection to varying frequencies and the integration of event data with RGB frames can enhance perception systems in autonomous vehicles, particularly for dynamic environments. While not explicitly focused on autonomous driving, the technology has direct applications in the field."}, "review": "```json\n{\n  \"score\": 9.2,\n  \"subfield\": \"\u4e8b\u4ef6\u76f8\u673a\u611f\u77e5 / \u591a\u4f20\u611f\u5668\u878d\u5408 (\u4e8b\u4ef6-\u5e27) / \u76ee\u6807\u68c0\u6d4b\",\n  \"sota_claim\": {\n    \"is_sota\": true,\n    \"dataset\": \"DSEC-Det, DSEC-Detection, DSEC-MOD\",\n    \"metrics\": \"mAP, AP50, AP75, APS, APM, APL\",\n    \"sota_details\": \"\u5728DSEC-Det\u6570\u636e\u96c6\u4e0amAP\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5DAGr-50 15.5% (57.4% vs 41.9%)\uff0c\u5728DSEC-Detection\u6570\u636e\u96c6\u4e0amAP\u8d85\u8d8aCAFR 9.4% (47.4% vs 38.0%)\uff0c\u5728DSEC-MOD\u6570\u636e\u96c6\u4e0amAP\u8d85\u8d8aRENet 7.9% (36.9% vs 29.0%)\u3002\u572820Hz\u5230180Hz\u7684\u5bbd\u6cdb\u64cd\u4f5c\u9891\u7387\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\uff0c\u5e76\u572820Hz\u523090Hz\u9891\u7387\u53d8\u5316\u4e0b\u6027\u80fd\u4fdd\u630196.2%\u3002\"\n  },\n  \"reason\": \"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlexEvent\u7684\u4e8b\u4ef6-\u5e27\u878d\u5408\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u68c0\u6d4b\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u64cd\u4f5c\u9891\u7387\u56fa\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u7684\u9650\u5236\u3002\u5176\u521b\u65b0\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u4e24\u70b9\uff1a\\n1. FlexFuse\u6a21\u5757\uff1a\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e8b\u4ef6-\u5e27\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u8f6f\u6743\u91cd\u548c\u5f15\u5165\u5b66\u4e60\u566a\u58f0\u7684\u95e8\u63a7\u51fd\u6570\uff0c\u6709\u6548\u5730\u5c06\u9ad8\u9891\u4e8b\u4ef6\u6570\u636e\u4e0eRGB\u5e27\u7684\u8bed\u4e49\u4fe1\u606f\u878d\u5408\uff0c\u514b\u670d\u4e86\u4e8b\u4ef6\u6570\u636e\u5728\u9ad8\u9891\u4e0b\u8bed\u4e49\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002\\n2. FlexTune\u673a\u5236\uff1a\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9891\u7387\u81ea\u9002\u5e94\u5fae\u8c03\u673a\u5236\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u9891\u8c03\u6574\u6807\u7b7e\u548c\u8fed\u4ee3\u81ea\u8bad\u7ec3\uff08\u5305\u62ec\u9ad8\u9891\u81ea\u4e3e\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u6821\u51c6\u548c\u5faa\u73af\u81ea\u8bad\u7ec3\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u624b\u52a8\u9ad8\u9891\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\uff0c\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u64cd\u4f5c\u9891\u7387\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u9891\u7387\u7684\u9c81\u68d2\u68c0\u6d4b\u3002\\n\\n\u5b9e\u9a8c\u90e8\u5206\u975e\u5e38\u5145\u5206\u548c\u4e25\u8c28\uff1a\\n- \u5728DSEC-Det\u3001DSEC-Detection\u548cDSEC-MOD\u4e09\u4e2a\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5747\u4e0e\u8f66\u7aef\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u9ad8\u5ea6\u76f8\u5173\u3002\\n- \u4e0e\u5927\u91cf\u6700\u5148\u8fdb\u7684\u7eaf\u4e8b\u4ef6\u548c\u4e8b\u4ef6-\u5e27\u878d\u5408\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u5305\u62ec\u4e86\u591a\u4e2aCVPR/ICCV/Nature\u7b49\u9876\u4f1a/\u671f\u520a\u7684\u6700\u65b0\u5de5\u4f5c\uff0c\u5e76\u660e\u786e\u6307\u51fa\u4e86\u57fa\u7ebf\u7684\u91cd\u8bad\u7ec3\u548c\u7ed3\u679c\u5f15\u7528\u65b9\u5f0f\uff0c\u786e\u4fdd\u4e86\u5bf9\u6bd4\u7684\u516c\u5e73\u6027\u3002\\n- \u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u6807\u51c6COCO\u6307\u6807\uff08mAP, AP50, AP75, APS, APM, APL\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684SOTA\u6027\u80fd\u63d0\u5347\u3002\\n- \u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u6e05\u6670\u5730\u8bc1\u660e\u4e86FlexFuse\u548cFlexTune\u6a21\u5757\u5404\u81ea\u7684\u8d21\u732e\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u878d\u5408\u7b56\u7565\u548c\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u3002\\n- \u6548\u7387\u5206\u6790\u8868\u660e\uff0cFlexEvent\u867d\u7136\u53c2\u6570\u7565\u591a\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u4e0eSAST\u76f8\u5f53\uff0c\u8fdc\u8d85DAGr\u7b49\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u65f6\u5e94\u7528\u6f5c\u529b\u3002\\n- \u5927\u91cf\u7684\u5b9a\u6027\u7ed3\u679c\u5c55\u793a\u4e86\u5728\u5feb\u901f\u79fb\u52a8\u3001\u906e\u6321\u3001\u4e0d\u540c\u5149\u7167\u548c\u9ad8\u9891\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0cFlexEvent\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u3002\\n\\n\u8be5\u7814\u7a76\u76f4\u63a5\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e8b\u4ef6\u76f8\u673a\u611f\u77e5\u7684\u5173\u952e\u6311\u6218\uff0c\u5373\u5728\u591a\u53d8\u52a8\u6001\u73af\u5883\u4e0b\u4fdd\u6301\u9ad8\u9891\u3001\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u7684\u7269\u4f53\u68c0\u6d4b\u80fd\u529b\u3002FlexFuse\u548cFlexTune\u7684\u521b\u65b0\u6027\u7ed3\u5408\uff0c\u4e0d\u4ec5\u5728\u6280\u672f\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e5f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u901f\u8fd0\u52a8\u3001\u590d\u6742\u5149\u7167\u7b49\u4f20\u7edf\u76f8\u673a\u96be\u4ee5\u80dc\u4efb\u7684\u573a\u666f\uff0c\u4ee5\u53ca\u7f13\u89e3\u9ad8\u9891\u6570\u636e\u6807\u6ce8\u6210\u672c\u65b9\u9762\u3002\u8bba\u6587\u7ed3\u6784\u6e05\u6670\uff0c\u8bba\u8bc1\u624e\u5b9e\uff0c\u6570\u636e\u652f\u6301\u5145\u5206\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u4e8b\u4ef6\u611f\u77e5\u53d1\u5c55\u5177\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002\"\n}\n```"}, "236": {"title": "FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error", "authors": ["Beilin Chu, Xuan Xu, Xin Wang, Yufei Zhang, Weike You, Linna Zhou"], "abstract": "arXiv:2412.07140v3 Announce Type: replace \nThe rapid advancement of diffusion models has significantly improved high-quality image generation, making generated content increasingly challenging to distinguish from real images and raising concerns about potential misuse. In this paper, we observe that diffusion models struggle to accurately reconstruct mid-band frequency information in real images, suggesting the limitation could serve as a cue for detecting diffusion model generated images. Motivated by this observation, we propose a novel method called Frequency-guided Reconstruction Error (FIRE), which, to the best of our knowledge, is the first to investigate the influence of frequency decomposition on reconstruction error. FIRE assesses the variation in reconstruction error before and after the frequency decomposition, offering a robust method for identifying diffusion model generated images. Extensive experiments show that FIRE generalizes effectively to unseen diffusion models and maintains robustness against diverse perturbations.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2412.07140", "pdf_url": "https://arxiv.org/pdf/2412.07140.pdf", "is_interesting": false}, "237": {"title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities", "authors": ["Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Sara Pieri, Saeed Yahya Alseiari, Shanavas Cholakkal, Khaled Aldahmani, Fahad Khan, Rao Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal"], "abstract": "arXiv:2412.07769v2 Announce Type: replace \nWe introduce BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model that supports text-based and image-based medical interactions. It enables multi-turn conversation in Arabic and English and supports diverse medical imaging modalities, including radiology, CT, and histology. To train BiMediX2, we curate BiMed-V, an extensive Arabic-English bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions. This dataset supports a range of medical Large Language Model (LLM) and Large Multimodal Model (LMM) tasks, including multi-turn medical conversations, report generation, and visual question answering (VQA). We also introduce BiMed-MBench, the first Arabic-English medical LMM evaluation benchmark, verified by medical experts. BiMediX2 demonstrates excellent performance across multiple medical LLM and LMM benchmarks, achieving state-of-the-art results compared to other open-sourced models. On BiMed-MBench, BiMediX2 outperforms existing methods by over 9% in English and more than 20% in Arabic evaluations. Additionally, it surpasses GPT-4 by approximately 9% in UPHILL factual accuracy evaluations and excels in various medical VQA, report generation, and report summarization tasks. Our trained models, instruction set, and source code are available at https://github.com/mbzuai-oryx/BiMediX2", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2412.07769", "pdf_url": "https://arxiv.org/pdf/2412.07769.pdf", "is_interesting": false}, "238": {"title": "Multi-scale Latent Point Consistency Models for 3D Shape Generation", "authors": ["Bi'an Du, Wei Hu, Renjie Liao"], "abstract": "arXiv:2412.19413v2 Announce Type: replace \nConsistency Models (CMs) have significantly accelerated the sampling process in diffusion models, yielding impressive results in synthesizing high-resolution images. To explore and extend these advancements to point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework and introduces hierarchical levels of latent representations, ranging from point-level to super-point levels, each corresponding to a different spatial resolution. We design a multi-scale latent integration module along with 3D spatial attention to effectively denoise the point-level latent representations conditioned on those from multiple super-point levels. Additionally, we propose a latent consistency model, learned through consistency distillation, that compresses the prior into a one-step generator. This significantly improves sampling efficiency while preserving the performance of the original teacher model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol demonstrate that MLPCM achieves a 100x speedup in the generation process, while surpassing state-of-the-art diffusion models in terms of both shape quality and diversity.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2412.19413", "pdf_url": "https://arxiv.org/pdf/2412.19413.pdf", "is_interesting": false}, "239": {"title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos", "authors": ["Haobo Yuan, Xiangtai Li, Tao Zhang, Yueyi Sun, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang"], "abstract": "arXiv:2501.04001v3 Announce Type: replace \nThis work presents Sa2VA, the first comprehensive, unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with MLLM, the advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves strong performance across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications. In addition, Sa2VA can be easily extended into various VLMs, including Qwen-VL and Intern-VL, which can be updated with rapid process in current open-sourced VLMs. Code and models have been provided to the community.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2501.04001", "pdf_url": "https://arxiv.org/pdf/2501.04001.pdf", "is_interesting": false}, "240": {"title": "BEN: Using Confidence-Guided Matting for Dichotomous Image Segmentation", "authors": ["Maxwell Meyer, Jack Spruyt"], "abstract": "arXiv:2501.06230v2 Announce Type: replace \nCurrent approaches to dichotomous image segmentation (DIS) treat image matting and object segmentation as fundamentally different tasks. As improvements in image segmentation become increasingly challenging to achieve, combining image matting and grayscale segmentation techniques offers promising new directions for architectural innovation. Inspired by the possibility of aligning these two model tasks, we propose a new architectural approach for DIS called Confidence-Guided Matting (CGM). We created the first CGM model called Background Erase Network (BEN). BEN consists of two components: BEN Base for initial segmentation and BEN Refiner for confidence-based refinement. Our approach achieves substantial improvements over current state-of-the-art methods on the DIS5K validation dataset, demonstrating that matting-based refinement can significantly enhance segmentation quality. This work introduces a new paradigm for integrating matting and segmentation techniques, improving fine-grained object boundary prediction in computer vision.", "categories": ["cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2501.06230", "pdf_url": "https://arxiv.org/pdf/2501.06230.pdf", "is_interesting": false}, "241": {"title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework", "authors": ["Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu"], "abstract": "arXiv:2501.12263v3 Announce Type: replace \nCollaborative perception significantly enhances individual vehicle perception performance through the exchange of sensory information among agents. However, real-world deployment faces challenges due to bandwidth constraints and inevitable calibration errors during information exchange. To address these issues, we propose mmCooper, a novel multi-agent, multi-stage, communication-efficient, and collaboration-robust cooperative perception framework. Our framework leverages a multi-stage collaboration strategy that dynamically and adaptively balances intermediate- and late-stage information to share among agents, enhancing perceptual performance while maintaining communication efficiency. To support robust collaboration despite potential misalignments and calibration errors, our framework prevents misleading low-confidence sensing information from transmission and refines the received detection results from collaborators to improve accuracy. The extensive evaluation results on both real-world and simulated datasets demonstrate the effectiveness of the mmCooper framework and its components.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2501.12263", "pdf_url": "https://arxiv.org/pdf/2501.12263.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u591a\u4f20\u611f\u5668\u878d\u5408 / \u8f66\u8def\u534f\u540c", "reason": "The paper discusses a multi-agent cooperative perception framework that improves vehicle perception through communication and collaboration. This approach is highly relevant to autonomous driving as it addresses challenges in cooperative perception, such as bandwidth constraints and calibration errors, which are directly related to sensor fusion and vehicle collaboration in autonomous systems."}}, "242": {"title": "A Study in Dataset Distillation for Image Super-Resolution", "authors": ["Tobias Dietz, Brian B. Moser, Tobias Nauen, Federico Raue, Stanislav Frolov, Andreas Dengel"], "abstract": "arXiv:2502.03656v2 Announce Type: replace \nDataset distillation aims to compress large datasets into compact yet highly informative subsets that preserve the training behavior of the original data. While this concept has gained traction in classification, its potential for image Super-Resolution (SR) remains largely untapped. In this work, we conduct the first systematic study of dataset distillation for SR, evaluating both pixel- and latent-space formulations. We show that a distilled dataset, occupying only 8.88% of the original size, can train SR models that retain nearly the same reconstruction fidelity as those trained on full datasets. Furthermore, we analyze how initialization strategies and distillation objectives affect efficiency, convergence, and visual quality. Our findings highlight the feasibility of SR dataset distillation and establish foundational insights for memory- and compute-efficient generative restoration models.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.03656", "pdf_url": "https://arxiv.org/pdf/2502.03656.pdf", "is_interesting": false}, "243": {"title": "SurGen: 1020 H&E-stained Whole Slide Images With Survival and Genetic Markers", "authors": ["Craig Myles, In Hwa Um, Craig Marshall, David Harris-Birtill, David J. Harrison"], "abstract": "arXiv:2502.04946v2 Announce Type: replace \nCancer remains one of the leading causes of morbidity and mortality worldwide. Comprehensive datasets that combine histopathological images with genetic and survival data across various tumour sites are essential for advancing computational pathology and personalised medicine. We present SurGen, a dataset comprising 1,020 H&amp;E-stained whole-slide images (WSIs) from 843 colorectal cancer cases. The dataset includes detailed annotations for key genetic mutations (KRAS, NRAS, BRAF) and mismatch repair status, as well as survival data for 426 cases. We illustrate SurGen's utility with a proof-of-concept model that predicts mismatch repair status directly from WSIs, achieving a test area under the receiver operating characteristic curve of 0.8273. These preliminary results underscore the dataset's potential to facilitate research in biomarker discovery, prognostic modelling, and advanced machine learning applications in colorectal cancer and beyond. SurGen offers a valuable resource for the scientific community, enabling studies that require high-quality WSIs linked with comprehensive clinical and genetic information on colorectal cancer. Our initial findings affirm the dataset's capacity to advance diagnostic precision and foster the development of personalised treatment strategies in colorectal oncology. Data available online: https://doi.org/10.6019/S-BIAD1285.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2502.04946", "pdf_url": "https://arxiv.org/pdf/2502.04946.pdf", "is_interesting": false}, "244": {"title": "MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification", "authors": ["Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild"], "abstract": "arXiv:2502.07409v5 Announce Type: replace \nWhole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.07409", "pdf_url": "https://arxiv.org/pdf/2502.07409.pdf", "is_interesting": false}, "245": {"title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation", "authors": ["Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, Lijuan Wang, Min Li"], "abstract": "arXiv:2502.07870v2 Announce Type: replace \nText-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2502.07870", "pdf_url": "https://arxiv.org/pdf/2502.07870.pdf", "is_interesting": false}, "246": {"title": "A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing", "authors": ["Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi"], "abstract": "arXiv:2502.14068v2 Announce Type: replace \nA significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at https://github.com/ghosh64/RaceGAN.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "abs_url": "https://arxiv.org/abs/2502.14068", "pdf_url": "https://arxiv.org/pdf/2502.14068.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "\u611f\u77e5 / \u8f68\u8ff9\u68c0\u6d4b / \u81ea\u52a8\u9a7e\u9a76\u7ade\u901f", "reason": "The paper introduces a dataset and baseline model for track detection in autonomous racing scenarios using multi-camera perception data collected from an autonomous race car (Dallara AV-21) in the Indy Autonomous Challenge. It directly focuses on perception for autonomous vehicles in high-speed racing environments, making it fully within the autonomous driving domain."}}, "247": {"title": "Surgical Scene Understanding in the Era of Foundation AI Models: A Comprehensive Review", "authors": ["Ufaq Khan, Umair Nawaz, Adnan Qayyum, Shazad Ashraf, Yutong Xie, Muhammad Haris Khan, Muhammad Bilal, Junaid Qadir"], "abstract": "arXiv:2502.14886v2 Announce Type: replace \nRecent advancements in machine learning (ML) and deep learning (DL), particularly through the introduction of Foundation Models (FMs), have significantly enhanced surgical scene understanding within minimally invasive surgery (MIS). This paper surveys the integration of state-of-the-art ML and DL technologies, including Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Foundation Models like the Segment Anything Model (SAM), into surgical workflows. These technologies improve segmentation accuracy, instrument tracking, and phase recognition in surgical scene understanding. The paper explores the challenges these technologies face, such as data variability and computational demands, and discusses ethical considerations and integration hurdles in clinical settings. Highlighting the roles of FMs, we bridge the technological capabilities with clinical needs and outline future research directions to enhance the adaptability, efficiency, and ethical alignment of AI applications in surgery. Our findings suggest that substantial progress has been made; however, more focused efforts are required to achieve seamless integration of these technologies into clinical workflows, ensuring they complement surgical practice by enhancing precision, reducing risks, and optimizing patient outcomes.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2502.14886", "pdf_url": "https://arxiv.org/pdf/2502.14886.pdf", "is_interesting": false}, "248": {"title": "New multimodal similarity measure for image registration via modeling local functional dependence with linear combination of learned basis functions", "authors": ["Joel Honkamaa, Pekka Marttinen"], "abstract": "arXiv:2503.05335v2 Announce Type: replace \nThe deformable registration of images of different modalities, essential in many medical imaging applications, remains challenging. The main challenge is developing a robust measure for image overlap despite the compared images capturing different aspects of the underlying tissue. Here, we explore similarity metrics based on functional dependence between intensity values of registered images. Although functional dependence is too restrictive on the global scale, earlier work has shown competitive performance in deformable registration when such measures are applied over small enough contexts. We confirm this finding and further develop the idea by modeling local functional dependence via the linear basis function model with the basis functions learned jointly with the deformation. The measure can be implemented via convolutions, making it efficient to compute on GPUs. We release the method as an easy-to-use tool and show good performance on three datasets compared to well-established baseline and earlier functional dependence-based methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2503.05335", "pdf_url": "https://arxiv.org/pdf/2503.05335.pdf", "is_interesting": false}, "249": {"title": "A Woman with a Knife or A Knife with a Woman? Measuring Directional Bias Amplification in Image Captions", "authors": ["Rahul Nair, Bhanu Tokas, Hannah Kerner"], "abstract": "arXiv:2503.07878v4 Announce Type: replace \nWhen we train models on biased datasets, they not only reproduce data biases, but can worsen them at test time - a phenomenon called bias amplification. Many of the current bias amplification metrics (e.g., BA (MALS), DPA) measure bias amplification only in classification datasets. These metrics are ineffective for image captioning datasets, as they cannot capture the language semantics of a caption. Recent work introduced Leakage in Captioning (LIC), a language-aware bias amplification metric that understands caption semantics. However, LIC has a crucial limitation: it cannot identify the source of bias amplification in captioning models. We propose Directional Bias Amplification in Captioning (DBAC), a language-aware and directional metric that can identify when captioning models amplify biases. DBAC has two more improvements over LIC: (1) it is less sensitive to sentence encoders (a hyperparameter in language-aware metrics), and (2) it provides a more accurate estimate of bias amplification in captions. Our experiments on gender and race attributes in the COCO captions dataset show that DBAC is the only reliable metric to measure bias amplification in captions.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.07878", "pdf_url": "https://arxiv.org/pdf/2503.07878.pdf", "is_interesting": false}, "250": {"title": "AdaSCALE: Adaptive Scaling for OOD Detection", "authors": ["Sudarshan Regmi"], "abstract": "arXiv:2503.08023v2 Announce Type: replace \nThe ability of the deep learning model to recognize when a sample falls outside its learned distribution is critical for safe and reliable deployment. Recent state-of-the-art out-of-distribution (OOD) detection methods leverage activation shaping to improve the separation between in-distribution (ID) and OOD inputs. These approaches resort to sample-specific scaling but apply a static percentile threshold across all samples regardless of their nature, resulting in suboptimal ID-OOD separability. In this work, we propose \\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the percentile threshold based on a sample's estimated OOD likelihood. This estimation leverages our key observation: OOD samples exhibit significantly more pronounced activation shifts at high-magnitude activations under minor perturbation compared to ID samples. AdaSCALE enables stronger scaling for likely ID samples and weaker scaling for likely OOD samples, yielding highly separable energy scores. Our approach achieves state-of-the-art OOD detection performance, outperforming the latest rival OptFS by 14.94% in near-OOD and 21.67% in far-OOD datasets in average FPR@95 metric on the ImageNet-1k benchmark across eight diverse architectures. The code is available at: https://github.com/sudarshanregmi/AdaSCALE/", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2503.08023", "pdf_url": "https://arxiv.org/pdf/2503.08023.pdf", "is_interesting": false}, "251": {"title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind", "authors": ["Junbin Xiao, Nanxin Huang, Hao Qiu, Zhulin Tao, Xun Yang, Richang Hong, Meng Wang, Angela Yao"], "abstract": "arXiv:2503.08221v4 Announce Type: replace \nWe present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,392 first-person videos from the daily lives of blind and visually impaired individuals. It also features 5,311 questions directly posed or verified by the blind to reflect their in-situation needs for visual assistance. Each question has an average of 3 manually annotated reference answers to reduce subjectiveness. Using EgoBlind, we comprehensively evaluate 16 advanced MLLMs and find that all models struggle. The best performers achieve an accuracy near 60\\%, which is far behind human performance of 87.4\\%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and explore heuristic solutions for improvement. With these efforts, we hope that EgoBlind will serve as a foundation for developing effective AI assistants to enhance the independence of the blind and visually impaired. Data and code are available at https://github.com/doc-doc/EgoBlind.", "categories": ["cs.CV", "cs.AI", "cs.MM"], "abs_url": "https://arxiv.org/abs/2503.08221", "pdf_url": "https://arxiv.org/pdf/2503.08221.pdf", "is_interesting": false}, "252": {"title": "LocDiff: Identifying Locations on Earth by Diffusing in the Hilbert Space", "authors": ["Zhangyu Wang, Zeping Liu, Jielu Zhang, Zhongliang Zhou, Qian Cao, Nemin Wu, Lan Mu, Yang Song, Yiqun Xie, Ni Lao, Gengchen Mai"], "abstract": "arXiv:2503.18142v2 Announce Type: replace \nImage geolocalization is a fundamental yet challenging task, aiming at inferring the geolocation on Earth where an image is taken. State-of-the-art methods employ either grid-based classification or gallery-based image-location retrieval, whose spatial generalizability significantly suffers if the spatial distribution of test im- ages does not align with the choices of grids and galleries. Recently emerging generative approaches, while getting rid of grids and galleries, use raw geographical coordinates and suffer quality losses due to their lack of multi-scale information. To address these limitations, we propose a multi-scale latent diffusion model called LocDiff for image geolocalization. We developed a novel positional encoding-decoding framework called Spherical Harmonics Dirac Delta (SHDD) Representations, which encodes points on a spherical surface (e.g., geolocations on Earth) into a Hilbert space of Spherical Harmonics coefficients and decodes points (geolocations) by mode-seeking on spherical probability distributions. We also propose a novel SirenNet-based architecture (CS-UNet) to learn an image-based conditional backward process in the latent SHDD space by minimizing a latent KL-divergence loss. To the best of our knowledge, LocDiff is the first image geolocalization model that performs latent diffusion in a multi-scale location encoding space and generates geolocations under the guidance of images. Experimental results show that LocDiff can outperform all state-of-the-art grid-based, retrieval-based, and diffusion-based baselines across 5 challenging global-scale image geolocalization datasets, and demonstrates significantly stronger generalizability to unseen geolocations.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.18142", "pdf_url": "https://arxiv.org/pdf/2503.18142.pdf", "is_interesting": false}, "253": {"title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion", "authors": ["Pihai Sun, Junjun Jiang, Yuanqi Yao, Youyu Chen, Wenbo Zhao, Kui Jiang, Xianming Liu"], "abstract": "arXiv:2503.19739v3 Announce Type: replace \nImage-event joint depth estimation methods leverage complementary modalities for robust perception, yet face challenges in generalizability stemming from two factors: 1) limited annotated image-event-depth datasets causing insufficient cross-modal supervision, and 2) inherent frequency mismatches between static images and dynamic event streams with distinct spatiotemporal patterns, leading to ineffective feature fusion. To address this dual challenge, we propose Frequency-decoupled Unified Self-supervised Encoder (FUSE) with two synergistic components: The Parameter-efficient Self-supervised Transfer (PST) establishes cross-modal knowledge transfer through latent space alignment with image foundation models, effectively mitigating data scarcity by enabling joint encoding without depth ground truth. Complementing this, we propose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple high-frequency edge features from low-frequency structural components, resolving modality-specific frequency mismatches through physics-aware fusion. This combined approach enables FUSE to construct a universal image-event encoder that only requires lightweight decoder adaptation for target datasets. Extensive experiments demonstrate state-of-the-art performance with 14% and 24.9% improvements in Abs .Rel on MVSEC and DENSE datasets. The framework exhibits remarkable zero-shot adaptability to challenging scenarios including extreme lighting and motion blur, significantly advancing real-world deployment capabilities. The source code for our method is publicly available at: https://github.com/sunpihai-up/FUSE", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2503.19739", "pdf_url": "https://arxiv.org/pdf/2503.19739.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.3, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on image-event joint depth estimation using a self-supervised encoder, which is primarily a computer vision technique. While the method could potentially be applied in autonomous driving for depth perception, it is not explicitly related to tasks such as vehicle perception, control, or decision-making in autonomous driving systems. The work addresses challenges in image-event modality fusion and frequency mismatches, which may have indirect relevance but is not directly tied to autonomous driving tasks."}}, "254": {"title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound", "authors": ["Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni"], "abstract": "arXiv:2503.20685v4 Announce Type: replace \nAccurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.20685", "pdf_url": "https://arxiv.org/pdf/2503.20685.pdf", "is_interesting": false}, "255": {"title": "SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting", "authors": ["Advaith V. Sethuraman, Max Rucker, Onur Bagoren, Pou-Chun Kung, Nibarkavi N. B. Amutha, Katherine A. Skinner"], "abstract": "arXiv:2504.00159v3 Announce Type: replace \nIn this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (77% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.00159", "pdf_url": "https://arxiv.org/pdf/2504.00159.pdf", "is_interesting": false}, "256": {"title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers", "authors": ["Qianhao Yuan, Qingyu Zhang, Yanjiang Liu, Jiawei Chen, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun"], "abstract": "arXiv:2504.00502v2 Announce Type: replace \nMultimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2504.00502", "pdf_url": "https://arxiv.org/pdf/2504.00502.pdf", "is_interesting": false}, "257": {"title": "OpenFACADES: An Open Framework for Architectural Caption and Attribute Data Enrichment via Street View Imagery", "authors": ["Xiucheng Liang, Jinheng Xie, Tianhong Zhao, Rudi Stouffs, Filip Biljecki"], "abstract": "arXiv:2504.02866v2 Announce Type: replace \nBuilding properties, such as height, usage, and material, play a crucial role in spatial data infrastructures, supporting various urban applications. Despite their importance, comprehensive building attribute data remain scarce in many urban areas. Recent advances have enabled the extraction of objective building attributes using remote sensing and street-level imagery. However, establishing a pipeline that integrates diverse open datasets, acquires holistic building imagery, and infers comprehensive building attributes at scale remains a significant challenge. Among the first, this study bridges the gaps by introducing OpenFACADES, an open framework that leverages multimodal crowdsourced data to enrich building profiles with both objective attributes and semantic descriptors through multimodal large language models. First, we integrate street-level image metadata from Mapillary with OpenStreetMap geometries via isovist analysis, identifying images that provide suitable vantage points for observing target buildings. Second, we automate the detection of building facades in panoramic imagery and tailor a reprojection approach to convert objects into holistic perspective views that approximate real-world observation. Third, we introduce an innovative approach that harnesses and investigates the capabilities of open-source large vision-language models (VLMs) for multi-attribute prediction and open-vocabulary captioning in building-level analytics, leveraging a globally sourced dataset of 31,180 labeled images from seven cities. Evaluation shows that fine-tuned VLM excel in multi-attribute inference, outperforming single-attribute computer vision models and zero-shot ChatGPT-4o. Further experiments confirm its superior generalization and robustness across culturally distinct region and varying image conditions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.02866", "pdf_url": "https://arxiv.org/pdf/2504.02866.pdf", "is_interesting": false}, "258": {"title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "authors": ["Zhiqiang Wang, Pengbin Feng, Yanbin Lin, Shuzhang Cai, Zongao Bian, Jinghua Yan, Xingquan Zhu"], "abstract": "arXiv:2504.03724v2 Announce Type: replace \nWe propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that integrates Group Relative Policy Optimization (GRPO) with a fuzzy reward function to enhance learning efficiency. Unlike the conventional binary 0/1 accuracy reward, our fuzzy reward model provides nuanced incentives, encouraging more precise outputs. Experimental results demonstrate that GRPO with a standard 0/1 accuracy reward underperforms compared to supervised fine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B), surpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across five in-domain datasets. On an out-of-domain dataset, FGRPR achieves performance comparable to SFT but excels when target values are larger, as its fuzzy reward function assigns higher rewards to closer approximations. This approach is broadly applicable to tasks where the precision of the answer is critical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2504.03724", "pdf_url": "https://arxiv.org/pdf/2504.03724.pdf", "is_interesting": false}, "259": {"title": "LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking", "authors": ["Mert Asim Karaoglu, Wenbo Ji, Ahmed Abbas, Nassir Navab, Benjamin Busam, Alexander Ladikos"], "abstract": "arXiv:2504.09904v2 Announce Type: replace \nTissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requirements necessary for real-time surgical applications. To address this limitation, we propose LiteTracker, a low-latency method for tissue tracking in endoscopic video streams. LiteTracker builds on a state-of-the-art long-term point tracking method, and introduces a set of training-free runtime optimizations. These optimizations enable online, frame-by-frame tracking by leveraging a temporal memory buffer for efficient feature reuse and utilizing prior motion for accurate track initialization. LiteTracker demonstrates significant runtime improvements being around 7x faster than its predecessor and 2x than the state-of-the-art. Beyond its primary focus on efficiency, LiteTracker delivers high-accuracy tracking and occlusion prediction, performing competitively on both the STIR and SuPer datasets. We believe LiteTracker is an important step toward low-latency tissue tracking for real-time surgical applications in the operating room. Our code is publicly available at https://github.com/ImFusionGmbH/lite-tracker.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.09904", "pdf_url": "https://arxiv.org/pdf/2504.09904.pdf", "is_interesting": false}, "260": {"title": "Efficient Remote Sensing Change Detection with Change State Space Models", "authors": ["Elman Ghazaei, Erchan Aptoula"], "abstract": "arXiv:2504.11080v2 Announce Type: replace \nDespite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.11080", "pdf_url": "https://arxiv.org/pdf/2504.11080.pdf", "is_interesting": false}, "261": {"title": "SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex Instruction Understanding", "authors": ["Qianqian Sun, Jixiang Luo, Dell Zhang, Xuelong Li"], "abstract": "arXiv:2504.12704v2 Announce Type: replace \nRecent advancements in image editing have utilized large-scale multimodal models to enable intuitive, natural instruction-driven interactions. However, conventional methods still face significant challenges, particularly in spatial reasoning, precise region segmentation, and maintaining semantic consistency, especially in complex scenes. To overcome these challenges, we introduce SmartFreeEdit, a novel end-to-end framework that integrates a multimodal large language model (MLLM) with a hypergraph-enhanced inpainting architecture, enabling precise, mask-free image editing guided exclusively by natural language instructions. The key innovations of SmartFreeEdit include:(1)the introduction of region aware tokens and a mask embedding paradigm that enhance the spatial understanding of complex scenes;(2) a reasoning segmentation pipeline designed to optimize the generation of editing masks based on natural language instructions;and (3) a hypergraph-augmented inpainting module that ensures the preservation of both structural integrity and semantic coherence during complex edits, overcoming the limitations of local-based image generation. Extensive experiments on the Reason-Edit benchmark demonstrate that SmartFreeEdit surpasses current state-of-the-art methods across multiple evaluation metrics, including segmentation accuracy, instruction adherence, and visual quality preservation, while addressing the issue of local information focus and improving global consistency in the edited image. Our project will be available at https://github.com/smileformylove/SmartFreeEdit.", "categories": ["cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2504.12704", "pdf_url": "https://arxiv.org/pdf/2504.12704.pdf", "is_interesting": false}, "262": {"title": "Transforming Hyperspectral Images Into Chemical Maps: A Novel End-to-End Deep Learning Approach", "authors": ["Ole-Christian Galbo Engstr{\\o}m, Michela Albano-Gaglio, Erik Schou Dreier, Yamine Bouzembrak, Maria Font-i-Furnols, Puneet Mishra, Kim Steenstrup Pedersen"], "abstract": "arXiv:2504.14131v5 Announce Type: replace \nCurrent approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. This study compares the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error that is 7% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.37% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2504.14131", "pdf_url": "https://arxiv.org/pdf/2504.14131.pdf", "is_interesting": false}, "263": {"title": "What Makes Good Synthetic Training Data for Zero-Shot Stereo Matching?", "authors": ["David Yan, Alexander Raistrick, Jia Deng"], "abstract": "arXiv:2504.16930v2 Announce Type: replace \nSynthetic datasets are a crucial ingredient for training stereo matching networks, but the question of what makes a stereo dataset effective remains underexplored. We investigate the design space of synthetic datasets by varying the parameters of a procedural dataset generator, and report the effects on zero-shot stereo matching performance using standard benchmarks. We validate our findings by collecting the best settings and creating a large-scale dataset. Training only on this dataset achieves better performance than training on a mixture of widely used datasets, and is competitive with training on the FoundationStereo dataset, with the additional benefit of open-source generation code and an accompanying parameter analysis to enable further research. We open-source our system at https://github.com/princeton-vl/InfinigenStereo to enable further research on procedural stereo datasets.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.16930", "pdf_url": "https://arxiv.org/pdf/2504.16930.pdf", "is_interesting": false}, "264": {"title": "A Genealogy of Foundation Models in Remote Sensing", "authors": ["Kevin Lane, Morteza Karimzadeh"], "abstract": "arXiv:2504.17177v2 Announce Type: replace \nFoundation models have garnered increasing attention for representation learning in remote sensing. Many such foundation models adopt approaches that have demonstrated success in computer vision with minimal domain-specific modification. However, the development and application of foundation models in this field are still burgeoning, as there are a variety of competing approaches for how to most effectively leverage remotely sensed data. This paper examines these approaches, along with their roots in the computer vision field. This is done to characterize potential advantages and pitfalls, while outlining future directions to further improve remote sensing-specific foundation models. We discuss the quality of the learned representations and methods to alleviate the need for massive compute resources. We first examine single-sensor remote foundation models to introduce concepts and provide context, and then place emphasis on incorporating the multi-sensor aspect of Earth observations into foundation models. In particular, we explore the extent to which existing approaches leverage multiple sensors in training foundation models in relation to multi-modal foundation models. Finally, we identify opportunities for further harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote sensing observations.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2504.17177", "pdf_url": "https://arxiv.org/pdf/2504.17177.pdf", "is_interesting": false}, "265": {"title": "Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras", "authors": ["Yunzhong Zhang, Bo Xiong, You Zhou, Changqing Su, Zhen Cheng, Zhaofei Yu, Xun Cao, Tiejun Huang"], "abstract": "arXiv:2504.18864v3 Announce Type: replace \nThe need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.18864", "pdf_url": "https://arxiv.org/pdf/2504.18864.pdf", "is_interesting": false}, "266": {"title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung Nodule Malignancy Prediction", "authors": ["Luoting Zhuang, Seyed Mohammad Hossein Tabatabaei, Ramin Salehi-Rad, Linh M. Tran, Denise R. Aberle, Ashley E. Prosper, William Hsu"], "abstract": "arXiv:2504.21344v3 Announce Type: replace \nMachine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, guiding the model to learn clinically relevant, robust, and explainable imaging features for predicting lung cancer. We obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST) with 1,261 nodules and semantic features. Additionally, the Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a pretrained Contrastive Language-Image Pretraining (CLIP) model with a parameter-efficient fine-tuning approach to align imaging and semantic text features and predict the one-year lung cancer diagnosis. Our model outperformed state-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and AUPRC of 0.776. It also showed robust results in external datasets. Using CLIP, we also obtained predictions on semantic features through zero-shot inference, such as nodule margin (AUROC: 0.807), nodule consistency (0.812), and pleural attachment (0.840). Our approach surpasses the SOTA models in predicting lung cancer across datasets collected from diverse clinical settings, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings. The code is available at https://github.com/luotingzhuang/CLIP_nodule.", "categories": ["cs.CV", "cs.AI", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2504.21344", "pdf_url": "https://arxiv.org/pdf/2504.21344.pdf", "is_interesting": false}, "267": {"title": "Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID", "authors": ["Priyank Pathak, Yogesh S Rawat"], "abstract": "arXiv:2505.12580v2 Announce Type: replace \nThis work focuses on Clothes Changing Re-IDentification (CC-ReID) for the real world. Existing works perform well with high-quality (HQ) images, but struggle with low-quality (LQ) where we can have artifacts like pixelation, out-of-focus blur, and motion blur. These artifacts introduce noise to not only external biometric attributes (e.g. pose, body shape, etc.) but also corrupt the model's internal feature representation. Models usually cluster LQ image features together, making it difficult to distinguish between them, leading to incorrect matches. We propose a novel framework Robustness against Low-Quality (RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse Attributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in alternate steps in a novel training mechanism. CAP enriches the model with external fine-grained attributes via coarse predictions, thereby reducing the effect of noisy inputs. On the other hand, TAD enhances the model's internal feature representation by bridging the gap between HQ and LQ features, via an external dataset through task-agnostic self-supervision and distillation. RLQ outperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets like LaST, and DeepChange, while showing consistent improvement of 5.3%-6% Top-1 on PRCC with competitive performance on LTCC. *The code will be made public soon.*", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.12580", "pdf_url": "https://arxiv.org/pdf/2505.12580.pdf", "is_interesting": false}, "268": {"title": "LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts", "authors": ["Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, Wentao Zhang"], "abstract": "arXiv:2505.13928v2 Announce Type: replace \nLong videos contain a vast amount of information, making video-text retrieval an essential and challenging task in multimodal learning. However, existing benchmarks suffer from limited video duration, low-quality captions, and coarse annotation granularity, which hinder the evaluation of advanced video-text retrieval methods. To address these limitations, we introduce LoVR, a benchmark specifically designed for long video-text retrieval. LoVR contains 467 long videos and over 40,804 fine-grained clips with high-quality captions. To overcome the issue of poor machine-generated annotations, we propose an efficient caption generation framework that integrates VLM automatic generation, caption quality scoring, and dynamic refinement. This pipeline improves annotation accuracy while maintaining scalability. Furthermore, we introduce a semantic fusion method to generate coherent full-video captions without losing important contextual information. Our benchmark introduces longer videos, more detailed captions, and a larger-scale dataset, presenting new challenges for video understanding and retrieval. Extensive experiments on various advanced embedding models demonstrate that LoVR is a challenging benchmark, revealing the limitations of current approaches and providing valuable insights for future research. We release the code and dataset link at https://github.com/TechNomad-ds/LoVR-benchmark", "categories": ["cs.CV", "cs.IR"], "abs_url": "https://arxiv.org/abs/2505.13928", "pdf_url": "https://arxiv.org/pdf/2505.13928.pdf", "is_interesting": false}, "269": {"title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds", "authors": ["Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, Junhui Hou"], "abstract": "arXiv:2505.17442v2 Announce Type: replace \nRegarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.17442", "pdf_url": "https://arxiv.org/pdf/2505.17442.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses a 3D object detection framework aimed at improving detection accuracy for compressed point clouds, which is a critical task in autonomous driving systems. The method focuses on enhancing detection robustness in low-bitrate transmission scenarios, directly related to the perception task in autonomous vehicles, specifically in the context of point cloud processing for object detection."}}, "270": {"title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply", "authors": ["Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach"], "abstract": "arXiv:2505.17955v3 Announce Type: replace \nUnderstanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark \\textsc{Self-Bench} comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.17955", "pdf_url": "https://arxiv.org/pdf/2505.17955.pdf", "is_interesting": false}, "271": {"title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding", "authors": ["Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu"], "abstract": "arXiv:2505.18079v4 Announce Type: replace \nLong-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Unlike previous video agents that rely on predefined workflows applied uniformly across different queries, our approach emphasizes the autonomous and adaptive nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code has been released at https://github.com/microsoft/DeepVideoDiscovery.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2505.18079", "pdf_url": "https://arxiv.org/pdf/2505.18079.pdf", "is_interesting": false}, "272": {"title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders", "authors": ["Savya Khosla, Sethuraman TV, Barnett Lee, Alexander Schwing, Derek Hoiem"], "abstract": "arXiv:2505.18153v2 Announce Type: replace \nWe introduce the Region Encoder Network (REN), a fast and effective model for generating region-based image representations using point prompts. Recent methods combine class-agnostic segmenters (e.g., SAM) with patch-based image encoders (e.g., DINO) to produce compact and effective region representations, but they suffer from high computational cost due to the segmentation step. REN bypasses this bottleneck using a lightweight module that directly generates region tokens, enabling 60x faster token generation with 35x less memory, while also improving token quality. It uses a few cross-attention blocks that take point prompts as queries and features from a patch-based image encoder as keys and values to produce region tokens that correspond to the prompted objects. We train REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that it can be extended to other encoders without dedicated training. We evaluate REN on semantic segmentation and retrieval tasks, where it consistently outperforms the original encoders in both performance and compactness, and matches or exceeds SAM-based region methods while being significantly faster. Notably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D benchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle challenge. Code and models are available at: https://github.com/savya08/REN.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.18153", "pdf_url": "https://arxiv.org/pdf/2505.18153.pdf", "is_interesting": false}, "273": {"title": "Policy Optimized Text-to-Image Pipeline Design", "authors": ["Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor"], "abstract": "arXiv:2505.21478v2 Announce Type: replace \nText-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.21478", "pdf_url": "https://arxiv.org/pdf/2505.21478.pdf", "is_interesting": false}, "274": {"title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding", "authors": ["Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, Slobodan Ilic"], "abstract": "arXiv:2505.22643v2 Announce Type: replace \nLeveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, we propose Spiral, a novel range-view LiDAR diffusion model that simultaneously generates depth, reflectance images, and semantic maps. Furthermore, we introduce novel semantic-aware metrics to evaluate the quality of the generated labeled range-view data. Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, outperforming two-step methods that combine the generative and segmentation models. Additionally, we validate that range images generated by Spiral can be effectively used for synthetic data augmentation in the downstream segmentation training, significantly reducing the labeling effort on LiDAR data.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.22643", "pdf_url": "https://arxiv.org/pdf/2505.22643.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u4eff\u771f\u8bc4\u4f30 / \u6570\u636e\u751f\u6210", "reason": "The paper proposes a semantic-aware LiDAR diffusion model (Spiral) for generating labeled 3D LiDAR scenes and validates its effectiveness on autonomous driving datasets such as SemanticKITTI and nuScenes. The work directly benefits autonomous driving perception tasks by enabling synthetic data generation for LiDAR-based 3D segmentation and scene understanding."}}, "275": {"title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding", "authors": ["Zhoufaran Yang, Yan Shu, Jing Wang, Zhifei Yang, Yan Zhang, Yu Li, Keyang Lu, Gangyan Zeng, Shaohui Liu, Yu Zhou, Nicu Sebe"], "abstract": "arXiv:2505.22810v2 Announce Type: replace \nVisual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.22810", "pdf_url": "https://arxiv.org/pdf/2505.22810.pdf", "is_interesting": false}, "276": {"title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals", "authors": ["Pedram Ghamisi, Weikang Yu, Xiaokang Zhang, Aldino Rizaldy, Jian Wang, Chufeng Zhou, Richard Gloaguen, Gustau Camps-Valls"], "abstract": "arXiv:2505.24528v2 Announce Type: replace \nFoundation Models (FMs) are large-scale, pre-trained artificial intelligence (AI) systems that have revolutionized natural language processing and computer vision, and are now advancing geospatial analysis and Earth Observation (EO). They promise improved generalization across tasks, scalability, and efficient adaptation with minimal labeled data. However, despite the rapid proliferation of geospatial FMs, their real-world utility and alignment with global sustainability goals remain underexplored. We introduce SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals with extremely diverse tasks ranging from asset wealth prediction to environmental hazard detection. This study provides a rigorous, interdisciplinary assessment of geospatial FMs and offers critical insights into their role in attaining sustainability goals. Our findings show: (1) While not universally superior, FMs often outperform traditional approaches across diverse tasks and datasets. (2) Evaluating FMs should go beyond accuracy to include transferability, generalization, and energy efficiency as key criteria for their responsible use. (3) FMs enable scalable, SDG-grounded solutions, offering broad utility for tackling complex sustainability challenges. Critically, we advocate for a paradigm shift from model-centric development to impact-driven deployment, and emphasize metrics such as energy efficiency, robustness to domain shifts, and ethical considerations.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.24528", "pdf_url": "https://arxiv.org/pdf/2505.24528.pdf", "is_interesting": false}, "277": {"title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "authors": ["Hyojin Bahng, Caroline Chan, Fredo Durand, Phillip Isola"], "abstract": "arXiv:2506.02095v2 Announce Type: replace \nMeasuring alignment between language and vision is a fundamental challenge, especially as multimodal data becomes increasingly detailed and complex. Existing methods often rely on collecting human or AI preferences, which can be costly and time-intensive. We propose an alternative approach that leverages cycle consistency as a supervisory signal. Given an image and generated text, we map the text back to image space using a text-to-image model and compute the similarity between the original image and its reconstruction. Analogously, for text-to-image generation, we measure the textual similarity between an input caption and its reconstruction through the cycle. We use the cycle consistency score to rank candidates and construct a preference dataset of 866K comparison pairs. The reward model trained on our dataset, CycleReward, outperforms state-of-the-art alignment metrics on detailed captioning, with superior inference-time scalability when used as a verifier for Best-of-N sampling, while maintaining speed and differentiability. Furthermore, performing DPO and Diffusion DPO using our dataset enhances performance across a wide range of vision-language tasks and text-to-image generation. Our dataset, model, and code are publicly released at https://cyclereward.github.io.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.02095", "pdf_url": "https://arxiv.org/pdf/2506.02095.pdf", "is_interesting": false}, "278": {"title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models", "authors": ["Mingzhe Li, Gehao Zhang, Zhenting Wang, Guanhong Tao, Siqi Pan, Richard Cartwright, Juan Zhai, Shiqing Ma"], "abstract": "arXiv:2506.03067v2 Announce Type: replace \nText-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \\sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.03067", "pdf_url": "https://arxiv.org/pdf/2506.03067.pdf", "is_interesting": false}, "279": {"title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities", "authors": ["Hugo Porta, Emanuele Dalsasso, Jessica L. McCarty, Devis Tuia"], "abstract": "arXiv:2506.08690v2 Announce Type: replace \nCanada experienced in 2023 one of the most severe wildfire seasons in recent history, causing damage across ecosystems, destroying communities, and emitting large quantities of CO2. This extreme wildfire season is symptomatic of a climate-change-induced increase in the length and severity of the fire season that affects the boreal ecosystem. Therefore, it is critical to empower wildfire management in boreal communities with better mitigation solutions. Wildfire probability maps represent an important tool for understanding the likelihood of wildfire occurrence and the potential severity of future wildfires. The massive increase in the availability of Earth observation data has enabled the development of deep learning-based wildfire forecasting models, aiming at providing precise wildfire probability maps at different spatial and temporal scales. A main limitation of such methods is their reliance on coarse-resolution environmental drivers and satellite products, leading to wildfire occurrence prediction of reduced resolution, typically around $\\sim 0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and baseline methods for high-resolution: 100 m wildfire forecasting across Canada, leveraging multi-modal data from high-resolution multi-spectral satellite images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and environmental factors (ERA5 reanalysis data). Our experiments consider two major deep learning architectures. We observe that using multi-modal temporal inputs outperforms single-modal temporal inputs across all metrics, achieving a peak performance of 60.3% in F1 score for the 2023 wildfire season, a season never seen during model training. This demonstrates the potential of multi-modal deep learning models for wildfire forecasting at high-resolution and continental scale.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.08690", "pdf_url": "https://arxiv.org/pdf/2506.08690.pdf", "is_interesting": false}, "280": {"title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "authors": ["Xulin Ma, Jiankai Tang, Zhang Jiang, Songqin Cheng, Yuanchun Shi, Dong LI, Xin Liu, Daniel McDuff, Xiaojing Liu, Yuntao Wang"], "abstract": "arXiv:2506.09718v2 Announce Type: replace \nRemote photoplethysmography (rPPG) enables non-contact, continuous monitoring of physiological signals and offers a practical alternative to traditional health sensing methods. Although rPPG is promising for daily health monitoring, its application in long-term personal care scenarios, such as mirror-facing routines in high-altitude environments, remains challenging due to ambient lighting variations, frequent occlusions from hand movements, and dynamic facial postures. To address these challenges, we present LADH (Long-term Altitude Daily Health), the first long-term rPPG dataset containing 240 synchronized RGB and infrared (IR) facial videos from 21 participants across five common personal care scenarios, along with ground-truth PPG, respiration, and blood oxygen signals. Our experiments demonstrate that combining RGB and IR video inputs improves the accuracy and robustness of non-contact physiological monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate estimation. Furthermore, we find that multi-task learning enhances performance across multiple physiological indicators simultaneously. Dataset and code are open at https://github.com/McJackTang/FusionVitals.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.09718", "pdf_url": "https://arxiv.org/pdf/2506.09718.pdf", "is_interesting": false}, "281": {"title": "Where and How to Perturb: On the Design of Perturbation Guidance in Diffusion and Flow Models", "authors": ["Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Sangwu Lee, Sayak Paul, Susung Hong, Seungryong Kim"], "abstract": "arXiv:2506.10978v4 Announce Type: replace \nRecent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.10978", "pdf_url": "https://arxiv.org/pdf/2506.10978.pdf", "is_interesting": false}, "282": {"title": "WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild", "authors": ["Morris Alper, David Novotny, Filippos Kokkinos, Hadar Averbuch-Elor, Tom Monnier"], "abstract": "arXiv:2506.13030v2 Announce Type: replace \nDespite recent advances in sparse novel view synthesis (NVS) applied to object-centric scenes, scene-level NVS remains a challenge. A central issue is the lack of available clean multi-view training data, beyond manually curated datasets with limited diversity, camera variation, or licensing issues. On the other hand, an abundance of diverse and permissively-licensed data exists in the wild, consisting of scenes with varying appearances (illuminations, transient occlusions, etc.) from sources such as tourist photos. To this end, we present WildCAT3D, a framework for generating novel views of scenes learned from diverse 2D scene image data captured in the wild. We unlock training on these data sources by explicitly modeling global appearance conditions in images, extending the state-of-the-art multi-view diffusion paradigm to learn from scene views of varying appearances. Our trained model generalizes to new scenes at inference time, enabling the generation of multiple consistent novel views. WildCAT3D provides state-of-the-art results on single-view NVS in object- and scene-level settings, while training on strictly less data sources than prior methods. Additionally, it enables novel applications by providing global appearance control during generation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.13030", "pdf_url": "https://arxiv.org/pdf/2506.13030.pdf", "is_interesting": false}, "283": {"title": "PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue", "authors": ["Eugene Vorontsov, George Shaikovski, Adam Casson, Julian Viret, Eric Zimmermann, Neil Tenenholtz, Yi Kan Wang, Jan H. Bernhard, Ran A. Godrich, Juan A. Retamero, Jinru Shia, Mithat Gonen, Martin R. Weiser, David S. Klimstra, Razik Yousfi, Nicolo Fusi, Thomas J. Fuchs, Kristen Severson, Siqi Liu"], "abstract": "arXiv:2506.13063v2 Announce Type: replace \nRecent rapid progress in the field of computational pathology has been enabled by foundation models. These models are beginning to move beyond encoding image patches towards whole-slide understanding but their clinical utility remains limited. In this work, we present PRISM2, a multimodal slide-level foundation model trained on data from 700,000 diagnostic specimen-report pairs, the largest vision (2.3 million whole slide images) and language (14M question-answer pairs) histopathology dataset to date. By learning through clinical-dialogue supervision, PRISM2 aligns histomorphologic features with the language of diagnostic reasoning, producing slide-level representations that support both direct diagnostic question-answering and transferable embeddings for downstream tasks. Without additional training, PRISM2 matches or exceeds the cancer-detection performance of clinical-grade products. This is observed without loss of generality on other tasks, where PRISM2 achieves top performance. Finally, using survival prediction as the example, we show that task-specific finetuning with a large dataset can outperform task-specific models, further improving performance. These results demonstrate how language-supervised pretraining provides a scalable, clinically grounded signal for learning generalizable pathology representations, bridging human diagnostic reasoning and foundation-model performance.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2506.13063", "pdf_url": "https://arxiv.org/pdf/2506.13063.pdf", "is_interesting": false}, "284": {"title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches", "authors": ["Yun Xing, Yue Cao, Nhat Chung, Jie Zhang, Ivor Tsang, Ming-Ming Cheng, Yang Liu, Lei Ma, Qing Guo"], "abstract": "arXiv:2506.16690v2 Announce Type: replace \nStereo depth estimation is a critical task in autonomous driving and robotics, where inaccuracies (such as misidentifying nearby objects as distant) can lead to dangerous situations. Adversarial attacks against stereo depth estimation can help reveal vulnerabilities before deployment. Previous works have shown that repeating optimized textures can effectively mislead stereo depth estimation in digital settings. However, our research reveals that these naively repeated textures perform poorly in physical implementations, i.e., when deployed as patches, limiting their practical utility for stress-testing stereo depth estimation systems. In this work, for the first time, we discover that introducing regular intervals among the repeated textures, creating a grid structure, significantly enhances the patch's attack performance. Through extensive experimentation, we analyze how variations of this novel structure influence the adversarial effectiveness. Based on these insights, we develop a novel stereo depth attack that jointly optimizes both the interval structure and texture elements. Our generated adversarial patches can be inserted into any scenes and successfully attack advanced stereo depth estimation methods of different paradigms, i.e., RAFT-Stereo and STTR. Most critically, our patch can also attack commercial RGB-D cameras (Intel RealSense) in real-world conditions, demonstrating their practical relevance for security assessment of stereo systems. The code is officially released at: https://github.com/WiWiN42/DepthVanish", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.16690", "pdf_url": "https://arxiv.org/pdf/2506.16690.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses stereo depth estimation and adversarial attacks on stereo depth systems, which are crucial for 3D perception in autonomous driving. The focus on enhancing the robustness of stereo depth estimation directly ties into the improvement of sensing capabilities in autonomous vehicles, making it highly relevant to the field."}}, "285": {"title": "Class Agnostic Instance-level Descriptor for Visual Instance Search", "authors": ["Qi-Ying Sun, Wan-Lei Zhao, Hui-Ying Xie, Yi-Bo Miao, Chong-Wah Ngo"], "abstract": "arXiv:2506.16745v2 Announce Type: replace \nDespite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance-level feature representation. Supervised or weakly supervised object detection methods are not the appropriate solutions due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance-level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of instance regions. On the one hand, this kind of hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in real scenarios. On the other hand, the non-leaf nodes and leaf nodes on the hierarchy correspond to the instance regions in different granularities within an image. Therefore, features in uniform length are produced for these instance regions, which may cover across a dominant image region, an integral of multiple instances, or various individual instances. Such a collection of features allows us to unify the image retrieval, multi-instance search, and instance search into one framework. The empirical studies on three benchmarks show that such an instance-level descriptor remains effective on both the known and unknown object categories. Moreover, the superior performance is achieved on single-instance and multi-instance search, as well as image retrieval tasks.", "categories": ["cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2506.16745", "pdf_url": "https://arxiv.org/pdf/2506.16745.pdf", "is_interesting": false}, "286": {"title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "authors": ["Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang"], "abstract": "arXiv:2506.23467v3 Announce Type: replace \nContrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.23467", "pdf_url": "https://arxiv.org/pdf/2506.23467.pdf", "is_interesting": false}, "287": {"title": "MotionGPT3: Human Motion as a Second Modality", "authors": ["Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen"], "abstract": "arXiv:2506.24086v3 Announce Type: replace \nWith the rapid progress of large language models (LLMs), multimodal frameworks that unify understanding and generation have become promising, yet they face increasing complexity as the number of modalities and tasks grows. We observe that motion quantization introduces approximation errors that cap motion quality, and that unifying discrete text and continuous motion within a single-stream backbone amplifies cross-modal interference. Motivated by recent multi-branch Transformer designs that separate signals from different modalities, we propose MotionGPT3, a bimodal motion-language model for both understanding and generation. MotionGPT3 encodes raw motion into a continuous latent space using a variational autoencoder (VAE), thereby avoiding quantization-induced artifacts, while leveraging the semantic prior of pretrained language models. A dual-stream Transformer with shared attention preserves modality-specific routes while enabling controlled, bidirectional information flow, which reduces interference, stabilizing optimization, and empirically accelerates convergence without degrading fidelity. For multimodal joint training, a generate-then-align three-stage schedule further improves stability and limits cross-task interference. Experiments show that MotionGPT3 achieves 2x faster convergence in training loss and up to 4x faster convergence in validation, while maintaining state-of-the-art performance on standard motion understanding and motion generation benchmarks.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2506.24086", "pdf_url": "https://arxiv.org/pdf/2506.24086.pdf", "is_interesting": false}, "288": {"title": "AI-Generated Video Detection via Perceptual Straightening", "authors": ["Christian Intern\\`o, Robert Geirhos, Markus Olhofer, Sunny Liu, Barbara Hammer, David Klindt"], "abstract": "arXiv:2507.00583v3 Announce Type: replace \nThe rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the \"perceptual straightening\" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2507.00583", "pdf_url": "https://arxiv.org/pdf/2507.00583.pdf", "is_interesting": false}, "289": {"title": "Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery", "authors": ["Jizhou Han, Shaokun Wang, Yuhang He, Chenhao Ding, Qiang Wang, Xinyuan Gao, SongLin Dong, Yihong Gong"], "abstract": "arXiv:2507.04725v2 Announce Type: replace \nGeneralized Category Discovery (GCD) focuses on classifying known categories while simultaneously discovering novel categories from unlabeled data. However, previous GCD methods face challenges due to inconsistent optimization objectives and category confusion. This leads to feature overlap and ultimately hinders performance on novel categories. To address these issues, we propose the Neural Collapse-inspired Generalized Category Discovery (NC-GCD) framework. By pre-assigning and fixing Equiangular Tight Frame (ETF) prototypes, our method ensures an optimal geometric structure and a consistent optimization objective for both known and novel categories. We introduce a Consistent ETF Alignment Loss that unifies supervised and unsupervised ETF alignment and enhances category separability. Additionally, a Semantic Consistency Matcher (SCM) is designed to maintain stable and consistent label assignments across clustering iterations. Our method achieves strong performance on multiple GCD benchmarks, significantly enhancing novel category accuracy and demonstrating its effectiveness.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.04725", "pdf_url": "https://arxiv.org/pdf/2507.04725.pdf", "is_interesting": false}, "290": {"title": "Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting", "authors": ["Mohsi Jawaid, Marcus M\\\"artens, Tat-Jun Chin"], "abstract": "arXiv:2507.05698v3 Announce Type: replace \nSpacecraft pose estimation is crucial for autonomous in-space operations, such as rendezvous, docking and on-orbit servicing. Vision-based pose estimation methods, which typically employ RGB imaging sensors, is a compelling solution for spacecraft pose estimation, but are challenged by harsh lighting conditions, which produce imaging artifacts such as glare, over-exposure, blooming and lens flare. Due to their much higher dynamic range, neuromorphic or event sensors are more resilient to extreme lighting conditions. However, event sensors generally have lower spatial resolution and suffer from reduced signal-to-noise ratio during periods of low relative motion. This work addresses these individual sensor limitations by introducing a sensor fusion approach combining RGB and event sensors. A beam-splitter prism was employed to achieve precise optical and temporal alignment. Then, a RANSAC-based technique was developed to fuse the information from the RGB and event channels to achieve pose estimation that leveraged the strengths of the two modalities. The pipeline was complemented by dropout uncertainty estimation to detect extreme conditions that affect either channel. To benchmark the performance of the proposed event-RGB fusion method, we collected a comprehensive real dataset of RGB and event data for satellite pose estimation in a laboratory setting under a variety of challenging illumination conditions. Encouraging results on the dataset demonstrate the efficacy of our event-RGB fusion approach and further supports the usage of event sensors for spacecraft pose estimation. To support community research on this topic, our dataset has been released publicly.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2507.05698", "pdf_url": "https://arxiv.org/pdf/2507.05698.pdf", "is_interesting": false}, "291": {"title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "authors": ["Hongyong Han, Wei Wang, Gaowei Zhang, Mingjie Li, Yi Wang"], "abstract": "arXiv:2507.10449v2 Announce Type: replace \nCoral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2507.10449", "pdf_url": "https://arxiv.org/pdf/2507.10449.pdf", "is_interesting": false}, "292": {"title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "authors": ["Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan"], "abstract": "arXiv:2507.12508v2 Announce Type: replace \nSpatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 7.7% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.", "categories": ["cs.CV", "cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2507.12508", "pdf_url": "https://arxiv.org/pdf/2507.12508.pdf", "is_interesting": false}, "293": {"title": "Semantic-Aware Representation Learning via Conditional Transport for Multi-Label Image Classification", "authors": ["Ren-Dong Xie, Zhi-Fen He, Bo Li, Bin Liu, Jin-Yan Hu"], "abstract": "arXiv:2507.14918v2 Announce Type: replace \nMulti-label image classification is a critical task in machine learning that aims to accurately assign multiple labels to a single image. While existing methods often utilize attention mechanisms or graph convolutional networks to model visual representations, their performance is still constrained by two critical limitations: the inability to learn discriminative semantic-aware features, and the lack of fine-grained alignment between visual representations and label embeddings. To tackle these issues in a unified framework, this paper proposes a novel approach named Semantic-aware representation learning via Conditional Transport for Multi-Label Image Classification (SCT). The proposed method introduces a semantic-related feature learning module that extracts discriminative label-specific features by emphasizing semantic relevance and interaction, along with a conditional transport-based alignment mechanism that enables precise visual-semantic alignment. Extensive experiments on two widely-used benchmark datasets, VOC2007 and MS-COCO, validate the effectiveness of SCT and demonstrate its superior performance compared to existing state-of-the-art methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.14918", "pdf_url": "https://arxiv.org/pdf/2507.14918.pdf", "is_interesting": false}, "294": {"title": "Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for Semi-Supervised Medical Image Segmentation", "authors": ["Chaowei Chen, Xiang Zhang, Honglie Guo, Shunfang Wang"], "abstract": "arXiv:2507.20729v2 Announce Type: replace \nWeak-strong consistency learning strategies are widely employed in semi-supervised medical image segmentation to train models by leveraging limited labeled data and enforcing weak-to-strong consistency. However, existing methods primarily focus on designing and combining various perturbation schemes, overlooking the inherent potential and limitations within the framework itself. In this paper, we first identify two critical deficiencies: (1) separated training data streams, which lead to confirmation bias dominated by the labeled stream; and (2) incomplete utilization of supervisory information, which limits exploration of strong-to-weak consistency. To tackle these challenges, we propose a style-aware blending and prototype-based cross-contrast consistency learning framework. Specifically, inspired by the empirical observation that the distribution mismatch between labeled and unlabeled data can be characterized by statistical moments, we design a style-guided distribution blending module to break the independent training data streams. Meanwhile, considering the potential noise in strong pseudo-labels, we introduce a prototype-based cross-contrast strategy to encourage the model to learn informative supervisory signals from both weak-to-strong and strong-to-weak predictions, while mitigating the adverse effects of noise. Experimental results demonstrate the effectiveness and superiority of our framework across multiple medical segmentation benchmarks under various semi-supervised settings.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.20729", "pdf_url": "https://arxiv.org/pdf/2507.20729.pdf", "is_interesting": false}, "295": {"title": "Adjustable Spatio-Spectral Hyperspectral Image Compression Network", "authors": ["Martin Hermann Paul Fuchs, Behnood Rasti, Beg\\\"um Demir"], "abstract": "arXiv:2507.23447v3 Announce Type: replace \nWith the rapid growth of hyperspectral data archives in remote sensing (RS), the need for efficient storage has become essential, driving significant attention toward learning-based hyperspectral image (HSI) compression. However, a comprehensive investigation of the individual and joint effects of spectral and spatial compression on learning-based HSI compression has not been thoroughly examined yet. Conducting such an analysis is crucial for understanding how the exploitation of spectral, spatial, and joint spatio-spectral redundancies affects HSI compression. To address this issue, we propose Adjustable Spatio-Spectral Hyperspectral Image Compression Network (HyCASS), a learning-based model designed for adjustable HSI compression in both spectral and spatial dimensions. HyCASS consists of six main modules: 1) spectral encoder module; 2) spatial encoder module; 3) compression ratio (CR) adapter encoder module; 4) CR adapter decoder module; 5) spatial decoder module; and 6) spectral decoder module. The modules employ convolutional layers and transformer blocks to capture both short-range and long-range redundancies. Experimental results on three HSI benchmark datasets demonstrate the effectiveness of our proposed adjustable model compared to existing learning-based compression models, surpassing the state of the art by up to 2.36 dB in terms of PSNR. Based on our results, we establish a guideline for effectively balancing spectral and spatial compression across different CRs, taking into account the spatial resolution of the HSIs. Our code and pre-trained model weights are publicly available at https://git.tu-berlin.de/rsim/hycass .", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.23447", "pdf_url": "https://arxiv.org/pdf/2507.23447.pdf", "is_interesting": false}, "296": {"title": "Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification", "authors": ["Shiben Liu, Mingyue Xu, Huijie Fan, Qiang Wang, Yandong Tang, Zhi Han"], "abstract": "arXiv:2508.03516v2 Announce Type: replace \nLifelong person re-identification (LReID) encounters a key challenge: balancing the preservation of old knowledge with adaptation to new information. Existing LReID methods typically employ knowledge distillation to enforce representation alignment. However, these approaches ignore two crucial aspects: specific distribution awareness and cross-domain unified knowledge learning, both of which are essential for addressing this challenge. To overcome these limitations, we propose a novel distribution-aware knowledge unification and association (DKUA) framework where domain-style modeling is performed for each instance to propagate domain-specific representations, enhancing anti-forgetting and generalization capacity. Specifically, we design a distribution-aware model to transfer instance-level representations of the current domain into the domain-specific representations with the different domain styles, preserving learned knowledge without storing old samples. Next, we propose adaptive knowledge consolidation (AKC) to dynamically generate the unified representation as a cross-domain representation center. To further mitigate forgetting, we develop a unified knowledge association (UKA) mechanism, which explores the unified representation as a bridge to explicitly model inter-domain associations, reducing inter-domain gaps. Finally, distribution-based knowledge transfer (DKT) is proposed to prevent the current domain distribution from deviating from the cross-domain distribution center, improving adaptation capacity. Experimental results show our DKUA outperforms the existing methods by 7.6%/5.3% average mAP/R@1 improvement on anti-forgetting and generalization capacity, respectively. Our code is available at https://github.com/LiuShiBen/DKUA.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2508.03516", "pdf_url": "https://arxiv.org/pdf/2508.03516.pdf", "is_interesting": false}, "297": {"title": "Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification", "authors": ["Samuel R\\\"aber, Till Aczel, Andreas Plesner, Roger Wattenhofer"], "abstract": "arXiv:2508.05489v3 Announce Type: replace \nPrevious work has suggested that preprocessing images through lossy compression can defend against adversarial perturbations, but comprehensive attack evaluations have been lacking. In this paper, we construct strong white-box and adaptive attacks against various compression models and identify a critical challenge for attackers: high realism in reconstructed images significantly increases attack difficulty. Through rigorous evaluation across multiple attack scenarios, we demonstrate that compression models capable of producing realistic, high-fidelity reconstructions are substantially more resistant to our attacks. In contrast, low-realism compression models can be broken. Our analysis reveals that this is not due to gradient masking. Rather, realistic reconstructions maintaining distributional alignment with natural images seem to offer inherent robustness. This work highlights a significant obstacle for future adversarial attacks and suggests that developing more effective techniques to overcome realism represents an essential challenge for comprehensive security evaluation.", "categories": ["cs.CV", "cs.LG", "eess.IV"], "abs_url": "https://arxiv.org/abs/2508.05489", "pdf_url": "https://arxiv.org/pdf/2508.05489.pdf", "is_interesting": false}, "298": {"title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "authors": ["Yingxian Chen, Jiahui Liu, Ruidi Fan, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu"], "abstract": "arXiv:2508.06350v2 Announce Type: replace \nUnderstanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (MLLMs) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal sparsity of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models (LLMs), we propose VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and LLMs through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2508.06350", "pdf_url": "https://arxiv.org/pdf/2508.06350.pdf", "is_interesting": false}, "299": {"title": "Combinative Matching for Geometric Shape Assembly", "authors": ["Nahyuk Lee, Juhong Min, Junhong Lee, Chunghyun Park, Minsu Cho"], "abstract": "arXiv:2508.09780v2 Announce Type: replace \nThis paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. In contrast, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art. Project page: https://nahyuklee.github.io/cmnet.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.09780", "pdf_url": "https://arxiv.org/pdf/2508.09780.pdf", "is_interesting": false}, "300": {"title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "authors": ["Shu Shen, C. L. Philip Chen, Tong Zhang"], "abstract": "arXiv:2508.19769v3 Announce Type: replace \nMultimodal learning has significantly enhanced machine learning performance but still faces numerous challenges and limitations. Imbalanced multimodal learning is one of the problems extensively studied in recent works and is typically mitigated by modulating the learning of each modality. However, we find that these methods typically hinder the dominant modality's learning to promote weaker modalities, which affects overall multimodal performance. We analyze the cause of this issue and highlight a commonly overlooked problem: optimization bias within networks. To address this, we propose Adaptive Intra-Network Modulation (AIM) to improve balanced modality learning. AIM accounts for differences in optimization state across parameters and depths within the network during modulation, achieving balanced multimodal learning without hindering either dominant or weak modalities for the first time. Specifically, AIM decouples the dominant modality's under-optimized parameters into Auxiliary Blocks and encourages reliance on these performance-degraded blocks for joint training with weaker modalities. This approach effectively prevents suppression of weaker modalities while enabling targeted optimization of under-optimized parameters to improve the dominant modality. Additionally, AIM assesses modality imbalance level across network depths and adaptively adjusts modulation strength at each depth. Experimental results demonstrate that AIM outperforms state-of-the-art imbalanced modality learning methods across multiple benchmarks and exhibits strong generalizability across different backbones, fusion strategies, and optimizers.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2508.19769", "pdf_url": "https://arxiv.org/pdf/2508.19769.pdf", "is_interesting": false}, "301": {"title": "Multi-Focused Video Group Activities Hashing", "authors": ["Zhongmiao Qi, Yan Jiang, Bolin Zhang, Lijun Guo, Chong Wang, Qiangbo Qian"], "abstract": "arXiv:2509.00490v2 Announce Type: replace \nWith the explosive growth of video data in various complex scenarios, quickly retrieving group activities has become an urgent problem. However, many tasks can only retrieve videos focusing on an entire video, not the activity granularity. To solve this problem, we propose a new STVH (spatiotemporal interleaved video hashing) technique for the first time. Through a unified framework, the STVH simultaneously models individual object dynamics and group interactions, capturing the spatiotemporal evolution on both group visual features and positional features. Moreover, in real-life video retrieval scenarios, it may sometimes require activity features, while at other times, it may require visual features of objects. We then further propose a novel M-STVH (multi-focused spatiotemporal video hashing) as an enhanced version to handle this difficult task. The advanced method incorporates hierarchical feature integration through multi-focused representation learning, allowing the model to jointly focus on activity semantics features and object visual features. We conducted comparative experiments on publicly available datasets, and both STVH and M-STVH can achieve excellent results.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.00490", "pdf_url": "https://arxiv.org/pdf/2509.00490.pdf", "is_interesting": false}, "302": {"title": "EndoGMDE: Generalizable Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes", "authors": ["Liangjing Shao, Chenkang Du, Benshuang Chen, Xueli Liu, Xinrong Chen"], "abstract": "arXiv:2509.01206v3 Announce Type: replace \nSelf-supervised monocular depth estimation is a significant task for low-cost and efficient 3D scene perception and measurement in endoscopy. However, the variety of illumination conditions and scene features is still the primary challenges for depth estimation in endoscopic scenes. In this work, a novel self-supervised framework is proposed for monocular depth estimation in diverse endoscopy. Firstly, considering the diverse features in endoscopic scenes with different tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to efficiently finetune the foundation model for endoscopic depth estimation. In the proposed module, based on the input feature, different experts with a small amount of trainable parameters are adaptively selected for weighted inference, from low-rank experts which are allocated based on the generalization of each block. Moreover, a novel self-supervised training framework is proposed to jointly cope with brightness inconsistency and reflectance interference. The proposed method outperforms state-of-the-art works on SCARED dataset and SimCol dataset. Furthermore, the proposed network also achieves the best generalization based on zero-shot depth estimation on C3VD, Hamlyn and SERV-CT dataset. The outstanding performance of our model is further demonstrated with 3D reconstruction and ego-motion estimation. The proposed method could contribute to accurate endoscopy for minimally invasive measurement and surgery. The evaluation codes will be released upon acceptance, while the demo videos can be found on: https://endo-gmde.netlify.app/.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.01206", "pdf_url": "https://arxiv.org/pdf/2509.01206.pdf", "is_interesting": false}, "303": {"title": "3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data", "authors": ["Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali"], "abstract": "arXiv:2509.12143v2 Announce Type: replace \nMajor depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.12143", "pdf_url": "https://arxiv.org/pdf/2509.12143.pdf", "is_interesting": false}, "304": {"title": "Bidirectional Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression", "authors": ["Xuan Deng, Xingtao Wang, Xiandong Meng, Longguang Wang, Tiange Zhang, Xiaopeng Fan, Debin Zhao"], "abstract": "arXiv:2509.14591v2 Announce Type: replace \nEfficient dynamic point cloud compression (DPCC) critically depends on accurate motion estimation and compensation. However, the inherently irregular structure and substantial local variations of point clouds make this task highly challenging. Existing approaches typically rely on explicit motion estimation, whose encoded motion vectors often fail to capture complex dynamics and inadequately exploit temporal correlations. To address these limitations, we propose a Bidirectional Feature-aligned Motion Transformation (Bi-FMT) framework that implicitly models motion in the feature space. Bi-FMT aligns features across both past and future frames to produce temporally consistent latent representations, which serve as predictive context in a conditional coding pipeline, forming a unified ``Motion + Conditional'' representation. Built upon this bidirectional feature alignment, we introduce a Cross-Transformer Refinement module (CTR) at the decoder side to adaptively refine locally aligned features. By modeling cross-frame dependencies with vector attention, CRT enhances local consistency and restores fine-grained spatial details that are often lost during motion alignment. Moreover, we design a Random Access (RA) reference strategy that treats the bidirectionally aligned features as conditional context, enabling frame-level parallel compression and eliminating the sequential encoding. Extensive experiments demonstrate that Bi-FMT surpasses D-DPCC and AdaDPCC in both compression efficiency and runtime, achieving BD-Rate reductions of 20% (D1) and 9.4% (D1), respectively.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.14591", "pdf_url": "https://arxiv.org/pdf/2509.14591.pdf", "is_interesting": false}, "305": {"title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity", "authors": ["Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong Fu, Jia Pan"], "abstract": "arXiv:2509.16527v2 Announce Type: replace \nThis work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.16527", "pdf_url": "https://arxiv.org/pdf/2509.16527.pdf", "is_interesting": false}, "306": {"title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices", "authors": ["Bishal Adhikari, Jiajia Li, Eric S. Michel, Jacob Dykes, Te-Ming Paul Tseng, Mary Love Tagert, Dong Chen"], "abstract": "arXiv:2509.20318v2 Announce Type: replace \nThe escalating economic losses in agriculture due to deer intrusion, estimated to be in the hundreds of millions of dollars annually in the U.S., highlight the inadequacy of traditional mitigation strategies such as hunting, fencing, use of repellents, and scare tactics. This underscores a critical need for intelligent, autonomous solutions capable of real-time deer detection and deterrence. But the progress in this field is impeded by a significant gap in the literature, mainly the lack of a domain-specific, practical dataset and limited study on the viability of deer detection systems on edge devices. To address this gap, this study presents a comprehensive evaluation of state-of-the-art deep learning models for deer detection in challenging real-world scenarios. We introduce a curated, publicly available dataset of 3,095 annotated images with bounding box annotations of deer. Then, we provide an extensive comparative analysis of 12 model variants across four recent YOLO architectures (v8 to v11). Finally, we evaluated their performance on two representative edge computing platforms: the CPU-based Raspberry Pi 5 and the GPU-accelerated NVIDIA Jetson AGX Xavier to assess feasibility for real-world field deployment. Results show that the real-time detection performance is not feasible on Raspberry Pi without hardware-specific model optimization, while NVIDIA Jetson provides greater than 30 frames per second (FPS) with 's' and 'n' series models. This study also reveals that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and YOLOv9s offer the optimal balance of high accuracy (Average Precision (AP) > 0.85) and computational efficiency (Inference Time < 34 milliseconds).", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.20318", "pdf_url": "https://arxiv.org/pdf/2509.20318.pdf", "is_interesting": false}, "307": {"title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?", "authors": ["Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong"], "abstract": "arXiv:2509.21278v3 Announce Type: replace \nImage composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.21278", "pdf_url": "https://arxiv.org/pdf/2509.21278.pdf", "is_interesting": false}, "308": {"title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "authors": ["Shimaa Elbana, Ahmad Kamal, Shahd Ahmed Ali, Ahmad Al-Kabbany"], "abstract": "arXiv:2510.03769v2 Announce Type: replace \nThe increasing size and complexity of medical imaging datasets, particularly in 3D formats, present significant barriers to collaborative research and transferability. This study investigates whether the ZFP compression technique can mitigate these challenges without compromising the performance of automated cerebrovascular segmentation, a critical first step in intracranial aneurysm detection. We apply ZFP in both its error tolerance and fixed-rate modes to a large scale, and one of the most recent, datasets in the literature, 3D medical dataset containing ground-truth vascular segmentations. The segmentation quality on the compressed volumes is rigorously compared to the uncompressed baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance mode--while maintaining a high degree of fidelity, with the mean Dice coefficient remaining high at 0.87656. These results demonstrate that ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration across the community.", "categories": ["cs.CV", "eess.SP"], "abs_url": "https://arxiv.org/abs/2510.03769", "pdf_url": "https://arxiv.org/pdf/2510.03769.pdf", "is_interesting": false}, "309": {"title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning", "authors": ["Yaxin Hou, Bo Han, Yuheng Jia, Hui Liu, Junhui Hou"], "abstract": "arXiv:2510.03993v5 Announce Type: replace \nCurrent long-tailed semi-supervised learning methods assume that labeled data exhibit a long-tailed distribution, and unlabeled data adhere to a typical predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed). However, the distribution of the unlabeled data is generally unknown and may follow an arbitrary distribution. To tackle this challenge, we propose a Controllable Pseudo-label Generation (CPG) framework, expanding the labeled dataset with the progressively identified reliable pseudo-labels from the unlabeled dataset and training the model on the updated labeled dataset with a known distribution, making it unaffected by the unlabeled data distribution. Specifically, CPG operates through a controllable self-reinforcing optimization cycle: (i) at each training step, our dynamic controllable filtering mechanism selectively incorporates reliable pseudo-labels from the unlabeled dataset into the labeled dataset, ensuring that the updated labeled dataset follows a known distribution; (ii) we then construct a Bayes-optimal classifier using logit adjustment based on the updated labeled data distribution; (iii) this improved classifier subsequently helps identify more reliable pseudo-labels in the next training step. We further theoretically prove that this optimization cycle can significantly reduce the generalization error under some conditions. Additionally, we propose a class-aware adaptive augmentation module to further improve the representation of minority classes, and an auxiliary branch to maximize data utilization by leveraging all labeled and unlabeled samples. Comprehensive evaluations on various commonly used benchmark datasets show that CPG achieves consistent improvements, surpassing state-of-the-art methods by up to $\\textbf{15.97%}$ in accuracy. The code is available at https://github.com/yaxinhou/CPG.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.03993", "pdf_url": "https://arxiv.org/pdf/2510.03993.pdf", "is_interesting": false}, "310": {"title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints", "authors": ["Viktor Koz\\'ak, Jan Chudoba, Libor P\\v{r}eu\\v{c}il"], "abstract": "arXiv:2510.04840v2 Announce Type: replace \nAn accurate and up-to-date model of a photovoltaic (PV) power plant is essential for its optimal operation and maintenance. However, such a model may not be easily available. This work introduces a novel approach for PV power plant mapping based on aerial overview images. It enables the automation of the mapping process while removing the reliance on third-party data. The presented mapping method takes advantage of the structural layout of the power plants to achieve detailed modeling down to the level of individual PV modules. The approach relies on visual segmentation of PV modules in overview images and the inference of structural information in each image, assigning modules to individual benches, rows, and columns. We identify visual keypoints related to the layout and use these to merge detections from multiple images while maintaining their structural integrity. The presented method was experimentally verified and evaluated on two different power plants. The final fusion of 3D positions and semantic structures results in a compact georeferenced model suitable for power plant maintenance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.04840", "pdf_url": "https://arxiv.org/pdf/2510.04840.pdf", "is_interesting": false}, "311": {"title": "OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search", "authors": ["Zexin Zheng, Huangyu Dai, Lingtao Mao, Xinyu Sun, Zihan Liang, Ben Chen, Yuqing Ding, Chenyi Lei, Wenwu Ou, Han Li, Kun Gai"], "abstract": "arXiv:2510.05759v3 Announce Type: replace \nTraditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.05759", "pdf_url": "https://arxiv.org/pdf/2510.05759.pdf", "is_interesting": false}, "312": {"title": "Dropping the D: RGB-D SLAM Without the Depth Sensor", "authors": ["Mert Kiray, Alican Karaomer, Benjamin Busam"], "abstract": "arXiv:2510.06216v2 Announce Type: replace \nWe present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network. Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features. These are processed by an unmodified RGB-D SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS on a single GPU. These results suggest that modern pretrained vision models can replace active depth sensors as reliable, real-time sources of metric scale, marking a step toward simpler and more cost-effective SLAM systems.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2510.06216", "pdf_url": "https://arxiv.org/pdf/2510.06216.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "SLAM / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses a monocular SLAM system that uses pretrained vision modules to replace active depth sensors. This technology can be directly applied to autonomous driving systems, particularly in improving the efficiency and cost-effectiveness of vehicle perception systems, where SLAM is crucial for navigation and mapping in real-time driving scenarios."}}, "313": {"title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language Models", "authors": ["Jonghyun Park, Minhyuk Seo, Jonghyun Choi"], "abstract": "arXiv:2510.13698v2 Announce Type: replace \nOne of the key challenges of modern AI models is ensuring that they provide helpful responses to benign queries while refusing malicious ones. But often, the models are vulnerable to multimodal queries with harmful intent embedded in images. One approach for safety alignment is training with extensive safety datasets at the significant costs in both dataset curation and training. Inference-time alignment mitigates these costs, but introduces two drawbacks: excessive refusals from misclassified benign queries and slower inference speed due to iterative output adjustments. To overcome these limitations, we propose to reformulate queries to strengthen cross-modal attention to safety-critical image regions, enabling accurate risk assessment at the query level. Using the assessed risk, it adaptively steers activations to generate responses that are safe and helpful without overhead from iterative output adjustments. We call this Risk-adaptive Activation Steering (RAS). Extensive experiments across multiple benchmarks on multimodal safety and utility demonstrate that the RAS significantly reduces attack success rates, preserves general task performance, and improves inference speed over prior inference-time defenses.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.13698", "pdf_url": "https://arxiv.org/pdf/2510.13698.pdf", "is_interesting": false}, "314": {"title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering", "authors": ["Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang"], "abstract": "arXiv:2510.14270v2 Announce Type: replace \nScene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2510.14270", "pdf_url": "https://arxiv.org/pdf/2510.14270.pdf", "is_interesting": false}, "315": {"title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data", "authors": ["Qi Chen, Xinze Zhou, Chen Liu, Hao Chen, Wenxuan Li, Zekun Jiang, Ziyan Huang, Yuxuan Zhao, Dexin Yu, Junjun He, Yefeng Zheng, Ling Shao, Alan Yuille, Zongwei Zhou"], "abstract": "arXiv:2510.14831v2 Announce Type: replace \nAI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation--based on lessons from the JHH dataset--for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.14831", "pdf_url": "https://arxiv.org/pdf/2510.14831.pdf", "is_interesting": false}, "316": {"title": "Hyperparameter Optimization and Reproducibility in Deep Learning Model Training", "authors": ["Usman Afzaal, Ziyu Su, Usama Sajjad, Hao Lu, Mostafa Rezapour, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi"], "abstract": "arXiv:2510.15164v2 Announce Type: replace \nReproducibility remains a critical challenge in foundation model training for histopathology, often hindered by software randomness, hardware non-determinism, and inconsistent hyperparameter reporting. To investigate these issues, we trained a CLIP model on the QUILT-1M dataset and systematically evaluated the impact of different hyperparameter settings and augmentation strategies across three downstream histopathology datasets (PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8 outperformed more aggressive (0.6) or conservative (0.9) settings, distributed training without local loss improved stability, and learning rates below 5.0e-5 consistently degraded performance across all datasets. The LC25000 (Colon) dataset consistently provided the most reproducible benchmark. These findings highlight that reproducibility in computational pathology depends not only on transparent documentation but also on carefully chosen experimental configurations, and we provide practical rules to guide future efforts in developing reproducible foundation models for digital pathology.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.15164", "pdf_url": "https://arxiv.org/pdf/2510.15164.pdf", "is_interesting": false}, "317": {"title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input", "authors": ["Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, Xiang Wang"], "abstract": "arXiv:2510.16926v2 Announce Type: replace \nMultimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.16926", "pdf_url": "https://arxiv.org/pdf/2510.16926.pdf", "is_interesting": false}, "318": {"title": "Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models", "authors": ["Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng"], "abstract": "arXiv:2510.18457v2 Announce Type: replace \nThe performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.18457", "pdf_url": "https://arxiv.org/pdf/2510.18457.pdf", "is_interesting": false}, "319": {"title": "Space Object Detection using Multi-frame Temporal Trajectory Completion Method", "authors": ["Xiaoqing Lan, Biqiao Xin, Bingshu Wang, Han Zhang, Rui Zhu, Laixian Zhang"], "abstract": "arXiv:2510.19220v2 Announce Type: replace \nSpace objects in Geostationary Earth Orbit (GEO) present significant detection challenges in optical imaging due to weak signals, complex stellar backgrounds, and environmental interference. In this paper, we enhance high-frequency features of GEO targets while suppressing background noise at the single-frame level through wavelet transform. Building on this, we propose a multi-frame temporal trajectory completion scheme centered on the Hungarian algorithm for globally optimal cross-frame matching. To effectively mitigate missing and false detections, a series of key steps including temporal matching and interpolation completion, temporal-consistency-based noise filtering, and progressive trajectory refinement are designed in the post-processing pipeline. Experimental results on the public SpotGEO dataset demonstrate the effectiveness of the proposed method, achieving an F_1 score of 90.14%.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.19220", "pdf_url": "https://arxiv.org/pdf/2510.19220.pdf", "is_interesting": false}, "320": {"title": "Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism", "authors": ["Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang"], "abstract": "arXiv:2510.19618v3 Announce Type: replace \nMulti-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at https://github.com/jeffreychou777/GenComm.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.19618", "pdf_url": "https://arxiv.org/pdf/2510.19618.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u8f66\u8def\u534f\u540c / \u591a\u4f20\u611f\u5668\u878d\u5408", "reason": "The paper discusses multi-agent collaboration for perception in heterogeneous systems, which is relevant to vehicle-to-everything (V2X) and collaborative sensing in autonomous driving. It addresses challenges in integrating new agents and sharing spatial information, which are crucial for improving vehicle perception and collaboration in autonomous driving systems."}}, "321": {"title": "Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization", "authors": ["Adinath Dukre, Ankan Deria, Yutong Xie, Imran Razzak"], "abstract": "arXiv:2510.22630v2 Announce Type: replace \nAtypical mitotic figures are important biomarkers of tumor aggressiveness in histopathology, yet reliable recognition remains challenging due to severe class imbalance and variability across imaging domains. We present a DenseNet-121-based framework tailored for atypical mitosis classification in the MIDOG 2025 (Track 2) setting. Our method integrates stain-aware augmentation (Macenko), geometric and intensity transformations, and imbalance-aware learning via weighted sampling with a hybrid objective combining class-weighted binary cross-entropy and focal loss. Trained end-to-end with AdamW and evaluated across multiple independent domains, the model demonstrates strong generalization under scanner and staining shifts, achieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and specificity 80.9% on the official test set. These results indicate that combining DenseNet-121 with stain-aware augmentation and imbalance-adaptive objectives yields a robust, domain-generalizable framework for atypical mitosis classification suitable for real-world computational pathology workflows.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.22630", "pdf_url": "https://arxiv.org/pdf/2510.22630.pdf", "is_interesting": false}, "322": {"title": "Cross-view Localization and Synthesis -- Datasets, Challenges and Opportunities", "authors": ["Ningli Xu, Rongjun Qin"], "abstract": "arXiv:2510.22736v2 Announce Type: replace \nCross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery. These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality. Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery. Both tasks remain challenging due to significant differences in viewing perspective, resolution, and occlusion, which are widely embedded in cross-view datasets. Recent years have witnessed rapid progress driven by the availability of large-scale datasets and novel approaches. Typically, cross-view localization is formulated as an image retrieval problem where ground-level features are matched with tiled overhead images feature, extracted by convolutional neural networks (CNNs) or vision transformers (ViTs) for cross-view feature embedding. Cross-view synthesis, on the other hand, seeks to generate ground-level views based on information from overhead imagery, generally using generative adversarial networks (GANs) or diffusion models. This paper presents a comprehensive survey of advances in cross-view localization and synthesis, reviewing widely used datasets, highlighting key challenges, and providing an organized overview of state-of-the-art techniques. Furthermore, it discusses current limitations, offers comparative analyses, and outlines promising directions for future research. We also include the project page via https://github.com/GDAOSU/Awesome-Cross-View-Methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.22736", "pdf_url": "https://arxiv.org/pdf/2510.22736.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on cross-view localization and synthesis, which are related to visual understanding but primarily for applications like urban planning and augmented reality. Although these tasks could have indirect applications in autonomous driving, the paper does not specifically address autonomous driving systems or related tasks such as perception, prediction, or control."}}, "323": {"title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity", "authors": ["Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi"], "abstract": "arXiv:2510.23603v2 Announce Type: replace \nMultimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.23603", "pdf_url": "https://arxiv.org/pdf/2510.23603.pdf", "is_interesting": false}, "324": {"title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "authors": ["Forouzan Fallah, Wenwen Li, Chia-Yu Hsu, Hyunho Lee, Yezhou Yang"], "abstract": "arXiv:2510.23816v2 Announce Type: replace \nSuper-resolution (SR) for remote sensing imagery often fails under out-of-distribution (OOD) conditions, such as rare geomorphic features captured by diverse sensors, producing visually plausible but physically inaccurate results. We present RareFlow, a physics-aware SR framework designed for OOD robustness. RareFlow's core is a dual-conditioning architecture. A Gated ControlNet preserves fine-grained geometric fidelity from the low-resolution input, while textual prompts provide semantic guidance for synthesizing complex features. To ensure physically sound outputs, we introduce a multifaceted loss function that enforces both spectral and radiometric consistency with sensor properties. Furthermore, the framework quantifies its own predictive uncertainty by employing a stochastic forward pass approach; the resulting output variance directly identifies unfamiliar inputs, mitigating feature hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor satellite imagery. In blind evaluations, geophysical experts rated our model's outputs as approaching the fidelity of ground truth imagery, significantly outperforming state-of-the-art baselines. This qualitative superiority is corroborated by quantitative gains in perceptual metrics, including a nearly 40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity synthesis in data-scarce scientific domains and offers a new paradigm for controlled generation under severe domain shift.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.23816", "pdf_url": "https://arxiv.org/pdf/2510.23816.pdf", "is_interesting": false}, "325": {"title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "authors": ["Ali Ahmad Faour, Nabil Amacha, Ali J. Ghandour"], "abstract": "arXiv:2510.24413v2 Announce Type: replace \nThe sustainable management of the Qaraaoun Reservoir, the largest surface water body in Lebanon located in the Bekaa Plain, depends on reliable monitoring of its storage volume despite frequent sensor malfunctions and limited maintenance capacity. This study introduces a sensor-free approach that integrates open-source satellite imagery, advanced water-extent segmentation, and machine learning to estimate the reservoir's surface area and, subsequently, its volume in near real time. Sentinel-2 and Landsat 1-9 images are processed, where surface water is delineated using a newly proposed water segmentation index. A machine learning model based on Support Vector Regression (SVR) is trained on a curated dataset that includes water surface area, water level, and water volume derived from a reservoir bathymetric survey. The model is then able to estimate the water body's volume solely from the extracted water surface, without the need for any ground-based measurements. Water segmentation using the proposed index aligns with ground truth for over 95% of the shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR performance, with an error below 1.5% of the full reservoir capacity and coefficients of determination exceeding 0.98. These results demonstrate the method's robustness and cost-effectiveness, offering a practical solution for continuous, sensor-independent monitoring of reservoir storage. The proposed methodology is applicable to other water bodies and generates over five decades of time-series data, offering valuable insights into climate change and environmental dynamics, with an emphasis on capturing temporal trends rather than exact water volume measurements.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.24413", "pdf_url": "https://arxiv.org/pdf/2510.24413.pdf", "is_interesting": false}, "326": {"title": "A Quantitative Evaluation Framework for Explainable AI in Semantic Segmentation", "authors": ["Reem Hammoud, Abdul karim Gizzini, Ali J. Ghandour"], "abstract": "arXiv:2510.24414v2 Announce Type: replace \nEnsuring transparency and trust in artificial intelligence (AI) models is essential as they are increasingly deployed in safety-critical and high-stakes domains. Explainable AI (XAI) has emerged as a promising approach to address this challenge; however, the rigorous evaluation of XAI methods remains vital for balancing the trade-offs between model complexity, predictive performance, and interpretability. While substantial progress has been made in evaluating XAI for classification tasks, strategies tailored to semantic segmentation remain limited. Moreover, objectively assessing XAI approaches is difficult, since qualitative visual explanations provide only preliminary insights. Such qualitative methods are inherently subjective and cannot ensure the accuracy or stability of explanations. To address these limitations, this work introduces a comprehensive quantitative evaluation framework for assessing XAI in semantic segmentation, accounting for both spatial and contextual task complexities. The framework systematically integrates pixel-level evaluation strategies with carefully designed metrics to yield fine-grained interpretability insights. Simulation results using recently adapted class activation mapping (CAM)-based XAI schemes demonstrate the efficiency, robustness, and reliability of the proposed methodology. These findings advance the development of transparent, trustworthy, and accountable semantic segmentation models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.24414", "pdf_url": "https://arxiv.org/pdf/2510.24414.pdf", "is_interesting": false}, "327": {"title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "authors": ["Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro"], "abstract": "arXiv:2510.24448v2 Announce Type: replace \nLarge language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.24448", "pdf_url": "https://arxiv.org/pdf/2510.24448.pdf", "is_interesting": false}, "328": {"title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "authors": ["Charles Javerliat, Pierre Raimbaud, Guillaume Lavou\\'e"], "abstract": "arXiv:2510.24464v2 Announce Type: replace \nMarkerless multiview motion capture is often constrained by the need for precise camera calibration, limiting accessibility for non-experts and in-the-wild captures. Existing calibration-free approaches mitigate this requirement but suffer from high computational cost and reduced reconstruction accuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless motion capture from videos captured by unsynchronized, uncalibrated, consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf detectors to simultaneously calibrate cameras, including Brown-Conrady distortion coefficients, and reconstruct 3D keypoints and dense scene point maps at metric scale. A confidence-driven spatio-temporal keypoint sampling strategy, combined with graph-based global optimization, ensures robust calibration at a fixed computational cost independent of sequence length. We further introduce a pairwise reprojection consensus score to quantify 3D reconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements over prior calibration-free methods. Compared to previous state-of-the-art approaches, Kineo reduces camera translation error by approximately 83-85%, camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by 83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage). The full pipeline and evaluation code are openly released to promote reproducibility and practical adoption at https://liris-xr.github.io/kineo/.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.24464", "pdf_url": "https://arxiv.org/pdf/2510.24464.pdf", "is_interesting": false}, "329": {"title": "D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction", "authors": ["Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang"], "abstract": "arXiv:2510.25173v2 Announce Type: replace \nRecently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.25173", "pdf_url": "https://arxiv.org/pdf/2510.25173.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper focuses on urban scene reconstruction and proposes a LiDAR-free method to enhance depth accuracy, which is crucial for autonomous driving. The approach improves geometry priors and depth maps, addressing challenges related to LiDAR data, which directly impacts 3D perception tasks in autonomous driving systems."}}, "330": {"title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks", "authors": ["Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu"], "abstract": "arXiv:2510.25760v2 Announce Type: replace \nHumans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.25760", "pdf_url": "https://arxiv.org/pdf/2510.25760.pdf", "is_interesting": false}, "331": {"title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "authors": ["Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang"], "abstract": "arXiv:2510.26466v2 Announce Type: replace \nObject-context shortcuts remain a persistent challenge in vision-language models, undermining zero-shot reliability when test-time scenes differ from familiar training co-occurrences. We recast this issue as a causal inference problem and ask: Would the prediction remain if the object appeared in a different environment? To answer this at inference time, we estimate object and background expectations within CLIP's representation space, and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions. By estimating the Total Direct Effect and simulating intervention, we further subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores. Without retraining or prompt design, our method substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing a new zero-shot state of the art. Beyond performance, our framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26466", "pdf_url": "https://arxiv.org/pdf/2510.26466.pdf", "is_interesting": false}, "332": {"title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment", "authors": ["Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou"], "abstract": "arXiv:2510.26781v2 Announce Type: replace \nCharts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.26781", "pdf_url": "https://arxiv.org/pdf/2510.26781.pdf", "is_interesting": false}, "333": {"title": "Dual-level Progressive Hardness-Aware Reweighting for Cross-View Geo-Localization", "authors": ["Guozheng Zheng, Jian Guan, Mingjie Xie, Xuanjia Zhao, Congyi Fan, Shiheng Zhang, Pengming Feng"], "abstract": "arXiv:2510.27181v2 Announce Type: replace \nCross-view geo-localization (CVGL) between drone and satellite imagery remains challenging due to severe viewpoint gaps and the presence of hard negatives, which are visually similar but geographically mismatched samples. Existing mining or reweighting strategies often use static weighting, which is sensitive to distribution shifts and prone to overemphasizing difficult samples too early, leading to noisy gradients and unstable convergence. In this paper, we present a Dual-level Progressive Hardness-aware Reweighting (DPHR) strategy. At the sample level, a Ratio-based Difficulty-Aware (RDA) module evaluates relative difficulty and assigns fine-grained weights to negatives. At the batch level, a Progressive Adaptive Loss Weighting (PALW) mechanism exploits a training-progress signal to attenuate noisy gradients during early optimization and progressively enhance hard-negative mining as training matures. Experiments on the University-1652 and SUES-200 benchmarks demonstrate the effectiveness and robustness of the proposed DPHR, achieving consistent improvements over state-of-the-art methods.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27181", "pdf_url": "https://arxiv.org/pdf/2510.27181.pdf", "is_interesting": false}, "334": {"title": "Enhancing Spatio-Temporal Zero-shot Action Recognition with Language-driven Description Attributes", "authors": ["Yehna Kim, Young-Eun Kim, Seong-Whan Lee"], "abstract": "arXiv:2510.27255v2 Announce Type: replace \nVision-Language Models (VLMs) have demonstrated impressive capabilities in zero-shot action recognition by learning to associate video embeddings with class embeddings. However, a significant challenge arises when relying solely on action classes to provide semantic context, particularly due to the presence of multi-semantic words, which can introduce ambiguity in understanding the intended concepts of actions. To address this issue, we propose an innovative approach that harnesses web-crawled descriptions, leveraging a large-language model to extract relevant keywords. This method reduces the need for human annotators and eliminates the laborious manual process of attribute data creation. Additionally, we introduce a spatio-temporal interaction module designed to focus on objects and action units, facilitating alignment between description attributes and video content. In our zero-shot experiments, our model achieves impressive results, attaining accuracies of 81.0%, 53.1%, and 68.9% on UCF-101, HMDB-51, and Kinetics-600, respectively, underscoring the model's adaptability and effectiveness across various downstream tasks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27255", "pdf_url": "https://arxiv.org/pdf/2510.27255.pdf", "is_interesting": false}, "335": {"title": "Image Hashing via Cross-View Code Alignment in the Age of Foundation Models", "authors": ["Ilyass Moummad, Kawtar Zaher, Herv\\'e Go\\\"eau, Alexis Joly"], "abstract": "arXiv:2510.27584v2 Announce Type: replace \nEfficient large-scale retrieval requires representations that are both compact and discriminative. Foundation models provide powerful visual and multimodal embeddings, but nearest neighbor search in these high-dimensional spaces is computationally expensive. Hashing offers an efficient alternative by enabling fast Hamming distance search with binary codes, yet existing approaches often rely on complex pipelines, multi-term objectives, designs specialized for a single learning paradigm, and long training times. We introduce CroVCA (Cross-View Code Alignment), a simple and unified principle for learning binary codes that remain consistent across semantically aligned views. A single binary cross-entropy loss enforces alignment, while coding-rate maximization serves as an anti-collapse regularizer to promote balanced and diverse codes. To implement this, we design HashCoder, a lightweight MLP hashing network with a final batch normalization layer to enforce balanced codes. HashCoder can be used as a probing head on frozen embeddings or to adapt encoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achieves state-of-the-art results in just 5 training epochs. At 16 bits, it particularly well-for instance, unsupervised hashing on COCO completes in under 2 minutes and supervised hashing on ImageNet100 in about 3 minutes on a single GPU. These results highlight CroVCA's efficiency, adaptability, and broad applicability.", "categories": ["cs.CV", "cs.IR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27584", "pdf_url": "https://arxiv.org/pdf/2510.27584.pdf", "is_interesting": false}, "336": {"title": "ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models", "authors": ["Shengming Li, Luping Liu, Runnan Li, Xu Tan"], "abstract": "arXiv:2301.12935v4 Announce Type: replace-cross \nThough denoising diffusion probabilistic models (DDPMs) have achieved remarkable generation results, the low sampling efficiency of DDPMs still limits further applications. Since DDPMs can be formulated as diffusion ordinary differential equations (ODEs), various fast sampling methods can be derived from solving diffusion ODEs. However, we notice that previous fast sampling methods with fixed analytical form are not able to robust with the various error patterns in the noise estimated from pretrained diffusion models. In this work, we construct an error-robust Adams solver (ERA-Solver), which utilizes the implicit Adams numerical method that consists of a predictor and a corrector. Different from the traditional predictor based on explicit Adams methods, we leverage a Lagrange interpolation function as the predictor, which is further enhanced with an error-robust strategy to adaptively select the Lagrange bases with lower errors in the estimated noise. The proposed solver can be directly applied to any pretrained diffusion models, without extra training. Experiments on Cifar10, CelebA, LSUN-Church, and ImageNet 64 x 64 (conditional) datasets demonstrate that our proposed ERA-Solver achieves 3.54, 5.06, 5.02, and 5.11 Frechet Inception Distance (FID) for image generation, with only 10 network evaluations.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2301.12935", "pdf_url": "https://arxiv.org/pdf/2301.12935.pdf", "is_interesting": false}, "337": {"title": "Knolling Bot: Teaching Robots the Human Notion of Tidiness", "authors": ["Yuhang Hu, Judah Goldfeder, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Jiong Lin, Hod Lipson"], "abstract": "arXiv:2310.04566v3 Announce Type: replace-cross \nFor robots to truly collaborate and assist humans, they must understand not only logic and instructions, but also the subtle emotions, aesthetics, and feelings that define our humanity. Human art and aesthetics are among the most elusive concepts-often difficult even for people to articulate-and without grasping these fundamentals, robots will be unable to help in many spheres of daily life. Consider the long-promised robotic butler: automating domestic chores demands more than motion planning. It requires an internal model of cleanliness and tidiness-a challenge largely unexplored by AI. To bridge this gap, we propose an approach that equips domestic robots to perform simple tidying tasks via knolling, the practice of arranging scattered items into neat, space-efficient layouts. Unlike the uniformity of industrial settings, household environments feature diverse objects and highly subjective notions of tidiness. Drawing inspiration from NLP, we treat knolling as a sequential prediction problem and employ a transformer based model to forecast each object's placement. Our method learns a generalizable concept of tidiness, generates diverse solutions adaptable to varying object sets, and incorporates human preferences for personalized arrangements. This work represents a step forward in building robots that internalize human aesthetic sense and can genuinely co-create in our living spaces.", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2310.04566", "pdf_url": "https://arxiv.org/pdf/2310.04566.pdf", "is_interesting": false}, "338": {"title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning", "authors": ["Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon"], "abstract": "arXiv:2406.04772v5 Announce Type: replace-cross \nRecent rehearsal-free continual learning (CL) methods guided by prompts achieve strong performance on vision tasks with non-stationary data but remain resource-intensive, hindering real-world edge deployment. We introduce resource-efficient prompting (REP), which improves the computational and memory efficiency of prompt-based rehearsal-free continual learning methods while minimizing accuracy trade-offs. Our approach employs swift prompt selection to refine input data using a carefully provisioned model and introduces adaptive token merging (AToM) and adaptive layer dropping (ALD) for efficient prompt updates. AToM and ALD selectively skip data and model layers while preserving task-specific features during the learning of new tasks. Extensive experiments on multiple image classification datasets demonstrate REP's superior resource efficiency over state-of-the-art rehearsal-free CL methods.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2406.04772", "pdf_url": "https://arxiv.org/pdf/2406.04772.pdf", "is_interesting": false}, "339": {"title": "Finite element-based space-time total variation-type regularization of the inverse problem in electrocardiographic imaging", "authors": ["Manuel Haas, Thomas Grandits, Thomas Pinetz, Thomas Beiert, Simone Pezzuto, Alexander Effland"], "abstract": "arXiv:2408.11573v2 Announce Type: replace-cross \nReconstructing cardiac electrical activity from body surface electric potential measurements results in the severely ill-posed inverse problem in electrocardiography. Many different regularization approaches have been proposed to improve numerical results and provide unique results. This work presents a novel approach for reconstructing the epicardial potential from body surface potential maps based on a space-time total variation-type regularization using finite elements, where a first-order primal-dual algorithm solves the underlying convex optimization problem. In several numerical experiments, the superior performance of this method and the benefit of space-time regularization for the reconstruction of epicardial potential on two-dimensional torso data and a three-dimensional rabbit heart compared to state-of-the-art methods are demonstrated.", "categories": ["math.NA", "cs.CV", "cs.NA"], "abs_url": "https://arxiv.org/abs/2408.11573", "pdf_url": "https://arxiv.org/pdf/2408.11573.pdf", "is_interesting": false}, "340": {"title": "FIPER: Factorized Features for Robust Image Super-Resolution and Compression", "authors": ["Yang-Che Sun, Cheng Yu Yeo, Ernie Chu, Jun-Cheng Chen, Yu-Lun Liu"], "abstract": "arXiv:2410.18083v4 Announce Type: replace-cross \nIn this work, we propose using a unified representation, termed Factorized Features, for low-level vision tasks, where we test on Single Image Super-Resolution (SISR) and \\textbf{Image Compression}. Motivated by the shared principles between these tasks, they require recovering and preserving fine image details, whether by enhancing resolution for SISR or reconstructing compressed data for Image Compression. Unlike previous methods that mainly focus on network architecture, our proposed approach utilizes a basis-coefficient decomposition as well as an explicit formulation of frequencies to capture structural components and multi-scale visual features in images, which addresses the core challenges of both tasks. We replace the representation of prior models from simple feature maps with Factorized Features to validate the potential for broad generalizability. In addition, we further optimize the compression pipeline by leveraging the mergeable-basis property of our Factorized Features, which consolidates shared structures on multi-frame compression. Extensive experiments show that our unified representation delivers state-of-the-art performance, achieving an average relative improvement of 204.4% in PSNR over the baseline in Super-Resolution (SR) and 9.35% BD-rate reduction in Image Compression compared to the previous SOTA. Project page: https://jayisaking.github.io/FIPER/", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2410.18083", "pdf_url": "https://arxiv.org/pdf/2410.18083.pdf", "is_interesting": false}, "341": {"title": "Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity", "authors": ["Mou\\\"in Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi"], "abstract": "arXiv:2411.02184v3 Announce Type: replace-cross \nOut-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance. Specifically, we aim to answer the following question: Does the Double Descent phenomenon manifest in post-hoc OOD detection? This question is crucial, as it can reveal whether overparameterization, which is already known to benefit generalization, can also enhance OOD detection. Despite the growing interest in these topics by the classic supervised machine learning community, this intersection remains unexplored for OOD detection. We empirically demonstrate that the Double Descent effect does indeed appear in post-hoc OOD detection. Furthermore, we provide theoretical insights to explain why this phenomenon emerges in such setting. Finally, we show that the overparameterized regime does not yield superior results consistently, and we propose a method to identify the optimal regime for OOD detection based on our observations.", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "math.ST", "stat.TH"], "abs_url": "https://arxiv.org/abs/2411.02184", "pdf_url": "https://arxiv.org/pdf/2411.02184.pdf", "is_interesting": false}, "342": {"title": "Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving", "authors": ["Xinyu Zhang, Zewei Zhou, Zhaoyi Wang, Yangjie Ji, Yanjun Huang, Hong Chen"], "abstract": "arXiv:2502.16589v3 Announce Type: replace-cross \nVehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2502.16589", "pdf_url": "https://arxiv.org/pdf/2502.16589.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u8f68\u8ff9\u9884\u6d4b / \u8f66\u8def\u534f\u540c", "reason": "The paper introduces a cooperative trajectory prediction framework (Co-MTP) with multi-temporal fusion, leveraging V2X technologies to improve trajectory prediction and planning tasks in autonomous driving. The use of history and future interactions between agents is directly relevant to trajectory prediction, which is a key task in autonomous driving systems, and the framework involves cooperative perception, which is integral to vehicle-to-everything (V2X) communication in autonomous driving."}}, "343": {"title": "Split Gibbs Discrete Diffusion Posterior Sampling", "authors": ["Wenda Chu, Zihui Wu, Yifan Chen, Yang Song, Yisong Yue"], "abstract": "arXiv:2503.01161v3 Announce Type: replace-cross \nWe study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SGDD. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate the convergence of SGDD to the target posterior distribution and verify this through controlled experiments on synthetic benchmarks. Our method enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, including DNA sequence design, discrete image inverse problems, and music infilling, achieving more than 30% improved performance compared to existing baselines. Our code is available at https://github.com/chuwd19/Split-Gibbs-Discrete-Diffusion-Posterior-Sampling.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2503.01161", "pdf_url": "https://arxiv.org/pdf/2503.01161.pdf", "is_interesting": false}, "344": {"title": "As Good as It KAN Get: High-Fidelity Audio Representation", "authors": ["Patryk Marsza{\\l}ek, Maciej Rut, Piotr Kawa, Przemys{\\l}aw Spurek, Piotr Syga"], "abstract": "arXiv:2503.02585v3 Announce Type: replace-cross \nImplicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KAN's utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at https://github.com/gmum/fewsound.git.", "categories": ["cs.SD", "cs.CV", "eess.AS"], "abs_url": "https://arxiv.org/abs/2503.02585", "pdf_url": "https://arxiv.org/pdf/2503.02585.pdf", "is_interesting": false}, "345": {"title": "Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata Integration", "authors": ["Taejin Jeong, Joohyeok Kim, Jaehoon Joo, Seong Jae Hwang"], "abstract": "arXiv:2503.18642v2 Announce Type: replace-cross \nGlaucoma is a major cause of irreversible blindness, with significant diagnostic subjectivity. This inherent uncertainty, combined with the overconfidence of models optimized solely for accuracy can lead to fatal issues such as overdiagnosis or missing critical diseases. To ensure clinical trust, model calibration is essential for reliable predictions, yet study in this field remains limited. Existing calibration study have overlooked glaucoma's systemic associations and high diagnostic subjectivity. To overcome these limitations, we propose V-ViT (Voting-based ViT), a framework that enhances calibration by integrating a patient's binocular information and metadata. Furthermore, to mitigate diagnostic subjectivity, V-ViT utilizes an iterative dropout-based Voting System to maximize calibration performance. The proposed framework achieved state-of-the-art performance across all metrics, including the primary calibration metrics. Our results demonstrate that V-ViT effectively resolves the issue of overconfidence in predictions in glaucoma diagnosis, providing highly reliable predictions for clinical use. Our source code is available at https://github.com/starforTJ/V-ViT.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2503.18642", "pdf_url": "https://arxiv.org/pdf/2503.18642.pdf", "is_interesting": false}, "346": {"title": "Improved visual-information-driven model for crowd simulation and its modular application", "authors": ["Xuanwen Liang, Jiayu Chen, Eric Wai Ming Lee, Wei Xie"], "abstract": "arXiv:2504.03758v3 Announce Type: replace-cross \nData-driven crowd simulation models offer advantages in enhancing the accuracy and realism of simulations, and improving their generalizability is essential for promoting application. Current data-driven approaches are primarily designed for a single scenario, with very few models validated across more than two scenarios. It is still an open question to develop data-driven crowd simulation models with strong generalizibility. We notice that the key to addressing this challenge lies in effectively and accurately capturing the core common influential features that govern pedestrians' navigation across diverse scenarios. Particularly, we believe that visual information is one of the most dominant influencing features. In light of this, this paper proposes a data-driven model incorporating a refined visual information extraction method and exit cues to enhance generalizability. The proposed model is examined on four common fundamental modules: bottleneck, corridor, corner and T-junction. The evaluation results demonstrate that our model performs excellently across these scenarios, aligning with pedestrian movement in real-world experiments, and significantly outperforms the classical knowledge-driven model. Furthermore, we introduce a modular approach to apply our proposed model in composite scenarios, and the results regarding trajectories and fundamental diagrams indicate that our simulations closely match real-world patterns in the composite scenario. The research outcomes can provide inspiration for the development of data-driven crowd simulation models with high generalizability and advance the application of data-driven approaches.This work has been submitted to Elsevier for possible publication.", "categories": ["cs.CY", "cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2504.03758", "pdf_url": "https://arxiv.org/pdf/2504.03758.pdf", "is_interesting": false}, "347": {"title": "Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2", "authors": ["Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski"], "abstract": "arXiv:2505.01854v2 Announce Type: replace-cross \nManual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on four public datasets covering organs, bones, and muscles across MRI, CT, and ultrasound videos. We show that the proposed method markedly outperforms the default SAM 2, achieving an average Dice Similarity Coefficient improvement of 0.14 and 0.10 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, reducing the time required to correct propagated masks by 60.575% per volume compared to SAM 2, making a notable step toward more accurate automated annotation of medical images for segmentation model development.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2505.01854", "pdf_url": "https://arxiv.org/pdf/2505.01854.pdf", "is_interesting": false}, "348": {"title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "authors": ["Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Kehai Chen, Min Zhang, Huajun Chen, Ningyu Zhang"], "abstract": "arXiv:2505.22633v2 Announce Type: replace-cross \nRecent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at https://github.com/zjunlp/Knowledge2Data.", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "abs_url": "https://arxiv.org/abs/2505.22633", "pdf_url": "https://arxiv.org/pdf/2505.22633.pdf", "is_interesting": false}, "349": {"title": "Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation", "authors": ["Nicholas Summerfield, Qisheng He, Alex Kuo, Ahmed I. Ghanem, Simeng Zhu, Chase Ruff, Joshua Pan, Anudeep Kumar, Prashant Nagpal, Jiwei Zhao, Ming Dong, Carri K. Glide-Hurst"], "abstract": "arXiv:2506.10797v2 Announce Type: replace-cross \nCardiac substructure delineation is emerging in treatment planning to minimize the risk of radiation-induced heart disease. Deep learning offers efficient methods to reduce contouring burden but currently lacks generalizability across different modalities and overlapping structures. This work introduces and validates a Modality-AGnostic Image Cascade (MAGIC) deep-learning pipeline for comprehensive and multi-modal cardiac substructure segmentation. MAGIC is implemented through replicated encoding and decoding branches of an nnU-Net backbone to handle multi-modality inputs and overlapping labels. First benchmarked on the multi-modality whole-heart segmentation (MMWHS) dataset including cardiac CT-angiography (CCTA) and MR modalities, twenty cardiac substructures (heart, chambers, great vessels (GVs), valves, coronary arteries (CAs), and conduction nodes) from clinical simulation CT (Sim-CT), low-field MR-Linac, and cardiac CT-angiography (CCTA) modalities were delineated to train semi-supervised (n=151), validate (n=15), and test (n=30) MAGIC. For comparison, fourteen single-modality comparison models (two MMWHS modalities and four subgroups across three clinical modalities) were trained. Methods were evaluated for efficiency and against reference contours through the Dice similarity coefficient (DSC) and two-tailed Wilcoxon Signed-Rank test (p<0.05). Average MMWHS DSC scores across CCTA and MR inputs were 0.88(0.08) and 0.87(0.04) respectively with significant improvement over unimodal baselines. Average 20-structure DSC scores were 0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. Furthermore, >80% and >70% reductions in training time and parameters were achieved, respectively. MAGIC offers an efficient, lightweight solution capable of segmenting multiple image modalities and overlapping structures in a single model without compromising segmentation accuracy.", "categories": ["physics.med-ph", "cs.CV"], "abs_url": "https://arxiv.org/abs/2506.10797", "pdf_url": "https://arxiv.org/pdf/2506.10797.pdf", "is_interesting": false}, "350": {"title": "Anti-Aliased 2D Gaussian Splatting", "authors": ["Mae Younes, Adnane Boukhayma"], "abstract": "arXiv:2506.11252v2 Announce Type: replace-cross \n2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an anti-aliased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world-space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object-space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.", "categories": ["cs.GR", "cs.CV"], "abs_url": "https://arxiv.org/abs/2506.11252", "pdf_url": "https://arxiv.org/pdf/2506.11252.pdf", "is_interesting": false}, "351": {"title": "Autoadaptive Medical Segment Anything Model", "authors": ["Tyler Ward, Meredith K. Owen, O'Kira Coleman, Brian Noehren, Abdullah-Al-Zubaer Imran"], "abstract": "arXiv:2507.01828v2 Announce Type: replace-cross \nMedical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose ADA-SAM (automated, domain-specific, and adaptive segment anything model), a novel multitask learning framework for medical image segmentation that leverages class activation maps from an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the Segment Anything (SAM) framework. Additionally, our ADA-SAM model employs a novel gradient feedback mechanism to create a learnable connection between the segmentation and classification branches by using the segmentation gradients to guide and improve the classification predictions. We validate ADA-SAM on real-world clinical data collected during rehabilitation trials, and demonstrate that our proposed method outperforms both fully-supervised and semi-supervised baselines by double digits in limited label settings. Our code is available at: https://github.com/tbwa233/ADA-SAM.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2507.01828", "pdf_url": "https://arxiv.org/pdf/2507.01828.pdf", "is_interesting": false}, "352": {"title": "MOSPA: Human Motion Generation Driven by Spatial Audio", "authors": ["Shuyang Xu, Zhiyang Dou, Mingyi Shi, Liang Pan, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang, Taku Komura"], "abstract": "arXiv:2507.11949v2 Announce Type: replace-cross \nEnabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse, realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation", "categories": ["cs.GR", "cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2507.11949", "pdf_url": "https://arxiv.org/pdf/2507.11949.pdf", "is_interesting": false}, "353": {"title": "Joint Lossless Compression and Steganography for Medical Images via Large Language Models", "authors": ["Pengcheng Zheng, Xiaorong Pu, Kecheng Chen, Jiaxin Huang, Meng Yang, Bai Feng, Yazhou Ren, Jianan Jiang"], "abstract": "arXiv:2508.01782v2 Announce Type: replace-cross \nRecently, large language models (LLMs) have driven promising progress in lossless image compression. However, directly adopting existing paradigms for medical images suffers from an unsatisfactory trade-off between compression performance and efficiency. Moreover, existing LLM-based compressors often overlook the security of the compression process, which is critical in modern medical scenarios. To this end, we propose a novel joint lossless compression and steganography framework. Inspired by bit plane slicing (BPS), we find it feasible to securely embed privacy messages into medical images in an invisible manner. Based on this insight, an adaptive modalities decomposition strategy is first devised to partition the entire image into two segments, providing global and local modalities for subsequent dual-path lossless compression. During this dual-path stage, we innovatively propose a segmented message steganography algorithm within the local modality path to ensure the security of the compression process. Coupled with the proposed anatomical priors-based low-rank adaptation (A-LoRA) fine-tuning strategy, extensive experimental results demonstrate the superiority of our proposed method in terms of compression ratios, efficiency, and security. The source code will be made publicly available.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2508.01782", "pdf_url": "https://arxiv.org/pdf/2508.01782.pdf", "is_interesting": false}, "354": {"title": "AI-Driven Detection and Analysis of Handwriting on Seized Ivory: A Tool to Uncover Criminal Networks in the Illicit Wildlife Trade", "authors": ["Will Fein, Ryan J. Horwitz, John E. Brown III, Amit Misra, Felipe Oviedo, Kevin White, Juan M. Lavista Ferres, Samuel K. Wasser"], "abstract": "arXiv:2508.10219v3 Announce Type: replace-cross \nThe transnational ivory trade continues to drive the decline of elephant populations across Africa, and trafficking networks remain difficult to disrupt. Tusks seized by law enforcement officials carry forensic information on the traffickers responsible for their export, including DNA evidence and handwritten markings made by traffickers. For 20 years, analyses of tusk DNA have identified where elephants were poached and established connections among shipments of ivory. While the links established using genetic evidence are extremely conclusive, genetic data is expensive and sometimes impossible to obtain. But though handwritten markings are easy to photograph, they are rarely documented or analyzed. Here, we present an AI-driven pipeline for extracting and analyzing handwritten markings on seized elephant tusks, offering a novel, scalable, and low-cost source of forensic evidence. Having collected 6,085 photographs from eight large seizures of ivory over a 6-year period (2014-2019), we used an object detection model to extract over 17,000 individual markings, which were then labeled and described using state-of-the-art AI tools. We identified 184 recurring \"signature markings\" that connect the tusks on which they appear. 20 signature markings were observed in multiple seizures, establishing forensic links between these seizures through traffickers involved in both shipments. This work complements other investigative techniques by filling in gaps where other data sources are unavailable. The study demonstrates the transformative potential of AI in wildlife forensics and highlights practical steps for integrating handwriting analysis into efforts to disrupt organized wildlife crime.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2508.10219", "pdf_url": "https://arxiv.org/pdf/2508.10219.pdf", "is_interesting": false}, "355": {"title": "SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning", "authors": ["Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song"], "abstract": "arXiv:2508.10298v3 Announce Type: replace-cross \nDeciphering how visual stimuli are transformed into cortical responses is a fundamental challenge in computational neuroscience. This visual-to-neural mapping is inherently a one-to-many relationship, as identical visual inputs reliably evoke variable hemodynamic responses across trials, contexts, and subjects. However, existing deterministic methods struggle to simultaneously model this biological variability while capturing the underlying functional consistency that encodes stimulus information. To address these limitations, we propose SynBrain, a generative framework that simulates the transformation from visual semantics to neural responses in a probabilistic and biologically interpretable manner. SynBrain introduces two key components: (i) BrainVAE models neural representations as continuous probability distributions via probabilistic learning while maintaining functional consistency through visual semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic transmission pathway, projecting visual semantics into the neural response manifold to facilitate high-fidelity fMRI synthesis. Experimental results demonstrate that SynBrain surpasses state-of-the-art methods in subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain adapts efficiently to new subjects with few-shot data and synthesizes high-quality fMRI signals that are effective in improving data-limited fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional consistency across trials and subjects, with synthesized signals capturing interpretable patterns shaped by biological neural variability. Our code is available at https://github.com/MichaelMaiii/SynBrain.", "categories": ["cs.LG", "cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2508.10298", "pdf_url": "https://arxiv.org/pdf/2508.10298.pdf", "is_interesting": false}, "356": {"title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs", "authors": ["Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Arnaud Dapogny, Alasdair Newson, Matthieu Cord"], "abstract": "arXiv:2508.12815v2 Announce Type: replace-cross \nSteering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines. Our code is publicly available at https://jayneelparekh.github.io/learn-to-steer/", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "abs_url": "https://arxiv.org/abs/2508.12815", "pdf_url": "https://arxiv.org/pdf/2508.12815.pdf", "is_interesting": false}, "357": {"title": "From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China", "authors": ["Weihuan Deng, Yaofu Huang, Luan Chen, Xun Li, Yu Gu, Yao Yao"], "abstract": "arXiv:2508.21738v2 Announce Type: replace-cross \nThe high cost of acquiring rural street view images has constrained comprehensive environmental perception in rural areas. Drone photographs, with their advantages of easy acquisition, broad coverage, and high spatial resolution, offer a viable approach for large-scale rural environmental perception. However, a systematic methodology for identifying key environmental elements from drone photographs and quantifying their impact on environmental perception remains lacking. To address this gap, a Vision-Language Contrastive Ranking Framework (VLCR) is designed for rural livability assessment in China. The framework employs chain-of-thought prompting strategies to guide multimodal large language models (MLLMs) in identifying visual features related to quality of life and ecological habitability from drone photographs. Subsequently, to address the instability in pairwise village comparison, a text description-constrained drone photograph comparison strategy is proposed. Finally, to overcome the efficiency bottleneck in nationwide pairwise village comparisons, an innovation ranking algorithm based on binary search interpolation is developed, which reduces the number of comparisons through automated selection of comparison targets. The proposed framework achieves superior performance with a Spearman Footrule distance of 0.74, outperforming mainstream commercial MLLMs by approximately 0.1. Moreover, the mechanism of concurrent comparison and ranking demonstrates a threefold enhancement in computational efficiency. Our framework has achieved data innovation and methodological breakthroughs in village livability assessment, providing strong support for large-scale village livability analysis.\n  Keywords: Drone photographs, Environmental perception, Rural livability assessment, Multimodal large language models, Chain-of-thought prompting.", "categories": ["cs.CY", "cs.CV"], "abs_url": "https://arxiv.org/abs/2508.21738", "pdf_url": "https://arxiv.org/pdf/2508.21738.pdf", "is_interesting": false}, "358": {"title": "Audio Driven Real-Time Facial Animation for Social Telepresence", "authors": ["Jiye Lee, Chenghui Li, Linh Tran, Shih-En Wei, Jason Saragih, Alexander Richard, Hanbyul Joo, Shaojie Bai"], "abstract": "arXiv:2510.01176v2 Announce Type: replace-cross \nWe present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.SD"], "abs_url": "https://arxiv.org/abs/2510.01176", "pdf_url": "https://arxiv.org/pdf/2510.01176.pdf", "is_interesting": false}, "359": {"title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation", "authors": ["Andreas Engelhardt, Mark Boss, Vikram Voleti, Chun-Han Yao, Hendrik P. A. Lensch, Varun Jampani"], "abstract": "arXiv:2510.08271v2 Announce Type: replace-cross \nWe present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.", "categories": ["cs.GR", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.08271", "pdf_url": "https://arxiv.org/pdf/2510.08271.pdf", "is_interesting": false}, "360": {"title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "authors": ["Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng"], "abstract": "arXiv:2510.12709v3 Announce Type: replace-cross \nMultimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.1% AUC gain.", "categories": ["cs.IR", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.12709", "pdf_url": "https://arxiv.org/pdf/2510.12709.pdf", "is_interesting": false}, "361": {"title": "VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation", "authors": ["Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He"], "abstract": "arXiv:2510.15530v4 Announce Type: replace-cross \nIn the context of imitation learning, visuomotor-based diffusion policy learning is one of the main directions in robotic manipulation. Most of these approaches rely on point clouds as observation inputs and construct scene representations through point clouds feature learning, which enables them to achieve remarkable accuracy. However, the existing literature lacks an in-depth exploration of vision-only solutions that have significant potential. In this paper, we propose a Vision-Only and single-view Diffusion Policy learning method (VO-DP) that leverages pretrained visual foundation models to achieve effective fusion of semantic and geometric features. We utilize intermediate features from VGGT incorporating semantic features from DINOv2 and geometric features from Alternating Attention blocks. Features are fused via cross-attention and spatially compressed with a CNN to form the input to the policy head. Extensive experiments demonstrate that VO-DP not only outperforms the vision-only baseline DP significantly but also exhibits distinct performance trends against the point cloud-based method DP3: in simulation tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0% and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%, outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further robustness evaluations confirm that VO-DP remains highly stable under varying conditions including color, size, background, and lighting. Lastly, we open-source a training library for robotic manipulation. Built on Accelerate, this library supports multi-machine and multi-GPU parallel training, as well as mixed precision training. It is compatible with visuomotor policies such as DP, DP3 and VO-DP, and also supports the RoboTwin simulator.", "categories": ["cs.RO", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.15530", "pdf_url": "https://arxiv.org/pdf/2510.15530.pdf", "is_interesting": false}, "362": {"title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation", "authors": ["Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang"], "abstract": "arXiv:2510.19755v3 Announce Type: replace-cross \nDiffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.\n  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.19755", "pdf_url": "https://arxiv.org/pdf/2510.19755.pdf", "is_interesting": false}, "363": {"title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs", "authors": ["Anand, Umberto Cappellazzo, Stavros Petridis, Maja Pantic"], "abstract": "arXiv:2510.22603v2 Announce Type: replace-cross \nLarge language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.", "categories": ["eess.AS", "cs.CV", "cs.SD"], "abs_url": "https://arxiv.org/abs/2510.22603", "pdf_url": "https://arxiv.org/pdf/2510.22603.pdf", "is_interesting": false}, "364": {"title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "authors": ["Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yu-Gang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu"], "abstract": "arXiv:2510.23763v3 Announce Type: replace-cross \nRecent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.", "categories": ["cs.RO", "cs.CL", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.23763", "pdf_url": "https://arxiv.org/pdf/2510.23763.pdf", "is_interesting": false}, "365": {"title": "DMVFC: Deep Learning Based Functionally Consistent Tractography Fiber Clustering Using Multimodal Diffusion MRI and Functional MRI", "authors": ["Bocheng Guo, Jin Wang, Yijie Li, Junyi Wang, Mingyu Gao, Puming Feng, Yuqian Chen, Jarrett Rushmore, Nikos Makris, Yogesh Rathi, Lauren J O'Donnell, Fan Zhang"], "abstract": "arXiv:2510.24770v2 Announce Type: replace-cross \nTractography fiber clustering using diffusion MRI (dMRI) is a crucial method for white matter (WM) parcellation to enable analysis of brains structural connectivity in health and disease. Current fiber clustering strategies primarily use the fiber geometric characteristics (i.e., the spatial trajectories) to group similar fibers into clusters, while neglecting the functional and microstructural information of the fiber tracts. There is increasing evidence that neural activity in the WM can be measured using functional MRI (fMRI), providing potentially valuable multimodal information for fiber clustering to enhance its functional coherence. Furthermore, microstructural features such as fractional anisotropy (FA) can be computed from dMRI as additional information to ensure the anatomical coherence of the clusters. In this paper, we develop a novel deep learning fiber clustering framework, namely Deep Multi-view Fiber Clustering (DMVFC), which uses joint multi-modal dMRI and fMRI data to enable functionally consistent WM parcellation. DMVFC can effectively integrate the geometric and microstructural characteristics of the WM fibers with the fMRI BOLD signals along the fiber tracts. DMVFC includes two major components: (1) a multi-view pretraining module to compute embedding features from each source of information separately, including fiber geometry, microstructure measures, and functional signals, and (2) a collaborative fine-tuning module to simultaneously refine the differences of embeddings. In the experiments, we compare DMVFC with two state-of-the-art fiber clustering methods and demonstrate superior performance in achieving functionally meaningful and consistent WM parcellation results.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.24770", "pdf_url": "https://arxiv.org/pdf/2510.24770.pdf", "is_interesting": false}, "366": {"title": "Generative diffusion modeling protocols for improving the Kikuchi pattern indexing in electron back-scatter diffraction", "authors": ["Meghraj Prajapat, Alankar Alankar"], "abstract": "arXiv:2510.26907v2 Announce Type: replace-cross \nElectron back-scatter diffraction (EBSD) has traditionally relied upon methods such as the Hough transform and dictionary Indexing to interpret diffraction patterns and extract crystallographic orientation. However, these methods encounter significant limitations, particularly when operating at high scanning speeds, where the exposure time per pattern is decreased beyond the operating sensitivity of CCD camera. Hence the signal to noise ratio decreases for the observed pattern which makes the pattern noisy, leading to reduced indexing accuracy. This research work aims to develop generative machine learning models for the post-processing or on-the-fly processing of Kikuchi patterns which are capable of restoring noisy EBSD patterns obtained at high scan speeds. These restored patterns can be used for the determination of crystal orientations to provide reliable indexing results. We compare the performance of such generative models in enhancing the quality of patterns captured at short exposure times (high scan speeds). An interesting observation is that the methodology is not data-hungry as typical machine learning methods.", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.26907", "pdf_url": "https://arxiv.org/pdf/2510.26907.pdf", "is_interesting": false}, "367": {"title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions", "authors": ["Lingyue Fu, Xin Ding, Yaoming Zhu, Shao Zhang, Lin Qiu, Weiwen Liu, Weinan Zhang, Xuezhi Cao, Xunliang Cai, Jiaxin Ding, Yong Yu"], "abstract": "arXiv:2510.26852v1 Announce Type: new \nLarge Language Model (LLM) agents have evolved from basic text generation to autonomously completing complex tasks through interaction with external tools. However, current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve. In this work, we emphasize the importance of learning ability, including both self-improvement and peer-learning, as a core driver for agent evolution toward human-level intelligence. We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback, thereby systematically evaluating their learning capabilities. To address the score saturation issue in current benchmarks, we introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring. By providing tasks without explicit upper score limits, CATArena enables continuous and dynamic evaluation of rapidly advancing agent capabilities. Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.26852", "pdf_url": "https://arxiv.org/pdf/2510.26852.pdf", "is_interesting": false}, "368": {"title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base", "authors": ["Yu Li, Yuan Huang, Tao Wang, Caiyu Fan, Xiansheng Cai, Sihan Hu, Xinzijian Liu, Cheng Shi, Mingjun Xu, Zhen Wang, Yan Wang, Xiangqi Jin, Tianhan Zhang, Linfeng Zhang, Lei Wang, Youjin Deng, Pan Zhang, Weijie Sun, Xingyu Li, Weinan E, Linfeng Zhang, Zhiyuan Yao, Kun Chen"], "abstract": "arXiv:2510.26854v1 Announce Type: new \nMost scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26854", "pdf_url": "https://arxiv.org/pdf/2510.26854.pdf", "is_interesting": false}, "369": {"title": "The Denario project: Deep knowledge AI agents for scientific discovery", "authors": ["Francisco Villaescusa-Navarro, Boris Bolliet, Pablo Villanueva-Domingo, Adrian E. Bayer, Aidan Acquah, Chetana Amancharla, Almog Barzilay-Siegal, Pablo Bermejo, Camille Bilodeau, Pablo C\\'ardenas Ram\\'irez, Miles Cranmer, Urbano L. Fran\\c{c}a, ChangHoon Hahn, Yan-Fei Jiang, Raul Jimenez, Jun-Young Lee, Antonio Lerario, Osman Mamun, Thomas Meier, Anupam A. Ojha, Pavlos Protopapas, Shimanto Roy, David N. Spergel, Pedro Taranc\\'on-\\'Alvarez, Ujjwal Tiwari, Matteo Viel, Digvijay Wadekar, Chi Wang, Bonny Y. Wang, Licong Xu, Yossi Yovel, Shuwen Yue, Wen-Han Zhou, Qiyao Zhu, Jiajun Zou, \\'I\\~nigo Zubeldia"], "abstract": "arXiv:2510.26887v1 Announce Type: new \nWe present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2510.26887", "pdf_url": "https://arxiv.org/pdf/2510.26887.pdf", "is_interesting": false}, "370": {"title": "Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations", "authors": ["Pedro Antonio Alarc\\'on Granadeno, Arturo Miguel Bernal Russell, Sofia Nelson, Demetrius Hernandez, Maureen Petterson, Michael Murphy, Walter J. Scheirer, Jane Cleland-Huang"], "abstract": "arXiv:2510.26905v1 Announce Type: new \nCyber-physical systems increasingly rely on Foundational Models such as Large Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy through enhanced perception, inference, and planning. However, these models also introduce new types of errors, such as hallucinations, overgeneralizations, and context misalignments, resulting in incorrect and flawed decisions. To address this, we introduce the concept of Cognition Envelopes, designed to establish reasoning boundaries that constrain AI-generated decisions while complementing the use of meta-cognition and traditional safety envelopes. As with safety envelopes, Cognition Envelopes require practical guidelines and systematic processes for their definition, validation, and assurance.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26905", "pdf_url": "https://arxiv.org/pdf/2510.26905.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u89c4\u5212\u63a7\u5236 / \u7aef\u5230\u7aef\u63a7\u5236", "reason": "The paper discusses the application of foundational models (like LLMs and VLMs) to enhance autonomy in cyber-physical systems, including AI reasoning for operations. This is relevant to autonomous driving as it touches on improving decision-making and planning in autonomous systems, though the primary focus is on broader cyber-physical systems rather than specifically autonomous vehicles."}}, "371": {"title": "SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation", "authors": ["Agorakis Bompotas, Konstantinos Koutras, Nikitas Rigas Kalogeropoulos, Panagiotis Kechagias, Dimitra Gariza, Athanasios P. Kalogeras, Christos Alexakos"], "abstract": "arXiv:2510.26989v1 Announce Type: new \nThe global agricultural sector is undergoing a transformative shift, driven by increasing food demands, climate variability and the need for sustainable practices. SUSTAINABLE is a smart farming platform designed to integrate IoT, AI, satellite imaging, and role-based task orchestration to enable efficient, traceable, and sustainable agriculture with a pilot usecase in viticulture. This paper explores current smart agriculture solutions, presents a comparative evaluation, and introduces SUSTAINABLE's key features, including satellite index integration, real-time environmental data, and role-aware task management tailored to Mediterranean vineyards.", "categories": ["cs.AI", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2510.26989", "pdf_url": "https://arxiv.org/pdf/2510.26989.pdf", "is_interesting": false}, "372": {"title": "Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models", "authors": ["Jared Junkin, Samuel Nathanson"], "abstract": "arXiv:2510.27009v1 Announce Type: new \nLanguage models are traditionally designed around causal masking. In domains with spatial or relational structure, causal masking is often viewed as inappropriate, and sequential linearizations are instead used. Yet the question of whether it is viable to accept the information loss introduced by causal masking on nonsequential data has received little direct study, in part because few domains offer both spatial and sequential representations of the same dataset. In this work, we investigate this issue in the domain of chess, which naturally supports both representations. We train language models with bidirectional and causal self-attention mechanisms on both spatial (board-based) and sequential (move-based) data. Our results show that models trained on spatial board states - \\textit{even with causal masking} - consistently achieve stronger playing strength than models trained on sequential data. While our experiments are conducted on chess, our results are methodological and may have broader implications: applying causal masking to spatial data is a viable procedure for training unimodal LLMs on spatial data, and in some domains is even preferable to sequentialization.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.27009", "pdf_url": "https://arxiv.org/pdf/2510.27009.pdf", "is_interesting": false}, "373": {"title": "e1: Learning Adaptive Control of Reasoning Effort", "authors": ["Michael Kleinman, Matthew Trager, Alessandro Achille, Wei Xia, Stefano Soatto"], "abstract": "arXiv:2510.27042v1 Announce Type: new \nIncreasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables approximately 3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27042", "pdf_url": "https://arxiv.org/pdf/2510.27042.pdf", "is_interesting": false}, "374": {"title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement", "authors": ["Aaditya Shukla, Sidney Knowles, Meenakshi Madugula, Dave Farris, Ryan Angilly, Santiago Pombo, Anbang Xu, Lu An, Abhinav Balasubramanian, Tan Yu, Jiaxiang Ren, Rama Akkiraju"], "abstract": "arXiv:2510.27051v1 Announce Type: new \nEnterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\\%) and query rephrasal errors (3.2\\%). Using NVIDIA NeMo microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\\% accuracy, a 10x reduction in model size, and 70\\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\\% gain in accuracy and a 40\\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27051", "pdf_url": "https://arxiv.org/pdf/2510.27051.pdf", "is_interesting": false}, "375": {"title": "CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning", "authors": ["Hamed Mahdavi (Pennsylvania State University), Pouria Mahdavinia (Pennsylvania State University), Alireza Farhadi (Amirkabir University of Technology), Pegah Mohammadipour (Pennsylvania State University), Samira Malek (Pennsylvania State University), Majid Daliri (New York University), Pedram Mohammadipour (Amirkabir University of Technology), Alireza Hashemi (City University of New York), Amir Khasahmadi (Autodesk), Vasant Honavar (Pennsylvania State University)"], "abstract": "arXiv:2510.27094v1 Announce Type: new \nState-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27094", "pdf_url": "https://arxiv.org/pdf/2510.27094.pdf", "is_interesting": false}, "376": {"title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization", "authors": ["Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan"], "abstract": "arXiv:2510.27176v1 Announce Type: new \nCan an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.", "categories": ["cs.AI", "cs.CL", "cs.DC"], "abs_url": "https://arxiv.org/abs/2510.27176", "pdf_url": "https://arxiv.org/pdf/2510.27176.pdf", "is_interesting": false}, "377": {"title": "From product to system network challenges in system of systems lifecycle management", "authors": ["Vahid Salehi, Josef Vilsmeier, Shirui Wang"], "abstract": "arXiv:2510.27194v1 Announce Type: new \nToday, products are no longer isolated artifacts, but nodes in networked systems. This means that traditional, linearly conceived life cycle models are reaching their limits: Interoperability across disciplines, variant and configuration management, traceability, and governance across organizational boundaries are becoming key factors. This collective contribution classifies the state of the art and proposes a practical frame of reference for SoS lifecycle management, model-based systems engineering (MBSE) as the semantic backbone, product lifecycle management (PLM) as the governance and configuration level, CAD-CAE as model-derived domains, and digital thread and digital twin as continuous feedback. Based on current literature and industry experience, mobility, healthcare, and the public sector, we identify four principles: (1) referenced architecture and data models, (2) end-to-end configuration sovereignty instead of tool silos, (3) curated models with clear review gates, and (4) measurable value contributions along time, quality, cost, and sustainability. A three-step roadmap shows the transition from product- to network- centric development: piloting with reference architecture, scaling across variant and supply chain spaces, organizational anchoring (roles, training, compliance). The results are increased change robustness, shorter throughput times, improved reuse, and informed sustainability decisions. This article is aimed at decision-makers and practitioners who want to make complexity manageable and design SoS value streams to be scalable.", "categories": ["cs.AI", "cs.SE"], "abs_url": "https://arxiv.org/abs/2510.27194", "pdf_url": "https://arxiv.org/pdf/2510.27194.pdf", "is_interesting": false}, "378": {"title": "Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering", "authors": ["Kounianhua Du, Jianxing Liu, Kangning Zhang, Wenxiang Jiao, Yuan Lu, Jiarui Jin, Weiwen Liu, Yong Yu, Weinan Zhang"], "abstract": "arXiv:2510.27206v1 Announce Type: new \nThe rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation methods, including personalized parameter-efficient fine-tuning and reward modeling emerge. However, these methods face limitations in handling dynamic user patterns and high data sparsity scenarios, due to low adaptability and data efficiency. To address these challenges, we propose a fine-grained and instance-tailored steering framework that dynamically generates sample-level interference vectors from user data and injects them into the model's forward pass for personalized adaptation. Our approach introduces two key technical innovations: a fine-grained steering component that captures nuanced signals by hooking activations from attention and MLP layers, and an input-aware aggregation module that synthesizes these signals into contextually relevant enhancements. The method demonstrates high flexibility and data efficiency, excelling in fast-changing distribution and high data sparsity scenarios. In addition, the proposed method is orthogonal to existing methods and operates as a plug-in component compatible with different personalization techniques. Extensive experiments across diverse scenarios--including short-to-long text generation, and web function calling--validate the effectiveness and compatibility of our approach. Results show that our method significantly enhances personalization performance in fast-shifting environments while maintaining robustness across varying interaction modes and context lengths. Implementation is available at https://github.com/KounianhuaDu/Fints.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27206", "pdf_url": "https://arxiv.org/pdf/2510.27206.pdf", "is_interesting": false}, "379": {"title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation", "authors": ["Tao Liu, Chongyu Wang, Rongjie Li, Yingchen Yu, Xuming He, Bai Song"], "abstract": "arXiv:2510.27210v1 Announce Type: new \nWhile Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.", "categories": ["cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27210", "pdf_url": "https://arxiv.org/pdf/2510.27210.pdf", "is_interesting": false}, "380": {"title": "Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines", "authors": ["Kristina Levina, Nikolaos Pappas, Athanasios Karapantelakis, Aneta Vulgarakis Feljan, Jendrik Seipp"], "abstract": "arXiv:2510.27329v1 Announce Type: new \nReward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27329", "pdf_url": "https://arxiv.org/pdf/2510.27329.pdf", "is_interesting": false}, "381": {"title": "Discriminative Rule Learning for Outcome-Guided Process Model Discovery", "authors": ["Ali Norouzifar, Wil van der Aalst"], "abstract": "arXiv:2510.27343v1 Announce Type: new \nEvent logs extracted from information systems offer a rich foundation for understanding and improving business processes. In many real-world applications, it is possible to distinguish between desirable and undesirable process executions, where desirable traces reflect efficient or compliant behavior, and undesirable ones may involve inefficiencies, rule violations, delays, or resource waste. This distinction presents an opportunity to guide process discovery in a more outcome-aware manner. Discovering a single process model without considering outcomes can yield representations poorly suited for conformance checking and performance analysis, as they fail to capture critical behavioral differences. Moreover, prioritizing one behavior over the other may obscure structural distinctions vital for understanding process outcomes. By learning interpretable discriminative rules over control-flow features, we group traces with similar desirability profiles and apply process discovery separately within each group. This results in focused and interpretable models that reveal the drivers of both desirable and undesirable executions. The approach is implemented as a publicly available tool and it is evaluated on multiple real-life event logs, demonstrating its effectiveness in isolating and visualizing critical process patterns.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27343", "pdf_url": "https://arxiv.org/pdf/2510.27343.pdf", "is_interesting": false}, "382": {"title": "An In-depth Study of LLM Contributions to the Bin Packing Problem", "authors": ["Julien Herrmann, Guillaume Pallez"], "abstract": "arXiv:2510.27353v1 Announce Type: new \nRecent studies have suggested that Large Language Models (LLMs) could provide interesting ideas contributing to mathematical discovery. This claim was motivated by reports that LLM-based genetic algorithms produced heuristics offering new insights into the online bin packing problem under uniform and Weibull distributions. In this work, we reassess this claim through a detailed analysis of the heuristics produced by LLMs, examining both their behavior and interpretability. Despite being human-readable, these heuristics remain largely opaque even to domain experts. Building on this analysis, we propose a new class of algorithms tailored to these specific bin packing instances. The derived algorithms are significantly simpler, more efficient, more interpretable, and more generalizable, suggesting that the considered instances are themselves relatively simple. We then discuss the limitations of the claim regarding LLMs' contribution to this problem, which appears to rest on the mistaken assumption that the instances had previously been studied. Our findings instead emphasize the need for rigorous validation and contextualization when assessing the scientific value of LLM-generated outputs.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27353", "pdf_url": "https://arxiv.org/pdf/2510.27353.pdf", "is_interesting": false}, "383": {"title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use", "authors": ["Mengjie Deng, Guanting Dong, Zhicheng Dou"], "abstract": "arXiv:2510.27363v1 Announce Type: new \nRecently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27363", "pdf_url": "https://arxiv.org/pdf/2510.27363.pdf", "is_interesting": false}, "384": {"title": "Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints", "authors": ["Yueyang Wang, Mehmet Dogar, Gustav Markkula"], "abstract": "arXiv:2510.27383v1 Announce Type: new \nModelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27383", "pdf_url": "https://arxiv.org/pdf/2510.27383.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u8f66\u8def\u534f\u540c / \u8f68\u8ff9\u9884\u6d4b", "reason": "The paper proposes a multi-agent reinforcement learning framework for modelling pedestrian-driver interactions, which is highly relevant for autonomous driving systems. By incorporating human sensory and motor constraints, the framework helps improve road user interaction modeling, a key aspect of autonomous vehicle decision-making and trajectory prediction in complex real-world scenarios."}}, "385": {"title": "Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry", "authors": ["Jianwen Sun, Yukang Feng, Yifan Chang, Chuanhao Li, Zizhen Li, Jiaxin Ai, Fanrui Zhang, Yu Dai, Kaipeng Zhang"], "abstract": "arXiv:2510.27410v1 Announce Type: new \nA fundamental bottleneck in human-AI collaboration is the \"intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27410", "pdf_url": "https://arxiv.org/pdf/2510.27410.pdf", "is_interesting": false}, "386": {"title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains", "authors": ["Tian Liang, Wenxiang Jiao, Zhiwei He, Jiahao Xu, Haitao Mi, Dong Yu"], "abstract": "arXiv:2510.27419v1 Announce Type: new \nLarge Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27419", "pdf_url": "https://arxiv.org/pdf/2510.27419.pdf", "is_interesting": false}, "387": {"title": "GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language", "authors": ["Yuhao Zhang, Dingxin Hu, Tinghao Yu, Hao Liu, Yiting Liu"], "abstract": "arXiv:2510.27448v1 Announce Type: new \nMulti-modal Large Language Models (MLLMs) have gained significant attention in both academia and industry for their capabilities in handling multi-modal tasks. However, these models face challenges in mathematical geometric reasoning due to the scarcity of high-quality geometric data. To address this issue, synthetic geometric data has become an essential strategy. Current methods for generating synthetic geometric data involve rephrasing or expanding existing problems and utilizing predefined rules and templates to create geometric images and problems. However, these approaches often produce data that lacks diversity or is prone to noise. Additionally, the geometric images synthesized by existing methods tend to exhibit limited variation and deviate significantly from authentic geometric diagrams. To overcome these limitations, we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses formal languages to explore combinations of conditions within metric space, generating high-fidelity geometric problems that differ from the originals while ensuring correctness through a symbolic engine. Experimental results show that our synthetic data significantly outperforms existing methods. The model trained with our data surpass the proprietary GPT-4o model by 18.7\\% on geometry problem-solving tasks in MathVista and by 16.5\\% on GeoQA. Additionally, it exceeds the performance of a leading open-source model by 5.7\\% on MathVista and by 2.7\\% on GeoQA.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27448", "pdf_url": "https://arxiv.org/pdf/2510.27448.pdf", "is_interesting": false}, "388": {"title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance", "authors": ["Nikolaus Holzer, William Fishell, Baishakhi Ray, Mark Santolucito"], "abstract": "arXiv:2510.27544v1 Announce Type: new \nLarge Language Models (LLMs) are increasingly excelling and outpacing human performance on many tasks. However, to improve LLM reasoning, researchers either rely on ad-hoc generated datasets or formal mathematical proof systems such as the Lean proof assistant. Whilst ad-hoc generated methods can capture the decision chains of real-world reasoning processes, they may encode some inadvertent bias in the space of reasoning they cover; they also cannot be formally verified. On the other hand, systems like Lean can guarantee verifiability, but are not well-suited to capture the nature of agentic decision chain-based tasks. This creates a gap both in performance for functions such as business agents or code assistants, and in the usefulness of LLM reasoning benchmarks, whereby these fall short in reasoning structure or real-world alignment. We introduce TempoBench, the first formally grounded and verifiable diagnostic benchmark that parametrizes difficulty to systematically analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks to break down reasoning ability. First, temporal trace evaluation (TTE) tests the ability of an LLM to understand and simulate the execution of a given multi-step reasoning system. Subsequently, temporal causal evaluation (TCE) tests an LLM's ability to perform multi-step causal reasoning and to distill cause-and-effect relations from complex systems. We find that models score 65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art LLMs clearly understand the TCE task but perform poorly as system complexity increases. Our code is available at our \\href{https://github.com/nik-hz/tempobench}{GitHub repository}.", "categories": ["cs.AI", "cs.FL"], "abs_url": "https://arxiv.org/abs/2510.27544", "pdf_url": "https://arxiv.org/pdf/2510.27544.pdf", "is_interesting": false}, "389": {"title": "SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning", "authors": ["Ali Asgarov, Umid Suleymanov, Aadyant Khatri"], "abstract": "arXiv:2510.27568v1 Announce Type: new \nSolving mathematical reasoning problems requires not only accurate access to relevant knowledge but also careful, multi-step thinking. However, current retrieval-augmented models often rely on a single perspective, follow inflexible search strategies, and struggle to effectively combine information from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning), a unified framework that orchestrates specialized agents to independently reason, perform targeted searches, and synthesize findings through a moderator mechanism. Each agent generates hypothetical passages to optimize retrieval for its analytic perspective, ensuring knowledge integration is both context-sensitive and computation-efficient. When evaluated on challenging benchmarks such as MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms both open- and closed-source systems, achieving an absolute performance improvement of 7.4%. Our results demonstrate that multi-agent, on-demand knowledge integration significantly enhances both reasoning accuracy and efficiency, offering a scalable approach for complex, knowledge-intensive problem-solving. We will release the code upon publication.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27568", "pdf_url": "https://arxiv.org/pdf/2510.27568.pdf", "is_interesting": false}, "390": {"title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research", "authors": ["Yunze Wu, Dayuan Fu, Weiye Si, Zhen Huang, Mohan Jiang, Keyu Li, Shijie Xia, Jie Sun, Tianze Xu, Xiangkun Hu, Pengrui Lu, Xiaojie Cai, Lyumanshan Ye, Wenhong Zhu, Yang Xiao, Pengfei Liu"], "abstract": "arXiv:2510.27598v2 Announce Type: new \nAI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27598", "pdf_url": "https://arxiv.org/pdf/2510.27598.pdf", "is_interesting": false}, "391": {"title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation", "authors": ["Heng Ping, Arijit Bhattacharjee, Peiyu Zhang, Shixuan Li, Wei Yang, Anzhe Cheng, Xiaole Zhang, Jesse Thomason, Ali Jannesari, Nesreen Ahmed, Paul Bogdan"], "abstract": "arXiv:2510.27617v1 Announce Type: new \nAutomation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27617", "pdf_url": "https://arxiv.org/pdf/2510.27617.pdf", "is_interesting": false}, "392": {"title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning", "authors": ["Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, Liang-Yan Gui, Yu-Xiong Wang, Huan Zhang, Heng Ji, Daniel Kang"], "abstract": "arXiv:2510.27623v1 Announce Type: new \nMultimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.", "categories": ["cs.AI", "cs.CL", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27623", "pdf_url": "https://arxiv.org/pdf/2510.27623.pdf", "is_interesting": false}, "393": {"title": "Validity Is What You Need", "authors": ["Sebastian Benthall, Andrew Clark"], "abstract": "arXiv:2510.27628v1 Announce Type: new \nWhile AI agents have long been discussed and studied in computer science, today's Agentic AI systems are something new. We consider other definitions of Agentic AI and propose a new realist definition. Agentic AI is a software delivery mechanism, comparable to software as a service (SaaS), which puts an application to work autonomously in a complex enterprise setting. Recent advances in large language models (LLMs) as foundation models have driven excitement in Agentic AI. We note, however, that Agentic AI systems are primarily applications, not foundations, and so their success depends on validation by end users and principal stakeholders. The tools and techniques needed by the principal users to validate their applications are quite different from the tools and techniques used to evaluate foundation models. Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models that handle core logic. When it comes to Agentic AI, validity is what you need. LLMs are one option that might achieve it.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27628", "pdf_url": "https://arxiv.org/pdf/2510.27628.pdf", "is_interesting": false}, "394": {"title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training", "authors": ["Dayuan Fu, Yunze Wu, Xiaojie Cai, Lyumanshan Ye, Shijie Xia, Zhen Huang, Weiye Si, Tianze Xu, Jie Sun, Keyu Li, Mohan Jiang, Junfei Wang, Qishuo Hua, Pengrui Lu, Yang Xiao, Pengfei Liu"], "abstract": "arXiv:2510.27630v2 Announce Type: new \nLarge Language Model (LLM) agents have recently shown strong potential in domains such as automated coding, deep research, and graphical user interface manipulation. However, training them to succeed on long-horizon, domain-specialized tasks remains challenging. Current methods primarily fall into two categories. The first relies on dense human annotations through behavior cloning, which is prohibitively expensive for long-horizon tasks that can take days or months. The second depends on outcome-driven sampling, which often collapses due to the rarity of valid positive trajectories on domain-specialized tasks. We introduce Apollo, a sampling framework that integrates asynchronous human guidance with action-level data filtering. Instead of requiring annotators to shadow every step, Apollo allows them to intervene only when the agent drifts from a promising trajectory, by providing prior knowledge, strategic advice, etc. This lightweight design makes it possible to sustain interactions for over 30 hours and produces valuable trajectories at a lower cost. Apollo then applies supervision control to filter out sub-optimal actions and prevent error propagation. Together, these components enable reliable and effective data collection in long-horizon environments. To demonstrate the effectiveness of Apollo, we evaluate it using InnovatorBench. Our experiments show that when applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves more than a 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling and the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27630", "pdf_url": "https://arxiv.org/pdf/2510.27630.pdf", "is_interesting": false}, "395": {"title": "MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design", "authors": ["Wei Zhang, Zekun Guo, Yingce Xia, Peiran Jin, Shufang Xie, Tao Qin, Xiang-Yang Li"], "abstract": "arXiv:2510.27671v1 Announce Type: new \nStructure-based drug design (SBDD), which maps target proteins to candidate molecular ligands, is a fundamental task in drug discovery. Effectively aligning protein structural representations with molecular representations, and ensuring alignment between generated drugs and their pharmacological properties, remains a critical challenge. To address these challenges, we propose MolChord, which integrates two key techniques: (1) to align protein and molecule structures with their textual descriptions and sequential representations (e.g., FASTA for proteins and SMILES for molecules), we leverage NatureLM, an autoregressive model unifying text, small molecules, and proteins, as the molecule generator, alongside a diffusion-based structure encoder; and (2) to guide molecules toward desired properties, we curate a property-aware dataset by integrating preference data and refine the alignment process using Direct Preference Optimization (DPO). Experimental results on CrossDocked2020 demonstrate that our approach achieves state-of-the-art performance on key evaluation metrics, highlighting its potential as a practical tool for SBDD.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27671", "pdf_url": "https://arxiv.org/pdf/2510.27671.pdf", "is_interesting": false}, "396": {"title": "A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture", "authors": ["Shang Wang, Huanrong Tang, Jianquan Ouyang"], "abstract": "arXiv:2505.01313v1 Announce Type: cross \nThis paper proposes a neural architecture search space using ResNet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. In addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. The experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the MNIST, Fashion-MNIST and CIFAR100 datasets.", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.01313", "pdf_url": "https://arxiv.org/pdf/2505.01313.pdf", "is_interesting": false}, "397": {"title": "A Transformer-based Neural Architecture Search Method", "authors": ["Shang Wang, Huanrong Tang, Jianquan Ouyang"], "abstract": "arXiv:2505.01314v1 Announce Type: cross \nThis paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abs_url": "https://arxiv.org/abs/2505.01314", "pdf_url": "https://arxiv.org/pdf/2505.01314.pdf", "is_interesting": false}, "398": {"title": "Detecting Prefix Bias in LLM-based Reward Models", "authors": ["Ashwin Kumar, Yuzi He, Aram H. Markosyan, Bobbie Chern, Imanol Arrieta-Ibarra"], "abstract": "arXiv:2505.13487v2 Announce Type: cross \nReinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.13487", "pdf_url": "https://arxiv.org/pdf/2505.13487.pdf", "is_interesting": false}, "399": {"title": "VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus", "authors": ["Chuyue Sun, Yican Sun, Daneshvar Amrollahi, Ethan Zhang, Shuvendu Lahiri, Shan Lu, David Dill, Clark Barrett"], "abstract": "arXiv:2510.25015v1 Announce Type: cross \nWe introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.25015", "pdf_url": "https://arxiv.org/pdf/2510.25015.pdf", "is_interesting": false}, "400": {"title": "EARS-UDE: Evaluating Auditory Response in Sensory Overload with Universal Differential Equations", "authors": ["Miheer Salunke, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat"], "abstract": "arXiv:2510.26804v1 Announce Type: cross \nAuditory sensory overload affects 50-70% of individuals with Autism Spectrum Disorder (ASD), yet existing approaches, such as mechanistic models (Hodgkin Huxley type, Wilson Cowan, excitation inhibition balance), clinical tools (EEG/MEG, Sensory Profile scales), and ML methods (Neural ODEs, predictive coding), either assume fixed parameters or lack interpretability, missing autism heterogeneity. We present a Scientific Machine Learning approach using Universal Differential Equations (UDEs) to model sensory adaptation dynamics in autism. Our framework combines ordinary differential equations grounded in biophysics with neural networks to capture both mechanistic understanding and individual variability. We demonstrate that UDEs achieve a 90.8% improvement over pure Neural ODEs while using 73.5% fewer parameters. The model successfully recovers physiological parameters within the 2% error and provides a quantitative risk assessment for sensory overload, predicting 17.2% risk for pulse stimuli with specific temporal patterns. This framework establishes foundations for personalized, evidence-based interventions in autism, with direct applications to wearable technology and clinical practice.", "categories": ["q-bio.NC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26804", "pdf_url": "https://arxiv.org/pdf/2510.26804.pdf", "is_interesting": false}, "401": {"title": "Reinforcement Learning for Accelerator Beamline Control: a simulation-based approach", "authors": ["Anwar Ibrahim, Alexey Petrenko, Maxim Kaledin, Ehab Suleiman, Fedor Ratnikov, Denis Derkach"], "abstract": "arXiv:2510.26805v1 Announce Type: cross \nParticle accelerators play a pivotal role in advancing scientific research, yet optimizing beamline configurations to maximize particle transmission remains a labor-intensive task requiring expert intervention. In this work, we introduce RLABC (Reinforcement Learning for Accelerator Beamline Control), a Python-based library that reframes beamline optimization as a reinforcement learning (RL) problem. Leveraging the Elegant simulation framework, RLABC automates the creation of an RL environment from standard lattice and element input files, enabling sequential tuning of magnets to minimize particle losses. We define a comprehensive state representation capturing beam statistics, actions for adjusting magnet parameters, and a reward function focused on transmission efficiency. Employing the Deep Deterministic Policy Gradient (DDPG) algorithm, we demonstrate RLABC's efficacy on two beamlines, achieving transmission rates of 94% and 91%, comparable to expert manual optimizations. This approach bridges accelerator physics and machine learning, offering a versatile tool for physicists and RL researchers alike to streamline beamline tuning.", "categories": ["physics.acc-ph", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26805", "pdf_url": "https://arxiv.org/pdf/2510.26805.pdf", "is_interesting": false}, "402": {"title": "Impact of clinical decision support systems (cdss) on clinical outcomes and healthcare delivery in low- and middle-income countries: protocol for a systematic review and meta-analysis", "authors": ["Garima Jain, Anand Bodade, Sanghamitra Pati"], "abstract": "arXiv:2510.26812v1 Announce Type: cross \nClinical decision support systems (CDSS) are used to improve clinical and service outcomes, yet evidence from low- and middle-income countries (LMICs) is dispersed. This protocol outlines methods to quantify the impact of CDSS on patient and healthcare delivery outcomes in LMICs. We will include comparative quantitative designs (randomized trials, controlled before-after, interrupted time series, comparative cohorts) evaluating CDSS in World Bank-defined LMICs. Standalone qualitative studies are excluded; mixed-methods studies are eligible only if they report comparative quantitative outcomes, for which we will extract the quantitative component. Searches (from inception to 30 September 2024) will cover MEDLINE, Embase, CINAHL, CENTRAL, Web of Science, Global Health, Scopus, IEEE Xplore, LILACS, African Index Medicus, and IndMED, plus grey sources. Screening and extraction will be performed in duplicate. Risk of bias will be assessed with RoB 2 (randomized trials) and ROBINS-I (non-randomized). Random-effects meta-analysis will be performed where outcomes are conceptually or statistically comparable; otherwise, a structured narrative synthesis will be presented. Heterogeneity will be explored using relative and absolute metrics and a priori subgroups or meta-regression (condition area, care level, CDSS type, readiness proxies, study design).", "categories": ["stat.ME", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26812", "pdf_url": "https://arxiv.org/pdf/2510.26812.pdf", "is_interesting": false}, "403": {"title": "Systematic Absence of Low-Confidence Nighttime Fire Detections in VIIRS Active Fire Product: Evidence of Undocumented Algorithmic Filtering", "authors": ["Rohit Rajendra Dhage"], "abstract": "arXiv:2510.26816v1 Announce Type: cross \nThe Visible Infrared Imaging Radiometer Suite (VIIRS) active fire product is widely used for global fire monitoring, yet its confidence classification scheme exhibits an undocumented systematic pattern. Through analysis of 21,540,921 fire detections spanning one year (January 2023 - January 2024), I demonstrate a complete absence of low-confidence classifications during nighttime observations. Of 6,007,831 nighttime fires, zero were classified as low confidence, compared to an expected 696,908 under statistical independence (chi-squared = 1,474,795, p < 10^-15, Z = -833). This pattern persists globally across all months, latitude bands, and both NOAA-20 and Suomi-NPP satellites. Machine learning reverse-engineering (88.9% accuracy), bootstrap simulation (1,000 iterations), and spatial-temporal analysis confirm this is an algorithmic constraint rather than a geophysical phenomenon. Brightness temperature analysis reveals nighttime fires below approximately 295K are likely excluded entirely rather than flagged as low-confidence, while daytime fires show normal confidence distributions. This undocumented behavior affects 27.9% of all VIIRS fire detections and has significant implications for fire risk assessment, day-night detection comparisons, confidence-weighted analyses, and any research treating confidence levels as uncertainty metrics. I recommend explicit documentation of this algorithmic constraint in VIIRS user guides and reprocessing strategies for affected analyses.", "categories": ["stat.AP", "astro-ph.IM", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26816", "pdf_url": "https://arxiv.org/pdf/2510.26816.pdf", "is_interesting": false}, "404": {"title": "GACA-DiT: Diffusion-based Dance-to-Music Generation with Genre-Adaptive Rhythm and Context-Aware Alignment", "authors": ["Jinting Wang, Chenxing Li, Li Liu"], "abstract": "arXiv:2510.26818v1 Announce Type: cross \nDance-to-music (D2M) generation aims to automatically compose music that is rhythmically and temporally aligned with dance movements. Existing methods typically rely on coarse rhythm embeddings, such as global motion features or binarized joint-based rhythm values, which discard fine-grained motion cues and result in weak rhythmic alignment. Moreover, temporal mismatches introduced by feature downsampling further hinder precise synchronization between dance and music. To address these problems, we propose \\textbf{GACA-DiT}, a diffusion transformer-based framework with two novel modules for rhythmically consistent and temporally aligned music generation. First, a \\textbf{genre-adaptive rhythm extraction} module combines multi-scale temporal wavelet analysis and spatial phase histograms with adaptive joint weighting to capture fine-grained, genre-specific rhythm patterns. Second, a \\textbf{context-aware temporal alignment} module resolves temporal mismatches using learnable context queries to align music latents with relevant dance rhythm features. Extensive experiments on the AIST++ and TikTok datasets demonstrate that GACA-DiT outperforms state-of-the-art methods in both objective metrics and human evaluation. Project page: https://beria-moon.github.io/GACA-DiT/.", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "abs_url": "https://arxiv.org/abs/2510.26818", "pdf_url": "https://arxiv.org/pdf/2510.26818.pdf", "is_interesting": false}, "405": {"title": "See the Speaker: Crafting High-Resolution Talking Faces from Speech with Prior Guidance and Region Refinement", "authors": ["Jinting Wang, Jun Wang, Hei Victor Cheng, Li Liu"], "abstract": "arXiv:2510.26819v1 Announce Type: cross \nUnlike existing methods that rely on source images as appearance references and use source speech to generate motion, this work proposes a novel approach that directly extracts information from the speech, addressing key challenges in speech-to-talking face. Specifically, we first employ a speech-to-face portrait generation stage, utilizing a speech-conditioned diffusion model combined with statistical facial prior and a sample-adaptive weighting module to achieve high-quality portrait generation. In the subsequent speech-driven talking face generation stage, we embed expressive dynamics such as lip movement, facial expressions, and eye movements into the latent space of the diffusion model and further optimize lip synchronization using a region-enhancement module. To generate high-resolution outputs, we integrate a pre-trained Transformer-based discrete codebook with an image rendering network, enhancing video frame details in an end-to-end manner. Experimental results demonstrate that our method outperforms existing approaches on the HDTF, VoxCeleb, and AVSpeech datasets. Notably, this is the first method capable of generating high-resolution, high-quality talking face videos exclusively from a single speech input.", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.SD"], "abs_url": "https://arxiv.org/abs/2510.26819", "pdf_url": "https://arxiv.org/pdf/2510.26819.pdf", "is_interesting": false}, "406": {"title": "Cross-Corpus Validation of Speech Emotion Recognition in Urdu using Domain-Knowledge Acoustic Features", "authors": ["Unzela Talpur, Zafi Sherhan Syed, Muhammad Shehram Shah Syed, Abbas Shah Syed"], "abstract": "arXiv:2510.26823v1 Announce Type: cross \nSpeech Emotion Recognition (SER) is a key affective computing technology that enables emotionally intelligent artificial intelligence. While SER is challenging in general, it is particularly difficult for low-resource languages such as Urdu. This study investigates Urdu SER in a cross-corpus setting, an area that has remained largely unexplored. We employ a cross-corpus evaluation framework across three different Urdu emotional speech datasets to test model generalization. Two standard domain-knowledge based acoustic feature sets, eGeMAPS and ComParE, are used to represent speech signals as feature vectors which are then passed to Logistic Regression and Multilayer Perceptron classifiers. Classification performance is assessed using unweighted average recall (UAR) whilst considering class-label imbalance. Results show that Self-corpus validation often overestimates performance, with UAR exceeding cross-corpus evaluation by up to 13%, underscoring that cross-corpus evaluation offers a more realistic measure of model robustness. Overall, this work emphasizes the importance of cross-corpus validation for Urdu SER and its implications contribute to advancing affective computing research for underrepresented language communities.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "abs_url": "https://arxiv.org/abs/2510.26823", "pdf_url": "https://arxiv.org/pdf/2510.26823.pdf", "is_interesting": false}, "407": {"title": "LeMat-Synth: a multi-modal toolbox to curate broad synthesis procedure databases from scientific literature", "authors": ["Magdalena Lederbauer, Siddharth Betala, Xiyao Li, Ayush Jain, Amine Sehaba, Georgia Channing, Gr\\'egoire Germain, Anamaria Leonescu, Faris Flaifil, Alfonso Amayuelas, Alexandre Nozadze, Stefan P. Schmid, Mohd Zaki, Sudheesh Kumar Ethirajan, Elton Pan, Mathilde Franckel, Alexandre Duval, N. M. Anoop Krishnan, Samuel P. Gleason"], "abstract": "arXiv:2510.26824v1 Announce Type: cross \nThe development of synthesis procedures remains a fundamental challenge in materials discovery, with procedural knowledge scattered across decades of scientific literature in unstructured formats that are challenging for systematic analysis. In this paper, we propose a multi-modal toolbox that employs large language models (LLMs) and vision language models (VLMs) to automatically extract and organize synthesis procedures and performance data from materials science publications, covering text and figures. We curated 81k open-access papers, yielding LeMat-Synth (v 1.0): a dataset containing synthesis procedures spanning 35 synthesis methods and 16 material classes, structured according to an ontology specific to materials science. The extraction quality is rigorously evaluated on a subset of 2.5k synthesis procedures through a combination of expert annotations and a scalable LLM-as-a-judge framework. Beyond the dataset, we release a modular, open-source software library designed to support community-driven extension to new corpora and synthesis domains. Altogether, this work provides an extensible infrastructure to transform unstructured literature into machine-readable information. This lays the groundwork for predictive modeling of synthesis procedures as well as modeling synthesis--structure--property relationships.", "categories": ["cs.DL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2510.26824", "pdf_url": "https://arxiv.org/pdf/2510.26824.pdf", "is_interesting": false}, "408": {"title": "R3GAN-based Optimal Strategy for Augmenting Small Medical Dataset", "authors": ["Tsung-Wei Pan, Chang-Hong Wu, Jung-Hua Wang, Ming-Jer Chen, Yu-Chiao Yi, Tsung-Hsien Lee"], "abstract": "arXiv:2510.26828v1 Announce Type: cross \nMedical image analysis often suffers from data scarcity and class imbalance, limiting the effectiveness of deep learning models in clinical applications. Using human embryo time-lapse imaging (TLI) as a case study, this work investigates how generative adversarial networks (GANs) can be optimized for small datasets to generate realistic and diagnostically meaningful images. Based on systematic experiments with R3GAN, we established effective training strategies and designed an optimized configuration for 256x256-resolution datasets, featuring a full burn-in phase and a low, gradually increasing gamma range (5 -> 40). The generated samples were used to balance an imbalanced embryo dataset, leading to substantial improvement in classification performance. The recall and F1-score of t3 increased from 0.06 to 0.69 and 0.11 to 0.60, respectively, without compromising other classes. These results demonstrate that tailored R3GAN training strategies can effectively alleviate data scarcity and improve model robustness in small-scale medical imaging tasks.", "categories": ["eess.IV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26828", "pdf_url": "https://arxiv.org/pdf/2510.26828.pdf", "is_interesting": false}, "409": {"title": "VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes", "authors": ["Simon Yu, Peilin Yu, Hongbo Zheng, Huajie Shao, Han Zhao, Lui Sha"], "abstract": "arXiv:2510.26833v1 Announce Type: cross \nWe present VISAT, a novel open dataset and benchmarking suite for evaluating model robustness in the task of traffic sign recognition with the presence of visual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our dataset introduces two benchmarks that respectively emphasize robustness against adversarial attacks and distribution shifts. For our adversarial attack benchmark, we employ the state-of-the-art Projected Gradient Descent (PGD) method to generate adversarial inputs and evaluate their impact on popular models. Additionally, we investigate the effect of adversarial attacks on attribute-specific multi-task learning (MTL) networks, revealing spurious correlations among MTL tasks. The MTL networks leverage visual attributes (color, shape, symbol, and text) that we have created for each traffic sign in our dataset. For our distribution shift benchmark, we utilize ImageNet-C's realistic data corruption and natural variation techniques to perform evaluations on the robustness of both base and MTL models. Moreover, we further explore spurious correlations among MTL tasks through synthetic alterations of traffic sign colors using color quantization techniques. Our experiments focus on two major backbones, ResNet-152 and ViT-B/32, and compare the performance between base and MTL models. The VISAT dataset and benchmarking framework contribute to the understanding of model robustness for traffic sign recognition, shedding light on the challenges posed by adversarial attacks and distribution shifts. We believe this work will facilitate advancements in developing more robust models for real-world applications in autonomous driving and cyber-physical systems.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26833", "pdf_url": "https://arxiv.org/pdf/2510.26833.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b / \u5f3a\u5065\u6027\u8bc4\u4f30", "reason": "The paper presents a dataset and benchmarking framework for evaluating the robustness of traffic sign recognition models, which is a crucial task for autonomous driving systems. The focus on adversarial attacks and distribution shifts is directly relevant to the robustness of perception systems in autonomous vehicles, making it highly applicable to autonomous driving."}}, "410": {"title": "Diffusion-Driven Generation of Minimally Preprocessed Brain MRI", "authors": ["Samuel W. Remedios, Aaron Carass, Jerry L. Prince, Blake E. Dewey"], "abstract": "arXiv:2510.26834v1 Announce Type: cross \nThe purpose of this study is to present and compare three denoising diffusion probabilistic models (DDPMs) that generate 3D $T_1$-weighted MRI human brain images. Three DDPMs were trained using 80,675 image volumes from 42,406 subjects spanning 38 publicly available brain MRI datasets. These images had approximately 1 mm isotropic resolution and were manually inspected by three human experts to exclude those with poor quality, field-of-view issues, and excessive pathology. The images were minimally preprocessed to preserve the visual variability of the data. Furthermore, to enable the DDPMs to produce images with natural orientation variations and inhomogeneity, the images were neither registered to a common coordinate system nor bias field corrected. Evaluations included segmentation, Frechet Inception Distance (FID), and qualitative inspection. Regarding results, all three DDPMs generated coherent MR brain volumes. The velocity and flow prediction models achieved lower FIDs than the sample prediction model. However, all three models had higher FIDs compared to real images across multiple cohorts. In a permutation experiment, the generated brain regional volume distributions differed statistically from real data. However, the velocity and flow prediction models had fewer statistically different volume distributions in the thalamus and putamen. In conclusion this work presents and releases the first 3D non-latent diffusion model for brain data without skullstripping or registration. Despite the negative results in statistical testing, the presented DDPMs are capable of generating high-resolution 3D $T_1$-weighted brain images. All model weights and corresponding inference code are publicly available at https://github.com/piksl-research/medforj .", "categories": ["eess.IV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26834", "pdf_url": "https://arxiv.org/pdf/2510.26834.pdf", "is_interesting": false}, "411": {"title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads", "authors": ["Chen Wang, Xunzhuo Liu, Yue Zhu, Alaa Youssef, Priya Nagpurkar, Huamin Chen"], "abstract": "arXiv:2510.26835v1 Announce Type: cross \nLLM serving systems process heterogeneous query workloads where different categories exhibit different characteristics. Code queries cluster densely in embedding space while conversational queries distribute sparsely. Content staleness varies from minutes (stock data) to months (code patterns). Query repetition patterns range from power-law (code) to uniform (conversation), producing long tail cache hit rate distributions: high-repetition categories achieve 40-60% hit rates while low-repetition or volatile categories achieve 5-15% hit rates. Vector databases must exclude the long tail because remote search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of production traffic uncached. Uniform cache policies compound this problem: fixed thresholds cause false positives in dense spaces and miss valid paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This paper presents category-aware semantic caching where similarity thresholds, TTLs, and quotas vary by query category. We present a hybrid architecture separating in-memory HNSW search from external document storage, reducing miss cost from 30ms to 2ms. This reduction makes low-hit-rate categories economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage across the entire workload distribution. Adaptive load-based policies extend this framework to respond to downstream model load, dynamically adjusting thresholds and TTLs to reduce traffic to overloaded models by 9-17% in theoretical projections.", "categories": ["cs.DB", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26835", "pdf_url": "https://arxiv.org/pdf/2510.26835.pdf", "is_interesting": false}, "412": {"title": "SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification", "authors": ["Rocky Klopfenstein, Yang He, Andrew Tremante, Yuepeng Wang, Nina Narodytska, Haoze Wu"], "abstract": "arXiv:2510.26840v1 Announce Type: cross \nCommunity-driven Text-to-SQL evaluation platforms play a pivotal role in tracking the state of the art of Text-to-SQL performance. The reliability of the evaluation process is critical for driving progress in the field. Current evaluation methods are largely test-based, which involves comparing the execution results of a generated SQL query and a human-labeled ground-truth on a static test database. Such an evaluation is optimistic, as two queries can coincidentally produce the same output on the test database while actually being different. In this work, we propose a new alternative evaluation pipeline, called SpotIt, where a formal bounded equivalence verification engine actively searches for a database that differentiates the generated and ground-truth SQL queries. We develop techniques to extend existing verifiers to support a richer SQL subset relevant to Text-to-SQL. A performance evaluation of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that test-based methods can often overlook differences between the generated query and the ground-truth. Further analysis of the verification results reveals a more complex picture of the current Text-to-SQL evaluation.", "categories": ["cs.DB", "cs.AI", "cs.FL", "cs.LO"], "abs_url": "https://arxiv.org/abs/2510.26840", "pdf_url": "https://arxiv.org/pdf/2510.26840.pdf", "is_interesting": false}, "413": {"title": "Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility", "authors": ["Kangkang Sun, Jun Wu, Minyi Guo, Jianhua Li, Jianwei Huang"], "abstract": "arXiv:2510.26841v1 Announce Type: cross \nFederated Learning (FL) enables collaborative model training without data sharing, yet participants face a fundamental challenge, e.g., simultaneously ensuring fairness across demographic groups while protecting sensitive client data. We introduce a differentially private fair FL algorithm (\\textit{FedPF}) that transforms this multi-objective optimization into a zero-sum game where fairness and privacy constraints compete against model utility. Our theoretical analysis reveals a surprising inverse relationship, i.e., stricter privacy protection fundamentally limits the system's ability to detect and correct demographic biases, creating an inherent tension between privacy and fairness. Counterintuitively, we prove that moderate fairness constraints initially improve model generalization before causing performance degradation, where a non-monotonic relationship that challenges conventional wisdom about fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 % discrimination reduction across three datasets while maintaining competitive accuracy, but more importantly, reveals that the privacy-fairness tension is unavoidable, i.e., achieving both objectives simultaneously requires carefully balanced compromises rather than optimization of either in isolation. The source code for our proposed algorithm is publicly accessible at https://github.com/szpsunkk/FedPF.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26841", "pdf_url": "https://arxiv.org/pdf/2510.26841.pdf", "is_interesting": false}, "414": {"title": "CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs", "authors": ["Zhiyuan Ning, Jiawei Shao, Ruge Xu, Xinfei Guo, Jun Zhang, Chi Zhang, Xuelong Li"], "abstract": "arXiv:2510.26843v1 Announce Type: cross \nSpeculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training. Cascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application. In this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization. Furthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods. We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction. Our CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\\times$ to $2.3\\times$ over autoregressive decoding across various LLMs and datasets. DyTC improves the average speedup by $47$\\% and $48$\\% over cascade-based baseline and tree-based baseline algorithms, respectively. CAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26843", "pdf_url": "https://arxiv.org/pdf/2510.26843.pdf", "is_interesting": false}, "415": {"title": "Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token", "authors": ["Shaked Zychlinski, Yuval Kainan"], "abstract": "arXiv:2510.26847v1 Announce Type: cross \nLarge Language Models (LLMs) are susceptible to jailbreak attacks where malicious prompts are disguised using ciphers and character-level encodings to bypass safety guardrails. While these guardrails often fail to interpret the encoded content, the underlying models can still process the harmful instructions. We introduce CPT-Filtering, a novel, model-agnostic with negligible-costs and near-perfect accuracy guardrail technique that aims to mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair Encoding (BPE) tokenizers. Our method is based on the principle that tokenizers, trained on natural language, represent out-of-distribution text, such as ciphers, using a significantly higher number of shorter tokens. Our technique uses a simple yet powerful artifact of using language models: the average number of Characters Per Token (CPT) in the text. This approach is motivated by the high compute cost of modern methods - relying on added modules such as dedicated LLMs or perplexity models. We validate our approach across a large dataset of over 100,000 prompts, testing numerous encoding schemes with several popular tokenizers. Our experiments demonstrate that a simple CPT threshold robustly identifies encoded text with high accuracy, even for very short inputs. CPT-Filtering provides a practical defense layer that can be immediately deployed for real-time text filtering and offline data curation.", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "abs_url": "https://arxiv.org/abs/2510.26847", "pdf_url": "https://arxiv.org/pdf/2510.26847.pdf", "is_interesting": false}, "416": {"title": "Leveraging Foundation Models for Enhancing Robot Perception and Action", "authors": ["Reihaneh Mirjalili"], "abstract": "arXiv:2510.26855v1 Announce Type: cross \nThis thesis investigates how foundation models can be systematically leveraged to enhance robotic capabilities, enabling more effective localization, interaction, and manipulation in unstructured environments. The work is structured around four core lines of inquiry, each addressing a fundamental challenge in robotics while collectively contributing to a cohesive framework for semantics-aware robotic intelligence.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26855", "pdf_url": "https://arxiv.org/pdf/2510.26855.pdf", "is_interesting": false}, "417": {"title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench", "authors": ["Fenfen Lin, Yesheng Liu, Haiyu Xu, Chen Yue, Zheqi He, Mingxuan Zhao, Miguel Hu Chen, Jiakang Liu, JG Yao, Xi Yang"], "abstract": "arXiv:2510.26865v1 Announce Type: cross \nReading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26865", "pdf_url": "https://arxiv.org/pdf/2510.26865.pdf", "is_interesting": false}, "418": {"title": "BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs", "authors": ["Mahsa Valizadeh, Rui Tuo, James Caverlee"], "abstract": "arXiv:2510.26892v1 Announce Type: cross \nGenerative Adversarial Networks (GANs) are proficient at generating synthetic data but continue to suffer from mode collapse, where the generator produces a narrow range of outputs that fool the discriminator but fail to capture the full data distribution. This limitation is particularly problematic, as generative models are increasingly deployed in real-world applications that demand both diversity and uncertainty awareness. In response, we introduce BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty into the generative process while maintaining computational efficiency. BI-DCGAN integrates Bayes by Backprop to learn a distribution over network weights and employs mean-field variational inference to efficiently approximate the posterior distribution during GAN training. We establishes the first theoretical proof, based on covariance matrix analysis, that Bayesian modeling enhances sample diversity in GANs. We validate this theoretical result through extensive experiments on standard generative benchmarks, demonstrating that BI-DCGAN produces more diverse and robust outputs than conventional DCGANs, while maintaining training efficiency. These findings position BI-DCGAN as a scalable and timely solution for applications where both diversity and uncertainty are critical, and where modern alternatives like diffusion models remain too resource-intensive.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26892", "pdf_url": "https://arxiv.org/pdf/2510.26892.pdf", "is_interesting": false}, "419": {"title": "How Similar Are Grokipedia and Wikipedia? A Multi-Dimensional Textual and Structural Comparison", "authors": ["Taha Yasseri"], "abstract": "arXiv:2510.26899v2 Announce Type: cross \nThe launch of Grokipedia, an AI-generated encyclopedia developed by Elon Musk's xAI, was presented as a response to perceived ideological and structural biases in Wikipedia, aiming to produce \"truthful\" entries via the large language model Grok. Yet whether an AI-driven alternative can escape the biases and limitations of human-edited platforms remains unclear. This study undertakes a large-scale computational comparison of 1,800 matched article pairs between Grokipedia and Wikipedia, drawn from the 2,000 most-edited Wikipedia pages. Using metrics across lexical richness, readability, structural organization, reference density, and semantic similarity, we assess how closely the two platforms align in form and substance. The results show that while Grokipedia exhibits strong semantic and stylistic alignment with Wikipedia, it typically produces longer but less lexically diverse articles, with fewer references per word and greater structural variability. These findings suggest that AI-generated encyclopedic content currently mirrors Wikipedia's informational scope but diverges in editorial norms, favoring narrative expansion over citation-based verification. The implications highlight new tensions around transparency, provenance, and the governance of knowledge in an era of automated text generation.", "categories": ["cs.CY", "cs.AI", "cs.SI"], "abs_url": "https://arxiv.org/abs/2510.26899", "pdf_url": "https://arxiv.org/pdf/2510.26899.pdf", "is_interesting": false}, "420": {"title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "authors": ["Zachary Ravichandran, Fernando Cladera, Ankit Prabhu, Jason Hughes, Varun Murali, Camillo Taylor, George J. Pappas, Vijay Kumar"], "abstract": "arXiv:2510.26915v1 Announce Type: cross \nHeterogeneous robot teams operating in realistic settings often must accomplish complex missions requiring collaboration and adaptation to information acquired online. Because robot teams frequently operate in unstructured environments -- uncertain, open-world settings without prior maps -- subtasks must be grounded in robot capabilities and the physical world. While heterogeneous teams have typically been designed for fixed specifications, generative intelligence opens the possibility of teams that can accomplish a wide range of missions described in natural language. However, current large language model (LLM)-enabled teaming methods typically assume well-structured and known environments, limiting deployment in unstructured environments. We present SPINE-HT, a framework that addresses these limitations by grounding the reasoning abilities of LLMs in the context of a heterogeneous robot team through a three-stage process. Given language specifications describing mission goals and team capabilities, an LLM generates grounded subtasks which are validated for feasibility. Subtasks are then assigned to robots based on capabilities such as traversability or perception and refined given feedback collected during online operation. In simulation experiments with closed-loop perception and control, our framework achieves nearly twice the success rate compared to prior LLM-enabled heterogeneous teaming approaches. In real-world experiments with a Clearpath Jackal, a Clearpath Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an 87\\% success rate in missions requiring reasoning about robot capabilities and refining subtasks with online feedback. More information is provided at https://zacravichandran.github.io/SPINE-HT.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26915", "pdf_url": "https://arxiv.org/pdf/2510.26915.pdf", "is_interesting": false}, "421": {"title": "Scale-Aware Curriculum Learning for Ddata-Efficient Lung Nodule Detection with YOLOv11", "authors": ["Yi Luo, Yike Guo, Hamed Hooshangnejad, Kai Ding"], "abstract": "arXiv:2510.26923v1 Announce Type: cross \nLung nodule detection in chest CT is crucial for early lung cancer diagnosis, yet existing deep learning approaches face challenges when deployed in clinical settings with limited annotated data. While curriculum learning has shown promise in improving model training, traditional static curriculum strategies fail in data-scarce scenarios. We propose Scale Adaptive Curriculum Learning (SACL), a novel training strategy that dynamically adjusts curriculum design based on available data scale. SACL introduces three key mechanisms:(1) adaptive epoch scheduling, (2) hard sample injection, and (3) scale-aware optimization. We evaluate SACL on the LUNA25 dataset using YOLOv11 as the base detector. Experimental results demonstrate that while SACL achieves comparable performance to static curriculum learning on the full dataset in mAP50, it shows significant advantages under data-limited conditions with 4.6%, 3.5%, and 2.0% improvements over baseline at 10%, 20%, and 50% of training data respectively. By enabling robust training across varying data scales without architectural modifications, SACL provides a practical solution for healthcare institutions to develop effective lung nodule detection systems despite limited annotation resources.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26923", "pdf_url": "https://arxiv.org/pdf/2510.26923.pdf", "is_interesting": false}, "422": {"title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification", "authors": ["Yunhao Yang, Neel P. Bhatt, Pranay Samineni, Rohan Siva, Zhanyang Wang, Ufuk Topcu"], "abstract": "arXiv:2510.26935v1 Announce Type: cross \nAs AI systems migrate to safety-critical domains, verifying that their actions comply with well-defined rules remains a challenge. Formal methods provide provable guarantees but demand hand-crafted temporal-logic specifications, offering limited expressiveness and accessibility. Deep learning approaches enable evaluation of plans against natural-language constraints, yet their opaque decision process invites misclassifications with potentially severe consequences. We introduce RepV, a neurosymbolic verifier that unifies both views by learning a latent space where safe and unsafe plans are linearly separable. Starting from a modest seed set of plans labeled by an off-the-shelf model checker, RepV trains a lightweight projector that embeds each plan, together with a language model-generated rationale, into a low-dimensional space; a frozen linear boundary then verifies compliance for unseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the likelihood of correct verification based on its position in the latent space. This guarantee enables a guarantee-driven refinement of the planner, improving rule compliance without human annotations. Empirical evaluations show that RepV improves compliance prediction accuracy by up to 15% compared to baseline methods while adding fewer than 0.2M parameters. Furthermore, our refinement framework outperforms ordinary fine-tuning baselines across various planning domains. These results show that safety-separable latent spaces offer a scalable, plug-and-play primitive for reliable neurosymbolic plan verification. Code and data are available at: https://repv-project.github.io/.", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "abs_url": "https://arxiv.org/abs/2510.26935", "pdf_url": "https://arxiv.org/pdf/2510.26935.pdf", "is_interesting": false}, "423": {"title": "Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction", "authors": ["Ashwin Kumar, Hanyu Zhang, David A. Schweidel, William Yeoh"], "abstract": "arXiv:2510.26940v1 Announce Type: cross \nNext location prediction underpins a growing number of mobility, retail, and public-health applications, yet its societal impacts remain largely unexplored. In this paper, we audit state-of-the-art mobility prediction models trained on a large-scale dataset, highlighting hidden disparities based on user demographics. Drawing from aggregate census data, we compute the difference in predictive performance on racial and ethnic user groups and show a systematic disparity resulting from the underlying dataset, resulting in large differences in accuracy based on location and user groups. To address this, we propose Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy designed for incremental data collection settings. Because individual-level demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a clustering method that partitions users in latent mobility space while enforcing census-derived group proportions. This yields proxy racial labels for the four largest groups in the state: Asian, Black, Hispanic, and White. Built on these labels, our sampling algorithm prioritizes users based on expected performance gains and current group representation. This method incrementally constructs training datasets that reduce demographic performance gaps while preserving overall accuracy. Our method reduces total disparity between groups by up to 40\\% with minimal accuracy trade-offs, as evaluated on a state-of-art MetaPath2Vec model and a transformer-encoder model. Improvements are most significant in early sampling stages, highlighting the potential for fairness-aware strategies to deliver meaningful gains even in low-resource settings. Our findings expose structural inequities in mobility prediction pipelines and demonstrate how lightweight, data-centric interventions can improve fairness with little added complexity, especially for low-data applications.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26940", "pdf_url": "https://arxiv.org/pdf/2510.26940.pdf", "is_interesting": false}, "424": {"title": "LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks", "authors": ["Seif Ikbarieh, Maanak Gupta, Elmahedi Mahalal"], "abstract": "arXiv:2510.26941v1 Announce Type: cross \nThe Internet of Things has expanded rapidly, transforming communication and operations across industries but also increasing the attack surface and security breaches. Artificial Intelligence plays a key role in securing IoT, enabling attack detection, attack behavior analysis, and mitigation suggestion. Despite advancements, evaluations remain purely qualitative, and the lack of a standardized, objective benchmark for quantitatively measuring AI-based attack analysis and mitigation hinders consistent assessment of model effectiveness. In this work, we propose a hybrid framework combining Machine Learning (ML) for multi-class attack detection with Large Language Models (LLMs) for attack behavior analysis and mitigation suggestion. After benchmarking several ML and Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we applied structured role-play prompt engineering with Retrieval-Augmented Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed, context-aware responses. We introduce novel evaluation metrics for quantitative assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o, DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the responses. Results show that Random Forest has the best detection model, and ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26941", "pdf_url": "https://arxiv.org/pdf/2510.26941.pdf", "is_interesting": false}, "425": {"title": "Can machines think efficiently?", "authors": ["Adam Winchell"], "abstract": "arXiv:2510.26954v1 Announce Type: cross \nThe Turing Test is no longer adequate for distinguishing human and machine intelligence. With advanced artificial intelligence systems already passing the original Turing Test and contributing to serious ethical and environmental concerns, we urgently need to update the test. This work expands upon the original imitation game by accounting for an additional factor: the energy spent answering the questions. By adding the constraint of energy, the new test forces us to evaluate intelligence through the lens of efficiency, connecting the abstract problem of thinking to the concrete reality of finite resources. Further, this proposed new test ensures the evaluation of intelligence has a measurable, practical finish line that the original test lacks. This additional constraint compels society to weigh the time savings of using artificial intelligence against its total resource cost.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2510.26954", "pdf_url": "https://arxiv.org/pdf/2510.26954.pdf", "is_interesting": false}, "426": {"title": "Using Salient Object Detection to Identify Manipulative Cookie Banners that Circumvent GDPR", "authors": ["Riley Grossman, Michael Smith, Cristian Borcea, Yi Chen"], "abstract": "arXiv:2510.26967v1 Announce Type: cross \nThe main goal of this paper is to study how often cookie banners that comply with the General Data Protection Regulation (GDPR) contain aesthetic manipulation, a design tactic to draw users' attention to the button that permits personal data sharing. As a byproduct of this goal, we also evaluate how frequently the banners comply with GDPR and the recommendations of national data protection authorities regarding banner designs. We visited 2,579 websites and identified the type of cookie banner implemented. Although 45% of the relevant websites have fully compliant banners, we found aesthetic manipulation on 38% of the compliant banners. Unlike prior studies of aesthetic manipulation, we use a computer vision model for salient object detection to measure how salient (i.e., attention-drawing) each banner element is. This enables the discovery of new types of aesthetic manipulation (e.g., button placement), and leads us to conclude that aesthetic manipulation is more common than previously reported (38% vs 27% of banners). To study the effects of user and/or website location on cookie banner design, we include websites within the European Union (EU), where privacy regulation enforcement is more stringent, and websites outside the EU. We visited websites from IP addresses in the EU and from IP addresses in the United States (US). We find that 13.9% of EU websites change their banner design when the user is from the US, and EU websites are roughly 48.3% more likely to use aesthetic manipulation than non-EU websites, highlighting their innovative responses to privacy regulation.", "categories": ["cs.CY", "cs.AI", "cs.CV", "cs.HC"], "abs_url": "https://arxiv.org/abs/2510.26967", "pdf_url": "https://arxiv.org/pdf/2510.26967.pdf", "is_interesting": false}, "427": {"title": "Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence", "authors": ["L\\'ivia Dutra, Arthur Lorenzi, La\\'is Berno, Franciany Campos, Karoline Biscardi, Kenneth Brown, Marcelo Viridiano, Frederico Belcavello, Ely Matos, Ol\\'ivia Guaranha, Erik Santos, Sofia Reinach, Tiago Timponi Torrent"], "abstract": "arXiv:2510.26969v1 Announce Type: cross \nWe introduce a methodology for the identification of notifiable events in the domain of healthcare. The methodology harnesses semantic frames to define fine-grained patterns and search them in unstructured data, namely, open-text fields in e-medical records. We apply the methodology to the problem of underreporting of gender-based violence (GBV) in e-medical records produced during patients' visits to primary care units. A total of eight patterns are defined and searched on a corpus of 21 million sentences in Brazilian Portuguese extracted from e-SUS APS. The results are manually evaluated by linguists and the precision of each pattern measured. Our findings reveal that the methodology effectively identifies reports of violence with a precision of 0.726, confirming its robustness. Designed as a transparent, efficient, low-carbon, and language-agnostic pipeline, the approach can be easily adapted to other health surveillance contexts, contributing to the broader, ethical, and explainable use of NLP in public health systems.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26969", "pdf_url": "https://arxiv.org/pdf/2510.26969.pdf", "is_interesting": false}, "428": {"title": "Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations", "authors": ["Jean-Philippe Corbeil, Asma Ben Abacha, Jerome Tremblay, Phillip Swazinna, Akila Jeeson Daniel, Miguel Del-Agua, Francois Beaulieu"], "abstract": "arXiv:2510.26974v1 Announce Type: cross \nClinical documentation increasingly uses automatic speech recognition and summarization, yet converting conversations into actionable medical orders for Electronic Health Records remains unexplored. A solution to this problem can significantly reduce the documentation burden of clinicians and directly impact downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first challenge on extracting medical orders from doctor-patient conversations. Six teams participated in the shared task and experimented with a broad range of approaches, and both closed- and open-weight large language models (LLMs). In this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking, and participants' solutions.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26974", "pdf_url": "https://arxiv.org/pdf/2510.26974.pdf", "is_interesting": false}, "429": {"title": "Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget", "authors": ["Zhichao Hou, Weizhi Gao, Xiaorui Liu"], "abstract": "arXiv:2510.26981v1 Announce Type: cross \nThis work tackles a critical challenge in AI safety research under limited compute: given a fixed computation budget, how can one maximize the strength of iterative adversarial attacks? Coarsely reducing the number of attack iterations lowers cost but substantially weakens effectiveness. To fulfill the attainable attack efficacy within a constrained budget, we propose a fine-grained control mechanism that selectively recomputes layer activations across both iteration-wise and layer-wise levels. Extensive experiments show that our method consistently outperforms existing baselines at equal cost. Moreover, when integrated into adversarial training, it attains comparable performance with only 30% of the original budget.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26981", "pdf_url": "https://arxiv.org/pdf/2510.26981.pdf", "is_interesting": false}, "430": {"title": "LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval", "authors": ["Elliot L. Epstein, John Winnicki, Thanawat Sornwanee, Rajat Dwaraknath"], "abstract": "arXiv:2510.26995v1 Announce Type: cross \nLarge language models (LLMs) excel at numerical estimation but struggle to correctly quantify uncertainty. We study how well LLMs construct confidence intervals around their own answers and find that they are systematically overconfident. To evaluate this behavior, we introduce FermiEval, a benchmark of Fermi-style estimation questions with a rigorous scoring rule for confidence interval coverage and sharpness. Across several modern models, nominal 99\\% intervals cover the true answer only 65\\% of the time on average. With a conformal prediction based approach that adjusts the intervals, we obtain accurate 99\\% observed coverage, and the Winkler interval score decreases by 54\\%. We also propose direct log-probability elicitation and quantile adjustment methods, which further reduce overconfidence at high confidence levels. Finally, we develop a perception-tunnel theory explaining why LLMs exhibit overconfidence: when reasoning under uncertainty, they act as if sampling from a truncated region of their inferred distribution, neglecting its tails.", "categories": ["stat.ME", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26995", "pdf_url": "https://arxiv.org/pdf/2510.26995.pdf", "is_interesting": false}, "431": {"title": "AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments", "authors": ["Adithya Neelakantan, Pratik Satpute, Prerna Shinde, Tejas Manjunatha Devang"], "abstract": "arXiv:2510.26999v1 Announce Type: cross \nThe AIoT-Based Smart Education System integrates Artificial Intelligence and IoT to address persistent challenges in contemporary classrooms: attendance fraud, lack of personalization, student disengagement, and inefficient resource use. The unified platform combines four core modules: (1) a dual-factor authentication system leveraging RFID-based ID scans and WiFi verification for secure, fraud-resistant attendance; (2) an AI-powered assistant that provides real-time, context-aware support and dynamic quiz generation based on instructor-supplied materials; (3) automated test generators to streamline adaptive assessment and reduce administrative overhead; and (4) the EcoSmart Campus module, which autonomously regulates classroom lighting, air quality, and temperature using IoT sensors and actuators. Simulated evaluations demonstrate the system's effectiveness in delivering robust real-time monitoring, fostering inclusive engagement, preventing fraudulent practices, and supporting operational scalability. Collectively, the AIoT-Based Smart Education System offers a secure, adaptive, and efficient learning environment, providing a scalable blueprint for future educational innovation and improved student outcomes through the synergistic application of artificial intelligence and IoT technologies.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26999", "pdf_url": "https://arxiv.org/pdf/2510.26999.pdf", "is_interesting": false}, "432": {"title": "A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms", "authors": ["Elise Wolf"], "abstract": "arXiv:2510.27001v1 Announce Type: cross \nMulti-armed bandit (MAB) problems serve as a fundamental building block for more complex reinforcement learning algorithms. However, evaluating and comparing MAB algorithms remains challenging due to the lack of standardized conditions and replicability. This is particularly problematic for variance-aware extensions of classical methods like UCB, whose performance can heavily depend on the underlying environment. In this study, we address how performance differences between bandit algorithms can be reliably observed, and under what conditions variance-aware algorithms outperform classical ones. We present a reproducible evaluation designed to systematically compare eight classical and variance-aware MAB algorithms. The evaluation framework, implemented in our Bandit Playground codebase, features clearly defined experimental setups, multiple performance metrics (reward, regret, reward distribution, value-at-risk, and action optimality), and an interactive evaluation interface that supports consistent and transparent analysis. We show that variance-aware algorithms can offer advantages in settings with high uncertainty where the difficulty arises from subtle differences between arm rewards. In contrast, classical algorithms often perform equally well or better in more separable scenarios or if fine-tuned extensively. Our contributions are twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2) insights into the conditions under which variance-aware approaches outperform their classical counterparts.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27001", "pdf_url": "https://arxiv.org/pdf/2510.27001.pdf", "is_interesting": false}, "433": {"title": "Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase", "authors": ["Mihir Mahajan, Alfred Nguyen, Franz Srambical, Stefan Bauer"], "abstract": "arXiv:2510.27002v1 Announce Type: cross \nWhile world models are increasingly positioned as a pathway to overcoming data scarcity in domains such as robotics, open training infrastructure for world modeling remains nascent. We introduce Jasmine, a performant JAX-based world modeling codebase that scales from single hosts to hundreds of accelerators with minimal code changes. Jasmine achieves an order-of-magnitude faster reproduction of the CoinRun case study compared to prior open implementations, enabled by performance optimizations across data loading, training and checkpointing. The codebase guarantees fully reproducible training and supports diverse sharding configurations. By pairing Jasmine with curated large-scale datasets, we establish infrastructure for rigorous benchmarking pipelines across model families and architectural ablations.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27002", "pdf_url": "https://arxiv.org/pdf/2510.27002.pdf", "is_interesting": false}, "434": {"title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics", "authors": ["Simindokht Jahangard, Mehrzad Mohammadi, Abhinav Dhall, Hamid Rezatofighi"], "abstract": "arXiv:2510.27033v1 Announce Type: cross \nVisual reasoning, particularly spatial reasoning, is a challenging cognitive task that requires understanding object relationships and their interactions within complex environments, especially in robotics domain. Existing vision_language models (VLMs) excel at perception tasks but struggle with fine-grained spatial reasoning due to their implicit, correlation-driven reasoning and reliance solely on images. We propose a novel neuro_symbolic framework that integrates both panoramic-image and 3D point cloud information, combining neural perception with symbolic reasoning to explicitly model spatial and logical relationships. Our framework consists of a perception module for detecting entities and extracting attributes, and a reasoning module that constructs a structured scene graph to support precise, interpretable queries. Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior performance and reliability in crowded, human_built environments while maintaining a lightweight design suitable for robotics and embodied AI applications.", "categories": ["cs.RO", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27033", "pdf_url": "https://arxiv.org/pdf/2510.27033.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.2, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on spatial reasoning and visual grounding in robotics, integrating panoramic images and 3D point clouds for reasoning. While relevant to perception tasks, it does not directly address autonomous driving systems or typical tasks like trajectory prediction, control, or sensor fusion, but it could be applied in the context of autonomous driving perception systems."}}, "435": {"title": "Elastic Architecture Search for Efficient Language Models", "authors": ["Shang Wang"], "abstract": "arXiv:2510.27037v1 Announce Type: cross \nAs large pre-trained language models become increasingly critical to natural language understanding (NLU) tasks, their substantial computational and memory requirements have raised significant economic and environmental concerns. Addressing these challenges, this paper introduces the Elastic Language Model (ELM), a novel neural architecture search (NAS) method optimized for compact language models. ELM extends existing NAS approaches by introducing a flexible search space with efficient transformer blocks and dynamic modules for dimension and head number adjustment. These innovations enhance the efficiency and flexibility of the search process, which facilitates more thorough and effective exploration of model architectures. We also introduce novel knowledge distillation losses that preserve the unique characteristics of each block, in order to improve the discrimination between architectural choices during the search process. Experiments on masked language modeling and causal language modeling tasks demonstrate that models discovered by ELM significantly outperform existing methods.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abs_url": "https://arxiv.org/abs/2510.27037", "pdf_url": "https://arxiv.org/pdf/2510.27037.pdf", "is_interesting": false}, "436": {"title": "Dataset Creation and Baseline Models for Sexism Detection in Hausa", "authors": ["Fatima Adam Muhammad, Shamsuddeen Muhammad Hassan, Isa Inuwa-Dutse"], "abstract": "arXiv:2510.27038v1 Announce Type: cross \nSexism reinforces gender inequality and social exclusion by perpetuating stereotypes, bias, and discriminatory norms. Noting how online platforms enable various forms of sexism to thrive, there is a growing need for effective sexism detection and mitigation strategies. While computational approaches to sexism detection are widespread in high-resource languages, progress remains limited in low-resource languages where limited linguistic resources and cultural differences affect how sexism is expressed and perceived. This study introduces the first Hausa sexism detection dataset, developed through community engagement, qualitative coding, and data augmentation. For cultural nuances and linguistic representation, we conducted a two-stage user study (n=66) involving native speakers to explore how sexism is defined and articulated in everyday discourse. We further experiment with both traditional machine learning classifiers and pre-trained multilingual language models and evaluating the effectiveness few-shot learning in detecting sexism in Hausa. Our findings highlight challenges in capturing cultural nuance, particularly with clarification-seeking and idiomatic expressions, and reveal a tendency for many false positives in such cases.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27038", "pdf_url": "https://arxiv.org/pdf/2510.27038.pdf", "is_interesting": false}, "437": {"title": "Detecting Data Contamination in LLMs via In-Context Learning", "authors": ["Micha{\\l} Zawalski, Meriem Boubdir, Klaudia Ba{\\l}azy, Besmira Nushi, Pablo Ribalta"], "abstract": "arXiv:2510.27055v1 Announce Type: cross \nWe present Contamination Detection via Context (CoDeC), a practical and accurate method to detect and quantify training data contamination in large language models. CoDeC distinguishes between data memorized during training and data outside the training distribution by measuring how in-context learning affects model performance. We find that in-context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns. Experiments show that CoDeC produces interpretable contamination scores that clearly separate seen and unseen datasets, and reveals strong evidence of memorization in open-weight models with undisclosed training corpora. The method is simple, automated, and both model- and dataset-agnostic, making it easy to integrate with benchmark evaluations.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27055", "pdf_url": "https://arxiv.org/pdf/2510.27055.pdf", "is_interesting": false}, "438": {"title": "Consistency Training Helps Stop Sycophancy and Jailbreaks", "authors": ["Alex Irpan, Alexander Matt Turner, Mark Kurzeja, David K. Elson, Rohin Shah"], "abstract": "arXiv:2510.27062v1 Announce Type: cross \nAn LLM's factuality and refusal training can be compromised by simple changes to a prompt. Models often adopt user beliefs (sycophancy) or satisfy inappropriate requests which are wrapped within special text (jailbreaking). We explore \\emph{consistency training}, a self-supervised paradigm that teaches a model to be invariant to certain irrelevant cues in the prompt. Instead of teaching the model what exact response to give on a particular prompt, we aim to teach the model to behave identically across prompt data augmentations (like adding leading questions or jailbreak text). We try enforcing this invariance in two ways: over the model's external outputs (\\emph{Bias-augmented Consistency Training} (BCT) from Chua et al. [2025]) and over its internal activations (\\emph{Activation Consistency Training} (ACT), a method we introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant cues. Because consistency training uses responses from the model itself as training data, it avoids issues that arise from stale training data, such as degrading model capabilities or enforcing outdated response guidelines. While BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak reduction. We think that BCT can simplify training pipelines by removing reliance on static datasets. We argue that some alignment problems are better viewed not in terms of optimal responses, but rather as consistency issues.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27062", "pdf_url": "https://arxiv.org/pdf/2510.27062.pdf", "is_interesting": false}, "439": {"title": "Towards a Measure of Algorithm Similarity", "authors": ["Shairoz Sohail, Taher Ali"], "abstract": "arXiv:2510.27063v1 Announce Type: cross \nGiven two algorithms for the same problem, can we determine whether they are meaningfully different? In full generality, the question is uncomputable, and empirically it is muddied by competing notions of similarity. Yet, in many applications (such as clone detection or program synthesis) a pragmatic and consistent similarity metric is necessary. We review existing equivalence and similarity notions and introduce EMOC: An Evaluation-Memory-Operations-Complexity framework that embeds algorithm implementations into a feature space suitable for downstream tasks. We compile PACD, a curated dataset of verified Python implementations across three problems, and show that EMOC features support clustering and classification of algorithm types, detection of near-duplicates, and quantification of diversity in LLM-generated programs. Code, data, and utilities for computing EMOC embeddings are released to facilitate reproducibility and future work on algorithm similarity.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "cs.SE", "math.IT"], "abs_url": "https://arxiv.org/abs/2510.27063", "pdf_url": "https://arxiv.org/pdf/2510.27063.pdf", "is_interesting": false}, "440": {"title": "Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation", "authors": ["Arnabh Borah, Md Tanvirul Alam, Nidhi Rastogi"], "abstract": "arXiv:2510.27080v1 Announce Type: cross \nSecurity applications are increasingly relying on large language models (LLMs) for cyber threat detection; however, their opaque reasoning often limits trust, particularly in decisions that require domain-specific cybersecurity knowledge. Because security threats evolve rapidly, LLMs must not only recall historical incidents but also adapt to emerging vulnerabilities and attack patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in general LLM applications, but its potential for cybersecurity remains underexplored. In this work, we introduce a RAG-based framework designed to contextualize cybersecurity data and enhance LLM accuracy in knowledge retention and temporal reasoning. Using external datasets and the Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid retrieval approach, and conduct a comparative analysis across multiple performance metrics. Our findings highlight the promise of hybrid retrieval in strengthening the adaptability and reliability of LLMs for cybersecurity tasks.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27080", "pdf_url": "https://arxiv.org/pdf/2510.27080.pdf", "is_interesting": false}, "441": {"title": "QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis", "authors": ["Yiwei Chen, Kehuan Yan, Yu Pan, Daoyi Dong"], "abstract": "arXiv:2510.27091v1 Announce Type: cross \nQuantum theory provides non-classical principles, such as superposition and entanglement, that inspires promising paradigms in machine learning. However, most existing quantum-inspired fusion models rely solely on unitary or unitary-like transformations to generate quantum entanglement. While theoretically expressive, such approaches often suffer from training instability and limited generalizability. In this work, we propose a Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal entanglement modelling. Each modality is firstly encoded as a quantum pure state, after which a differentiable module simulating the QJ operator transforms the separable product state into the entangled representation. By jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates controllable cross-modal entanglement among modalities with dissipative dynamics, where structured stochasticity and steady-state attractor properties serve to stabilize training and constrain entanglement shaping. The resulting entangled states are projected onto trainable measurement vectors to produce predictions. In addition to achieving superior performance over the state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI, and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through von-Neumann entanglement entropy. This work establishes a principled framework for entangled multimodal fusion and paves the way for quantum-inspired approaches in modelling complex cross-modal correlations.", "categories": ["cs.LG", "cs.AI", "quant-ph"], "abs_url": "https://arxiv.org/abs/2510.27091", "pdf_url": "https://arxiv.org/pdf/2510.27091.pdf", "is_interesting": false}, "442": {"title": "Expressive Range Characterization of Open Text-to-Audio Models", "authors": ["Jonathan Morse, Azadeh Naderi, Swen Gaudl, Mark Cartwright, Amy K. Hoover, Mark J. Nelson"], "abstract": "arXiv:2510.27102v1 Announce Type: cross \nText-to-audio models are a type of generative model that produces audio output in response to a given textual prompt. Although level generators and the properties of the functional content that they create (e.g., playability) dominate most discourse in procedurally generated content (PCG), games that emotionally resonate with players tend to weave together a range of creative and multimodal content (e.g., music, sounds, visuals, narrative tone), and multimodal models have begun seeing at least experimental use for this purpose. However, it remains unclear what exactly such models generate, and with what degree of variability and fidelity: audio is an extremely broad class of output for a generative system to target.\n  Within the PCG community, expressive range analysis (ERA) has been used as a quantitative way to characterize generators' output space, especially for level generators. This paper adapts ERA to text-to-audio models, making the analysis tractable by looking at the expressive range of outputs for specific, fixed prompts. Experiments are conducted by prompting the models with several standardized prompts derived from the Environmental Sound Classification (ESC-50) dataset. The resulting audio is analyzed along key acoustic dimensions (e.g., pitch, loudness, and timbre). More broadly, this paper offers a framework for ERA-based exploratory evaluation of generative audio models.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "abs_url": "https://arxiv.org/abs/2510.27102", "pdf_url": "https://arxiv.org/pdf/2510.27102.pdf", "is_interesting": false}, "443": {"title": "AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys", "authors": ["Jinwen Tang, Yi Shang"], "abstract": "arXiv:2510.27126v1 Announce Type: cross \nConventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.12 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.", "categories": ["cs.HC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27126", "pdf_url": "https://arxiv.org/pdf/2510.27126.pdf", "is_interesting": false}, "444": {"title": "ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding", "authors": ["Haonan Wang, Jingyu Lu, Hongrui Li, Xiaomeng Li"], "abstract": "arXiv:2510.27128v1 Announce Type: cross \nRecent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: https://github.com/xmed-lab/ZEBRA.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27128", "pdf_url": "https://arxiv.org/pdf/2510.27128.pdf", "is_interesting": false}, "445": {"title": "Exploring Landscapes for Better Minima along Valleys", "authors": ["Tong Zhao, Jiacheng Li, Yuanchang Zhou, Guangming Tan, Weile Jia"], "abstract": "arXiv:2510.27153v1 Announce Type: cross \nFinding lower and better-generalizing minima is crucial for deep learning. However, most existing optimizers stop searching the parameter space once they reach a local minimum. Given the complex geometric properties of the loss landscape, it is difficult to guarantee that such a point is the lowest or provides the best generalization. To address this, we propose an adaptor \"E\" for gradient-based optimizers. The adapted optimizer tends to continue exploring along landscape valleys (areas with low and nearly identical losses) in order to search for potentially better local minima even after reaching a local minimum. This approach increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization. We also provide a proof of convergence for the adapted optimizers in both convex and non-convex scenarios for completeness. Finally, we demonstrate their effectiveness in an important but notoriously difficult training scenario, large-batch training, where Lamb is the benchmark optimizer. Our testing results show that the adapted Lamb, ALTO, increases the test accuracy (generalization) of the current state-of-the-art optimizer by an average of 2.5% across a variety of large-batch training tasks. This work potentially opens a new research direction in the design of optimization algorithms.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.27153", "pdf_url": "https://arxiv.org/pdf/2510.27153.pdf", "is_interesting": false}, "446": {"title": "MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems", "authors": ["Jieshan Chen, Suyu Ma, Qinghua Lu, Sung Une Lee, Liming Zhu"], "abstract": "arXiv:2510.27163v1 Announce Type: cross \nBefore deploying an AI system to replace an existing process, it must be compared with the incumbent to ensure improvement without added risk. Traditional evaluation relies on ground truth for both systems, but this is often unavailable due to delayed or unknowable outcomes, high costs, or incomplete data, especially for long-standing systems deemed safe by convention. The more practical solution is not to compute absolute risk but the difference between systems. We therefore propose a marginal risk assessment framework, that avoids dependence on ground truth or absolute risk. It emphasizes three kinds of relative evaluation methodology, including predictability, capability and interaction dominance. By shifting focus from absolute to relative evaluation, our approach equips software teams with actionable guidance: identifying where AI enhances outcomes, where it introduces new risks, and how to adopt such systems responsibly.", "categories": ["cs.SE", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2510.27163", "pdf_url": "https://arxiv.org/pdf/2510.27163.pdf", "is_interesting": false}, "447": {"title": "Generating Accurate and Detailed Captions for High-Resolution Images", "authors": ["Hankyeol Lee, Gawon Seo, Kyounggyu Lee, Dogun Kim, Kyungwoo Song, Jiyoung Jung"], "abstract": "arXiv:2510.27164v1 Announce Type: cross \nVision-language models (VLMs) often struggle to generate accurate and detailed captions for high-resolution images since they are typically pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels). Downscaling high-resolution images to these dimensions may result in the loss of visual details and the omission of important objects. To address this limitation, we propose a novel pipeline that integrates vision-language models, large language models (LLMs), and object detection systems to enhance caption quality. Our proposed pipeline refines captions through a novel, multi-stage process. Given a high-resolution image, an initial caption is first generated using a VLM, and key objects in the image are then identified by an LLM. The LLM predicts additional objects likely to co-occur with the identified key objects, and these predictions are verified by object detection systems. Newly detected objects not mentioned in the initial caption undergo focused, region-specific captioning to ensure they are incorporated. This process enriches caption detail while reducing hallucinations by removing references to undetected objects. We evaluate the enhanced captions using pairwise comparison and quantitative scoring from large multimodal models, along with a benchmark for hallucination detection. Experiments on a curated dataset of high-resolution images demonstrate that our pipeline produces more detailed and reliable image captions while effectively minimizing hallucinations.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27164", "pdf_url": "https://arxiv.org/pdf/2510.27164.pdf", "is_interesting": false}, "448": {"title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models", "authors": ["Mingyu Sung, Il-Min Kim, Sangseok Yun, Jae-Mo Kang"], "abstract": "arXiv:2510.27171v1 Announce Type: cross \nDiffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27171", "pdf_url": "https://arxiv.org/pdf/2510.27171.pdf", "is_interesting": false}, "449": {"title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler", "authors": ["Zixuan Hu, Li Shen, Zhenyi Wang, Yongxian Wei, Dacheng Tao"], "abstract": "arXiv:2510.27172v1 Announce Type: cross \nHarmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27172", "pdf_url": "https://arxiv.org/pdf/2510.27172.pdf", "is_interesting": false}, "450": {"title": "FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction", "authors": ["Jiaxin Yuan, Haizhao Yang, Maria Cameron"], "abstract": "arXiv:2510.27173v1 Announce Type: cross \nFast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.", "categories": ["cs.CE", "cs.AI", "cs.LG", "math.DS"], "abs_url": "https://arxiv.org/abs/2510.27173", "pdf_url": "https://arxiv.org/pdf/2510.27173.pdf", "is_interesting": false}, "451": {"title": "Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications", "authors": ["Zixuan Hu, Yongxian Wei, Li Shen, Zhenyi Wang, Lei Li, Chun Yuan, Dacheng Tao"], "abstract": "arXiv:2510.27186v1 Announce Type: cross \nModel inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations--a phenomenon we term \"hallucination\" in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to 3.79 faster) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at https://github.com/Egg-Hu/SMI.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27186", "pdf_url": "https://arxiv.org/pdf/2510.27186.pdf", "is_interesting": false}, "452": {"title": "Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures", "authors": ["Dominik Schwarz"], "abstract": "arXiv:2510.27190v1 Announce Type: cross \nAs Large Language Models (LLMs) are increasingly integrated into automated, multi-stage pipelines, risk patterns that arise from unvalidated trust between processing stages become a practical concern. This paper presents a mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs. The analysis shows that inputs are often interpreted non-neutrally and can trigger implementation-shaped responses or unintended state changes even without explicit commands. We argue that these behaviors constitute architectural failure modes and that string-level filtering alone is insufficient. To mitigate such cross-stage vulnerabilities, we recommend zero-trust architectural principles, including provenance enforcement, context sealing, and plan revalidation, and we introduce \"Countermind\" as a conceptual blueprint for implementing these defenses.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27190", "pdf_url": "https://arxiv.org/pdf/2510.27190.pdf", "is_interesting": false}, "453": {"title": "Vectorized Online POMDP Planning", "authors": ["Marcus Hoerger, Muhammad Sudrajat, Hanna Kurniawati"], "abstract": "arXiv:2510.27191v1 Announce Type: cross \nPlanning under partial observability is an essential capability of autonomous robots. The Partially Observable Markov Decision Process (POMDP) provides a powerful framework for planning under partial observability problems, capturing the stochastic effects of actions and the limited information available through noisy observations. POMDP solving could benefit tremendously from massive parallelization of today's hardware, but parallelizing POMDP solvers has been challenging. They rely on interleaving numerical optimization over actions with the estimation of their values, which creates dependencies and synchronization bottlenecks between parallel processes that can quickly offset the benefits of parallelization. In this paper, we propose Vectorized Online POMDP Planner (VOPP), a novel parallel online solver that leverages a recent POMDP formulation that analytically solves part of the optimization component, leaving only the estimation of expectations for numerical computation. VOPP represents all data structures related to planning as a collection of tensors and implements all planning steps as fully vectorized computations over this representation. The result is a massively parallel solver with no dependencies and synchronization bottlenecks between parallel computations. Experimental results indicate that VOPP is at least 20X more efficient in computing near-optimal solutions compared to an existing state-of-the-art parallel online solver.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27191", "pdf_url": "https://arxiv.org/pdf/2510.27191.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c4\u5212\u63a7\u5236", "reason": "The paper focuses on planning under partial observability using the POMDP framework and introduces a parallel solver for optimization. While the problem and approach are relevant to robotic planning, it does not specifically discuss autonomous driving or tasks directly related to vehicle perception, decision-making, or control in autonomous driving contexts."}}, "454": {"title": "MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models", "authors": ["Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Yayue Deng, Jing Ma"], "abstract": "arXiv:2510.27196v1 Announce Type: cross \nThe proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27196", "pdf_url": "https://arxiv.org/pdf/2510.27196.pdf", "is_interesting": false}, "455": {"title": "Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models", "authors": ["Hamed Najafi, Dongsheng Luo, Jason Liu"], "abstract": "arXiv:2510.27207v1 Announce Type: cross \nExplainable AI (XAI) is critical for building trust in complex machine learning models, yet mainstream attribution methods often provide an incomplete, static picture of a model's final state. By collapsing a feature's role into a single score, they are confounded by non-linearity and interactions. To address this, we introduce Feature-Function Curvature Analysis (FFCA), a novel framework that analyzes the geometry of a model's learned function. FFCA produces a 4-dimensional signature for each feature, quantifying its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction. Crucially, we extend this framework into Dynamic Archetype Analysis, which tracks the evolution of these signatures throughout the training process. This temporal view moves beyond explaining what a model learned to revealing how it learns. We provide the first direct, empirical evidence of hierarchical learning, showing that models consistently learn simple linear effects before complex interactions. Furthermore, this dynamic analysis provides novel, practical diagnostics for identifying insufficient model capacity and predicting the onset of overfitting. Our comprehensive experiments demonstrate that FFCA, through its static and dynamic components, provides the essential geometric context that transforms model explanation from simple quantification to a nuanced, trustworthy analysis of the entire learning process.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27207", "pdf_url": "https://arxiv.org/pdf/2510.27207.pdf", "is_interesting": false}, "456": {"title": "Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks", "authors": ["Jiaxin Zhang, Zehong Zhu, Junye Deng, Yunqin Li, and Bowen Wang"], "abstract": "arXiv:2510.27208v1 Announce Type: cross \nVillages areas hold significant importance in the study of human-land relationships. However, with the advancement of urbanization, the gradual disappearance of spatial characteristics and the homogenization of landscapes have emerged as prominent issues. Existing studies primarily adopt a single-disciplinary perspective to analyze villages spatial morphology and its influencing factors, relying heavily on qualitative analysis methods. These efforts are often constrained by the lack of digital infrastructure and insufficient data. To address the current research limitations, this paper proposes a Hierarchical Graph Neural Network (HGNN) model that integrates multi-source data to conduct an in-depth analysis of villages spatial morphology. The framework includes two types of nodes-input nodes and communication nodes-and two types of edges-static input edges and dynamic communication edges. By combining Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), the proposed model efficiently integrates multimodal features under a two-stage feature update mechanism. Additionally, based on existing principles for classifying villages spatial morphology, the paper introduces a relational pooling mechanism and implements a joint training strategy across 17 subtypes. Experimental results demonstrate that this method achieves significant performance improvements over existing approaches in multimodal fusion and classification tasks. Additionally, the proposed joint optimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83 (independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Our method provides scientific evidence for exploring villages spatial patterns and generative logic.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27208", "pdf_url": "https://arxiv.org/pdf/2510.27208.pdf", "is_interesting": false}, "457": {"title": "Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness", "authors": ["Ren Tasai, Guang Li, Ren Togo, Takahiro Ogawa, Kenji Hirata, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Noriko Nishioka, Yukie Shimizu, Kohsuke Kudo, Miki Haseyama"], "abstract": "arXiv:2510.27213v1 Announce Type: cross \nWe propose a novel continual self-supervised learning (CSSL) framework for simultaneously learning diverse features from multi-window-obtained chest computed tomography (CT) images and ensuring data privacy. Achieving a robust and highly generalizable model in medical image diagnosis is challenging, mainly because of issues, such as the scarcity of large-scale, accurately annotated datasets and domain shifts inherent to dynamic healthcare environments. Specifically, in chest CT, these domain shifts often arise from differences in window settings, which are optimized for distinct clinical purposes. Previous CSSL frameworks often mitigated domain shift by reusing past data, a typically impractical approach owing to privacy constraints. Our approach addresses these challenges by effectively capturing the relationship between previously learned knowledge and new information across different training stages through continual pretraining on unlabeled images. Specifically, by incorporating a latent replay-based mechanism into CSSL, our method mitigates catastrophic forgetting due to domain shifts during continual pretraining while ensuring data privacy. Additionally, we introduce a feature distillation technique that integrates Wasserstein distance-based knowledge distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of the model to learn meaningful, domain-shift-robust representations. Finally, we validate our approach using chest CT images obtained across two different window settings, demonstrating superior performance compared with other approaches.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27213", "pdf_url": "https://arxiv.org/pdf/2510.27213.pdf", "is_interesting": false}, "458": {"title": "Soft Task-Aware Routing of Experts for Equivariant Representation Learning", "authors": ["Jaebyeong Jeon, Hyeonseo Jang, Jy-yong Sohn, Kibok Lee"], "abstract": "arXiv:2510.27222v1 Announce Type: cross \nEquivariant representation learning aims to capture variations induced by input transformations in the representation space, whereas invariant representation learning encodes semantic information by disregarding such transformations. Recent studies have shown that jointly learning both types of representations is often beneficial for downstream tasks, typically by employing separate projection heads. However, this design overlooks information shared between invariant and equivariant learning, which leads to redundant feature learning and inefficient use of model capacity. To address this, we introduce Soft Task-Aware Routing (STAR), a routing strategy for projection heads that models them as experts. STAR induces the experts to specialize in capturing either shared or task-specific information, thereby reducing redundant feature learning. We validate this effect by observing lower canonical correlations between invariant and equivariant embeddings. Experimental results show consistent improvements across diverse transfer learning tasks. The code is available at https://github.com/YonseiML/star.", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.27222", "pdf_url": "https://arxiv.org/pdf/2510.27222.pdf", "is_interesting": false}, "459": {"title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries", "authors": ["Chuxuan Hu, Maxwell Yang, James Weiland, Yeji Lim, Suhas Palawala, Daniel Kang"], "abstract": "arXiv:2510.27238v1 Announce Type: cross \nManually conducting real-world data analyses is labor-intensive and inefficient. Despite numerous attempts to automate data science workflows, none of the existing paradigms or systems fully demonstrate all three key capabilities required to support them effectively: (1) open-domain data collection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that answers users' analytic queries in natural language on large-scale open-domain data. DRAMA unifies data collection, transformation, and analysis as a single pipeline. To quantitatively evaluate system performance on tasks representative of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories of tasks: claim verification and question answering, each comprising 100 instances. These tasks are derived from real-world applications that have gained significant public attention and require the retrieval and analysis of open-domain data. We develop DRAMA-Bot, a multi-agent system designed following DRAMA. It comprises a data retriever that collects and transforms data by coordinating the execution of sub-agents, and a data analyzer that performs structured reasoning over the retrieved data. We evaluate DRAMA-Bot on DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is publicly available at https://github.com/uiuc-kang-lab/drama.", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "abs_url": "https://arxiv.org/abs/2510.27238", "pdf_url": "https://arxiv.org/pdf/2510.27238.pdf", "is_interesting": false}, "460": {"title": "Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes", "authors": ["Ora Nova Fandina, Gal Amram, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Rami Katan, Alice Podolsky, Orna Raz"], "abstract": "arXiv:2510.27244v1 Announce Type: cross \nApplication modernization in legacy languages such as COBOL, PL/I, and REXX faces an acute shortage of resources, both in expert availability and in high-quality human evaluation data. While Large Language Models as a Judge (LaaJ) offer a scalable alternative to expert review, their reliability must be validated before being trusted in high-stakes workflows. Without principled validation, organizations risk a circular evaluation loop, where unverified LaaJs are used to assess model outputs, potentially reinforcing unreliable judgments and compromising downstream deployment decisions. Although various automated approaches to validating LaaJs have been proposed, alignment with human judgment remains a widely used and conceptually grounded validation strategy. In many real-world domains, the availability of human-labeled evaluation data is severely limited, making it difficult to assess how well a LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework for assessing LaaJ alignment with sparse human-labeled data. SparseAlign combines a novel pairwise-confidence concept with a score-sensitive alignment metric that jointly capture ranking consistency and score proximity, enabling reliable evaluator selection even when traditional statistical methods are ineffective due to limited annotated examples. SparseAlign was applied internally to select LaaJs for COBOL code explanation. The top-aligned evaluators were integrated into assessment workflows, guiding model release decisions. We present a case study of four LaaJs to demonstrate SparseAlign's utility in real-world evaluation scenarios.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27244", "pdf_url": "https://arxiv.org/pdf/2510.27244.pdf", "is_interesting": false}, "461": {"title": "Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs", "authors": ["Mohammad Tavakoli, Alireza Salemi, Carrie Ye, Mohamed Abdalla, Hamed Zamani, J Ross Mitchell"], "abstract": "arXiv:2510.27246v1 Announce Type: cross \nEvaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2510.27246", "pdf_url": "https://arxiv.org/pdf/2510.27246.pdf", "is_interesting": false}, "462": {"title": "Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication", "authors": ["Deok-Seon Kim, Seo-Hyun Lee, Kang Yin, Seong-Whan Lee"], "abstract": "arXiv:2510.27247v1 Announce Type: cross \nBrain-to-speech (BTS) systems represent a groundbreaking approach to human communication by enabling the direct transformation of neural activity into linguistic expressions. While recent non-invasive BTS studies have largely focused on decoding predefined words or sentences, achieving open-vocabulary neural communication comparable to natural human interaction requires decoding unconstrained speech. Additionally, effectively integrating diverse signals derived from speech is crucial for developing personalized and adaptive neural communication and rehabilitation solutions for patients. This study investigates the potential of speech synthesis for previously unseen sentences across various speech modes by leveraging phoneme-level information extracted from high-density electroencephalography (EEG) signals, both independently and in conjunction with electromyography (EMG) signals. Furthermore, we examine the properties affecting phoneme decoding accuracy during sentence reconstruction and offer neurophysiological insights to further enhance EEG decoding for more effective neural communication solutions. Our findings underscore the feasibility of biosignal-based sentence-level speech synthesis for reconstructing unseen sentences, highlighting a significant step toward developing open-vocabulary neural communication systems adapted to diverse patient needs and conditions. Additionally, this study provides meaningful insights into the development of communication and rehabilitation solutions utilizing EEG-based decoding technologies.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27247", "pdf_url": "https://arxiv.org/pdf/2510.27247.pdf", "is_interesting": false}, "463": {"title": "Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation", "authors": ["Qiyan Deng, Changqian Zheng, Lianpeng Qiao, Yuping Wang, Chengliang Chai, Lei Cao"], "abstract": "arXiv:2510.27253v1 Announce Type: cross \nDataset distillation condenses large datasets into synthetic subsets, achieving performance comparable to training on the full dataset while substantially reducing storage and computation costs. Most existing dataset distillation methods assume that all real instances contribute equally to the process. In practice, real-world datasets contain both informative and redundant or even harmful instances, and directly distilling the full dataset without considering data quality can degrade model performance. In this work, we present Influence-Weighted Distillation IWD, a principled framework that leverages influence functions to explicitly account for data quality in the distillation process. IWD assigns adaptive weights to each instance based on its estimated impact on the distillation objective, prioritizing beneficial data while downweighting less useful or harmful ones. Owing to its modular design, IWD can be seamlessly integrated into diverse dataset distillation frameworks. Our empirical results suggest that integrating IWD tends to improve the quality of distilled datasets and enhance model performance, with accuracy gains of up to 7.8%.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27253", "pdf_url": "https://arxiv.org/pdf/2510.27253.pdf", "is_interesting": false}, "464": {"title": "Languages are Modalities: Cross-Lingual Alignment via Encoder Injection", "authors": ["Rajan Agarwal, Aarush Gupta"], "abstract": "arXiv:2510.27254v1 Announce Type: cross \nInstruction-tuned Large Language Models (LLMs) underperform on low resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling. We present LLINK (Latent Language Injection for Non-English Knowledge), a compute efficient language-as-modality method that conditions an instruction-tuned decoder without changing the tokenizer or retraining the decoder. First, we align sentence embeddings from a frozen multilingual encoder to the decoder's latent embedding space at a reserved position via a lightweight contrastive projector. Second, the vector is expanded into K soft slots and trained with minimal adapters so the frozen decoder consumes the signal. LLINK substantially improves bilingual retrieval and achieves 81.3% preference over the base model and 63.6% over direct fine-tuning in LLM-judged Q&amp;A evaluations. We further find that improvements can be attributed to reduced tokenization inflation and a stronger cross lingual alignment, despite the model having residual weaknesses in numeric fidelity. Treating low resource languages as a modality offers a practical path to stronger cross-lingual alignment in lightweight LLMs.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27254", "pdf_url": "https://arxiv.org/pdf/2510.27254.pdf", "is_interesting": false}, "465": {"title": "Higher-order Linear Attention", "authors": ["Yifan Zhang, Zhen Qin, Quanquan Gu"], "abstract": "arXiv:2510.27258v1 Announce Type: cross \nThe quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \\times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27258", "pdf_url": "https://arxiv.org/pdf/2510.27258.pdf", "is_interesting": false}, "466": {"title": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models", "authors": ["Kangkun Mao, Jinru Ding, Jiayuan Chen, Mouxiao Bian, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu"], "abstract": "arXiv:2510.27267v1 Announce Type: cross \nAs large language models (LLMs) enter the medical domain, most benchmarks evaluate them on question answering or descriptive reasoning, overlooking quantitative reasoning critical to clinical decision-making. Existing datasets like MedCalc-Bench cover few calculation tasks and fail to reflect real-world computational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical calculation abilities, comprising 700+ tasks across two types: equation-based (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale). These tasks span diverse specialties including internal medicine, surgery, pediatrics, and cardiology, offering a broader and more challenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, enabling multi-step clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this environment achieves state-of-the-art results on MedCalc-Eval, with notable gains in numerical sensitivity, formula selection, and reasoning robustness. Remaining challenges include unit conversion, multi-condition logic, and contextual understanding.\n  Code and datasets are available at https://github.com/maokangkun/MedCalc-Eval.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27267", "pdf_url": "https://arxiv.org/pdf/2510.27267.pdf", "is_interesting": false}, "467": {"title": "Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?", "authors": ["Deokhyung Kang, Seonjeong Hwang, Daehui Kim, Hyounghun Kim, Gary Geunbae Lee"], "abstract": "arXiv:2510.27269v1 Announce Type: cross \nReasoning language models (RLMs) achieve strong performance on complex reasoning tasks, yet they still suffer from a multilingual reasoning gap, performing better in high-resource languages than in low-resource ones. While recent efforts have reduced this gap, its underlying causes remain largely unexplored. In this paper, we address this by showing that the multilingual reasoning gap largely stems from failures in language understanding-the model's inability to represent the multilingual input meaning into the dominant language (i.e., English) within its reasoning trace. This motivates us to examine whether understanding failures can be detected, as this ability could help mitigate the multilingual reasoning gap. To this end, we evaluate a range of detection methods and find that understanding failures can indeed be identified, with supervised approaches performing best. Building on this, we propose Selective Translation, a simple yet effective strategy that translates the multilingual input into English only when an understanding failure is detected. Experimental results show that Selective Translation bridges the multilingual reasoning gap, achieving near full-translation performance while using translation for only about 20% of inputs. Together, our work demonstrates that understanding failures are the primary cause of the multilingual reasoning gap and can be detected and selectively mitigated, providing key insight into its origin and a promising path toward more equitable multilingual reasoning. Our code and data are publicly available at https://github.com/deokhk/RLM_analysis.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27269", "pdf_url": "https://arxiv.org/pdf/2510.27269.pdf", "is_interesting": false}, "468": {"title": "FOCUS: Efficient Keyframe Selection for Long Video Understanding", "authors": ["Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Zhenheng Yang, Yang You"], "abstract": "arXiv:2510.27280v1 Announce Type: cross \nMultimodal large language models (MLLMs) represent images and video frames as visual tokens. Scaling from single images to hour-long videos, however, inflates the token budget far beyond practical limits. Popular pipelines therefore either uniformly subsample or apply keyframe selection with retrieval-style scoring using smaller vision-language models. However, these keyframe selection methods still rely on pre-filtering before selection to reduce the inference cost and can miss the most informative moments.\n  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a training-free, model-agnostic keyframe selection module that selects query-relevant frames under a strict token budget. FOCUS formulates keyframe selection as a combinatorial pure-exploration (CPE) problem in multi-armed bandits: it treats short temporal clips as arms, and uses empirical means and Bernstein confidence radius to identify informative regions while preserving exploration of uncertain areas. The resulting two-stage exploration-exploitation procedure reduces from a sequential policy with theoretical guarantees, first identifying high-value temporal regions, then selecting top-scoring frames within each region On two long-video question-answering benchmarks, FOCUS delivers substantial accuracy improvements while processing less than 2% of video frames. For videos longer than 20 minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating its effectiveness as a keyframe selection method and providing a simple and general solution for scalable long-video understanding with MLLMs.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27280", "pdf_url": "https://arxiv.org/pdf/2510.27280.pdf", "is_interesting": false}, "469": {"title": "HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction", "authors": ["Minghui Li, Yuanhang Wang, Peijin Guo, Wei Wan, Shengshan Hu, Shengqing Hu"], "abstract": "arXiv:2510.27281v1 Announce Type: cross \nAccurate prediction of Drug-Target Affinity (DTA) is crucial for reducing experimental costs and accelerating early screening in computational drug discovery. While sequence-based deep learning methods avoid reliance on costly 3D structures, they still overlook simultaneous modeling of global sequence semantic features and local topological structural features within drugs and proteins, and represent drugs as flat sequences without atomic-level, substructural-level, and molecular-level multi-scale features. We propose HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract both global sequence semantic and local topological features from drug and protein sequences, and models drugs multi-scale to learn atomic, substructural, and molecular representations fused via a multi-scale bilinear attention module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms state-of-the-art baselines, with ablations confirming the importance of global-local extraction and multi-scale fusion.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27281", "pdf_url": "https://arxiv.org/pdf/2510.27281.pdf", "is_interesting": false}, "470": {"title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments", "authors": ["Harsh Vishwakarma, Ankush Agarwal, Ojas Patil, Chaitanya Devaguptapu, Mahesh Chandran"], "abstract": "arXiv:2510.27287v1 Announce Type: cross \nEnterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27287", "pdf_url": "https://arxiv.org/pdf/2510.27287.pdf", "is_interesting": false}, "471": {"title": "Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity", "authors": ["Philipp Davydov, Ameya Prabhu, Matthias Bethge, Elisa Nguyen, Seong Joon Oh"], "abstract": "arXiv:2510.27313v1 Announce Type: cross \nUnderstanding how language-model outputs relate to the pretraining corpus is central to studying model behavior. Most training data attribution (TDA) methods ask which training examples causally influence a given output, often using leave-one-out tests. We invert the question: which outputs cannot be attributed to any pretraining example? We introduce un-attributability as an operational measure of semantic novelty: an output is novel if the pretraining corpus contains no semantically similar context. We approximate this with a simple two-stage retrieval pipeline: index the corpus with lightweight GIST embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the nearest corpus item is less attributable than a human-generated text reference, we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2 and report three findings: (1) models draw on pretraining data across much longer spans than previously reported; (2) some domains systematically promote or suppress novelty; and (3) instruction tuning not only alters style but also increases novelty. Reframing novelty assessment around un-attributability enables efficient analysis at pretraining scale. We release ~20 TB of corpus chunks and index artifacts to support replication and large-scale extension of our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27313", "pdf_url": "https://arxiv.org/pdf/2510.27313.pdf", "is_interesting": false}, "472": {"title": "CASR-Net: An Image Processing-focused Deep Learning-based Coronary Artery Segmentation and Refinement Network for X-ray Coronary Angiogram", "authors": ["Alvee Hassan, Rusab Sarmun, Muhammad E. H. Chowdhury, M. Murugappan, Md. Sakib Abrar Hossain, Sakib Mahmud, Abdulrahman Alqahtani, Sohaib Bassam Zoghoul, Amith Khandakar, Susu M. Zughaier, Somaya Al-Maadeed, Anwarul Hasan"], "abstract": "arXiv:2510.27315v1 Announce Type: cross \nEarly detection of coronary artery disease (CAD) is critical for reducing mortality and improving patient treatment planning. While angiographic image analysis from X-rays is a common and cost-effective method for identifying cardiac abnormalities, including stenotic coronary arteries, poor image quality can significantly impede clinical diagnosis. We present the Coronary Artery Segmentation and Refinement Network (CASR-Net), a three-stage pipeline comprising image preprocessing, segmentation, and refinement. A novel multichannel preprocessing strategy combining CLAHE and an improved Ben Graham method provides incremental gains, increasing Dice Score Coefficient (DSC) by 0.31-0.89% and Intersection over Union (IoU) by 0.40-1.16% compared with using the techniques individually. The core innovation is a segmentation network built on a UNet with a DenseNet121 encoder and a Self-organized Operational Neural Network (Self-ONN) based decoder, which preserves the continuity of narrow and stenotic vessel branches. A final contour refinement module further suppresses false positives. Evaluated with 5-fold cross-validation on a combination of two public datasets that contain both healthy and stenotic arteries, CASR-Net outperformed several state-of-the-art models, achieving an IoU of 61.43%, a DSC of 76.10%, and clDice of 79.36%. These results highlight a robust approach to automated coronary artery segmentation, offering a valuable tool to support clinicians in diagnosis and treatment planning.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27315", "pdf_url": "https://arxiv.org/pdf/2510.27315.pdf", "is_interesting": false}, "473": {"title": "Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis", "authors": ["Weiming Chen, Yijia Wang, Zhihan Zhu, Zhihai He"], "abstract": "arXiv:2510.27324v1 Announce Type: cross \nWe consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27324", "pdf_url": "https://arxiv.org/pdf/2510.27324.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.5, "subfield": "\u89c6\u89c9\u901a\u4fe1 / \u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "\u8be5\u8bba\u6587\u4e3b\u8981\u7814\u7a76\u8d85\u4f4e\u6bd4\u7279\u7387\u89c6\u89c9\u901a\u4fe1\u4e0e\u56fe\u50cf\u751f\u6210\uff0c\u7528\u4e8e\u8fdc\u7a0b\u89c6\u89c9\u5206\u6790\u3001\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u573a\u666f\u3002\u867d\u7136\u672a\u76f4\u63a5\u8ba8\u8bba\u81ea\u52a8\u9a7e\u9a76\uff0c\u4f46\u5176\u5728\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u538b\u7f29\u4e0e\u91cd\u5efa\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u8f66\u8f7d\u7cfb\u7edf\u7684\u8fdc\u7a0b\u611f\u77e5\u6216V2X\u901a\u4fe1\u4e2d\uff0c\u56e0\u6b64\u4e0e\u81ea\u52a8\u9a7e\u9a76\u6709\u4e00\u5b9a\u5173\u8054\u3002"}}, "474": {"title": "Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V", "authors": ["Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu"], "abstract": "arXiv:2510.27364v1 Announce Type: cross \nWe present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27364", "pdf_url": "https://arxiv.org/pdf/2510.27364.pdf", "is_interesting": false}, "475": {"title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity", "authors": ["Austin Meek, Eitan Sprejer, Iv\\'an Arcuschin, Austin J. Brockmeier, Steven Basart"], "abstract": "arXiv:2510.27378v1 Announce Type: cross \nChain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27378", "pdf_url": "https://arxiv.org/pdf/2510.27378.pdf", "is_interesting": false}, "476": {"title": "Spiking Neural Networks: The Future of Brain-Inspired Computing", "authors": ["Sales G. Aribe Jr"], "abstract": "arXiv:2510.27379v1 Announce Type: cross \nSpiking Neural Networks (SNNs) represent the latest generation of neural computation, offering a brain-inspired alternative to conventional Artificial Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals, SNNs operate using distinct spike events, making them inherently more energy-efficient and temporally dynamic. This study presents a comprehensive analysis of SNN design models, training algorithms, and multi-dimensional performance metrics, including accuracy, energy consumption, latency, spike count, and convergence behavior. Key neuron models such as the Leaky Integrate-and-Fire (LIF) and training strategies, including surrogate gradient descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP), are examined in depth. Results show that surrogate gradient-trained SNNs closely approximate ANN accuracy (within 1-2%), with faster convergence by the 20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve competitive performance but require higher spike counts and longer simulation windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike counts and energy consumption (as low as 5 millijoules per inference), making them optimal for unsupervised and low-power tasks. These findings reinforce the suitability of SNNs for energy-constrained, latency-sensitive, and adaptive applications such as robotics, neuromorphic vision, and edge AI systems. While promising, challenges persist in hardware standardization and scalable training. This study concludes that SNNs, with further refinement, are poised to propel the next phase of neuromorphic computing.", "categories": ["cs.NE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27379", "pdf_url": "https://arxiv.org/pdf/2510.27379.pdf", "is_interesting": false}, "477": {"title": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs", "authors": ["Jiahao Liu, Zijian Wang, Kuo Zhao, Dong Hu"], "abstract": "arXiv:2510.27400v1 Announce Type: cross \nKnowledge editing has emerged as an efficient approach for updating factual knowledge in large language models (LLMs). It typically locates knowledge storage modules and then modifies their parameters. However, most existing methods focus on the weights of multilayer perceptron (MLP) modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness. We perform comprehensive knowledge localization experiments on advanced LLMs and find that Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers. Based on these insights, we propose IntAttn-Edit, a method that extends the associative memory paradigm to jointly update both MLP and Attn modules. Our approach uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage. Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27400", "pdf_url": "https://arxiv.org/pdf/2510.27400.pdf", "is_interesting": false}, "478": {"title": "FedMuon: Accelerating Federated Learning with Matrix Orthogonalization", "authors": ["Junkang Liu, Fanhua Shang, Junchao Zhou, Hongying Liu, Yuanyuan Liu, Jin Liu"], "abstract": "arXiv:2510.27403v1 Announce Type: cross \nThe core bottleneck of Federated Learning (FL) lies in the communication rounds. That is, how to achieve more effective local updates is crucial for reducing communication rounds. Existing FL methods still primarily use element-wise local optimizers (Adam/SGD), neglecting the geometric structure of the weight matrices. This often leads to the amplification of pathological directions in the weights during local updates, leading deterioration in the condition number and slow convergence. Therefore, we introduce the Muon optimizer in local, which has matrix orthogonalization to optimize matrix-structured parameters. Experimental results show that, in IID setting, Local Muon significantly accelerates the convergence of FL and reduces communication rounds compared to Local SGD and Local AdamW. However, in non-IID setting, independent matrix orthogonalization based on the local distributions of each client induces strong client drift. Applying Muon in non-IID FL poses significant challenges: (1) client preconditioner leading to client drift; (2) moment reinitialization. To address these challenges, we propose a novel Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1) momentum aggregation, where clients use the aggregated momentum for local initialization; (2) local-global alignment, where the local gradients are aligned with the global update direction to significantly reduce client drift. Theoretically, we prove that \\texttt{FedMuon} achieves a linear speedup convergence rate without the heterogeneity assumption, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. Empirically, we validate the effectiveness of FedMuon on language and vision models. Compared to several baselines, FedMuon significantly reduces communication rounds and improves test accuracy.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27403", "pdf_url": "https://arxiv.org/pdf/2510.27403.pdf", "is_interesting": false}, "479": {"title": "Atlas-Alignment: Making Interpretability Transferable Across Language Models", "authors": ["Bruno Puri, Jim Berend, Sebastian Lapuschkin, Wojciech Samek"], "abstract": "arXiv:2510.27413v1 Announce Type: cross \nInterpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27413", "pdf_url": "https://arxiv.org/pdf/2510.27413.pdf", "is_interesting": false}, "480": {"title": "Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset", "authors": ["Aditya Parikh, Sneha Das, Aasa Feragen"], "abstract": "arXiv:2510.27421v1 Announce Type: cross \nDeep learning models aim to improve diagnostic workflows, but fairness evaluation remains underexplored beyond classification, e.g., in image segmentation. Unaddressed segmentation bias can lead to disparities in the quality of care for certain populations, potentially compounded across clinical decision points and amplified through iterative model development. Here, we audit the fairness of the automated segmentation labels provided in the breast cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation quality across age, ethnicity, and data source. Our analysis reveals an intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source. We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems. Finally, we show how aggregating data from multiple data sources influences site-specific ethnic biases, underscoring the necessity of investigating data at a granular level.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27421", "pdf_url": "https://arxiv.org/pdf/2510.27421.pdf", "is_interesting": false}, "481": {"title": "Learning Soft Robotic Dynamics with Active Exploration", "authors": ["Hehui Zheng, Bhavya Sukhija, Chenhao Li, Klemens Iten, Andreas Krause, Robert K. Katzschmann"], "abstract": "arXiv:2510.27428v1 Announce Type: cross \nSoft robots offer unmatched adaptability and safety in unstructured environments, yet their compliant, high-dimensional, and nonlinear dynamics make modeling for control notoriously difficult. Existing data-driven approaches often fail to generalize, constrained by narrowly focused task demonstrations or inefficient random exploration. We introduce SoftAE, an uncertainty-aware active exploration framework that autonomously learns task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE employs probabilistic ensemble models to estimate epistemic uncertainty and actively guides exploration toward underrepresented regions of the state-action space, achieving efficient coverage of diverse behaviors without task-specific supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a continuum arm, an articulated fish in fluid, and a musculoskeletal leg with hybrid actuation -- and on a pneumatically actuated continuum soft arm in the real world. Compared with random exploration and task-specific model-based reinforcement learning, SoftAE produces more accurate dynamics models, enables superior zero-shot control on unseen tasks, and maintains robustness under sensing noise, actuation delays, and nonlinear material effects. These results demonstrate that uncertainty-driven active exploration can yield scalable, reusable dynamics models across diverse soft robotic morphologies, representing a step toward more autonomous, adaptable, and data-efficient control in compliant robots.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27428", "pdf_url": "https://arxiv.org/pdf/2510.27428.pdf", "is_interesting": false}, "482": {"title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval", "authors": ["WonJun Moon, MinSeok Jung, Gilhan Park, Tae-Young Kim, Cheol-Ho Cho, Woojin Jun, Jae-Pil Heo"], "abstract": "arXiv:2510.27432v1 Announce Type: cross \nPartially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query. Existing methods treat every annotated text-video pair as a positive and all others as negatives, ignoring the rich semantic variation both within a single video and across different videos. Consequently, embeddings of both queries and their corresponding video-clip segments for distinct events within the same video collapse together, while embeddings of semantically similar queries and segments from different videos are driven apart. This limits retrieval performance when videos contain multiple, diverse events. This paper addresses the aforementioned problems, termed as semantic collapse, in both the text and video embedding spaces. We first introduce Text Correlation Preservation Learning, which preserves the semantic relationships encoded by the foundation model across text queries. To address collapse in video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive alignment method that disentangles hierarchical video representations across temporal scales. Subsequently, we introduce order-preserving token merging and adaptive CBVA to enhance alignment by producing video segments that are internally coherent yet mutually distinctive. Extensive experiments on PRVR benchmarks demonstrate that our framework effectively prevents semantic collapse and substantially improves retrieval accuracy.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27432", "pdf_url": "https://arxiv.org/pdf/2510.27432.pdf", "is_interesting": false}, "483": {"title": "CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging", "authors": ["Aon Safdar, Mohamed Saadeldin"], "abstract": "arXiv:2510.27442v1 Announce Type: cross \nVision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27442", "pdf_url": "https://arxiv.org/pdf/2510.27442.pdf", "is_interesting": false}, "484": {"title": "VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision", "authors": ["Xuan Gong, Senmiao Wang, Hanbo Huang, Ruoyu Sun, Shiyu Liang"], "abstract": "arXiv:2510.27462v1 Announce Type: cross \nSupervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled \\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at https://github.com/coder-gx/VCORE.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27462", "pdf_url": "https://arxiv.org/pdf/2510.27462.pdf", "is_interesting": false}, "485": {"title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling", "authors": ["Uzay Macar, Paul C. Bogdan, Senthooran Rajamanoharan, Neel Nanda"], "abstract": "arXiv:2510.27484v1 Announce Type: cross \nMost work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In \"agentic misalignment\" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.27484", "pdf_url": "https://arxiv.org/pdf/2510.27484.pdf", "is_interesting": false}, "486": {"title": "FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models", "authors": ["Junkang Liu, Fanhua Shang, Kewen Zhu, Hongying Liu, Yuanyuan Liu, Jin Liu"], "abstract": "arXiv:2510.27486v1 Announce Type: cross \nAdamW has become one of the most effective optimizers for training large-scale models. We have also observed its effectiveness in the context of federated learning (FL). However, directly applying AdamW in federated learning settings poses significant challenges: (1) due to data heterogeneity, AdamW often yields high variance in the second-moment estimate $\\boldsymbol{v}$; (2) the local overfitting of AdamW may cause client drift; and (3) Reinitializing moment estimates ($\\boldsymbol{v}$, $\\boldsymbol{m}$) at each round slows down convergence. To address these challenges, we propose the first \\underline{Fed}erated \\underline{AdamW} algorithm, called \\texttt{FedAdamW}, for training and fine-tuning various large models. \\texttt{FedAdamW} aligns local updates with the global update using both a \\textbf{local correction mechanism} and decoupled weight decay to mitigate local overfitting. \\texttt{FedAdamW} efficiently aggregates the \\texttt{mean} of the second-moment estimates to reduce their variance and reinitialize them. Theoretically, we prove that \\texttt{FedAdamW} achieves a linear speedup convergence rate of $\\mathcal{O}(\\sqrt{(L \\Delta \\sigma_l^2)/(S K R \\epsilon^2)}+(L \\Delta)/R)$ without \\textbf{heterogeneity assumption}, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. We also employ PAC-Bayesian generalization analysis to explain the effectiveness of decoupled weight decay in local training. Empirically, we validate the effectiveness of \\texttt{FedAdamW} on language and vision Transformer models. Compared to several baselines, \\texttt{FedAdamW} significantly reduces communication rounds and improves test accuracy. The code is available in https://github.com/junkangLiu0/FedAdamW.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27486", "pdf_url": "https://arxiv.org/pdf/2510.27486.pdf", "is_interesting": false}, "487": {"title": "InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames", "authors": ["Haorui Li, Weitao Du, Yuqiang Li, Hongyu Guo, Shengchao Liu"], "abstract": "arXiv:2510.27497v1 Announce Type: cross \nTransformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored. The gap stems from two fundamental challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens that is invariant to both SE(3) transformations and atom index permutations, and (2) designing an architecture capable of modeling hybrid atom-based tokens that couple discrete atom types with continuous 3D coordinates. To address these challenges, we introduce InertialAR. InertialAR devises a canonical tokenization that aligns molecules to their inertial frames and reorders atoms to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the attention mechanism with geometric awareness via geometric rotary positional encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive paradigm to predict the next atom-based token, predicting the atom type first and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR achieves state-of-the-art performance on 7 of the 10 evaluation metrics for unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover, it significantly outperforms strong baselines in controllable generation for targeted chemical functionality, attaining state-of-the-art results across all 5 metrics.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27497", "pdf_url": "https://arxiv.org/pdf/2510.27497.pdf", "is_interesting": false}, "488": {"title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm", "authors": ["Junkang Liu, Yuxuan Tian, Fanhua Shang, Yuanyuan Liu, Hongying Liu, Junchao Zhou, Daorui Ding"], "abstract": "arXiv:2510.27504v1 Announce Type: cross \nTo prevent inference attacks in Federated Learning (FL) and reduce the leakage of sensitive information, Client-level Differentially Private Federated Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually result in sharper loss landscapes, which leads to a decrease in model generalization after differential privacy protection. By using Sharpness Aware Minimization (SAM), the current popular federated learning methods are to find a local flat minimum value to alleviate this problem. However, the local flatness may not reflect the global flatness in CL-DPFL. Therefore, to address this issue and seek global flat minima of models, we propose a new CL-DPFL algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to the local loss to find the global flat minimum. Moreover, by using our global gradient norm penalty, we not only find a flatter global minimum but also reduce the locally updated norm, which means that we further reduce the error of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN mitigates the performance degradation caused by DP. Meanwhile, the proposed DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves fast convergence. We also use R\\'enyi DP to provide strict privacy guarantees and provide sensitivity analysis for local updates. Finally, we conduct effectiveness tests on both ResNet and Transformer models, and achieve significant improvements in six visual and natural language processing tasks compared to existing state-of-the-art algorithms. The code is available at https://github.com/junkangLiu0/DP-FedPGN", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27504", "pdf_url": "https://arxiv.org/pdf/2510.27504.pdf", "is_interesting": false}, "489": {"title": "Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation", "authors": ["Elena Mulero Ayll\\'on, Linlin Shen, Pierangelo Veltri, Fabrizia Gelardi, Arturo Chiti, Paolo Soda, Matteo Tortora"], "abstract": "arXiv:2510.27508v1 Announce Type: cross \nAccurate lung tumor segmentation is vital for improving diagnosis and treatment planning, and effectively combining anatomical and functional information from PET and CT remains a major challenge. In this study, we propose vMambaX, a lightweight multimodal framework integrating PET and CT scan images through a Context-Gated Cross-Modal Perception Module (CGM). Built on the Visual Mamba architecture, vMambaX adaptively enhances inter-modality feature interaction, emphasizing informative regions while suppressing noise. Evaluated on the PCLT20K dataset, the model outperforms baseline models while maintaining lower computational complexity. These results highlight the effectiveness of adaptive cross-modal gating for multimodal tumor segmentation and demonstrate the potential of vMambaX as an efficient and scalable framework for advanced lung cancer analysis. The code is available at https://github.com/arco-group/vMambaX.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27508", "pdf_url": "https://arxiv.org/pdf/2510.27508.pdf", "is_interesting": false}, "490": {"title": "Leveraging Generic Time Series Foundation Models for EEG Classification", "authors": ["Th\\'eo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko"], "abstract": "arXiv:2510.27522v1 Announce Type: cross \nFoundation models for time series are emerging as powerful general-purpose backbones, yet their potential for domain-specific biomedical signals such as electroencephalography (EEG) remains rather unexplored. In this work, we investigate the applicability a recently proposed time series classification foundation model, to a different EEG tasks such as motor imagery classification and sleep stage prediction. We test two pretraining regimes: (a) pretraining on heterogeneous real-world time series from multiple domains, and (b) pretraining on purely synthetic data. We find that both variants yield strong performance, consistently outperforming EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. These results suggest that generalist time series foundation models, even when pretrained on data of non-neural origin or on synthetic signals, can transfer effectively to EEG. Our findings highlight the promise of leveraging cross-domain pretrained models for brain signal analysis, suggesting that EEG may benefit from advances in the broader time series literature.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27522", "pdf_url": "https://arxiv.org/pdf/2510.27522.pdf", "is_interesting": false}, "491": {"title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control", "authors": ["Yuxiang Chen, Xiaoming Xu, Pengle Zhang, Michael Beyer, Martin Rapp, Jun Zhu, Jianfei Chen"], "abstract": "arXiv:2510.27527v1 Announce Type: cross \nLarge Language Models (LLMs) training is prohibitively expensive, driving interest in low-precision fully-quantized training (FQT). While novel 4-bit formats like NVFP4 offer substantial efficiency gains, achieving near-lossless training at such low precision remains challenging. We introduce TetraJet-v2, an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights, and gradients in all linear layers. We identify two critical issues hindering low-precision LLM training: weight oscillation and outliers. To address these, we propose: 1) an unbiased double-block quantization method for NVFP4 linear layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3) OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently outperforms prior FP4 training methods on pre-training LLMs across varying model sizes up to 370M and data sizes up to 200B tokens, reducing the performance gap to full-precision training by an average of 51.3%.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27527", "pdf_url": "https://arxiv.org/pdf/2510.27527.pdf", "is_interesting": false}, "492": {"title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models", "authors": ["Malik H. Altakrori, Nizar Habash, Abdelhakim Freihat, Younes Samih, Kirill Chirkunov, Muhammed AbuOdeh, Radu Florian, Teresa Lynn, Preslav Nakov, Alham Fikri Aji"], "abstract": "arXiv:2510.27543v1 Announce Type: cross \nWe present DialectalArabicMMLU, a new benchmark for evaluating the performance of large language models (LLMs) across Arabic dialects. While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains (22K QA pairs when also including English and MSA). The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA, supporting both task-based and linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27543", "pdf_url": "https://arxiv.org/pdf/2510.27543.pdf", "is_interesting": false}, "493": {"title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities", "authors": ["Travis Davies, Yiqi Huang, Alexi Gladstone, Yunxin Liu, Xiang Chen, Heng Ji, Huxian Liu, Luhui Hu"], "abstract": "arXiv:2510.27545v1 Announce Type: cross \nImplicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27545", "pdf_url": "https://arxiv.org/pdf/2510.27545.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on energy-based models and generative approaches for robotics, specifically addressing issues like computational cost and policy learning. While these topics may have indirect applications in autonomous driving, such as improving robot behavior and dynamic compute allocation, it does not directly discuss autonomous driving systems or related tasks like perception, prediction, or control."}}, "494": {"title": "Sybil-Resistant Service Discovery for Agent Economies", "authors": ["David Shi, Kevin Joo"], "abstract": "arXiv:2510.27554v1 Announce Type: cross \nx402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.", "categories": ["cs.CR", "cs.AI", "cs.SI"], "abs_url": "https://arxiv.org/abs/2510.27554", "pdf_url": "https://arxiv.org/pdf/2510.27554.pdf", "is_interesting": false}, "495": {"title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "authors": ["Sushil Samuel Dinesh, Shinkyu Park"], "abstract": "arXiv:2510.27558v1 Announce Type: cross \nThis paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27558", "pdf_url": "https://arxiv.org/pdf/2510.27558.pdf", "is_interesting": false}, "496": {"title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments", "authors": ["Forough Mehralian, Ryan Shar, James R. Rae, Alireza Hashemi"], "abstract": "arXiv:2510.27565v1 Announce Type: cross \nAs large language models become increasingly capable of generating code, evaluating their performance remains a complex and evolving challenge. Existing benchmarks primarily focus on functional correctness, overlooking the diversity of real-world coding tasks and developer expectations. To this end, we introduce a multi-language benchmark that evaluates LLM instruction-following capabilities and is extensible to operate on any set of standalone coding problems. Our benchmark evaluates instruction following in two key settings: adherence to pre-defined constraints specified with the initial problem, and the ability to perform refinements based on follow-up instructions. For this paper's analysis, we empirically evaluated our benchmarking pipeline with programming tasks from LiveBench, that are also automatically translated from Python into Java and JavaScript. Our automated benchmark reveals that models exhibit differing levels of performance across multiple dimensions of instruction-following. Our benchmarking pipeline provides a more comprehensive evaluation of code generation models, highlighting their strengths and limitations across languages and generation goals.", "categories": ["cs.SE", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2510.27565", "pdf_url": "https://arxiv.org/pdf/2510.27565.pdf", "is_interesting": false}, "497": {"title": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum", "authors": ["Zhuoning Guo, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Xiaowen Chu"], "abstract": "arXiv:2510.27571v1 Announce Type: cross \nThe prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27571", "pdf_url": "https://arxiv.org/pdf/2510.27571.pdf", "is_interesting": false}, "498": {"title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning", "authors": ["Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang"], "abstract": "arXiv:2510.27606v1 Announce Type: cross \nSpatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27606", "pdf_url": "https://arxiv.org/pdf/2510.27606.pdf", "is_interesting": false}, "499": {"title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models", "authors": ["Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper G\\\"otting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika"], "abstract": "arXiv:2510.27629v2 Announce Type: cross \nOpen-weight bio-foundation models present a dual-use dilemma. While holding great promise for accelerating scientific research and drug development, they could also enable bad actors to develop more deadly bioweapons. To mitigate the risk posed by these models, current approaches focus on filtering biohazardous data during pre-training. However, the effectiveness of such an approach remains unclear, particularly against determined actors who might fine-tune these models for malicious use. To address this gap, we propose \\eval, a framework to evaluate the robustness of procedures that are intended to reduce the dual-use capabilities of bio-foundation models. \\eval assesses models' virus understanding through three lenses, including sequence modeling, mutational effects prediction, and virulence prediction. Our results show that current filtering practices may not be particularly effective: Excluded knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits broader generalizability in sequence modeling. Furthermore, dual-use signals may already reside in the pretrained representations, and can be elicited via simple linear probing. These findings highlight the challenges of data filtering as a standalone procedure, underscoring the need for further research into robust safety and security strategies for open-weight bio-foundation models.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27629", "pdf_url": "https://arxiv.org/pdf/2510.27629.pdf", "is_interesting": false}, "500": {"title": "Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation", "authors": ["Riccardo Brioschi, Aleksandr Alekseev, Emanuele Nevali, Berkay D\\\"oner, Omar El Malki, Blagoj Mitrevski, Leandro Kieliger, Mark Collier, Andrii Maksai, Jesse Berent, Claudiu Musat, Efi Kokiopoulou"], "abstract": "arXiv:2510.27632v1 Announce Type: cross \nGraphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27632", "pdf_url": "https://arxiv.org/pdf/2510.27632.pdf", "is_interesting": false}, "501": {"title": "VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images", "authors": ["Cesar H. Comin, Wesley N. Galv\\~ao"], "abstract": "arXiv:2510.27646v1 Announce Type: cross \nSemantic segmentation of blood vessels is an important task in medical image analysis, but its progress is often hindered by the scarcity of large annotated datasets and the poor generalization of models across different imaging modalities. A key aspect is the tendency of Convolutional Neural Networks (CNNs) to learn texture-based features, which limits their performance when applied to new domains with different visual characteristics. We hypothesize that leveraging geometric priors of vessel shapes, such as their tubular and branching nature, can lead to more robust and data-efficient models. To investigate this, we introduce VessShape, a methodology for generating large-scale 2D synthetic datasets designed to instill a shape bias in segmentation models. VessShape images contain procedurally generated tubular geometries combined with a wide variety of foreground and background textures, encouraging models to learn shape cues rather than textures. We demonstrate that a model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets from different domains, requiring only four to ten samples for fine-tuning. Furthermore, the model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training. Our results indicate that pre-training with a strong shape bias can be an effective strategy to overcome data scarcity and improve model generalization in blood vessel segmentation.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27646", "pdf_url": "https://arxiv.org/pdf/2510.27646.pdf", "is_interesting": false}, "502": {"title": "Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition", "authors": ["Shuyan Lyu, Zhanzimo Wu, Junliang Du"], "abstract": "arXiv:2510.27651v1 Announce Type: cross \nModern deep neural networks (DNNs) are typically trained with a global cross-entropy loss in a supervised end-to-end manner: neurons need to store their outgoing weights; training alternates between a forward pass (computation) and a top-down backward pass (learning) which is biologically implausible. Alternatively, greedy layer-wise training eliminates the need for cross-entropy loss and backpropagation. By avoiding the computation of intermediate gradients and the storage of intermediate outputs, it reduces memory usage and helps mitigate issues such as vanishing or exploding gradients. However, most existing layer-wise training approaches have been evaluated only on relatively small datasets with simple deep architectures. In this paper, we first systematically analyze the training dynamics of popular convolutional neural networks (CNNs) trained by stochastic gradient descent (SGD) through an information-theoretic lens. Our findings reveal that networks converge layer-by-layer from bottom to top and that the flow of information adheres to a Markov information bottleneck principle. Building on these observations, we propose a novel layer-wise training approach based on the recently developed deterministic information bottleneck (DIB) and the matrix-based R\\'enyi's $\\alpha$-order entropy functional. Specifically, each layer is trained jointly with an auxiliary classifier that connects directly to the output layer, enabling the learning of minimal sufficient task-relevant representations. We empirically validate the effectiveness of our training procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further demonstrate its applicability to a practical task involving traffic sign recognition. Our approach not only outperforms existing layer-wise training baselines but also achieves performance comparable to SGD.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27651", "pdf_url": "https://arxiv.org/pdf/2510.27651.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.3, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on improving deep neural network training methods and demonstrates its application in traffic sign recognition. While the paper does not explicitly discuss autonomous driving systems, the technique could be applicable to tasks such as traffic sign recognition, a common task in autonomous driving, making it indirectly relevant to the field."}}, "503": {"title": "Community Detection on Model Explanation Graphs for Explainable AI", "authors": ["Ehsan Moradi"], "abstract": "arXiv:2510.27655v1 Announce Type: cross \nFeature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.", "categories": ["cs.SI", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.27655", "pdf_url": "https://arxiv.org/pdf/2510.27655.pdf", "is_interesting": false}, "504": {"title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems", "authors": ["Alireza Saleh Abadi, Leen-Kiat Soh"], "abstract": "arXiv:2510.27659v1 Announce Type: cross \nIn the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2510.27659", "pdf_url": "https://arxiv.org/pdf/2510.27659.pdf", "is_interesting": false}, "505": {"title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting", "authors": ["Danyal Maqbool, Changhee Lee, Zachary Huemann, Samuel D. Church, Matthew E. Larson, Scott B. Perlman, Tomas A. Romero, Joshua D. Warner, Meghan Lubner, Xin Tie, Jameson Merkow, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw"], "abstract": "arXiv:2510.27680v1 Announce Type: cross \nRecent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27680", "pdf_url": "https://arxiv.org/pdf/2510.27680.pdf", "is_interesting": false}, "506": {"title": "Continuous Autoregressive Language Models", "authors": ["Chenze Shao, Darren Li, Fandong Meng, Jie Zhou"], "abstract": "arXiv:2510.27688v1 Announce Type: cross \nThe efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.27688", "pdf_url": "https://arxiv.org/pdf/2510.27688.pdf", "is_interesting": false}, "507": {"title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods", "authors": ["Yang Zhang, Hanlei Jin, Dan Meng, Jun Wang, Jinghua Tan"], "abstract": "arXiv:2403.02901v3 Announce Type: replace \nAutomatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2403.02901", "pdf_url": "https://arxiv.org/pdf/2403.02901.pdf", "is_interesting": false}, "508": {"title": "VRoPE: Rotary Position Embedding for Video Large Language Models", "authors": ["Zikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, Jing Liu"], "abstract": "arXiv:2502.11664v4 Announce Type: replace \nRotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code is available at https://github.com/johncaged/VRoPE.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2502.11664", "pdf_url": "https://arxiv.org/pdf/2502.11664.pdf", "is_interesting": false}, "509": {"title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models", "authors": ["Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay"], "abstract": "arXiv:2503.16724v3 Announce Type: replace \nSemantic interpretability in Reinforcement Learning (RL) enables transparency and verifiability of decision-making. Achieving semantic interpretability in reinforcement learning requires (1) a feature space composed of human-understandable concepts and (2) a policy that is interpretable and verifiable. However, constructing such a feature space has traditionally relied on manual human specification, which often fails to generalize to unseen environments. Moreover, even when interpretable features are available, most reinforcement learning algorithms employ black-box models as policies, thereby hindering transparency. We introduce interpretable Tree-based Reinforcement learning via Automated Concept Extraction (iTRACE), an automated framework that leverages pre-trained vision-language models (VLM) for semantic feature extraction and train a interpretable tree-based model via RL. To address the impracticality of running VLMs in RL loops, we distill their outputs into a lightweight model. By leveraging Vision-Language Models (VLMs) to automate tree-based reinforcement learning, iTRACE loosens the reliance the need for human annotation that is traditionally required by interpretable models. In addition, it addresses key limitations of VLMs alone, such as their lack of grounding in action spaces and their inability to directly optimize policies. We evaluate iTRACE across three domains: Atari games, grid-world navigation, and driving. The results show that iTRACE outperforms other interpretable policy baselines and matches the performance of black-box policies on the same interpretable feature space.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.16724", "pdf_url": "https://arxiv.org/pdf/2503.16724.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.4, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses the use of vision-language models (VLMs) for semantic feature extraction and interpretable reinforcement learning in a driving domain. Although the primary focus is on reinforcement learning and semantic interpretability, the inclusion of the driving domain suggests potential applications in autonomous driving, particularly in areas like decision-making transparency and policy optimization for autonomous vehicles."}}, "510": {"title": "A Framework for Objective-Driven Dynamical Stochastic Fields", "authors": ["Yibo Jacky Zhang, Sanmi Koyejo"], "abstract": "arXiv:2504.16115v2 Announce Type: replace \nFields offer a versatile approach for describing complex systems composed of interacting and dynamic components. In particular, some of these dynamical and stochastic systems may exhibit goal-directed behaviors aimed at achieving specific objectives, which we refer to as $\\textit{intelligent fields}$. However, due to their inherent complexity, it remains challenging to develop a formal theoretical description of such systems and to effectively translate these descriptions into practical applications. In this paper, we propose three fundamental principles to establish a theoretical framework for understanding intelligent fields: complete configuration, locality, and purposefulness. Moreover, we explore methodologies for designing such fields from the perspective of artificial intelligence applications. This initial investigation aims to lay the groundwork for future theoretical developments and practical advances in understanding and harnessing the potential of such objective-driven dynamical stochastic fields.", "categories": ["cs.AI", "cs.LG", "cs.MA", "nlin.AO"], "abs_url": "https://arxiv.org/abs/2504.16115", "pdf_url": "https://arxiv.org/pdf/2504.16115.pdf", "is_interesting": false}, "511": {"title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "authors": ["Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, Keith Ross"], "abstract": "arXiv:2505.14216v2 Announce Type: replace \nRecent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy (pass@1) but often fails to improve capability (pass@k) of LLMs in reasoning tasks, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR struggles to improve capability as it focuses on improving the accuracy of the easier questions to the detriment of the accuracy of the most difficult questions. Second, we show that RLVR does not merely increase the success probability for the easier questions, but in our small model settings, produces quality responses that were absent in its original output distribution. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, from the experiment distilling teacher responses to in-distribution problems, we find that capability does not always improve with distillation. We conjecture that capability improves only when new knowledge is introduced, whereas distilling reasoning patterns only improves accuracy but not capability, sacrificing performance on the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in LLMs", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2505.14216", "pdf_url": "https://arxiv.org/pdf/2505.14216.pdf", "is_interesting": false}, "512": {"title": "Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning", "authors": ["Alexis R. Tudor, Yankai Zeng, Huaduo Wang, Joaquin Arias, Gopal Gupta"], "abstract": "arXiv:2506.12667v2 Announce Type: replace \nCurrent advances in AI and its applicability have highlighted the need to ensure its trustworthiness for legal, ethical, and even commercial reasons. Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning but hallucinate and their decisions cannot be explained or audited (crucial aspects for trustworthiness). On the other hand, rule-based reasoners, such as Cyc, are able to provide the chain of reasoning steps but are complex and use a large number of reasoners. We propose a middle ground using s(CASP), a goal-directed constraint-based answer set programming reasoner that employs a small number of mechanisms to emulate reliable and explainable human-style commonsense reasoning. In this paper, we explain how s(CASP) supports the 16 desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023), and two additional ones: inconsistency detection and the assumption of alternative worlds. To illustrate the feasibility and synergies of s(CASP), we present a range of diverse applications, including a conversational chatbot and a virtually embodied reasoner.", "categories": ["cs.AI", "cs.LO"], "abs_url": "https://arxiv.org/abs/2506.12667", "pdf_url": "https://arxiv.org/pdf/2506.12667.pdf", "is_interesting": false}, "513": {"title": "Don't throw the baby out with the bathwater: How and why deep learning for ARC", "authors": ["Jack Cole, Mohamed Osman"], "abstract": "arXiv:2506.14276v2 Announce Type: replace \nThe Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.14276", "pdf_url": "https://arxiv.org/pdf/2506.14276.pdf", "is_interesting": false}, "514": {"title": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale Orchestration", "authors": ["Yan Jiang, Hao Zhou, LiZhong GU, Ai Han, TianLong Li"], "abstract": "arXiv:2506.19500v2 Announce Type: replace \nLarge language models (LLMs) have recently demonstrated the ability to act as function call agents by invoking external tools, enabling them to solve tasks beyond their static knowledge. However, existing agents typically call tools step by step at a time without a global view of task structure. As tools depend on each other, this leads to error accumulation and limited scalability, particularly when scaling to thousands of tools. To address these limitations, we propose NaviAgent, a novel bilevel architecture that decouples task planning from tool execution through graph-based modeling of the tool ecosystem. At the task-planning level, the LLM-based agent decides whether to respond directly, clarify user intent, invoke a toolchain, or execute tool outputs, ensuring broad coverage of interaction scenarios independent of inter-tool complexity. At the execution level, a continuously evolving Tool World Navigation Model (TWNM) encodes structural and behavioral relations among tools, guiding the agent to generate scalable and robust invocation sequences. By incorporating feedback from real tool interactions, NaviAgent supports closed-loop optimization of planning and execution, moving beyond tool calling toward adaptive navigation of large-scale tool ecosystems. Experiments show that NaviAgent achieves the best task success rates across models and tasks, and integrating TWMN further boosts performance by up to 17 points on complex tasks, underscoring its key role in toolchain orchestration.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.19500", "pdf_url": "https://arxiv.org/pdf/2506.19500.pdf", "is_interesting": false}, "515": {"title": "HiRA: A Hierarchical Reasoning Framework for Decoupled Planning and Execution in Deep Search", "authors": ["Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou"], "abstract": "arXiv:2507.02652v2 Announce Type: replace \nComplex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.", "categories": ["cs.AI", "cs.CL", "cs.IR"], "abs_url": "https://arxiv.org/abs/2507.02652", "pdf_url": "https://arxiv.org/pdf/2507.02652.pdf", "is_interesting": false}, "516": {"title": "Red Teaming AI Red Teaming", "authors": ["Subhabrata Majumdar, Brian Pendleton, Abhishek Gupta"], "abstract": "arXiv:2507.05538v2 Announce Type: replace \nRed teaming has evolved from its origins in military applications to become a widely adopted methodology in cybersecurity and AI. In this paper, we take a critical look at the practice of AI red teaming. We argue that despite its current popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI. Current AI red teaming efforts focus predominantly on individual model vulnerabilities while overlooking the broader sociotechnical systems and emergent behaviors that arise from complex interactions between models, users, and environments. To address this deficiency, we propose a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming. Drawing on cybersecurity experience and systems theory, we further propose a set of six recommendations. In these, we emphasize that effective AI red teaming requires multifunctional teams that examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors.", "categories": ["cs.AI", "cs.CR", "cs.CY"], "abs_url": "https://arxiv.org/abs/2507.05538", "pdf_url": "https://arxiv.org/pdf/2507.05538.pdf", "is_interesting": false}, "517": {"title": "Why Isn't Relational Learning Taking Over the World?", "authors": ["David Poole"], "abstract": "arXiv:2507.13558v4 Announce Type: replace \nArtificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.", "categories": ["cs.AI", "cs.DB", "cs.LG"], "abs_url": "https://arxiv.org/abs/2507.13558", "pdf_url": "https://arxiv.org/pdf/2507.13558.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "518": {"title": "Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind", "authors": ["Myung Ho Kim"], "abstract": "arXiv:2507.16184v2 Announce Type: replace \nWe report a structural convergence among four influential theories of mind: Kahneman's dual-system theory, Friston's predictive processing, Minsky's society of mind, and Clark's extended mind, emerging unintentionally within a practical AI architecture known as Agentic Flow. Designed to address the limitations of large language models (LLMs), Agentic Flow comprises five interlocking modules: Retrieval, Cognition, Control, Memory, and Action, organized into a repeatable cognitive loop. Although originally inspired only by Minsky and Clark, subsequent analysis revealed that its structure echoes computational motifs from all four theories, suggesting that theoretical convergence can emerge naturally from implementation demands rather than deliberate synthesis. Controlled evaluations confirmed this: the structured agent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating robust constraint adherence and reproducible reasoning. We describe this convergence under a broader descriptive meta-architecture called PEACE, highlighting recurring design patterns such as predictive modeling, associative recall, and error-sensitive control. Later formalized as the Structured Cognitive Loop (SCL), this framework generalizes the same principles as a foundation for behavioral intelligence in LLM-based agents. Rather than claiming theoretical unification, this paper proposes that intelligent architectures may evolve toward shared structural patterns shaped by practical constraints. As a position paper, it aims to frame this convergence as an interpretive reflection rather than a finalized theory, inviting further theoretical and experimental dialogue. Agentic Flow, or equivalently the Structured Cognitive Loop, thus offers a glimpse of how a unified cognitive form can arise not from abstraction, but from the necessities of real-world reasoning.", "categories": ["cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2507.16184", "pdf_url": "https://arxiv.org/pdf/2507.16184.pdf", "is_interesting": false}, "519": {"title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "authors": ["Thassilo M. Schiepanski, Nicholas Pi\\\"el"], "abstract": "arXiv:2508.04412v2 Announce Type: replace \nFrontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation - referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) - within the same input token order of magnitude (1e3). Our best evaluated configurations - one token order above, but within the model's context window - outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.", "categories": ["cs.AI", "cs.CL", "cs.HC"], "abs_url": "https://arxiv.org/abs/2508.04412", "pdf_url": "https://arxiv.org/pdf/2508.04412.pdf", "is_interesting": false}, "520": {"title": "Artificially intelligent agents in the social and behavioral sciences: A history and outlook", "authors": ["Petter Holme, Milena Tsvetkova"], "abstract": "arXiv:2510.05743v2 Announce Type: replace \nWe review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.", "categories": ["cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2510.05743", "pdf_url": "https://arxiv.org/pdf/2510.05743.pdf", "is_interesting": false}, "521": {"title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction", "authors": ["Tian Xia, Tianrun Gao, Wenhao Deng, Long Wei, Xiaowei Qian, Yixian Jiang, Chenglei Yu, Tailin Wu"], "abstract": "arXiv:2510.16559v3 Announce Type: replace \nEngineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.16559", "pdf_url": "https://arxiv.org/pdf/2510.16559.pdf", "is_interesting": false}, "522": {"title": "RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs", "authors": ["Joe Meyer, Divyansha Lachi, Mahmoud Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski"], "abstract": "arXiv:2510.19954v3 Announce Type: replace \nRelational multi-table data is common in domains such as e-commerce, healthcare, and scientific research, and can be naturally represented as heterogeneous temporal graphs with multi-modal node attributes. Existing graph neural networks (GNNs) rely on schema-specific feature encoders, requiring separate modules for each node type and feature column, which hinders scalability and parameter sharing. We introduce RELATE (Relational Encoder for Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature encoder that can be used with any general purpose GNN. RELATE employs shared modality-specific encoders for categorical, numerical, textual, and temporal attributes, followed by a Perceiver-style cross-attention module that aggregates features into a fixed-size, permutation-invariant node representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark, where it achieves performance within 3% of schema-specific encoders while reducing parameter counts by up to 5x. This design supports varying schemas and enables multi-dataset pretraining for general-purpose GNNs, paving the way toward foundation models for relational graph data.", "categories": ["cs.AI", "cs.DB", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.19954", "pdf_url": "https://arxiv.org/pdf/2510.19954.pdf", "is_interesting": false}, "523": {"title": "Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms", "authors": ["Riccardo Guidotti, Martina Cinquini, Marta Marchiori Manerba, Mattia Setzu, Francesco Spinnato"], "abstract": "arXiv:2510.20621v2 Announce Type: replace \nInterpretable-by-design models are crucial for fostering trust, accountability, and safe adoption of automated decision-making models in real-world applications. In this paper we formalize the ground for the MIMOSA (Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties. We formally define here the supervised learning setting across diverse decision-making tasks and data types, including tabular data, time series, images, text, transactions, and trajectories. We characterize three major families of interpretable models: feature importance, rule, and instance based models. For each family, we analyze their interpretability dimensions, reasoning mechanisms, and complexity. Beyond interpretability, we formalize three critical ethical properties, namely causality, fairness, and privacy, providing formal definitions, evaluation metrics, and verification procedures for each. We then examine the inherent trade-offs between these properties and discuss how privacy requirements, fairness constraints, and causal reasoning can be embedded within interpretable pipelines. By evaluating ethical measures during model generation, this framework establishes the theoretical foundations for developing AI systems that are not only accurate and interpretable but also fair, privacy-preserving, and causally aware, i.e., trustworthy.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.20621", "pdf_url": "https://arxiv.org/pdf/2510.20621.pdf", "is_interesting": false}, "524": {"title": "A Survey of AI Scientists", "authors": ["Guiyao Tie, Pan Zhou, Lichao Sun"], "abstract": "arXiv:2510.23045v3 Announce Type: replace \nArtificial intelligence is undergoing a profound transition from a computational instrument to an autonomous originator of scientific knowledge. This emerging paradigm, the AI scientist, is architected to emulate the complete scientific workflow-from initial hypothesis generation to the final synthesis of publishable findings-thereby promising to fundamentally reshape the pace and scale of discovery. However, the rapid and unstructured proliferation of these systems has created a fragmented research landscape, obscuring overarching methodological principles and developmental trends. This survey provides a systematic and comprehensive synthesis of this domain by introducing a unified, six-stage methodological framework that deconstructs the end-to-end scientific process into: Literature Review, Idea Generation, Experimental Preparation, Experimental Execution, Scientific Writing, and Paper Generation. Through this analytical lens, we chart the field's evolution from early Foundational Modules (2022-2023) to integrated Closed-Loop Systems (2024), and finally to the current frontier of Scalability, Impact, and Human-AI Collaboration (2025-present). By rigorously synthesizing these developments, this survey not only clarifies the current state of autonomous science but also provides a critical roadmap for overcoming remaining challenges in robustness and governance, ultimately guiding the next generation of systems toward becoming trustworthy and indispensable partners in human scientific inquiry.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.23045", "pdf_url": "https://arxiv.org/pdf/2510.23045.pdf", "is_interesting": false}, "525": {"title": "Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives", "authors": ["Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada"], "abstract": "arXiv:2510.26606v2 Announce Type: replace \nNormative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.26606", "pdf_url": "https://arxiv.org/pdf/2510.26606.pdf", "is_interesting": false}, "526": {"title": "Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey", "authors": ["Qin Yang, Rui Liu"], "abstract": "arXiv:2306.09445v2 Announce Type: replace-cross \nAs a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer relationships among agents' interactions. Then, we survey existing literature in relevant fields to support it and propose several promising research directions along with some open problems deemed necessary for further investigations.", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.NE", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2306.09445", "pdf_url": "https://arxiv.org/pdf/2306.09445.pdf", "is_interesting": false}, "527": {"title": "Continual Vision-and-Language Navigation", "authors": ["Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang"], "abstract": "arXiv:2403.15049v3 Announce Type: replace-cross \nDeveloping Vision-and-Language Navigation (VLN) agents typically assumes a \\textit{train-once-deploy-once} strategy, which is unrealistic as deployed agents continually encounter novel environments. To address this, we propose the Continual Vision-and-Language Navigation (CVLN) paradigm, where agents learn and adapt incrementally across multiple \\textit{scene domains}. CVLN includes two setups: Initial-instruction based CVLN for instruction-following, and Dialogue-based CVLN for dialogue-guided navigation. We also introduce two simple yet effective baselines for sequential decision-making: Perplexity Replay (PerpR), which replays difficult episodes, and Episodic Self-Replay (ESR), which stores and revisits action logits during training. Experiments show that existing continual learning methods fall short for CVLN, while PerpR and ESR achieve better performance by efficiently utilizing replay memory.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2403.15049", "pdf_url": "https://arxiv.org/pdf/2403.15049.pdf", "is_interesting": false}, "528": {"title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher", "authors": ["Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao"], "abstract": "arXiv:2407.20183v2 Announce Type: replace-cross \nInformation seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2407.20183", "pdf_url": "https://arxiv.org/pdf/2407.20183.pdf", "is_interesting": false}, "529": {"title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories", "authors": ["Qinyun Wu, Chao Peng, Pengfei Gao, Ruida Hu, Haoyu Gan, Bo Jiang, Jinhe Tang, Zhiwen Deng, Zhanming Guan, Cuiyun Gao, Xia Liu, Ping Yang"], "abstract": "arXiv:2408.03519v2 Announce Type: replace-cross \nWith the growing reliance on automated code completion tools in software development, the need for comprehensive evaluation benchmarks has become critical. Existing benchmarks focus more on code completion in function and class level by providing text descriptions to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes existing evaluation benchmarks poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 10 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report variance in model performance in real-world scenarios. The deployment of RepoMasterEval also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2408.03519", "pdf_url": "https://arxiv.org/pdf/2408.03519.pdf", "is_interesting": false}, "530": {"title": "Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI", "authors": ["Elisa Nguyen, Johannes Bertram, Evgenii Kortukov, Jean Y. Song, Seong Joon Oh"], "abstract": "arXiv:2409.16978v2 Announce Type: replace-cross \nExplainable AI (XAI) aims to make AI systems more transparent, yet many practices emphasise mathematical rigour over practical user needs. We propose an alternative to this model-centric approach by following a design thinking process for the emerging XAI field of training data attribution (TDA), which risks repeating solutionist patterns seen in other subfields. However, because TDA is in its early stages, there is a valuable opportunity to shape its direction through user-centred practices. We engage directly with machine learning developers via a needfinding interview study (N=6) and a scenario-based interactive user study (N=31) to ground explanations in real workflows. Our exploration of the TDA design space reveals novel tasks for data-centric explanations useful to developers, such as grouping training samples behind specific model behaviours or identifying undersampled data. We invite the TDA, XAI, and HCI communities to engage with these tasks to strengthen their research's practical relevance and human impact.", "categories": ["cs.HC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2409.16978", "pdf_url": "https://arxiv.org/pdf/2409.16978.pdf", "is_interesting": false}, "531": {"title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks", "authors": ["Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang"], "abstract": "arXiv:2410.05102v3 Announce Type: replace-cross \nDirect alignment algorithms have proven an effective step for aligning language models to human-desired behaviors. Current variants of the Direct Preference Optimization objective have focused on a strict setting where all tokens are contributing signals of KL divergence and rewards to the loss function. However, human preference is not affected equally by each word in a sequence but is often dependent on specific words or phrases, e.g. existence of toxic terms leads to non-preferred responses. Based on this observation, we argue that not all tokens should be weighted equally during PO and propose a flexible objective termed SparsePO, that aims to automatically learn to weight the KL divergence and reward corresponding to each token during PO training. We propose two different variants of weight-masks that can either be derived from the reference model itself or learned on the fly. Notably, our method induces sparsity in the learned masks, allowing the model to learn how to best balance reward and KL divergence contributions at the token level, learning an optimal level of mask sparsity. Extensive experiments illustrate the effectiveness of our approach at aligning to preference proxies, including sentiment control, helpfulness and harmlessness, and summary quality. Our method obtains +10% and +3% win rate points in summarization and dialogue scenarios, respectively, without compromising model reasoning or the relevancy and faithfulness of the summary response.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2410.05102", "pdf_url": "https://arxiv.org/pdf/2410.05102.pdf", "is_interesting": false}, "532": {"title": "A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification", "authors": ["Flavio Corradini, Flavio Gerosa, Marco Gori, Carlo Lucheroni, Marco Piangerelli, Martina Zannotti"], "abstract": "arXiv:2410.22377v3 Announce Type: replace-cross \nIn recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture, at once, dependencies among variables and across time points. The objective of this systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and 366 papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive review of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in their studies. To the best of our knowledge, this is the first and broadest systematic literature review presenting a detailed comparison of results from current spatio-temporal GNN models applied to different domains. In its final part, this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability. This paper is complemented by a GitHub repository at https://github.com/FlaGer99/SLR-Spatio-Temporal-GNN.git providing additional interactive tools to further explore the presented findings.", "categories": ["cs.LG", "cs.AI", "physics.data-an"], "abs_url": "https://arxiv.org/abs/2410.22377", "pdf_url": "https://arxiv.org/pdf/2410.22377.pdf", "is_interesting": false}, "533": {"title": "Representative Social Choice: From Learning Theory to AI Alignment", "authors": ["Tianyi Qiu"], "abstract": "arXiv:2410.23953v4 Announce Type: replace-cross \nSocial choice theory is the study of preference aggregation across a population, used both in mechanism design for human agents and in the democratic alignment of language models. In this study, we propose the representative social choice framework for the modeling of democratic representation in collective decisions, where the number of issues and individuals are too large for mechanisms to consider all preferences directly. These scenarios are widespread in real-world decision-making processes, such as jury trials, legislation, corporate governance, and, more recently, language model alignment. In representative social choice, the population is represented by a finite sample of individual-issue pairs based on which social choice decisions are made. We show that many of the deepest questions in representative social choice can be formulated as statistical learning problems, and prove the generalization properties of social choice mechanisms using the theory of machine learning. We further formulate axioms for representative social choice, and prove Arrow-like impossibility theorems with new combinatorial tools of analysis. Our framework introduces the representative approach to social choice, opening up research directions at the intersection of social choice, learning theory, and AI alignment.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.GT"], "abs_url": "https://arxiv.org/abs/2410.23953", "pdf_url": "https://arxiv.org/pdf/2410.23953.pdf", "is_interesting": false}, "534": {"title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models", "authors": ["Nam V. Nguyen, Thong T. Doan, Luong Tran, Van Nguyen, Quang Pham"], "abstract": "arXiv:2411.00918v2 Announce Type: replace-cross \nMixture of experts (MoE) architectures have become a cornerstone for scaling up and are a key component in most large language models such as GPT-OSS, DeepSeek-V3, Llama-4, and Gemini-2.5. However, systematic research on MoE remains severely constrained by the prohibitive computational costs of training and evaluation, restricting large-scale studies accessible to most researchers. We introduce LibMoE, a unified framework for reproducible, efficient, and extensible MoE research that supports both pretraining and sparse-upcycling regimes. Beyond unified implementations, the framework provides transparent analytical tools for probing routing and expert dynamics. Leveraging this foundation, we conduct a comprehensive analysis along three dimensions: (i) routing dynamics, covering expert selection patterns, routing stability and optimality, and how routing entropy reveals task specialization and expert diversity; (ii) the effect of lightweight initialization on load balancing, demonstrating how subtle changes in router initialization shape early expert utilization; and (iii) training regime differences, revealing how sparse upcycling and full pretraining exhibit distinct routing patterns and stability profiles. By lowering the barrier to entry and standardizing evaluation, along with our comprehensive analysis, LibMoE broadens access to MoE research and establishes a reliable benchmark to guide future innovations. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2411.00918", "pdf_url": "https://arxiv.org/pdf/2411.00918.pdf", "is_interesting": false}, "535": {"title": "Robust Offline Reinforcement Learning with Linearly Structured f-Divergence Regularization", "authors": ["Cheng Tang, Zhishuai Liu, Pan Xu"], "abstract": "arXiv:2411.18612v2 Announce Type: replace-cross \nThe Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abs_url": "https://arxiv.org/abs/2411.18612", "pdf_url": "https://arxiv.org/pdf/2411.18612.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0", "reason": "The paper focuses on reinforcement learning for robust policy learning in dynamic environments with a specific focus on offline reinforcement learning and regularization techniques, which are not directly or indirectly related to autonomous driving systems or tasks such as perception, prediction, or decision making in autonomous vehicles."}}, "536": {"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "authors": ["Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen"], "abstract": "arXiv:2412.13178v5 Announce Type: replace-cross \nWith the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs' safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.", "categories": ["cs.CR", "cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2412.13178", "pdf_url": "https://arxiv.org/pdf/2412.13178.pdf", "is_interesting": false}, "537": {"title": "Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude", "authors": ["Wentao Xu, Yile Yan, Yuqi Zhu"], "abstract": "arXiv:2501.10484v5 Announce Type: replace-cross \nRecent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2501.10484", "pdf_url": "https://arxiv.org/pdf/2501.10484.pdf", "is_interesting": false}, "538": {"title": "Multilingual State Space Models for Structured Question Answering in Indic Languages", "authors": ["Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha"], "abstract": "arXiv:2502.01673v3 Announce Type: replace-cross \nThe diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.01673", "pdf_url": "https://arxiv.org/pdf/2502.01673.pdf", "is_interesting": false}, "539": {"title": "On-device Computation of Single-lead ECG Parameters for Real-time Remote Cardiac Health Assessment: A Real-world Validation Study", "authors": ["Sumei Fan, Deyun Zhang, Yue Wang, Shijia Geng, Kun Lu, Meng Sang, Weilun Xu, Haixue Wang, Qinghao Zhao, Chuandong Cheng, Peng Wang, Shenda Hong"], "abstract": "arXiv:2502.17499v3 Announce Type: replace-cross \nAccurate, continuous out-of-hospital electrocardiogram (ECG) parameter measurement is vital for real-time cardiac health monitoring and telemedicine. On-device computation of single-lead ECG parameters enables timely assessment without reliance on centralized data processing, advancing personalized, ubiquitous cardiac care-yet comprehensive validation across heterogeneous real-world populations remains limited. This study validated the on-device algorithm FeatureDB (https://github.com/PKUDigitalHealth/FeatureDB) using two datasets: HeartVoice-ECG-lite (369 participants with single-lead ECGs annotated by two physicians) and PTB-XL/PTB-XL+ (21,354 patients with 12-lead ECGs and physicians' diagnostic annotations). FeatureDB computed PR, QT, and QTc intervals, with accuracy evaluated against physician annotations via mean absolute error (MAE), correlation analysis, and Bland-Altman analysis. Diagnostic performance for first-degree atrioventricular block (AVBI, PR-based) and long QT syndrome (LQT, QTc-based) was benchmarked against commercial 12-lead systems (12SL, Uni-G) and open-source algorithm Deli, using AUC, accuracy, sensitivity, and specificity. Results showed high concordance with expert annotations (Pearson correlations: 0.836-0.960), MAEs matching inter-observer variability, and minimal bias. AVBI AUC reached 0.787 (12SL: 0.859; Uni-G: 0.812; Deli: 0.501); LQT AUC was 0.684 (12SL: 0.716; Uni-G: 0.605; Deli: 0.569)-comparable to commercial tools and superior to open-source alternatives. FeatureDB delivers physician-level parameter accuracy and commercial-grade abnormality detection via single-lead devices, supporting scalable telemedicine, decentralized cardiac screening, and continuous monitoring in community and outpatient settings.", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "abs_url": "https://arxiv.org/abs/2502.17499", "pdf_url": "https://arxiv.org/pdf/2502.17499.pdf", "is_interesting": false}, "540": {"title": "Training a Generally Curious Agent", "authors": ["Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J Zico Kolter, Jeff Schneider, Ruslan Salakhutdinov"], "abstract": "arXiv:2502.17543v4 Announce Type: replace-cross \nEfficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present Paprika, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2502.17543", "pdf_url": "https://arxiv.org/pdf/2502.17543.pdf", "is_interesting": false}, "541": {"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "authors": ["Zhewei Kang, Xuandong Zhao, Dawn Song"], "abstract": "arXiv:2502.18581v2 Announce Type: replace-cross \nBest-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.18581", "pdf_url": "https://arxiv.org/pdf/2502.18581.pdf", "is_interesting": false}, "542": {"title": "Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing", "authors": ["Xuyang Zhong, Yixiao Huang, Chen Liu"], "abstract": "arXiv:2502.21041v2 Announce Type: replace-cross \nThis paper studies fast adversarial training against sparse adversarial perturbations bounded by $l_0$ norm. We demonstrate the challenges of employing $1$-step attacks on $l_0$ bounded perturbations for fast adversarial training, including degraded performance and the occurrence of catastrophic overfitting (CO). We highlight that CO in $l_0$ adversarial training is caused by sub-optimal perturbation locations of $1$-step attack. Theoretical and empirical analyses reveal that the loss landscape of $l_0$ adversarial training is more craggy compared to its $l_\\infty$, $l_2$ and $l_1$ counterparts. Moreover, we corroborate that the craggy loss landscape can aggravate CO. To address these issues, we propose Fast-LS-$l_0$ that incorporates soft labels and the trade-off loss function to smooth the adversarial loss landscape. Extensive experiments demonstrate our method can overcome the challenge of catastrophic overfitting, achieve state-of-the-art performance, and narrow down the performance gap between $1$-step and multi-step adversarial training against sparse attacks.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.21041", "pdf_url": "https://arxiv.org/pdf/2502.21041.pdf", "is_interesting": false}, "543": {"title": "More of the Same: Persistent Representational Harms Under Increased Representation", "authors": ["Jennifer Mickel, Maria De-Arteaga, Leqi Liu, Kevin Tian"], "abstract": "arXiv:2503.00333v3 Announce Type: replace-cross \nTo recognize and mitigate the harms of generative AI systems, it is crucial to consider whether and how different societal groups are represented by these systems. A critical gap emerges when naively measuring or improving who is represented, as this does not consider how people are represented. In this work, we develop GAS(P), an evaluation methodology for surfacing distribution-level group representational biases in generated text, tackling the setting where groups are unprompted (i.e., groups are not specified in the input to generative systems). We apply this novel methodology to investigate gendered representations in occupations across state-of-the-art large language models. We show that, even though the gender distribution when models are prompted to generate biographies leads to a large representation of women, even representational biases persist in how different genders are represented. Our evaluation methodology reveals that there are statistically significant distribution-level differences in the word choice used to describe biographies and personas of different genders across occupations, and we show that many of these differences are associated with representational harms and stereotypes. Our empirical findings caution that naively increasing (unprompted) representation may inadvertently proliferate representational biases, and our proposed evaluation methodology enables systematic and rigorous measurement of the problem.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.00333", "pdf_url": "https://arxiv.org/pdf/2503.00333.pdf", "is_interesting": false}, "544": {"title": "(How) Do Language Models Track State?", "authors": ["Belinda Z. Li, Zifan Carl Guo, Jacob Andreas"], "abstract": "arXiv:2503.02854v3 Announce Type: replace-cross \nTransformer language models (LMs) exhibit behaviors -- from storytelling to code generation -- that seem to require tracking the unobserved state of an evolving world. How do they do this? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the \"associative scan\" construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, and then refines this with an associative scan. LMs that learn the former algorithm tend to generalize better and converge faster, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pre-trained or fine-tuned, can learn to implement efficient and interpretable state-tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.02854", "pdf_url": "https://arxiv.org/pdf/2503.02854.pdf", "is_interesting": false}, "545": {"title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models", "authors": ["Palakorn Achananuparp, Ee-Peng Lim, Yao Lu"], "abstract": "arXiv:2503.12989v3 Announce Type: replace-cross \nAutomatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.", "categories": ["cs.CL", "cs.AI", "cs.SI"], "abs_url": "https://arxiv.org/abs/2503.12989", "pdf_url": "https://arxiv.org/pdf/2503.12989.pdf", "is_interesting": false}, "546": {"title": "Modelling Emotions in Face-to-Face Setting: The Interplay of Eye-Tracking, Personality, and Temporal Dynamics", "authors": ["Meisam Jamshidi Seikavandi, Jostein Fimland, Maria Barrett, Paolo Burelli"], "abstract": "arXiv:2503.16532v2 Announce Type: replace-cross \nAccurate emotion recognition is pivotal for nuanced and engaging human-computer interactions, yet remains difficult to achieve, especially in dynamic, conversation-like settings. In this study, we showcase how integrating eye-tracking data, temporal dynamics, and personality traits can substantially enhance the detection of both perceived and felt emotions. Seventy-three participants viewed short, speech-containing videos from the CREMA-D dataset, while being recorded for eye-tracking signals (pupil size, fixation patterns), Big Five personality assessments, and self-reported emotional states. Our neural network models combined these diverse inputs including stimulus emotion labels for contextual cues and yielded marked performance gains compared to the state-of-the-art. Specifically, perceived valence predictions reached a macro F1-score of 0.76, and models incorporating personality traits and stimulus information demonstrated significant improvements in felt emotion accuracy. These results highlight the benefit of unifying physiological, individual and contextual factors to address the subjectivity and complexity of emotional expression. Beyond validating the role of user-specific data in capturing subtle internal states, our findings inform the design of future affective computing and human-agent systems, paving the way for more adaptive and cross-individual emotional intelligence in real-world interactions.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.16532", "pdf_url": "https://arxiv.org/pdf/2503.16532.pdf", "is_interesting": false}, "547": {"title": "Face Spoofing Detection using Deep Learning", "authors": ["Najeebullah, Maaz Salman, Zar Nawab Khan Swati"], "abstract": "arXiv:2503.19223v2 Announce Type: replace-cross \nDigital image spoofing has emerged as a significant security threat in biometric authentication systems, particularly those relying on facial recognition. This study evaluates the performance of three vision based models, MobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in image classification, utilizing a dataset of 150,986 images divided into training , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof detection is critical for enhancing the security of image recognition systems, and this research compares the models effectiveness through accuracy, precision, recall, and F1 score metrics. Results reveal that MobileNetV2 outperforms other architectures on the test dataset, achieving an accuracy of 91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared to ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation dataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17% accuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during training and superior generalization to unseen data, despite both models showing signs of overfitting. These findings highlight MobileNetV2 balanced performance and robustness, making it the preferred choice for spoof detection applications where reliability on new data is essential. The study underscores the importance of model selection in security sensitive contexts and suggests MobileNetV2 as a practical solution for real world deployment.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.19223", "pdf_url": "https://arxiv.org/pdf/2503.19223.pdf", "is_interesting": false}, "548": {"title": "On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations", "authors": ["Rajdeep Singh Hundal, Yan Xiao, Xiaochun Cao, Jin Song Dong, Manuel Rigger"], "abstract": "arXiv:2503.22575v2 Announce Type: replace-cross \nDeep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.22575", "pdf_url": "https://arxiv.org/pdf/2503.22575.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on deep reinforcement learning (DRL) and the inconsistencies in algorithm implementations, with applications in areas like driving simulators and video games. However, it does not directly address or relate to autonomous driving tasks such as vehicle perception, prediction, control, or other core AD-related domains."}}, "549": {"title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models", "authors": ["Wenkang Ji, Huaben Chen, Mingyang Chen, Guobin Zhu, Lufeng Xu, Roderich Gro{\\ss}, Rui Zhou, Ming Cao, Shiyu Zhao"], "abstract": "arXiv:2503.23875v2 Announce Type: replace-cross \nThe development of control policies for multi-robot systems traditionally follows a complex and labor-intensive process, often lacking the flexibility to adapt to dynamic tasks. This has motivated research on methods to automatically create control policies. However, these methods require iterative processes of manually crafting and refining objective functions, thereby prolonging the development cycle. This work introduces \\textit{GenSwarm}, an end-to-end system that leverages large language models to automatically generate and deploy control policies for multi-robot tasks based on simple user instructions in natural language. As a multi-language-agent system, GenSwarm achieves zero-shot learning, enabling rapid adaptation to altered or unseen tasks. The white-box nature of the code policies ensures strong reproducibility and interpretability. With its scalable software and hardware architectures, GenSwarm supports efficient policy deployment on both simulated and real-world multi-robot systems, realizing an instruction-to-execution end-to-end functionality that could prove valuable for robotics specialists and non-specialists alike.The code of the proposed GenSwarm system is available online: https://github.com/WindyLab/GenSwarm.", "categories": ["cs.RO", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2503.23875", "pdf_url": "https://arxiv.org/pdf/2503.23875.pdf", "is_interesting": false}, "550": {"title": "RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm", "authors": ["Yongyi Yang, Jianyang Gao, Wei Hu"], "abstract": "arXiv:2504.03717v2 Announce Type: replace-cross \nPost-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. In this paper, we propose RaanA, a unified PTQ framework that overcomes these challenges by introducing two novel components: 1) RaBitQ-H, a variant of a randomized vector quantization method RaBitQ, designed for fast, accurate, and highly efficient quantization; and 2) AllocateBits, an algorithm that optimally allocates bit-widths across layers based on their quantization sensitivity. RaanA achieves competitive performance with state-of-the-art quantization methods while being extremely fast, requiring minimal calibration data, and enabling flexible bit allocation. Extensive experiments demonstrate RaanA's efficacy in balancing efficiency and accuracy. The code is publicly available at https://github.com/FFTYYY/RaanA .", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.03717", "pdf_url": "https://arxiv.org/pdf/2504.03717.pdf", "is_interesting": false}, "551": {"title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers", "authors": ["Xuyang Zhong, Haochen Luo, Chen Liu"], "abstract": "arXiv:2504.15827v2 Announce Type: replace-cross \nExisting machine unlearning (MU) approaches exhibit significant sensitivity to hyperparameters, requiring meticulous tuning that limits practical deployment. In this work, we first empirically demonstrate the instability and suboptimal performance of existing popular MU methods when deployed in different scenarios. To address this issue, we propose Dual Optimizer (DualOptim), which incorporates adaptive learning rate and decoupled momentum factors. Empirical and theoretical evidence demonstrates that DualOptim contributes to effective and stable unlearning. Through extensive experiments, we show that DualOptim can significantly boost MU efficacy and stability across diverse tasks, including image classification, image generation, and large language models, making it a versatile approach to empower existing MU algorithms.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.15827", "pdf_url": "https://arxiv.org/pdf/2504.15827.pdf", "is_interesting": false}, "552": {"title": "AVA: Towards Agentic Video Analytics with Vision Language Models", "authors": ["Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu"], "abstract": "arXiv:2505.00254v5 Announce Type: replace-cross \nAI-driven video analytics has become increasingly important across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Vision Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively-significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%. The source code of AVA is available at https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at https://huggingface.co/datasets/iesc/Ava-100.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.00254", "pdf_url": "https://arxiv.org/pdf/2505.00254.pdf", "is_interesting": false}, "553": {"title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "authors": ["Sungbok Shin, Hyeon Jeon, Sanghyun Hong, Niklas Elmqvist"], "abstract": "arXiv:2505.00455v4 Announce Type: replace-cross \nEffective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. Motivated by growing demands to surface tacit knowledge, we present the Data Therapist, a web-based system that helps domain experts externalize such implicit knowledge through a mixed-initiative process combining iterative Q&amp;A with interactive annotation. Powered by a large language model, the system automatically analyzes user-supplied datasets, prompts users with targeted questions, and supports annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. A qualitative study with expert pairs from Accounting, Political Science, and Computer Security revealed recurring patterns in how expert reason about their data and highlighted opportunities for AI support to enhance visualization design.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.00455", "pdf_url": "https://arxiv.org/pdf/2505.00455.pdf", "is_interesting": false}, "554": {"title": "Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness", "authors": ["H\\'eber H. Arcolezi, Mina Alishahi, Adda-Akram Bendoukha, Nesrine Kaaniche"], "abstract": "arXiv:2505.07985v2 Announce Type: replace-cross \nMachine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to fourfold. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "abs_url": "https://arxiv.org/abs/2505.07985", "pdf_url": "https://arxiv.org/pdf/2505.07985.pdf", "is_interesting": false}, "555": {"title": "Variational Visual Question Answering for Uncertainty-Aware Selective Prediction", "authors": ["Tobias Jan Wieczorek, Nathalie Daun, Mohammad Emtiyaz Khan, Marcus Rohrbach"], "abstract": "arXiv:2505.09591v2 Announce Type: replace-cross \nDespite remarkable progress in recent years, vision language models (VLMs) remain prone to overconfidence and hallucinations on tasks such as Visual Question Answering (VQA) and Visual Reasoning. Bayesian methods can potentially improve reliability by helping models selectively predict, that is, models respond only when they are sufficiently confident. Unfortunately, Bayesian methods are often assumed to be costly and ineffective for large models, and so far there exists little evidence to show otherwise, especially for multimodal applications. Here, we show the effectiveness and competitive edge of variational Bayes for selective prediction in VQA for the first time. We build on recent advances in variational methods for deep learning and propose an extension called \"Variational VQA\". This method improves calibration and yields significant gains for selective prediction on VQA and Visual Reasoning, particularly when the error tolerance is low ($\\leq 1\\%$). Often, just one posterior sample can yield more reliable answers than those obtained by models trained with AdamW. In addition, we propose a new risk-averse selector that outperforms standard sample averaging by considering the variance of predictions. Overall, we present compelling evidence that variational learning is a viable option to make large VLMs safer and more trustworthy.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.09591", "pdf_url": "https://arxiv.org/pdf/2505.09591.pdf", "is_interesting": false}, "556": {"title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "authors": ["Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen"], "abstract": "arXiv:2505.11594v2 Announce Type: replace-cross \nThe efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "abs_url": "https://arxiv.org/abs/2505.11594", "pdf_url": "https://arxiv.org/pdf/2505.11594.pdf", "is_interesting": false}, "557": {"title": "Scaling Diffusion Transformers Efficiently via $\\mu$P", "authors": ["Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li"], "abstract": "arXiv:2505.15270v3 Announce Type: replace-cross \nDiffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\\mu$P of mainstream diffusion Transformers, including U-ViT, DiT, PixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\\mu$P on text-to-image generation by scaling PixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\\mu$P as a principled and efficient framework for scaling diffusion Transformers.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2505.15270", "pdf_url": "https://arxiv.org/pdf/2505.15270.pdf", "is_interesting": false}, "558": {"title": "R$^2$ec: Towards Large Recommender Models with Reasoning", "authors": ["Runyang You, Yongqi Li, Xinyu Lin, Xin Zhang, Wenjie Wang, Wenjie Li, Liqiang Nie"], "abstract": "arXiv:2505.16994v3 Announce Type: replace-cross \nLarge recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. In this work, we propose R$^2$ec, a unified large recommender model with intrinsic reasoning capability. R$^2$ec introduces a dual-head architecture that supports both reasoning chain generation and efficient item prediction in a single model, significantly reducing inference latency. To overcome the lack of annotated reasoning data, we design RecPO, a reinforcement learning framework that optimizes reasoning and recommendation jointly with a novel fused reward mechanism. Extensive experiments on three datasets demonstrate that R$^2$ec outperforms traditional, LLM-based, and reasoning-augmented recommender baselines, while further analyses validate its competitive efficiency among conventional LLM-based recommender baselines and strong adaptability to diverse recommendation scenarios. Code and checkpoints available at https://github.com/YRYangang/RRec.", "categories": ["cs.IR", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2505.16994", "pdf_url": "https://arxiv.org/pdf/2505.16994.pdf", "is_interesting": false}, "559": {"title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics", "authors": ["Ata Akbari Asanjan, Olivia Alexander, Tom Berg, Stephen Peng, Jad Makki, Clara Zhang, Matt Yang, Disha Shidham, Srija Chakraborty, William Bender, Cara Crawford, Arun Ravindran, Olivier Raiman, David Potere, David Bell"], "abstract": "arXiv:2505.18179v2 Announce Type: replace-cross \nWe introduce GAIA (Geospatial Artificial Intelligence for Atmospheres), a hybrid self-supervised geospatial foundation model that fuses Masked Autoencoders (MAE) with self-distillation with no labels (DINO) to generate semantically rich representations from global geostationary satellite imagery. Pre-trained on 15 years of globally-merged infrared observations (2001-2015), GAIA learns disentangled representations that capture atmospheric dynamics rather than trivial diurnal patterns, as evidenced by distributed principal component structure and temporal coherence analysis. We demonstrate robust reconstruction capabilities across varying data availability (30-95% masking), achieving superior gap-filling performance on real missing data patterns. When transferred to downstream tasks, GAIA consistently outperforms an MAE-only baseline: improving atmospheric river segmentation (F1: 0.58 vs 0.52), enhancing tropical cyclone detection (storm-level recall: 81% vs 75%, early detection: 29% vs 17%), and maintaining competitive precipitation estimation performance. Analysis reveals that GAIA's hybrid objectives encourage learning of spatially coherent, object-centric features distributed across multiple principal components rather than concentrated representations focused on reconstruction. This work demonstrates that combining complementary self-supervised objectives yields more transferable representations for diverse atmospheric modeling tasks. Model weights and code are available at: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.18179", "pdf_url": "https://arxiv.org/pdf/2505.18179.pdf", "is_interesting": false}, "560": {"title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "authors": ["Boyi Wei, Benedikt Stroebl, Jiacen Xu, Joie Zhang, Zhou Li, Peter Henderson"], "abstract": "arXiv:2505.18384v5 Announce Type: replace-cross \nFoundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.18384", "pdf_url": "https://arxiv.org/pdf/2505.18384.pdf", "is_interesting": false}, "561": {"title": "Rethinking Metrics and Benchmarks of Video Anomaly Detection", "authors": ["Zihao Liu, Xiaoyu Wu, Wenna Li, Linlin Yang, Shengjin Wang"], "abstract": "arXiv:2505.19022v2 Announce Type: replace-cross \nVideo Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.19022", "pdf_url": "https://arxiv.org/pdf/2505.19022.pdf", "is_interesting": false}, "562": {"title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA", "authors": ["Minrui Luo, Fuhang Kuang, Yu Wang, Zirui Liu, Tianxing He"], "abstract": "arXiv:2505.23724v3 Announce Type: replace-cross \nParameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.23724", "pdf_url": "https://arxiv.org/pdf/2505.23724.pdf", "is_interesting": false}, "563": {"title": "Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting", "authors": ["ChengAo Shen, Wenchao Yu, Ziming Zhao, Dongjin Song, Wei Cheng, Haifeng Chen, Jingchao Ni"], "abstract": "arXiv:2505.24003v2 Announce Type: replace-cross \nTime series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, the state-of-the-art (SOTA) LVM-based forecaster poses an inductive bias towards \"forecasting periods\". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast-residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 SOTA models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets. The code for this paper is available at: https://github.com/D2I-Group/dmmv.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.24003", "pdf_url": "https://arxiv.org/pdf/2505.24003.pdf", "is_interesting": false}, "564": {"title": "Artificial Empathy: AI based Mental Health", "authors": ["Aditya Naik, Jovi Thomas, Teja Sree Mandava, Himavanth Reddy Vemula"], "abstract": "arXiv:2506.00081v2 Announce Type: replace-cross \nMany people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a \"Five minute therapist\" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.", "categories": ["q-bio.OT", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2506.00081", "pdf_url": "https://arxiv.org/pdf/2506.00081.pdf", "is_interesting": false}, "565": {"title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding", "authors": ["Daniel Israel, Guy Van den Broeck, Aditya Grover"], "abstract": "arXiv:2506.00413v2 Announce Type: replace-cross \nThe generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "abs_url": "https://arxiv.org/abs/2506.00413", "pdf_url": "https://arxiv.org/pdf/2506.00413.pdf", "is_interesting": false}, "566": {"title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation", "authors": ["Kai Lion, Liang Zhang, Bingcong Li, Niao He"], "abstract": "arXiv:2506.03133v2 Announce Type: replace-cross \nWe show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.", "categories": ["cs.LG", "cs.AI", "eess.SP", "math.OC"], "abs_url": "https://arxiv.org/abs/2506.03133", "pdf_url": "https://arxiv.org/pdf/2506.03133.pdf", "is_interesting": false}, "567": {"title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare", "authors": ["Pengfei Hu, Xiaoxue Han, Fei Wang, Yue Ning"], "abstract": "arXiv:2506.06977v2 Announce Type: replace-cross \nHealthcare providers often divide patient populations into cohorts based on shared clinical factors, such as medical history, to deliver personalized healthcare services. This idea has also been adopted in clinical prediction models, where it presents a vital challenge: capturing both global and cohort-specific patterns while enabling model generalization to unseen domains. Addressing this challenge falls under the scope of domain generalization (DG). However, conventional DG approaches often struggle in clinical settings due to the absence of explicit domain labels and the inherent gap in medical knowledge. To address this, we propose UdonCare, a hierarchy-guided method that iteratively divides patients into latent domains and decomposes domain-invariant (label) information from patient data. Our method identifies patient domains by pruning medical ontologies (e.g. ICD-9-CM hierarchy). On two public datasets, MIMIC-III and MIMIC-IV, UdonCare shows superiority over eight baselines across four clinical prediction tasks with substantial domain gaps, highlighting the untapped potential of medical knowledge in guiding clinical domain generalization problems.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.06977", "pdf_url": "https://arxiv.org/pdf/2506.06977.pdf", "is_interesting": false}, "568": {"title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "authors": ["Jinyoung Park, Jeehye Na, Jinyoung Kim, Hyunwoo J. Kim"], "abstract": "arXiv:2506.07464v4 Announce Type: replace-cross \nRecent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.07464", "pdf_url": "https://arxiv.org/pdf/2506.07464.pdf", "is_interesting": false}, "569": {"title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix", "authors": ["Shuo Jiang, Min Xie, Jianxi Luo"], "abstract": "arXiv:2506.09749v2 Announce Type: replace-cross \nIn complex engineering systems, the dependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that rely solely on mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) to address such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM sequencing-a common CO problem. Experiments on various DSM cases demonstrate that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.", "categories": ["cs.CE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.09749", "pdf_url": "https://arxiv.org/pdf/2506.09749.pdf", "is_interesting": false}, "570": {"title": "Graph Diffusion that can Insert and Delete", "authors": ["Matteo Ninniri, Marco Podda, Davide Bacciu"], "abstract": "arXiv:2506.15725v2 Announce Type: replace-cross \nGenerative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.15725", "pdf_url": "https://arxiv.org/pdf/2506.15725.pdf", "is_interesting": false}, "571": {"title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "authors": ["Tien Huu Do, Antoine Masquelier, Nae Eoun Lee, Jonathan Crowther"], "abstract": "arXiv:2507.23607v2 Announce Type: replace-cross \nClinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2507.23607", "pdf_url": "https://arxiv.org/pdf/2507.23607.pdf", "is_interesting": false}, "572": {"title": "LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring", "authors": ["Chloe Li, Mary Phuong, Noah Y. Siegel"], "abstract": "arXiv:2508.00943v2 Announce Type: replace-cross \nTrustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.00943", "pdf_url": "https://arxiv.org/pdf/2508.00943.pdf", "is_interesting": false}, "573": {"title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "authors": ["Zihao Sheng, Zilin Huang, Yansong Qu, Jiancong Chen, Yuhao Luo, Yen-Jung Chen, Yue Leng, Sikai Chen"], "abstract": "arXiv:2508.06763v3 Announce Type: replace-cross \nMultimodal large language models (MLLMs) have achieved remarkable progress across a range of vision-language tasks and demonstrate strong potential for traffic accident understanding. However, existing MLLMs in this domain primarily focus on coarse-grained image-level or video-level comprehension and often struggle to handle fine-grained visual details or localized scene components, limiting their applicability in complex accident scenarios. To address these limitations, we propose SafePLUG, a novel framework that empowers MLLMs with both Pixel-Level Understanding and temporal Grounding for comprehensive traffic accident analysis. SafePLUG supports both arbitrary-shaped visual prompts for region-aware question answering and pixel-level segmentation based on language instructions, while also enabling the recognition of temporally anchored events in traffic accident scenarios. To advance the development of MLLMs for traffic accident understanding, we curate a new dataset containing multimodal question-answer pairs centered on diverse accident scenarios, with detailed pixel-level annotations and temporal event boundaries. Experimental results show that SafePLUG achieves strong performance on multiple tasks, including region-based question answering, pixel-level segmentation, temporal event localization, and accident event understanding. These capabilities lay a foundation for fine-grained understanding of complex traffic scenes, with the potential to improve driving safety and enhance situational awareness in smart transportation systems. The code, dataset, and model checkpoints will be made publicly available at: https://zihaosheng.github.io/SafePLUG", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.06763", "pdf_url": "https://arxiv.org/pdf/2508.06763.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u9a7e\u9a76\u573a\u666f\u7406\u89e3 / \u591a\u6a21\u6001\u878d\u5408", "reason": "The paper proposes SafePLUG, a framework for traffic accident understanding, which includes pixel-level segmentation and temporal event localization. These capabilities are relevant to autonomous driving for improving safety and situational awareness in complex traffic environments, particularly through multimodal integration of vision and temporal data."}}, "574": {"title": "Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms", "authors": ["Hangcheng Zhao, Ron Berman"], "abstract": "arXiv:2508.08325v2 Announce Type: replace-cross \nWhen online sellers use AI learning algorithms to automatically compete on e-commerce platforms, there is concern that they will learn to coordinate on higher than competitive prices. However, this concern was primarily raised in single-dimension price competition. We investigate whether this prediction holds when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. We analyze competition in multi-agent reinforcement learning, and use a large-scale dataset from Amazon.com to provide empirical evidence. We show that when consumers have high search costs, learning algorithms can coordinate on prices lower than competitive prices, facilitating a win-win-win for consumers, sellers, and platforms. This occurs because algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices and enlarging demand on the platform. We also show that our results generalize to any learning algorithm that uses exploration of price and advertising bids. Consistent with our predictions, an empirical analysis shows that price levels exhibit a negative interaction between estimated consumer search costs and algorithm usage index. We analyze the platform's strategic response and find that reserve price adjustments will not increase platform profits, but commission adjustments will, while maintaining the beneficial outcomes for both sellers and consumers.", "categories": ["econ.GN", "cs.AI", "cs.CE", "cs.GT", "cs.LG", "q-fin.EC"], "abs_url": "https://arxiv.org/abs/2508.08325", "pdf_url": "https://arxiv.org/pdf/2508.08325.pdf", "is_interesting": false}, "575": {"title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization", "authors": ["Ishaan Verma"], "abstract": "arXiv:2509.05831v2 Announce Type: replace-cross \nLarge Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as , aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.05831", "pdf_url": "https://arxiv.org/pdf/2509.05831.pdf", "is_interesting": false}, "576": {"title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations", "authors": ["Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana"], "abstract": "arXiv:2509.15981v2 Announce Type: replace-cross \nIn reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abs_url": "https://arxiv.org/abs/2509.15981", "pdf_url": "https://arxiv.org/pdf/2509.15981.pdf", "is_interesting": false}, "577": {"title": "PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features", "authors": ["Lingyao Li, Haolun Wu, Zhenkun Li, Jiabei Hu, Yu Wang, Xiaoshan Huang, Wenyue Hua, Wenqian Wang"], "abstract": "arXiv:2509.24046v2 Announce Type: replace-cross \nHigh-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.", "categories": ["cs.MA", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.24046", "pdf_url": "https://arxiv.org/pdf/2509.24046.pdf", "is_interesting": false}, "578": {"title": "BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation", "authors": ["Zelin Liu, Sicheng Dong, Bocheng Li, Yixuan Yang, Jiacheng Ruan, Chenxu Zhou, Suncheng Xiang"], "abstract": "arXiv:2509.24204v2 Announce Type: replace-cross \nVision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAM's Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.24204", "pdf_url": "https://arxiv.org/pdf/2509.24204.pdf", "is_interesting": false}, "579": {"title": "PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control", "authors": ["Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan"], "abstract": "arXiv:2509.24591v2 Announce Type: replace-cross \nWe present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.24591", "pdf_url": "https://arxiv.org/pdf/2509.24591.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on a conditional diffusion model for robot pose estimation and control within a unified framework. While it presents techniques that could be relevant to robot perception and control, it does not explicitly discuss autonomous driving or its related tasks like vehicle perception, trajectory prediction, or motion planning."}}, "580": {"title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration", "authors": ["Wenhao Deng, Long Wei, Chenglei Yu, Tailin Wu"], "abstract": "arXiv:2510.03865v2 Announce Type: replace-cross \nReinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.03865", "pdf_url": "https://arxiv.org/pdf/2510.03865.pdf", "is_interesting": false}, "581": {"title": "Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising", "authors": ["Kangjia Yan, Chenxi Liu, Hao Miao, Xinle Wu, Yan Zhao, Chenjuan Guo, Bin Yang"], "abstract": "arXiv:2510.05589v2 Announce Type: replace-cross \nThe proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.05589", "pdf_url": "https://arxiv.org/pdf/2510.05589.pdf", "is_interesting": false}, "582": {"title": "LLM Based Long Code Translation using Identifier Replacement", "authors": ["Manojit Chakraborty, Madhusudan Ghosh, Rishabh Gupta"], "abstract": "arXiv:2510.09045v2 Announce Type: replace-cross \nIn the domain of software development, LLMs have been utilized to automate tasks such as code translation, where source code from one programming language is translated to another while preserving its functionality. However, LLMs often struggle with long source codes that don't fit into the context window, which produces inaccurate translations. To address this, we propose a novel zero-shot code translation method that incorporates identifier replacement. By substituting user-given long identifiers with generalized placeholders during translation, our method allows the LLM to focus on the logical structure of the code, by reducing token count and memory usage, which improves the efficiency and cost-effectiveness of long code translation. Our empirical results demonstrate that our approach preserves syntactical and hierarchical information and produces translation results with reduced tokens.", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.09045", "pdf_url": "https://arxiv.org/pdf/2510.09045.pdf", "is_interesting": false}, "583": {"title": "SVTime: Small Time Series Forecasting Models Informed by \"Physics\" of Large Vision Model Forecasters", "authors": ["ChengAo Shen, Ziming Zhao, Hanghang Tong, Dongjin Song, Dongsheng Luo, Qingsong Wen, Jingchao Ni"], "abstract": "arXiv:2510.09780v2 Announce Type: replace-cross \nTime series AI is crucial for analyzing dynamic web content, driving a surge of pre-trained large models known for their strong knowledge encoding and transfer capabilities across diverse tasks. However, given their energy-intensive training, inference, and hardware demands, using large models as a one-fits-all solution raises serious concerns about carbon footprint and sustainability. For a specific task, a compact yet specialized, high-performing model may be more practical and affordable, especially for resource-constrained users such as small businesses. This motivates the question: Can we build cost-effective lightweight models with large-model-like performance on core tasks such as forecasting? This paper addresses this question by introducing SVTime, a novel Small model inspired by large Vision model (LVM) forecasters for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as powerful tools for LTSF. We identify a set of key inductive biases of LVM forecasters -- analogous to the \"physics\" governing their behaviors in LTSF -- and design small models that encode these biases through meticulously crafted linear layers and constraint functions. Across 21 baselines spanning lightweight, complex, and pre-trained large models on 8 benchmark datasets, SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large models with 10^3 fewer parameters than LVMs, while enabling efficient training and inference in low-resource settings.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.09780", "pdf_url": "https://arxiv.org/pdf/2510.09780.pdf", "is_interesting": false}, "584": {"title": "Generative AI and Firm Productivity: Field Experiments in Online Retail", "authors": ["Lu Fang, Zhe Yuan, Kaifu Zhang, Dante Donati, Miklos Sarvary"], "abstract": "arXiv:2510.12049v2 Announce Type: replace-cross \nWe quantify the impact of Generative Artificial Intelligence (GenAI) on firm productivity through a series of large-scale randomized field experiments involving millions of users and products at a leading cross-border online retail platform. Over six months in 2023-2024, GenAI-based enhancements were integrated into seven consumer-facing business workflows. We find that GenAI adoption significantly increases sales, with treatment effects ranging from $0\\%$ to $16.3\\%$, depending on GenAI's marginal contribution relative to existing firm practices. Because inputs and prices were held constant across experimental arms, these gains map directly into total factor productivity improvements. Across the four GenAI applications with positive effects, the implied annual incremental value is approximately $\\$ 5$ per consumer-an economically meaningful impact given the retailer's scale and the early stage of GenAI adoption. The primary mechanism operates through higher conversion rates, consistent with GenAI reducing frictions in the marketplace and improving consumer experience. We also document substantial heterogeneity: smaller and newer sellers, as well as less experienced consumers, exhibit disproportionately larger gains. Our findings provide novel, large-scale causal evidence on the productivity effects of GenAI in online retail, highlighting both its immediate value and broader potential.", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "abs_url": "https://arxiv.org/abs/2510.12049", "pdf_url": "https://arxiv.org/pdf/2510.12049.pdf", "is_interesting": false}, "585": {"title": "DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization", "authors": ["Jiyan Qiu, Lyulin Kuang, Guan Wang, Yichen Xu, Leiyao Cui, Shaotong Fu, Yixin Zhu, Ruihua Zhang"], "abstract": "arXiv:2510.16857v2 Announce Type: replace-cross \nVehicle aerodynamics optimization has become critical for automotive electrification, where drag reduction directly determines electric vehicle range and energy efficiency. Traditional approaches face an intractable trade-off: computationally expensive Computational Fluid Dynamics (CFD) simulations requiring weeks per design iteration, or simplified models that sacrifice production-grade accuracy. While machine learning offers transformative potential, existing datasets exhibit fundamental limitations -- inadequate mesh resolution, missing vehicle components, and validation errors exceeding 5% -- preventing deployment in industrial workflows. We present DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations generated using STAR-CCM+${}^\\unicode{xAE}$ software. The dataset systematically explores three vehicle configurations through 20 Computer Aided Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including complete engine compartments and cooling systems with realistic internal airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a five-fold improvement over existing datasets -- through refined mesh strategies with strict wall $y^+$ control. Benchmarks demonstrate that models trained on this data achieve production-ready accuracy while reducing computational costs from weeks to minutes. This represents the first dataset bridging academic machine learning research and industrial CFD practice, establishing a new standard for data-driven aerodynamic optimization in automotive development. Beyond automotive applications, DrivAerStar demonstrates a paradigm for integrating high-fidelity physics simulations with Artificial Intelligence (AI) across engineering disciplines where computational constraints currently limit innovation.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.16857", "pdf_url": "https://arxiv.org/pdf/2510.16857.pdf", "is_interesting": false}, "586": {"title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams", "authors": ["Junhao Zhao, Zishuai Liu, Ruili Fang, Jin Lu, Linghan Zhang, Fei Dou"], "abstract": "arXiv:2510.16988v2 Announce Type: replace-cross \nThe recognition of Activities of Daily Living (ADLs) from event-triggered ambient sensors is an essential task in Ambient Assisted Living, yet existing methods remain constrained by representation-level limitations. Sequence-based approaches preserve temporal order of sensor activations but are sensitive to noise and lack spatial awareness, while image-based approaches capture global patterns and implicit spatial correlations but compress fine-grained temporal dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation) fail to enforce alignment between sequence- and image-based representation views, underutilizing their complementary strengths. We propose Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an end-to-end framework that jointly optimizes representation learning via Sequence-Image Contrastive Alignment (SICA) and classification via cross-entropy, ensuring both cross-representation alignment and task-specific discriminability. CARE integrates (i) time-aware, noise-resilient sequence encoding with (ii) spatially-informed and frequency-sensitive image representations, and employs (iii) a joint contrastive-classification objective for end-to-end learning of aligned and discriminative embeddings. Evaluated on three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to sensor malfunctions and layout variability, highlighting its potential for reliable ADL recognition in smart homes.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.16988", "pdf_url": "https://arxiv.org/pdf/2510.16988.pdf", "is_interesting": false}, "587": {"title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging", "authors": ["Tiancheng Hu, Benjamin Minixhofer, Nigel Collier"], "abstract": "arXiv:2510.17426v2 Announce Type: replace-cross \nThe \"alignment tax\" of post-training is typically framed as a drop in task accuracy. We show it also involves a severe loss of calibration, making models overconfident, less reliable, and model outputs less diverse. We show that this trade-off can be navigated effectively via a simple post-hoc intervention: interpolating between a model's weights before and after alignment. Crucially, this is not a strict trade-off. We find that the process consistently reveals Pareto-optimal interpolations - models that improve accuracy beyond both parents while substantially recovering the calibration lost during alignment. Our work demonstrates that simple model merging provides a computationally efficient method for mitigating the full scope of the alignment tax, yielding models that are more capable and more reliable.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.17426", "pdf_url": "https://arxiv.org/pdf/2510.17426.pdf", "is_interesting": false}, "588": {"title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models", "authors": ["Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi"], "abstract": "arXiv:2510.17496v2 Announce Type: replace-cross \nWe introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.17496", "pdf_url": "https://arxiv.org/pdf/2510.17496.pdf", "is_interesting": false}, "589": {"title": "{\\epsilon}-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data", "authors": ["Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug"], "abstract": "arXiv:2510.18637v2 Announce Type: replace-cross \nSemantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce {\\epsilon}-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of {\\epsilon}-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that {\\epsilon}-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.18637", "pdf_url": "https://arxiv.org/pdf/2510.18637.pdf", "is_interesting": false}, "590": {"title": "SmartMixed: A Two-Phase Training Strategy for Adaptive Activation Function Learning in Neural Networks", "authors": ["Amin Omidvar"], "abstract": "arXiv:2510.22450v2 Announce Type: replace-cross \nThe choice of activation function plays a critical role in neural networks, yet most architectures still rely on fixed, uniform activation functions across all neurons. We introduce SmartMixed, a two-phase training strategy that allows networks to learn optimal per-neuron activation functions while preserving computational efficiency at inference. In the first phase, neurons adaptively select from a pool of candidate activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, ELU, SELU) using a differentiable hard-mixture mechanism. In the second phase, each neuron's activation function is fixed according to the learned selection, resulting in a computationally efficient network that supports continued training with optimized vectorized operations. We evaluate SmartMixed on the MNIST dataset using feedforward neural networks of varying depths. The analysis shows that neurons in different layers exhibit distinct preferences for activation functions, providing insights into the functional diversity within neural architectures.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22450", "pdf_url": "https://arxiv.org/pdf/2510.22450.pdf", "is_interesting": false}, "591": {"title": "E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker", "authors": ["Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao"], "abstract": "arXiv:2510.22733v2 Announce Type: replace-cross \nText embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2510.22733", "pdf_url": "https://arxiv.org/pdf/2510.22733.pdf", "is_interesting": false}, "592": {"title": "DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications", "authors": ["Malaisree P, Youwai S, Kitkobsin T, Janrungautai S, Amorndechaphon D, Rojanavasu P"], "abstract": "arXiv:2510.25140v2 Announce Type: replace-cross \nObject detection in civil engineering applications is constrained by limited annotated data in specialized domains. We introduce DINO-YOLO, a hybrid architecture combining YOLOv12 with DINOv3 self-supervised vision transformers for data-efficient detection. DINOv3 features are strategically integrated at two locations: input preprocessing (P0) and mid-backbone enhancement (P3). Experimental validation demonstrates substantial improvements: Tunnel Segment Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while maintaining real-time inference (30-47 FPS). Systematic ablation across five YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead (21-33ms versus 8-16ms baseline) remains acceptable for field deployment on NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil engineering datasets (<10K images) while preserving computational efficiency, providing practical solutions for construction safety monitoring and infrastructure inspection in data-constrained environments.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.25140", "pdf_url": "https://arxiv.org/pdf/2510.25140.pdf", "is_interesting": false}, "593": {"title": "Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning", "authors": ["Yogesh Thakku Suresh, Vishwajeet Shivaji Hogale, Luca-Alexandru Zamfira, Anandavardhana Hegde"], "abstract": "arXiv:2510.25164v2 Announce Type: replace-cross \nWe present a transformer-based multimodal framework for generating clinically relevant captions for MRI scans. Our system combines a DEiT-Small vision transformer as an image encoder, MediCareBERT for caption embedding, and a custom LSTM-based decoder. The architecture is designed to semantically align image and textual embeddings, using hybrid cosine-MSE loss and contrastive inference via vector similarity. We benchmark our method on the MultiCaRe dataset, comparing performance on filtered brain-only MRIs versus general MRI images against state-of-the-art medical image captioning methods including BLIP, R2GenGPT, and recent transformer-based approaches. Results show that focusing on domain-specific data improves caption accuracy and semantic alignment. Our work proposes a scalable, interpretable solution for automated medical image reporting.", "categories": ["eess.IV", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.25164", "pdf_url": "https://arxiv.org/pdf/2510.25164.pdf", "is_interesting": false}, "594": {"title": "'Studies for': A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model", "authors": ["Chihiro Nagashima, Akira Takahashi, Zhi Zhong, Shusuke Takahashi, Yuki Mitsufuji"], "abstract": "arXiv:2510.25228v2 Announce Type: replace-cross \nThis paper explores the integration of AI technologies into the artistic workflow through the creation of Studies for, a generative sound installation developed in collaboration with sound artist Evala (https://www.ntticc.or.jp/en/archive/works/studies-for/). The installation employs SpecMaskGIT, a lightweight yet high-quality sound generation AI model, to generate and playback eight-channel sound in real-time, creating an immersive auditory experience over the course of a three-month exhibition. The work is grounded in the concept of a \"new form of archive,\" which aims to preserve the artistic style of an artist while expanding beyond artists' past artworks by continued generation of new sound elements. This speculative approach to archival preservation is facilitated by training the AI model on a dataset consisting of over 200 hours of Evala's past sound artworks.\n  By addressing key requirements in the co-creation of art using AI, this study highlights the value of the following aspects: (1) the necessity of integrating artist feedback, (2) datasets derived from an artist's past works, and (3) ensuring the inclusion of unexpected, novel outputs. In Studies for, the model was designed to reflect the artist's artistic identity while generating new, previously unheard sounds, making it a fitting realization of the concept of \"a new form of archive.\" We propose a Human-AI co-creation framework for effectively incorporating sound generation AI models into the sound art creation process and suggest new possibilities for creating and archiving sound art that extend an artist's work beyond their physical existence. Demo page: https://sony.github.io/studies-for/", "categories": ["cs.SD", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.25228", "pdf_url": "https://arxiv.org/pdf/2510.25228.pdf", "is_interesting": false}, "595": {"title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding", "authors": ["Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang"], "abstract": "arXiv:2510.25327v3 Announce Type: replace-cross \nReal-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.25327", "pdf_url": "https://arxiv.org/pdf/2510.25327.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u591a\u6a21\u6001\u878d\u5408", "reason": "The paper discusses a multimodal inference framework for resource-constrained edge devices, which can be applied to autonomous driving systems, particularly in handling multiple sensor inputs (e.g., cameras, LiDAR) for real-time decision-making. Although the primary focus is on the optimization of multi-modal systems, its potential application in autonomous driving tasks involving sensor fusion and early decision-making under resource constraints makes it relevant to the field."}}, "596": {"title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting", "authors": ["Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter"], "abstract": "arXiv:2510.25502v2 Announce Type: replace-cross \nFoundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the vast majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.25502", "pdf_url": "https://arxiv.org/pdf/2510.25502.pdf", "is_interesting": false}, "597": {"title": "A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows", "authors": ["Ant\\'ia Dorado, Iv\\'an Folgueira, Sof\\'ia Mart\\'in, Gonzalo Mart\\'in, \\'Alvaro Porto, Alejandro Ramos, John Wallace"], "abstract": "arXiv:2510.25935v2 Announce Type: replace-cross \nCodeSight is an end-to-end system designed to anticipate deadline compliance in software development workflows. It captures development and deployment data directly from GitHub, transforming it into process mining logs for detailed analysis. From these logs, the system generates metrics and dashboards that provide actionable insights into PR activity patterns and workflow efficiency. Building on this structured representation, CodeSight employs an LSTM model that predicts remaining PR resolution times based on sequential activity traces and static features, enabling early identification of potential deadline breaches. In tests, the system demonstrates high precision and F1 scores in predicting deadline compliance, illustrating the value of integrating process mining with machine learning for proactive software project management.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.25935", "pdf_url": "https://arxiv.org/pdf/2510.25935.pdf", "is_interesting": false}, "598": {"title": "Aeolus: A Multi-structural Flight Delay Dataset", "authors": ["Lin Xu, Xinyun Yuan, Yuxuan Liang, Suwan Yin, Yuankai Wu"], "abstract": "arXiv:2510.26616v2 Announce Type: replace-cross \nWe introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed to advance research on flight delay prediction and support the development of foundation models for tabular data. Existing datasets in this domain are typically limited to flat tabular structures and fail to capture the spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this limitation by providing three aligned modalities: (i) a tabular dataset with rich operational, meteorological, and airportlevel features for over 50 million flights; (ii) a flight chain module that models delay propagation along sequential flight legs, capturing upstream and downstream dependencies; and (iii) a flight network graph that encodes shared aircraft, crew, and airport resource connections, enabling cross-flight relational reasoning. The dataset is carefully constructed with temporal splits, comprehensive features, and strict leakage prevention to support realistic and reproducible machine learning evaluation. Aeolus supports a broad range of tasks, including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities. We release baseline experiments and preprocessing tools to facilitate adoption. Aeolus fills a key gap for both domain-specific modeling and general-purpose structured data research.Our source code and data can be accessed at https://github.com/Flnny/Delay-data", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26616", "pdf_url": "https://arxiv.org/pdf/2510.26616.pdf", "is_interesting": false}, "599": {"title": "The End of Manual Decoding: Towards Truly End-to-End Language Models", "authors": ["Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang"], "abstract": "arXiv:2510.26697v2 Announce Type: replace-cross \nThe \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end\" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., \"generate with low randomness\") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26697", "pdf_url": "https://arxiv.org/pdf/2510.26697.pdf", "is_interesting": false}, "600": {"title": "On the limitation of evaluating machine unlearning using only a single training seed", "authors": ["Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma"], "abstract": "arXiv:2510.26714v2 Announce Type: replace-cross \nMachine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comparisons of MU algorithms should also reflect the variability across different model training seeds.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26714", "pdf_url": "https://arxiv.org/pdf/2510.26714.pdf", "is_interesting": false}, "601": {"title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off", "authors": ["Muhammad Faraz Ul Abrar, Nicol\\`o Michelusi"], "abstract": "arXiv:2510.26722v2 Announce Type: replace-cross \nOver-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SP", "eess.SY"], "abs_url": "https://arxiv.org/abs/2510.26722", "pdf_url": "https://arxiv.org/pdf/2510.26722.pdf", "is_interesting": false}, "602": {"title": "Faithful and Fast Influence Function via Advanced Sampling", "authors": ["Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang"], "abstract": "arXiv:2510.26776v2 Announce Type: replace-cross \nHow can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26776", "pdf_url": "https://arxiv.org/pdf/2510.26776.pdf", "is_interesting": false}, "603": {"title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "abstract": "arXiv:2511.00039v1 Announce Type: new \nDynamic pricing in retail requires policies that adapt to shifting demand while coordinating decisions across related products. We present a systematic empirical study of multi-agent reinforcement learning for retail price optimization, comparing a strong MAPPO baseline with a graph-attention-augmented variant (MAPPO+GAT) that leverages learned interactions among products. Using a simulated pricing environment derived from real transaction data, we evaluate profit, stability across random seeds, fairness across products, and training efficiency under a standardized evaluation protocol. The results indicate that MAPPO provides a robust and reproducible foundation for portfolio-level price control, and that MAPPO+GAT further enhances performance by sharing information over the product graph without inducing excessive price volatility. These results indicate that graph-integrated MARL provides a more scalable and stable solution than independent learners for dynamic retail pricing, offering practical advantages in multi-product decision-making.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00039", "pdf_url": "https://arxiv.org/pdf/2511.00039.pdf", "is_interesting": false}, "604": {"title": "GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0", "authors": ["Martin Bicher, Maximilian Viehauser, Daniele Giannandrea, Hannah Kastinger, Dominik Brunmeir, Claire Rippinger, Christoph Urach, Niki Popper"], "abstract": "arXiv:2511.00048v1 Announce Type: new \nGEPOC, short for Generic Population Concept, is a collection of models and methods for analysing population-level research questions. For the valid application of the models for a specific country or region, stable and reproducible data processes are necessary, which provide valid and ready-to-use model parameters. This work contains a complete description of the data-processing methods for computation of model parameters for Austria, based exclusively on freely and publicly accessible data. In addition to the description of the source data used, this includes all algorithms used for aggregation, disaggregation, fusion, cleansing or scaling of the data, as well as a description of the resulting parameter files. The document places particular emphasis on the computation of parameters for the most important GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An extensive validation study using this particular model was made and is presented at the end of this work.", "categories": ["cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.00048", "pdf_url": "https://arxiv.org/pdf/2511.00048.pdf", "is_interesting": false}, "605": {"title": "QuantumBench: A Benchmark for Quantum Problem Solving", "authors": ["Shunya Minami, Tatsuya Ishigaki, Ikko Hamamura, Taku Mikuriya, Youmi Ma, Naoaki Okazaki, Hiroya Takamura, Yohichi Suzuki, Tadashi Kadowaki"], "abstract": "arXiv:2511.00092v1 Announce Type: new \nLarge language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "abs_url": "https://arxiv.org/abs/2511.00092", "pdf_url": "https://arxiv.org/pdf/2511.00092.pdf", "is_interesting": false}, "606": {"title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "authors": ["Ran Xu, Yupeng Qi, Jingsen Feng, Xu Chu"], "abstract": "arXiv:2511.00122v1 Announce Type: new \nIn modern engineering practice, human engineers collaborate in specialized teams to design complex products, with each expert completing their respective tasks while communicating and exchanging results and data with one another. While this division of expertise is essential for managing multidisciplinary complexity, it demands substantial development time and cost. Recently, we introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer for computational fluid dynamics, and turbulence.ai, which can conduct end-to-end research in fluid mechanics draft publications and PhD theses. Building upon these foundations, we present Engineering.ai, a platform for teams of AI engineers in computational design. The framework employs a hierarchical multi-agent architecture where a Chief Engineer coordinates specialized agents consisting of Aerodynamics, Structural, Acoustic, and Optimization Engineers, each powered by LLM with domain-specific knowledge. Agent-agent collaboration is achieved through file-mediated communication for data provenance and reproducibility, while a comprehensive memory system maintains project context, execution history, and retrieval-augmented domain knowledge to ensure reliable decision-making across the workflow. The system integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis, enabling parallel multidisciplinary simulations while maintaining computational accuracy. The framework is validated through UAV wing optimization. This work demonstrates that agentic-AI-enabled AI engineers has the potential to perform complex engineering tasks autonomously. Remarkably, the automated workflow achieved a 100% success rate across over 400 parametric configurations, with zero mesh generation failures, solver convergence issues, or manual interventions required, validating that the framework is trustworthy.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00122", "pdf_url": "https://arxiv.org/pdf/2511.00122.pdf", "is_interesting": false}, "607": {"title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "authors": ["Michael D. Moffitt"], "abstract": "arXiv:2511.00162v2 Announce Type: new \nThe Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\\langle$ input, output $\\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00162", "pdf_url": "https://arxiv.org/pdf/2511.00162.pdf", "is_interesting": false}, "608": {"title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "authors": ["Jovial Cheukam Ngouonou, Ramiz Gindullin, Claude-Guy Quimper, Nicolas Beldiceanu, Remi Douence"], "abstract": "arXiv:2511.00194v1 Announce Type: new \nWe present an improved incremental selection algorithm of the selection algorithm presented in [1] and prove all the selected conjectures.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00194", "pdf_url": "https://arxiv.org/pdf/2511.00194.pdf", "is_interesting": false}, "609": {"title": "Advancing Cognitive Science with LLMs", "authors": ["Dirk U. Wulff, Rui Mata"], "abstract": "arXiv:2511.00206v1 Announce Type: new \nCognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00206", "pdf_url": "https://arxiv.org/pdf/2511.00206.pdf", "is_interesting": false}, "610": {"title": "Advancing AI Challenges for the United States Department of the Air Force", "authors": ["Christian Prothmann, Vijay Gadepally, Jeremy Kepner, Koley Borchard, Luca Carlone, Zachary Folcik, J. Daniel Grith, Michael Houle, Jonathan P. How, Nathan Hughes, Ifueko Igbinedion, Hayden Jananthan, Tejas Jayashankar, Michael Jones, Sertac Karaman, Binoy G. Kurien, Alejandro Lancho, Giovanni Lavezzi, Gary C. F. Lee, Charles E. Leiserson, Richard Linares, Lindsey McEvoy, Peter Michaleas, Chasen Milner, Alex Pentland, Yury Polyanskiy, Jovan Popovich, Jeffrey Price, Tim W. Reid, Stephanie Riley, Siddharth Samsi, Peter Saunders, Olga Simek, Mark S. Veillette, Amir Weiss, Gregory W. Wornell, Daniela Rus, Scott T. Ruppel"], "abstract": "arXiv:2511.00267v1 Announce Type: new \nThe DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00267", "pdf_url": "https://arxiv.org/pdf/2511.00267.pdf", "is_interesting": false}, "611": {"title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "authors": ["Manan Roy Choudhury, Adithya Chandramouli, Mannan Anand, Vivek Gupta"], "abstract": "arXiv:2511.00340v1 Announce Type: new \nThe rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs' ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00340", "pdf_url": "https://arxiv.org/pdf/2511.00340.pdf", "is_interesting": false}, "612": {"title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "authors": ["Jiahao Wang, Songkai Xue, Jinghui Li, Xiaozhen Wang"], "abstract": "arXiv:2511.00379v1 Announce Type: new \nEnsuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00379", "pdf_url": "https://arxiv.org/pdf/2511.00379.pdf", "is_interesting": false}, "613": {"title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "authors": ["Mina Taraghi, Yann Pequignot, Amin Nikanjam, Mohamed Amine Merzouk, Foutse Khomh"], "abstract": "arXiv:2511.00382v1 Announce Type: new \nOrganizations are increasingly adopting and adapting Large Language Models (LLMs) hosted on public repositories such as HuggingFace. Although these adaptations often improve performance on specialized downstream tasks, recent evidence indicates that they can also degrade a model's safety or fairness. Since different fine-tuning techniques may exert distinct effects on these critical dimensions, this study undertakes a systematic assessment of their trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA, IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235 fine-tuned variants are evaluated across eleven safety hazard categories and nine demographic fairness dimensions. The results show that adapter-based approaches (LoRA, IA3) tend to improve safety scores and are the least disruptive to fairness, retaining higher accuracy and lower bias scores. In contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts are strongly moderated by base model type: LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest safety decline, and Mistral, which is released without an internal moderation layer, displays the greatest variance. Improvements in safety do not necessarily translate into improvements in fairness, and no single configuration optimizes all fairness metrics simultaneously, indicating an inherent trade-off between these objectives. These findings suggest a practical guideline for safety-critical deployments: begin with a well-aligned base model, favour adapter-based PEFT, and conduct category-specific audits of both safety and fairness.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00382", "pdf_url": "https://arxiv.org/pdf/2511.00382.pdf", "is_interesting": false}, "614": {"title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "authors": ["Ashutosh Anshul, Gumpili Sai Pranav, Mohammad Zia Ur Rehman, Nagendra Kumar"], "abstract": "arXiv:2511.00424v1 Announce Type: new \nThe recent coronavirus disease (Covid-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this paper, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user's emotional state, we propose (i) an extrinsic feature by harnessing the URLs present in tweets and (ii) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. Additionally, we introduce a Deep Learning model, the Visual Neural Network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated Covid-19 dataset of depressed and non-depressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over a benchmark dataset by 2%-8% and produces promising results on the Covid-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users' mental and emotional states.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00424", "pdf_url": "https://arxiv.org/pdf/2511.00424.pdf", "is_interesting": false}, "615": {"title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "authors": ["Chunyu Wei, Wenji Hu, Xingjia Hao, Xin Wang, Yifan Yang, Yueguo Chen, Yang Tian, Yunhai Wang"], "abstract": "arXiv:2511.00457v1 Announce Type: new \nLarge Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00457", "pdf_url": "https://arxiv.org/pdf/2511.00457.pdf", "is_interesting": false}, "616": {"title": "Reimagining Safety Alignment with An Image", "authors": ["Yifan Xia, Guorui Chen, Wenqian Yu, Zhijiang Li, Philip Torr, Jindong Gu"], "abstract": "arXiv:2511.00509v1 Announce Type: new \nLarge language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.", "categories": ["cs.AI", "cs.CR"], "abs_url": "https://arxiv.org/abs/2511.00509", "pdf_url": "https://arxiv.org/pdf/2511.00509.pdf", "is_interesting": false}, "617": {"title": "Efficient Generation of Binary Magic Squares", "authors": ["Alain Riou"], "abstract": "arXiv:2511.00547v1 Announce Type: new \nWe propose a simple algorithm for generating Binary Magic Squares (BMS), i.e., square binary matrices where the sum of all rows and all columns are equal. We show by induction that our algorithm always returns valid BMS with optimal theoretical complexity. We then extend our study to non-square Binary Magic Squares, formalize conditions on the sum of rows and columns for these BMS to exist, and show that a slight variant of our first algorithm can generate provably generate them. Finally, we publicly release two implementations of our algorithm as Python packages, including one that can generate several BMS in parallel using GPU acceleration.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00547", "pdf_url": "https://arxiv.org/pdf/2511.00547.pdf", "is_interesting": false}, "618": {"title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "authors": ["Qiang Li, Ningjing Zeng, Lina Yu"], "abstract": "arXiv:2511.00551v1 Announce Type: new \nSeveral studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method's potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00551", "pdf_url": "https://arxiv.org/pdf/2511.00551.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.2, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses the use of reinforcement learning for regional adaptive traffic signal control (ATSC) but does not directly focus on autonomous driving systems, vehicle perception, or control. While the approach could be indirectly applicable to vehicle systems that require traffic coordination, the primary focus is on traffic management rather than autonomous driving technology."}}, "619": {"title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "authors": ["Shengqi Xu, Xinpeng Zhou, Yabo Zhang, Ming Liu, Tao Liang, Tianyu Zhang, Yalong Bai, Zuxuan Wu, Wangmeng Zuo"], "abstract": "arXiv:2511.00609v1 Announce Type: new \nPersonalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \\textit{predict-then-assess} paradigm: it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00609", "pdf_url": "https://arxiv.org/pdf/2511.00609.pdf", "is_interesting": false}, "620": {"title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "authors": ["Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander S. Szalay, Zirui Liu, Vladimir Braverman"], "abstract": "arXiv:2511.00640v1 Announce Type: new \nLarge Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00640", "pdf_url": "https://arxiv.org/pdf/2511.00640.pdf", "is_interesting": false}, "621": {"title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "authors": ["Chenhua Shi, Bhavika Jalli, Gregor Macdonald, John Zou, Wanlu Lei, Mridul Jain, Joji Philip"], "abstract": "arXiv:2511.00651v1 Announce Type: new \nTelecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "abs_url": "https://arxiv.org/abs/2511.00651", "pdf_url": "https://arxiv.org/pdf/2511.00651.pdf", "is_interesting": false}, "622": {"title": "Lifted Successor Generation in Numeric Planning", "authors": ["Dominik Drexler"], "abstract": "arXiv:2511.00673v1 Announce Type: new \nMost planners ground numeric planning tasks, given in a first-order-like language, into a ground task representation. However, this can lead to an exponential blowup in task representation size, which occurs in practice for hard-to-ground tasks. We extend a state-of-the-art lifted successor generator for classical planning to support numeric precondition applicability. The method enumerates maximum cliques in a substitution consistency graph. Each maximum clique represents a substitution for the variables of the action schema, yielding a ground action. We augment this graph with numeric action preconditions and prove the successor generator is exact under formally specified conditions. When the conditions fail, our generator may list inapplicable ground actions; a final applicability check filters these without affecting completeness. However, this cannot happen in 23 of 25 benchmark domains, and it occurs only in 1 domain. To the authors' knowledge, no other lifted successor generator supports numeric action preconditions. This enables future research on lifted planning for a very rich planning fragment.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00673", "pdf_url": "https://arxiv.org/pdf/2511.00673.pdf", "is_interesting": false}, "623": {"title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "authors": ["Minghe Shen, Zhuo Zhi, Chonghan Liu, Shuo Xing, Zhengzhong Tu, Che Liu"], "abstract": "arXiv:2511.00710v1 Announce Type: new \nWhile Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00710", "pdf_url": "https://arxiv.org/pdf/2511.00710.pdf", "is_interesting": false}, "624": {"title": "A CPU-Centric Perspective on Agentic AI", "authors": ["Ritik Raj, Hong Wang, Tushar Krishna"], "abstract": "arXiv:2511.00739v1 Announce Type: new \nAgentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2511.00739", "pdf_url": "https://arxiv.org/pdf/2511.00739.pdf", "is_interesting": false}, "625": {"title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "authors": ["Chiyan Loo"], "abstract": "arXiv:2511.00751v1 Announce Type: new \nThis study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00751", "pdf_url": "https://arxiv.org/pdf/2511.00751.pdf", "is_interesting": false}, "626": {"title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "authors": ["Hong Su"], "abstract": "arXiv:2511.00758v1 Announce Type: new \nReal-world artificial intelligence (AI) systems are increasingly required to operate autonomously in dynamic, uncertain, and continuously changing environments. However, most existing AI models rely on predefined objectives, static training data, and externally supplied feedback, which restrict their ability to adapt, reflect, and improve independently. In this paper, we propose the Active Thinking Model (ATM)- a unified cognitive framework that integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture. Unlike conventional systems that passively execute fixed procedures, ATM actively evaluates its performance through logical reasoning and environmental indicators, reuses effective methods to solve new problems, and generates novel strategies for unseen situations via a continuous self-improvement loop. A mathematically grounded theoretical analysis demonstrates that ATM can autonomously evolve from suboptimal to optimal behavior without external supervision and maintain bounded tracking regret under changing environmental conditions.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00758", "pdf_url": "https://arxiv.org/pdf/2511.00758.pdf", "is_interesting": false}, "627": {"title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "authors": ["Wanda Hou, Leon Zhou, Hong-Ye Hu, Yi-Zhuang You, Xiao-Liang Qi"], "abstract": "arXiv:2511.00763v1 Announce Type: new \nWe investigate the performance of large language models on repetitive deterministic prediction tasks and study how the sequence accuracy rate scales with output length. Each such task involves repeating the same operation n times. Examples include letter replacement in strings following a given rule, integer addition, and multiplication of string operators in many body quantum mechanics. If the model performs the task through a simple repetition algorithm, the success rate should decay exponentially with sequence length. In contrast, our experiments on leading large language models reveal a sharp double exponential drop beyond a characteristic length scale, forming an accuracy cliff that marks the transition from reliable to unstable generation. This indicates that the models fail to execute each operation independently. To explain this phenomenon, we propose a statistical physics inspired model that captures the competition between external conditioning from the prompt and internal interference among generated tokens. The model quantitatively reproduces the observed crossover and provides an interpretable link between attention induced interference and sequence level failure. Fitting the model to empirical results across multiple models and tasks yields effective parameters that characterize the intrinsic error rate and error accumulation factor for each model task pair, offering a principled framework for understanding the limits of deterministic accuracy in large language models.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00763", "pdf_url": "https://arxiv.org/pdf/2511.00763.pdf", "is_interesting": false}, "628": {"title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "authors": ["Jifan Gao, Michael Rosenthal, Brian Wolpin, Simona Cristea"], "abstract": "arXiv:2511.00782v1 Announce Type: new \nStructured electronic health records (EHR) are essential for clinical prediction. While count-based learners continue to perform strongly on such data, no benchmarking has directly compared them against more recent mixture-of-agents LLM pipelines, which have been reported to outperform single LLMs in various NLP tasks. In this study, we evaluated three categories of methodologies for EHR prediction using the EHRSHOT dataset: count-based models built from ontology roll-ups with two time bins, based on LightGBM and the tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR); and a mixture-of-agents pipeline that converts tabular histories to natural-language summaries followed by a text classifier. We assessed eight outcomes using the EHRSHOT dataset. Across the eight evaluation tasks, head-to-head wins were largely split between the count-based and the mixture-of-agents methods. Given their simplicity and interpretability, count-based models remain a strong candidate for structured EHR benchmarking. The source code is available at: https://github.com/cristea-lab/Structured_EHR_Benchmark.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00782", "pdf_url": "https://arxiv.org/pdf/2511.00782.pdf", "is_interesting": false}, "629": {"title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "authors": ["Bowen Fang, Ruijian Zha, Xuan Di"], "abstract": "arXiv:2511.00808v1 Announce Type: new \nPredicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00808", "pdf_url": "https://arxiv.org/pdf/2511.00808.pdf", "is_interesting": false}, "630": {"title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "authors": ["Kyung-Hoon Kim"], "abstract": "arXiv:2511.00926v2 Announce Type: new \nAs Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the \"Guess 2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00926", "pdf_url": "https://arxiv.org/pdf/2511.00926.pdf", "is_interesting": false}, "631": {"title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "authors": ["Tianming Liu, Jirong Yang, Yafeng Yin, Manzi Li, Linghao Wang, Zheng Zhu"], "abstract": "arXiv:2511.00993v1 Announce Type: new \nEffective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00993", "pdf_url": "https://arxiv.org/pdf/2511.00993.pdf", "is_interesting": false}, "632": {"title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "authors": ["Hui-Lee Ooi, Nicholas Mitsakakis, Margerie Huet Dastarac, Roger Zemek, Amy C. Plint, Jeff Gilchrist, Khaled El Emam, Dhenuka Radhakrishnan"], "abstract": "arXiv:2511.01018v1 Announce Type: new \nRecurrent exacerbations remain a common yet preventable outcome for many children with asthma. Machine learning (ML) algorithms using electronic medical records (EMR) could allow accurate identification of children at risk for exacerbations and facilitate referral for preventative comprehensive care to avoid this morbidity. We developed ML algorithms to predict repeat severe exacerbations (i.e. asthma-related emergency department (ED) visits or future hospital admissions) for children with a prior asthma ED visit at a tertiary care children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from the Children's Hospital of Eastern Ontario (CHEO) linked with environmental pollutant exposure and neighbourhood marginalization information was used to train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from CHEO. Models were compared using the area under the curve (AUC) and F1 scores, with SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage acuity scale, medical complexity, food allergy, prior ED visits for non-asthma respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This is a nontrivial improvement over the current decision rule which has F1=0.334. While the most predictive features in the AIRE-KIDS_HOSP model included medical complexity, prior asthma ED visit, average wait time in the ED, the pediatric respiratory assessment measure score at triage and food allergy.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01018", "pdf_url": "https://arxiv.org/pdf/2511.01018.pdf", "is_interesting": false}, "633": {"title": "On the Emergence of Induction Heads for In-Context Learning", "authors": ["Tiberiu Musat, Tiago Pimentel, Lorenzo Noci, Alessandro Stolfo, Mrinmaya Sachan, Thomas Hofmann"], "abstract": "arXiv:2511.01033v1 Announce Type: new \nTransformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.01033", "pdf_url": "https://arxiv.org/pdf/2511.01033.pdf", "is_interesting": false}, "634": {"title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "authors": ["Yeawon Lee, Christopher C. Yang, Chia-Hsuan Chang, Grace Lu-Yao"], "abstract": "arXiv:2511.01052v1 Announce Type: new \nCancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.", "categories": ["cs.AI", "physics.med-ph"], "abs_url": "https://arxiv.org/abs/2511.01052", "pdf_url": "https://arxiv.org/pdf/2511.01052.pdf", "is_interesting": false}, "635": {"title": "Efficient Test-Time Retrieval Augmented Generation", "authors": ["Hailong Yin, Bin Zhu, Jingjing Chen, Chong-Wah Ngo"], "abstract": "arXiv:2511.01059v1 Announce Type: new \nAlthough Large Language Models (LLMs) demonstrate significant capabilities, their reliance on parametric knowledge often leads to inaccuracies. Retrieval Augmented Generation (RAG) mitigates this by incorporating external knowledge, but these methods may introduce irrelevant retrieved documents, leading to inaccurate responses. While the integration methods filter out incorrect answers from multiple responses, but lack external knowledge like RAG methods, and their high costs require balancing overhead with performance gains. To address these issues, we propose an Efficient Test-Time Retrieval-Augmented Generation Framework named ET2RAG to improve the performance of LLMs while maintaining efficiency. Specifically, ET2RAG is a training-free method, that first retrieves the most relevant documents and augments the LLMs to efficiently generate diverse candidate responses by managing response length. Then we compute the similarity of candidate responses and employ a majority voting mechanism to select the most suitable response as the final output. In particular, we discover that partial generation is sufficient to capture the key information necessary for consensus calculation, allowing us to effectively perform majority voting without the need for fully generated responses. Thus, we can reach a balance between computational cost and performance by managing the response length for the number of retrieved documents for majority voting. Experimental results demonstrate that ET2RAG significantly enhances performance across three tasks, including open-domain question answering, recipe generation and image captioning.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01059", "pdf_url": "https://arxiv.org/pdf/2511.01059.pdf", "is_interesting": false}, "636": {"title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "authors": ["Shuaidong Pan, Di Wu"], "abstract": "arXiv:2511.01149v1 Announce Type: new \nThis paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01149", "pdf_url": "https://arxiv.org/pdf/2511.01149.pdf", "is_interesting": false}, "637": {"title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "authors": ["Ruofan Zhang, Bin Xia, Zhen Cheng, Cairen Jian, Minglun Yang, Ngai Wong, Yuan Cheng"], "abstract": "arXiv:2511.01170v1 Announce Type: new \nAdaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \\textbf{DART}, a supervised \\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01170", "pdf_url": "https://arxiv.org/pdf/2511.01170.pdf", "is_interesting": false}, "638": {"title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "authors": ["Cuong Van Duc, Thai Tran Quoc, Minh Nguyen Dinh Tuan, Tam Vu Duc, Son Nguyen Van, Hanh Nguyen Thi"], "abstract": "arXiv:2511.01182v1 Announce Type: new \nDetecting student misconceptions in open-ended responses is a longstanding challenge, demanding semantic precision and logical reasoning. We propose MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion, a novel framework for automated misconception detection in mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a large candidate pool to a semantically relevant subset; (2) a Reasoning module employs chain-of-thought generation to expose logical inconsistencies in student solutions; and (3) a Reranking module refines predictions by aligning them with the reasoning. These components are unified through an ensemble-fusion strategy that enhances robustness and interpretability. On mathematics datasets, MiRAGE achieves Mean Average Precision scores of 0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules. By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces dependence on large-scale language models while delivering a scalable and effective solution for educational assessment.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01182", "pdf_url": "https://arxiv.org/pdf/2511.01182.pdf", "is_interesting": false}, "639": {"title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "authors": ["Hainan Fang, Yuanbo Wen, Jun Bi, Yihan Wang, Tonghui He, Yanlin Tang, Di Huang, Jiaming Guo, Rui Zhang, Qi Guo, Yunji Chen"], "abstract": "arXiv:2511.01183v1 Announce Type: new \nCompilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities. Experiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01183", "pdf_url": "https://arxiv.org/pdf/2511.01183.pdf", "is_interesting": false}, "640": {"title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "authors": ["Chuyue Lou, M. Amine Atoui"], "abstract": "arXiv:2511.01258v1 Announce Type: new \nRecently, fault diagnosis methods for marine machinery systems based on deep learning models have attracted considerable attention in the shipping industry. Most existing studies assume fault classes are consistent and known between the training and test datasets, and these methods perform well under controlled environment. In practice, however, previously unseen or unknown fault types (i.e., out-of-distribution or open-set observations not present during training) can occur, causing such methods to fail and posing a significant challenge to their widespread industrial deployment. To address this challenge, this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework that enhances and extends the applicability of deep learning models in open-set fault diagnosis scenarios. The framework includes a reliability subset construction process, which uses a multi-layer fusion feature representation extracted by a supervised feature learning model to select an unlabeled test subset. The labeled training set and pseudo-labeled test subset are then fed into a semi-supervised diagnosis model to learn discriminative features for each class, enabling accurate classification of known faults and effective detection of unknown samples. Experimental results on a public maritime benchmark dataset demonstrate the effectiveness and superiority of the proposed SOFD framework.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01258", "pdf_url": "https://arxiv.org/pdf/2511.01258.pdf", "is_interesting": false}, "641": {"title": "llmSHAP: A Principled Approach to LLM Explainability", "authors": ["Filip Naudot, Tobias Sundqvist, Timotheus Kampik"], "abstract": "arXiv:2511.01311v1 Announce Type: new \nFeature attribution methods help make machine learning-based inference explainable by determining how much one or several features have contributed to a model's output. A particularly popular attribution method is based on the Shapley value from cooperative game theory, a measure that guarantees the satisfaction of several desirable principles, assuming deterministic inference. We apply the Shapley value to feature attribution in large language model (LLM)-based decision support systems, where inference is, by design, stochastic (non-deterministic). We then demonstrate when we can and cannot guarantee Shapley value principle satisfaction across different implementation variants applied to LLM-based decision support, and analyze how the stochastic nature of LLMs affects these guarantees. We also highlight trade-offs between explainable inference speed, agreement with exact Shapley value attributions, and principle attainment.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01311", "pdf_url": "https://arxiv.org/pdf/2511.01311.pdf", "is_interesting": false}, "642": {"title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "authors": ["Ziqi Wang, Hailiang Zhao, Yuhao Yang, Daojiang Hu, Cheng Bao, Mingyi Liu, Kai Di, Schahram Dustdar, Zhongjie Wang, Shuiguang Deng"], "abstract": "arXiv:2511.01320v1 Announce Type: new \nAccurate and timely prediction of tool conditions is critical for intelligent manufacturing systems, where unplanned tool failures can lead to quality degradation and production downtime. In modern industrial environments, predictive maintenance is increasingly implemented as an intelligent service that integrates sensing, analysis, and decision support across production processes. To meet the demand for reliable and service-oriented operation, we present OmniFuser, a multimodal learning framework for predictive maintenance of milling tools that leverages both visual and sensor data. It performs parallel feature extraction from high-resolution tool images and cutting-force signals, capturing complementary spatiotemporal patterns across modalities. To effectively integrate heterogeneous features, OmniFuser employs a contamination-free cross-modal fusion mechanism that disentangles shared and modality-specific components, allowing for efficient cross-modal interaction. Furthermore, a recursive refinement pathway functions as an anchor mechanism, consistently retaining residual information to stabilize fusion dynamics. The learned representations can be encapsulated as reusable maintenance service modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled) and multi-step force signal forecasting. Experiments on real-world milling datasets demonstrate that OmniFuser consistently outperforms state-of-the-art baselines, providing a dependable foundation for building intelligent industrial maintenance services.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01320", "pdf_url": "https://arxiv.org/pdf/2511.01320.pdf", "is_interesting": false}, "643": {"title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework", "authors": ["Ying Song, Yijing Wang, Hui Yang, Weihan Jin, Jun Xiong, Congyi Zhou, Jialin Zhu, Xiang Gao, Rong Chen, HuaGuang Deng, Ying Dai, Fei Xiao, Haihong Tang, Bo Zheng, KaiFu Zhang"], "abstract": "arXiv:2511.01329v1 Announce Type: new \nEvaluating platform-level interventions in search-based two-sided marketplaces is fundamentally challenged by systemic effects such as spillovers and network interference. While widely used for causal inference, the PSM (Propensity Score Matching) - DID (Difference-in-Differences) framework remains susceptible to selection bias and cross-unit interference from unaccounted spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel causal framework that integrates propensity score matching with competitive isolation to enable platform-level effect measurement (e.g., order volume, GMV) instead of item-level metrics in search systems.\n  Our approach provides theoretically guaranteed unbiased estimation under mutual exclusion conditions, with an open dataset released to support reproducible research on marketplace interference (github.com/xxxx). Extensive experiments demonstrate significant reductions in interference effects and estimation variance compared to baseline methods. Successful deployment in a large-scale marketplace confirms the framework's practical utility for platform-level causal inference.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01329", "pdf_url": "https://arxiv.org/pdf/2511.01329.pdf", "is_interesting": false}, "644": {"title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "authors": ["Giuseppe Riva, Brenda K. Wiederhold, Fabrizia Mantovani"], "abstract": "arXiv:2511.01363v1 Announce Type: new \nThe cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01363", "pdf_url": "https://arxiv.org/pdf/2511.01363.pdf", "is_interesting": false}, "645": {"title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "authors": ["Hamin Koo, Minseon Kim, Jaehyung Kim"], "abstract": "arXiv:2511.01375v1 Announce Type: new \nIdentifying the vulnerabilities of large language models (LLMs) is crucial for improving their safety by addressing inherent weaknesses. Jailbreaks, in which adversaries bypass safeguards with crafted input prompts, play a central role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors. Recent optimization-based jailbreak approaches iteratively refine attack prompts by leveraging LLMs. However, they often rely heavily on either binary attack success rate (ASR) signals, which are sparse, or manually crafted scoring templates, which introduce human bias and uncertainty in the scoring outcomes. To address these limitations, we introduce AMIS (Align to MISalign), a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through a bi-level structure. In the inner loop, prompts are refined using fine-grained and dense feedback using a fixed scoring template. In the outer loop, the template is optimized using an ASR alignment score, gradually evolving to better reflect true attack outcomes across queries. This co-optimization process yields progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors demonstrate that AMIS achieves state-of-the-art performance, including 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming existing baselines by substantial margins.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01375", "pdf_url": "https://arxiv.org/pdf/2511.01375.pdf", "is_interesting": false}, "646": {"title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "authors": ["Cl\\'ement Yvernes (APTIKAL), Emilie Devijver (APTIKAL), Ad\\`ele H. Ribeiro (IECL), Marianne Clausel--Lesourd (IECL), \\'Eric Gaussier (LIG, APTIKAL)"], "abstract": "arXiv:2511.01396v1 Announce Type: new \nCluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes represent clusters of variables, and edges encode both cluster-level causal relationships and dependencies arisen from unobserved confounding. C-DAGs define an equivalence class of acyclic causal graphs that agree on cluster-level relationships, enabling causal reasoning at a higher level of abstraction. However, when the chosen clustering induces cycles in the resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG semantics. In this work, we extend the C-DAG framework to support arbitrary variable clusterings by relaxing the partition admissibility constraint, thereby allowing cyclic C-DAG representations. We extend the notions of d-separation and causal calculus to this setting, significantly broadening the scope of causal reasoning across clusters and enabling the application of C-DAGs in previously intractable scenarios. Our calculus is both sound and atomically complete with respect to the do-calculus: all valid interventional queries at the cluster level can be derived using our rules, each corresponding to a primitive do-calculus step.", "categories": ["cs.AI", "stat.ME"], "abs_url": "https://arxiv.org/abs/2511.01396", "pdf_url": "https://arxiv.org/pdf/2511.01396.pdf", "is_interesting": false}, "647": {"title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "authors": ["Amrapali Pednekar, \\'Alvaro Garrido-P\\'erez, Yara Khaluf, Pieter Simoens"], "abstract": "arXiv:2511.01415v1 Announce Type: new \nThis study explores the interference in temporal processing within a dual-task paradigm from an artificial intelligence (AI) perspective. In this context, the dual-task setup is implemented as a simplified version of the Overcooked environment with two variations, single task (T) and dual task (T+N). Both variations involve an embedded time production task, but the dual task (T+N) additionally involves a concurrent number comparison task. Two deep reinforcement learning (DRL) agents were separately trained for each of these tasks. These agents exhibited emergent behavior consistent with human timing research. Specifically, the dual task (T+N) agent exhibited significant overproduction of time relative to its single task (T) counterpart. This result was consistent across four target durations. Preliminary analysis of neural dynamics in the agents' LSTM layers did not reveal any clear evidence of a dedicated or intrinsic timer. Hence, further investigation is needed to better understand the underlying time-keeping mechanisms of the agents and to provide insights into the observed behavioral patterns. This study is a small step towards exploring parallels between emergent DRL behavior and behavior observed in biological systems in order to facilitate a better understanding of both.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01415", "pdf_url": "https://arxiv.org/pdf/2511.01415.pdf", "is_interesting": false}, "648": {"title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "authors": ["Huiting Huang, Tieliang Gong, Kai He, Jialun Wu, Erik Cambria, Mengling Feng"], "abstract": "arXiv:2511.01444v1 Announce Type: new \nMultimodal sentiment analysis has received significant attention across diverse research domains. Despite advancements in algorithm design, existing approaches suffer from two critical limitations: insufficient learning of noise-contaminated unimodal data, leading to corrupted cross-modal interactions, and inadequate fusion of multimodal representations, resulting in discarding discriminative unimodal information while retaining multimodal redundant information. To address these challenges, this paper proposes a Double Information Bottleneck (DIB) strategy to obtain a powerful, unified compact multimodal representation. Implemented within the framework of low-rank Renyi's entropy functional, DIB offers enhanced robustness against diverse noise sources and computational tractability for high-dimensional data, as compared to the conventional Shannon entropy-based methods. The DIB comprises two key modules: 1) learning a sufficient and compressed representation of individual unimodal data by maximizing the task-relevant information and discarding the superfluous information, and 2) ensuring the discriminative ability of multimodal representation through a novel attention bottleneck fusion mechanism. Consequently, DIB yields a multimodal representation that effectively filters out noisy information from unimodal data while capturing inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI, CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI respectively.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01444", "pdf_url": "https://arxiv.org/pdf/2511.01444.pdf", "is_interesting": false}, "649": {"title": "From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation", "authors": ["ChengZhang Yu, YingRu He, Hongyan Cheng, nuo Cheng, Zhixing Liu, Dongxu Mu, Zhangrui Shen, Zhanpeng Jin"], "abstract": "arXiv:2511.01445v1 Announce Type: new \nGlobal healthcare systems face critical challenges from increasing patient volumes and limited consultation times, with primary care visits averaging under 5 minutes in many countries. While pre-consultation processes encompassing triage and structured history-taking offer potential solutions, they remain limited by passive interaction paradigms and context management challenges in existing AI systems. This study introduces a hierarchical multi-agent framework that transforms passive medical AI systems into proactive inquiry agents through autonomous task orchestration. We developed an eight-agent architecture with centralized control mechanisms that decomposes pre-consultation into four primary tasks: Triage ($T_1$), History of Present Illness collection ($T_2$), Past History collection ($T_3$), and Chief Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13 domain-specific subtasks. Evaluated on 1,372 validated electronic health records from a Chinese medical platform across multiple foundation models (GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for primary department triage and 80.5% for secondary department classification, with task completion rates reaching 98.2% using agent-driven scheduling versus 93.1% with sequential processing. Clinical quality scores from 18 physicians averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and 4.69 for Past History on a 5-point scale, with consultations completed within 12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic architecture maintained high performance across different foundation models while preserving data privacy through local deployment, demonstrating the potential for autonomous AI systems to enhance pre-consultation efficiency and quality in clinical settings.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01445", "pdf_url": "https://arxiv.org/pdf/2511.01445.pdf", "is_interesting": false}, "650": {"title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "authors": ["Hanwen Xu, Xuyao Huang, Yuzhe Liu, Kai Yu, Zhijie Deng"], "abstract": "arXiv:2511.01527v1 Announce Type: new \nLarge language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available https://github.com/hanwenxu1/mcp-agent.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01527", "pdf_url": "https://arxiv.org/pdf/2511.01527.pdf", "is_interesting": false}, "651": {"title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "authors": ["Ujjwal Sharma, Stevan Rudinac, Ana Mi\\'ckovi\\'c, Willemijn van Dolen, Marcel Worring"], "abstract": "arXiv:2511.01550v1 Announce Type: new \nIn this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate messaging on platforms such as X (formerly Twitter), we employ an ensemble of large language models (LLMs) to annotate a large corpus of corporate tweets on their topical alignment with the 17 Sustainable Development Goals (SDGs). This approach avoids the need for costly, task-specific annotations and explores the potential of such models as ad-hoc annotators for social media data that can efficiently capture both explicit and implicit references to sustainability themes in a scalable manner. Complementing this textual analysis, we utilize vision-language models (VLMs), within a visual understanding framework that uses semantic clusters to uncover patterns in visual sustainability communication. This integrated approach reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, environmental, social, governance (ESG) risks, and consumer engagement. Our methods-automatic label generation and semantic visual clustering-are broadly applicable to other domains and offer a flexible framework for large-scale social media analysis.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01550", "pdf_url": "https://arxiv.org/pdf/2511.01550.pdf", "is_interesting": false}, "652": {"title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "authors": ["Chengzhang Yu, Zening Lu, Chenyang Zheng, Chiyue Wang, Yiming Zhang, Zhanpeng Jin"], "abstract": "arXiv:2511.01581v1 Announce Type: new \nLarge language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. We design a differentiable two-stage retrieval mechanism with efficient coarse-grained filtering via product key decomposition (reducing complexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot |I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training. Inspired by dual-system cognitive theory, we partition knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained through Exponential Moving Average updates for stability. ExplicitLM achieves up to 43.67% improvement on knowledge-intensive tasks versus standard Transformers, with 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows strong correlations between memory retrieval and performance, with correct predictions achieving 49% higher hit rates. Unlike RAG systems with frozen retrieval, our jointly optimized architecture demonstrates that interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01581", "pdf_url": "https://arxiv.org/pdf/2511.01581.pdf", "is_interesting": false}, "653": {"title": "IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization", "authors": ["Sicheng Wang, Shuhao Chen, Jingran Zhou, Chengyi Tu"], "abstract": "arXiv:2511.01639v1 Announce Type: new \nGlobal food trade plays a crucial role in ensuring food security and maintaining supply chain stability. However, its network structure evolves dynamically under the influence of geopolitical, economic, and environmental factors, making it challenging to model and predict future trade links. Effectively capturing temporal patterns in food trade networks is therefore essential for improving the accuracy and robustness of link prediction. This study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed to model evolving trade structures and predict future links in global food trade networks. To the best of our knowledge, this is the first work to apply dynamic graph neural networks to this domain, significantly enhancing predictive performance. Building upon the original IVGAE framework, the proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture the temporal evolution of trade networks, jointly modeling short-term fluctuations and long-term structural dependencies. A momentum-based structural memory mechanism further improves predictive stability and performance. In addition, Bayesian optimization is used to automatically tune key hyperparameters, enhancing generalization across diverse trade scenarios. Extensive experiments on five crop-specific datasets demonstrate that IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic baselines by effectively modeling temporal dependencies, while Bayesian optimization further boosts performance in IVGAE-TAMA-BO. These results highlight the proposed framework as a robust and scalable solution for structural prediction in global trade networks, with strong potential for applications in food security monitoring and policy decision support.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01639", "pdf_url": "https://arxiv.org/pdf/2511.01639.pdf", "is_interesting": false}, "654": {"title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "authors": ["Yueqing Xi, Yifan Bai, Huasen Luo, Weiliang Wen, Hui Liu, Haoliang Li"], "abstract": "arXiv:2511.01668v1 Announce Type: new \nAs artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01668", "pdf_url": "https://arxiv.org/pdf/2511.01668.pdf", "is_interesting": false}, "655": {"title": "Simulating Environments with Reasoning Models for Agent Training", "authors": ["Yuetai Li, Huseyin A Inan, Xiang Yue, Wei-Ning Chen, Lukas Wutschitz, Janardhan Kulkarni, Radha Poovendran, Robert Sim, Saravan Rajmohan"], "abstract": "arXiv:2511.01824v1 Announce Type: new \nLLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01824", "pdf_url": "https://arxiv.org/pdf/2511.01824.pdf", "is_interesting": false}, "656": {"title": "A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering", "authors": ["Alexandre Valentin Jamet, Georgios Vavouliotis, Daniel A. Jim\\'enez, Lluc Alvarez, Marc Casas"], "abstract": "arXiv:2403.15181v1 Announce Type: cross \nTo alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D). TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP). FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component. The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features. TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multi-level perceptron hardware approach. TLP only requires 7KB of storage. To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads. Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions. As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively. In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic.", "categories": ["cs.AR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2403.15181", "pdf_url": "https://arxiv.org/pdf/2403.15181.pdf", "is_interesting": false}, "657": {"title": "Sorting by Strip Swaps is NP-Hard", "authors": ["Swapnoneel Roy, Asai Asaithambi, Debajyoti Mukhopadhyay"], "abstract": "arXiv:2511.00015v1 Announce Type: cross \nWe show that \\emph{Sorting by Strip Swaps} (SbSS) is NP-hard by a polynomial reduction of \\emph{Block Sorting}. The key idea is a local gadget, a \\emph{cage}, that replaces every decreasing adjacency $(a_i,a_{i+1})$ by a guarded triple $a_i,m_i,a_{i+1}$ enclosed by guards $L_i,U_i$, so the only decreasing adjacencies are the two inside the cage. Small \\emph{hinge} gadgets couple adjacent cages that share an element and enforce that a strip swap that removes exactly two adjacencies corresponds bijectively to a block move that removes exactly one decreasing adjacency in the source permutation. This yields a clean equivalence between exact SbSS schedules and perfect block schedules, establishing NP-hardness.", "categories": ["cs.DS", "cs.AI", "cs.CC"], "abs_url": "https://arxiv.org/abs/2511.00015", "pdf_url": "https://arxiv.org/pdf/2511.00015.pdf", "is_interesting": false}, "658": {"title": "Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model", "authors": ["Haotian Hang, Yueyang Shen, Vicky Zhu, Jose Cruz, Michelle Li"], "abstract": "arXiv:2511.00024v1 Announce Type: cross \nIn the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "abs_url": "https://arxiv.org/abs/2511.00024", "pdf_url": "https://arxiv.org/pdf/2511.00024.pdf", "is_interesting": false}, "659": {"title": "Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?", "authors": ["Josu Eguiluz Casta\\~neira, Axel Brando, Migle Laukyte, Marc Serra-Vidal"], "abstract": "arXiv:2511.00027v1 Announce Type: cross \nArtificial intelligence (AI) now permeates critical infrastructures and decision-making systems where failures produce social, economic, and democratic harm. This position paper challenges the entrenched belief that regulation and innovation are opposites. As evidenced by analogies from aviation, pharmaceuticals, and welfare systems and recent cases of synthetic misinformation, bias and unaccountable decision-making, the absence of well-designed regulation has already created immeasurable damage. Regulation, when thoughtful and adaptive, is not a brake on innovation -- it is its foundation. The present position paper examines the EU AI Act as a model of risk-based, responsibility-driven regulation that addresses the Collingridge Dilemma: acting early enough to prevent harm, yet flexibly enough to sustain innovation. Its adaptive mechanisms -- regulatory sandboxes, small and medium enterprises (SMEs) support, real-world testing, fundamental rights impact assessment (FRIA) -- demonstrate how regulation can accelerate responsibly, rather than delay, technological progress. The position paper summarises how governance tools transform perceived burdens into tangible advantages: legal certainty, consumer trust, and ethical competitiveness. Ultimately, the paper reframes progress: innovation and regulation advance together. By embedding transparency, impact assessments, accountability, and AI literacy into design and deployment, the EU framework defines what responsible innovation truly means -- technological ambition disciplined by democratic values and fundamental rights.", "categories": ["cs.CY", "cs.AI", "cs.LG", "stat.AP"], "abs_url": "https://arxiv.org/abs/2511.00027", "pdf_url": "https://arxiv.org/pdf/2511.00027.pdf", "is_interesting": false}, "660": {"title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts", "authors": ["Samaksh Bhargav, Zining Zhu"], "abstract": "arXiv:2511.00029v1 Announce Type: cross \nLarge Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00029", "pdf_url": "https://arxiv.org/pdf/2511.00029.pdf", "is_interesting": false}, "661": {"title": "Probing Knowledge Holes in Unlearned LLMs", "authors": ["Myeongseob Ko, Hoang Anh Just, Charles Fleming, Ming Jin, Ruoxi Jia"], "abstract": "arXiv:2511.00030v1 Announce Type: cross \nMachine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00030", "pdf_url": "https://arxiv.org/pdf/2511.00030.pdf", "is_interesting": false}, "662": {"title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators", "authors": ["Lei Liu, Zhongyi Yu, Hong Wang, Huanshuo Dong, Haiyang Xin, Hongwei Zhao, Bin Li"], "abstract": "arXiv:2511.00032v2 Announce Type: cross \nIn recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs). However, their application to large-scale engineering tasks suffers from significant computational overhead. And the fact that current models impose a uniform computational cost while physical fields exhibit vastly different complexities constitutes a fundamental mismatch, which is the root of this inefficiency. For instance, in turbulence flows, intricate vortex regions require deeper network processing compared to stable flows. To address this, we introduce a framework: Skip-Block Routing (SBR), a general framework designed for Transformer-based neural operators, capable of being integrated into their multi-layer architectures. First, SBR uses a routing mechanism to learn the complexity and ranking of tokens, which is then applied during inference. Then, in later layers, it decides how many tokens are passed forward based on this ranking. This way, the model focuses more processing capacity on the tokens that are more complex. Experiments demonstrate that SBR is a general framework that seamlessly integrates into various neural operators. Our method reduces computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while still delivering up to 2x faster inference without sacrificing accuracy.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00032", "pdf_url": "https://arxiv.org/pdf/2511.00032.pdf", "is_interesting": false}, "663": {"title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "authors": ["Diqi He, Xuehao Gao, Hao Li, Junwei Han, Dingwen Zhang"], "abstract": "arXiv:2511.00033v1 Announce Type: cross \nThe Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00033", "pdf_url": "https://arxiv.org/pdf/2511.00033.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on vision-and-language navigation in continuous environments, addressing the challenges of zero-shot navigation and task alignment. While it involves spatial decision-making and navigation, it is more related to general navigation tasks rather than autonomous driving specifically."}}, "664": {"title": "Semi-Supervised Preference Optimization with Limited Feedback", "authors": ["Seonggyun Lee, Sungjun Lim, Seojin Park, Soeun Cheon, Kyungwoo Song"], "abstract": "arXiv:2511.00040v1 Announce Type: cross \nThe field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00040", "pdf_url": "https://arxiv.org/pdf/2511.00040.pdf", "is_interesting": false}, "665": {"title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "authors": ["Yingzhao Jian, Zhongan Wang, Yi Yang, Hehe Fan"], "abstract": "arXiv:2511.00041v1 Announce Type: cross \nHumanoid agents often struggle to handle flexible and diverse interactions in open environments. A common solution is to collect massive datasets to train a highly capable model, but this approach can be prohibitively expensive. In this paper, we explore an alternative solution: empowering off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents, thereby leveraging their strong open-world generalization to mitigate the need for extensive data collection. To this end, we present \\textbf{BiBo} (\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf VLMs). It consists of two key components: (1) an \\textbf{embodied instruction compiler}, which enables the VLM to perceive the environment and precisely translate high-level user instructions (e.g., {\\small\\itshape ``have a rest''}) into low-level primitive commands with control parameters (e.g., {\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and (2) a diffusion-based \\textbf{motion executor}, which generates human-like motions from these commands, while dynamically adapting to physical feedback from the environment. In this way, BiBo is capable of handling not only basic interactions but also diverse and complex motions. Experiments demonstrate that BiBo achieves an interaction task success rate of 90.2\\% in open environments, and improves the precision of text-guided motion execution by 16.3\\% over prior methods. The code will be made publicly available.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00041", "pdf_url": "https://arxiv.org/pdf/2511.00041.pdf", "is_interesting": false}, "666": {"title": "DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection", "authors": ["Omkar Kulkarni, Rohitash Chandra"], "abstract": "arXiv:2511.00047v1 Announce Type: cross \nFinancial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.", "categories": ["cs.LG", "cs.AI", "cs.CE"], "abs_url": "https://arxiv.org/abs/2511.00047", "pdf_url": "https://arxiv.org/pdf/2511.00047.pdf", "is_interesting": false}, "667": {"title": "Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting", "authors": ["Yao Liu"], "abstract": "arXiv:2511.00049v1 Announce Type: cross \nAccurate and robust weather forecasting remains a fundamental challenge due to the inherent spatio-temporal complexity of atmospheric systems. In this paper, we propose a novel self-supervised learning framework that leverages spatio-temporal structures to improve multi-variable weather prediction. The model integrates a graph neural network (GNN) for spatial reasoning, a self-supervised pretraining scheme for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis datasets demonstrate that our approach achieves superior performance compared to traditional numerical weather prediction (NWP) models and recent deep learning methods. Quantitative evaluations and visual analyses in Beijing and Shanghai confirm the model's capability to capture fine-grained meteorological patterns. The proposed framework provides a scalable and label-efficient solution for future data-driven weather forecasting systems.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00049", "pdf_url": "https://arxiv.org/pdf/2511.00049.pdf", "is_interesting": false}, "668": {"title": "FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs", "authors": ["Dhananjaya Gowda, Seoha Song, Junhyun Lee, Harshith Goka"], "abstract": "arXiv:2511.00050v1 Announce Type: cross \nAs the large language models (LLMs) grow in size each day, efficient training and fine-tuning has never been as important as nowadays. This resulted in the great interest in parameter efficient fine-tuning (PEFT), and effective methods including low-rank adapters (LoRA) has emerged. Although the various PEFT methods have been studied extensively in the recent years, the greater part of the subject remains unexplored with the huge degree of freedom. In this paper, we propose FLoRA, a family of fused forward-backward adapters (FFBA) for parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine ideas from the popular LoRA and parallel adapters to improve the overall fine-tuning accuracies. At the same time, latencies are minimized by fusing the forward and backward adapters into existing projection layers of the base model. Experimental results show that the proposed FFB adapters perform significantly better than the popularly used LoRA in both accuracy and latency for a similar parameter budget.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00050", "pdf_url": "https://arxiv.org/pdf/2511.00050.pdf", "is_interesting": false}, "669": {"title": "Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT", "authors": ["Da Chang, Peng Xue, Yu Li, Yongxiang Liu, Pengxiang Xu, Shixun Zhang"], "abstract": "arXiv:2511.00051v1 Announce Type: cross \nParameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \\textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \\textbf{S}kewed \\textbf{O}rthogonal \\textbf{R}otation \\textbf{A}daptation (\\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at https://github.com/MaeChd/SORA.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00051", "pdf_url": "https://arxiv.org/pdf/2511.00051.pdf", "is_interesting": false}, "670": {"title": "Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models", "authors": ["Hao Wang, Licheng Pan, Yuan Lu, Zhichao Chen, Tianqiao Liu, Shuting He, Zhixuan Chu, Qingsong Wen, Haoxuan Li, Zhouchen Lin"], "abstract": "arXiv:2511.00053v1 Announce Type: cross \nThe design of training objective is central to training time-series forecasting models. Existing training objectives such as mean squared error mostly treat each future step as an independent, equally weighted task, which we found leading to the following two issues: (1) overlook the label autocorrelation effect among future steps, leading to biased training objective; (2) fail to set heterogeneous task weights for different forecasting tasks corresponding to varying future steps, limiting the forecasting performance. To fill this gap, we propose a novel quadratic-form weighted training objective, addressing both of the issues simultaneously. Specifically, the off-diagonal elements of the weighting matrix account for the label autocorrelation effect, whereas the non-uniform diagonals are expected to match the most preferable weights of the forecasting tasks with varying future steps. To achieve this, we propose a Quadratic Direct Forecast (QDF) learning algorithm, which trains the forecast model using the adaptively updated quadratic-form weighting matrix. Experiments show that our QDF effectively improves performance of various forecast models, achieving state-of-the-art results. Code is available at https://anonymous.4open.science/r/QDF-8937.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00053", "pdf_url": "https://arxiv.org/pdf/2511.00053.pdf", "is_interesting": false}, "671": {"title": "SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation", "authors": ["Gio Huh, Dhruv Sheth, Rayhan Zirvi, Frank Xiao"], "abstract": "arXiv:2511.00054v1 Announce Type: cross \nWhile Vision-Language Models (VLMs) excel in many areas, they struggle with complex spatial reasoning, which requires problem decomposition and strategic tool use. Fine-tuning smaller, more deployable models offers an efficient path to strong performance, but this is hampered by a major bottleneck: the absence of high-quality, step-by-step reasoning data. To address this data-efficiency gap, we introduce SpatialTraceGen, a framework to distill the reasoning processes of a large teacher model into a high-quality dataset of multi-hop, multi-tool reasoning traces. A key innovation is our automated Verifier, which scalably ensures the fidelity of each reasoning step, providing a cost-effective alternative to manual human annotation. On the CLEVR-Humans benchmark, this verifier-guided process improves the average quality score of traces by 17\\% while reducing quality variance by over 40\\%. SpatialTraceGen delivers a dataset of expert traces, providing the structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00054", "pdf_url": "https://arxiv.org/pdf/2511.00054.pdf", "is_interesting": false}, "672": {"title": "Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches", "authors": ["Leonhard Duda, Khadijeh Alibabaei, Elena Vollmer, Leon Klug, Valentin Kozlov, Lisana Berberi, Mishal Benz, Rebekka Volk, Juan Pedro Guti\\'errez Hermosillo Muriedas, Markus G\\\"otz, Judith S\\'a\\'inz-Pardo D\\'iaz, \\'Alvaro L\\'opez Garc\\'ia, Frank Schultmann, Achim Streit"], "abstract": "arXiv:2511.00055v2 Announce Type: cross \nFederated Learning (FL) is an approach for training a shared Machine Learning (ML) model with distributed training data and multiple participants. FL allows bypassing limitations of the traditional Centralized Machine Learning CL if data cannot be shared or stored centrally due to privacy or technical restrictions -- the participants train the model locally with their training data and do not need to share it among the other participants. This paper investigates the practical implementation and effectiveness of FL in a real-world scenario, specifically focusing on unmanned aerial vehicle (UAV)-based thermal images for common thermal feature detection in urban environments. The distributed nature of the data arises naturally and makes it suitable for FL applications, as images captured in two German cities are available. This application presents unique challenges due to non-identical distribution and feature characteristics of data captured at both locations. The study makes several key contributions by evaluating FL algorithms in real deployment scenarios rather than simulation. We compare several FL approaches with a centralized learning baseline across key performance metrics such as model accuracy, training time, communication overhead, and energy usage. This paper also explores various FL workflows, comparing client-controlled workflows and server-controlled workflows. The findings of this work serve as a valuable reference for understanding the practical application and limitations of the FL methods in segmentation tasks in UAV-based imaging.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00055", "pdf_url": "https://arxiv.org/pdf/2511.00055.pdf", "is_interesting": false}, "673": {"title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling", "authors": ["Yuxi Liu, Renjia Deng, Yutong He, Xue Wang, Tao Yao, Kun Yuan"], "abstract": "arXiv:2511.00056v1 Announce Type: cross \nThe substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00056", "pdf_url": "https://arxiv.org/pdf/2511.00056.pdf", "is_interesting": false}, "674": {"title": "Automatically Finding Rule-Based Neurons in OthelloGPT", "authors": ["Aditya Singh, Zihang Wen, Srujananjali Medicherla, Adam Karvonen, Can Rager"], "abstract": "arXiv:2511.00059v1 Announce Type: cross \nOthelloGPT, a transformer trained to predict valid moves in Othello, provides an ideal testbed for interpretability research. The model is complex enough to exhibit rich computational patterns, yet grounded in rule-based game logic that enables meaningful reverse-engineering. We present an automated approach based on decision trees to identify and interpret MLP neurons that encode rule-based game logic. Our method trains regression decision trees to map board states to neuron activations, then extracts decision paths where neurons are highly active to convert them into human-readable logical forms. These descriptions reveal highly interpretable patterns; for instance, neurons that specifically detect when diagonal moves become legal. Our findings suggest that roughly half of the neurons in layer 5 can be accurately described by compact, rule-based decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder likely participate in more distributed or non-rule-based computations. We verify the causal relevance of patterns identified by our decision trees through targeted interventions. For a specific square, for specific game patterns, we ablate neurons corresponding to those patterns and find an approximately 5-10 fold stronger degradation in the model's ability to predict legal moves along those patterns compared to control patterns. To facilitate future work, we provide a Python tool that maps rule-based game behaviors to their implementing neurons, serving as a resource for researchers to test whether their interpretability methods recover meaningful computational structures.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00059", "pdf_url": "https://arxiv.org/pdf/2511.00059.pdf", "is_interesting": false}, "675": {"title": "Aligning Brain Signals with Multimodal Speech and Vision Embeddings", "authors": ["Kateryna Shapovalenko, Quentin Auster"], "abstract": "arXiv:2511.00065v1 Announce Type: cross \nWhen we hear the word \"house\", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00065", "pdf_url": "https://arxiv.org/pdf/2511.00065.pdf", "is_interesting": false}, "676": {"title": "Latent Domain Prompt Learning for Vision-Language Models", "authors": ["Zhixing Li, Arsham Gholamzadeh Khoee, Yinan Yu"], "abstract": "arXiv:2511.00067v1 Announce Type: cross \nThe objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead study the DG setting where models must generalize well without access to explicit domain labels. Our key idea is to represent an unseen target domain as a combination of latent domains automatically discovered from training data, enabling the model to adaptively transfer knowledge across domains. To realize this, we perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between the input image and each latent domain. Experiments on four benchmarks show that this strategy yields consistent gains over VLM-based baselines and provides new insights into improving robustness under domain shift.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00067", "pdf_url": "https://arxiv.org/pdf/2511.00067.pdf", "is_interesting": false}, "677": {"title": "Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design", "authors": ["Muhammad Bilal Awan, Abdul Razzaq, Abdul Shahid"], "abstract": "arXiv:2511.00070v1 Announce Type: cross \nThis paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00070", "pdf_url": "https://arxiv.org/pdf/2511.00070.pdf", "is_interesting": false}, "678": {"title": "RailEstate: An Interactive System for Metro Linked Property Trends", "authors": ["Chen-Wei Chang, Yu-Chieh Cheng, Yun-En Tsai, Fanglan Chen, Chang-Tien Lu"], "abstract": "arXiv:2511.00078v1 Announce Type: cross \nAccess to metro systems plays a critical role in shaping urban housing markets by enhancing neighborhood accessibility and driving property demand. We present RailEstate, a novel web based system that integrates spatial analytics, natural language interfaces, and interactive forecasting to analyze how proximity to metro stations influences residential property prices in the Washington metropolitan area. Unlike static mapping tools or generic listing platforms, RailEstate combines 25 years of historical housing data with transit infrastructure to support low latency geospatial queries, time series visualizations, and predictive modeling. Users can interactively explore ZIP code level price patterns, investigate long term trends, and forecast future housing values around any metro station. A key innovation is our natural language chatbot, which translates plain-English questions e.g., What is the highest price in Falls Church in the year 2000? into executable SQL over a spatial database. This unified and interactive platform empowers urban planners, investors, and residents to derive actionable insights from metro linked housing data without requiring technical expertise.", "categories": ["cs.CY", "cs.AI", "cs.DB"], "abs_url": "https://arxiv.org/abs/2511.00078", "pdf_url": "https://arxiv.org/pdf/2511.00078.pdf", "is_interesting": false}, "679": {"title": "Fixed-point graph convolutional networks against adversarial attacks", "authors": ["Shakib Khan, A. Ben Hamza, Amr Youssef"], "abstract": "arXiv:2511.00083v1 Announce Type: cross \nAdversarial attacks present a significant risk to the integrity and performance of graph neural networks, particularly in tasks where graph structure and node features are vulnerable to manipulation. In this paper, we present a novel model, called fixed-point iterative graph convolutional network (Fix-GCN), which achieves robustness against adversarial perturbations by effectively capturing higher-order node neighborhood information in the graph without additional memory or computational complexity. Specifically, we introduce a versatile spectral modulation filter and derive the feature propagation rule of our model using fixed-point iteration. Unlike traditional defense mechanisms that rely on additional design elements to counteract attacks, the proposed graph filter provides a flexible-pass filtering approach, allowing it to selectively attenuate high-frequency components while preserving low-frequency structural information in the graph signal. By iteratively updating node representations, our model offers a flexible and efficient framework for preserving essential graph information while mitigating the impact of adversarial manipulation. We demonstrate the effectiveness of the proposed model through extensive experiments on various benchmark graph datasets, showcasing its resilience against adversarial attacks.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00083", "pdf_url": "https://arxiv.org/pdf/2511.00083.pdf", "is_interesting": false}, "680": {"title": "Application of predictive machine learning in pen & paper RPG game design", "authors": ["Jolanta \\'Sliwa"], "abstract": "arXiv:2511.00084v1 Announce Type: cross \nIn recent years, the pen and paper RPG market has experienced significant growth. As a result, companies are increasingly exploring the integration of AI technologies to enhance player experience and gain a competitive edge.\n  One of the key challenges faced by publishers is designing new opponents and estimating their challenge level. Currently, there are no automated methods for determining a monster's level; the only approaches used are based on manual testing and expert evaluation. Although these manual methods can provide reasonably accurate estimates, they are time-consuming and resource-intensive.\n  Level prediction can be approached using ordinal regression techniques. This thesis presents an overview and evaluation of state-of-the-art methods for this task. It also details the construction of a dedicated dataset for level estimation. Furthermore, a human-inspired model was developed to serve as a benchmark, allowing comparison between machine learning algorithms and the approach typically employed by pen and paper RPG publishers. In addition, a specialized evaluation procedure, grounded in domain knowledge, was designed to assess model performance and facilitate meaningful comparisons.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00084", "pdf_url": "https://arxiv.org/pdf/2511.00084.pdf", "is_interesting": false}, "681": {"title": "MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning", "authors": ["Peilin Tan, Chuanqi Shi, Dian Tu, Liang Xie"], "abstract": "arXiv:2511.00085v1 Announce Type: cross \nStock trend prediction is crucial for profitable trading strategies and portfolio management yet remains challenging due to market volatility, complex temporal dynamics and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships. This work introduces MaGNet, a novel Mamba dual-hyperGraph Network for stock prediction, integrating three key innovations: (1) a MAGE block, which leverages bidirectional Mamba with adaptive gating mechanisms for contextual temporal modeling and integrates a sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market conditions, alongside multi-head attention for capturing global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable precise fusion of multivariate features and cross-stock dependencies, effectively enhancing informativeness while preserving intrinsic data structures, bridging temporal modeling with relational reasoning; and (3) a dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH) that captures fine-grained causal dependencies with temporal constraints, and Global Probabilistic Hypergraph (GPH) that models market-wide patterns through soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism, jointly disentangling localized temporal influences from instantaneous global structures for multi-scale relational learning. Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities. Codes available at: https://github.com/PeilinTime/MaGNet.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00085", "pdf_url": "https://arxiv.org/pdf/2511.00085.pdf", "is_interesting": false}, "682": {"title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "authors": ["Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao, Zhiwei Zhang, Xianfeng Tang, Hui Liu, Qi He, Suhang Wang"], "abstract": "arXiv:2511.00086v1 Announce Type: cross \nTest-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00086", "pdf_url": "https://arxiv.org/pdf/2511.00086.pdf", "is_interesting": false}, "683": {"title": "Adding New Capability in Existing Scientific Application with LLM Assistance", "authors": ["Anshu Dubey, Akash Dhruv"], "abstract": "arXiv:2511.00087v1 Announce Type: cross \nWith the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an important research topic. Many efforts are underway and literature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training dataset would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code-translation tool, Code-Scribe, for new code generation.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00087", "pdf_url": "https://arxiv.org/pdf/2511.00087.pdf", "is_interesting": false}, "684": {"title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "authors": ["NVIDIA,  :, Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, Liang Feng, Greg Heinrich, Jack Huang, Peter Karkus, Boyi Li, Pinyi Li, Tsung-Yi Lin, Dongran Liu, Ming-Yu Liu, Langechuan Liu, Zhijian Liu, Jason Lu, Yunxiang Mao, Pavlo Molchanov, Lindsey Pavao, Zhenghao Peng, Mike Ranzinger, Ed Schmerling, Shida Shen, Yunfei Shi, Sarah Tariq, Ran Tian, Tilman Wekel, Xinshuo Weng, Tianjun Xiao, Eric Yang, Xiaodong Yang, Yurong You, Xiaohui Zeng, Wenyuan Zhang, Boris Ivanovic, Marco Pavone"], "abstract": "arXiv:2511.00088v1 Announce Type: cross \nEnd-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00088", "pdf_url": "https://arxiv.org/pdf/2511.00088.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "\u51b3\u7b56/\u63a7\u5236 / \u8f68\u8ff9\u9884\u6d4b / \u7aef\u5230\u7aef\u63a7\u5236", "reason": "The paper discusses an end-to-end architecture for autonomous driving, focusing on trajectory planning and decision-making in complex driving scenarios. It integrates causal reasoning with trajectory prediction and real-time control, which are directly relevant to autonomous driving systems. The approach improves planning accuracy, reduces off-road and close encounter rates, and is tested in real-world urban deployment, making it highly relevant to autonomous driving research."}}, "685": {"title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments", "authors": ["Angelos Alexopoulos, Agorakis Bompotas, Nikitas Rigas Kalogeropoulos, Panagiotis Kechagias, Athanasios P. Kalogeras, Christos Alexakos"], "abstract": "arXiv:2511.00094v1 Announce Type: cross \nRobotic systems have become integral to smart environments, enabling applications ranging from urban surveillance and automated agriculture to industrial automation. However, their effective operation in dynamic settings - such as smart cities and precision farming - is challenged by continuously evolving topographies and environmental conditions. Traditional control systems often struggle to adapt quickly, leading to inefficiencies or operational failures. To address this limitation, we propose a novel framework for autonomous and dynamic reconfiguration of robotic controllers using Digital Twin technology. Our approach leverages a virtual replica of the robot's operational environment to simulate and optimize movement trajectories in response to real-world changes. By recalculating paths and control parameters in the Digital Twin and deploying the updated code to the physical robot, our method ensures rapid and reliable adaptation without manual intervention. This work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00094", "pdf_url": "https://arxiv.org/pdf/2511.00094.pdf", "is_interesting": false}, "686": {"title": "Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System", "authors": ["Shangyu Lou"], "abstract": "arXiv:2511.00096v1 Announce Type: cross \nUrban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by comparing multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, positioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:https://github.com/THETUREHOOHA/UrbanMAS", "categories": ["cs.MA", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.00096", "pdf_url": "https://arxiv.org/pdf/2511.00096.pdf", "is_interesting": false}, "687": {"title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation", "authors": ["Zihao Guo, Qingyun Sun, Ziwei Zhang, Haonan Yuan, Huiping Zhuang, Xingcheng Fu, Jianxin Li"], "abstract": "arXiv:2511.00097v1 Announce Type: cross \nGraph incremental learning (GIL), which continuously updates graph models by sequential knowledge acquisition, has garnered significant interest recently. However, existing GIL approaches focus on task-incremental and class-incremental scenarios within a single domain. Graph domain-incremental learning (Domain-IL), aiming at updating models across multiple graph domains, has become critical with the development of graph foundation models (GFMs), but remains unexplored in the literature. In this paper, we propose Graph Domain-Incremental Learning via Knowledge Dientanglement and Preservation (GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from the perspectives of embedding shifts and decision boundary deviations. Specifically, to prevent embedding shifts and confusion across incremental graph domains, we first propose the domain-specific parameter-efficient fine-tuning together with intra- and inter-domain disentanglement objectives. Consequently, to maintain a stable decision boundary, we introduce deviation-free knowledge preservation to continuously fit incremental domains. Additionally, for graphs with unobservable domains, we perform domain-aware distribution discrimination to obtain precise embeddings. Extensive experiments demonstrate the proposed GraphKeeper achieves state-of-the-art results with 6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover, we show GraphKeeper can be seamlessly integrated with various representative GFMs, highlighting its broad applicative potential.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00097", "pdf_url": "https://arxiv.org/pdf/2511.00097.pdf", "is_interesting": false}, "688": {"title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving", "authors": ["Yuchen Zhang, Hanyue Du, Chun Cao, Jingwei Xu"], "abstract": "arXiv:2511.00101v1 Announce Type: cross \nLow-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00101", "pdf_url": "https://arxiv.org/pdf/2511.00101.pdf", "is_interesting": false}, "689": {"title": "Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers", "authors": ["Vivan Doshi"], "abstract": "arXiv:2511.00102v1 Announce Type: cross \nThe discovery of conservation laws is a cornerstone of scientific progress. However, identifying these invariants from observational data remains a significant challenge. We propose a hybrid framework to automate the discovery of conserved quantities from noisy trajectory data. Our approach integrates three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that learns a continuous model of the system's dynamics, (2) a Transformer that generates symbolic candidate invariants conditioned on the learned vector field, and (3) a symbolic-numeric verifier that provides a strong numerical certificate for the validity of these candidates. We test our framework on canonical physical systems and show that it significantly outperforms baselines that operate directly on trajectory data. This work demonstrates the robustness of a decoupled learn-then-search approach for discovering mathematical principles from imperfect data.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00102", "pdf_url": "https://arxiv.org/pdf/2511.00102.pdf", "is_interesting": false}, "690": {"title": "Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges", "authors": ["Majid Memari, Krista Ruggles"], "abstract": "arXiv:2511.00105v1 Announce Type: cross \nArtificial intelligence (AI) is transforming elementary STEM education, yet evidence remains fragmented. This systematic review synthesizes 258 studies (2020-2025) examining AI applications across eight categories: intelligent tutoring systems (45% of studies), learning analytics (18%), automated assessment (12%), computer vision (8%), educational robotics (7%), multimodal sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content generation. The analysis shows that most studies focus on upper elementary grades (65%) and mathematics (38%), with limited cross-disciplinary STEM integration (15%). While conversational AI demonstrates moderate effectiveness (d = 0.45-0.70 where reported), only 34% of studies include standardized effect sizes. Eight major gaps limit real-world impact: fragmented ecosystems, developmental inappropriateness, infrastructure barriers, lack of privacy frameworks, weak STEM integration, equity disparities, teacher marginalization, and narrow assessment scopes. Geographic distribution is also uneven, with 90% of studies originating from North America, East Asia, and Europe. Future directions call for interoperable architectures that support authentic STEM integration, grade-appropriate design, privacy-preserving analytics, and teacher-centered implementations that enhance rather than replace human expertise.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00105", "pdf_url": "https://arxiv.org/pdf/2511.00105.pdf", "is_interesting": false}, "691": {"title": "Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies", "authors": ["Anuj Gupta, Ann Shivers-McNair"], "abstract": "arXiv:2511.00106v1 Announce Type: cross \nIn this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00106", "pdf_url": "https://arxiv.org/pdf/2511.00106.pdf", "is_interesting": false}, "692": {"title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence", "authors": ["Yi Zhang, Che Liu, Xiancong Ren, Hanchu Ni, Shuai Zhang, Zeyuan Ding, Jiayu Hu, Hanzhe Shan, Zhenwei Niu, Zhaoyang Liu, Yue Zhao, Junbo Qi, Qinfan Zhang, Dengjie Li, Yidong Wang, Jiachen Luo, Yong Dai, Jian Tang, Xiaozhu Ju"], "abstract": "arXiv:2511.00108v1 Announce Type: cross \nThis report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.00108", "pdf_url": "https://arxiv.org/pdf/2511.00108.pdf", "is_interesting": false}, "693": {"title": "Real-DRL: Teach and Learn in Reality", "authors": ["Yanbing Mao, Yihao Cai, Lui Sha"], "abstract": "arXiv:2511.00112v1 Announce Type: cross \nThis paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00112", "pdf_url": "https://arxiv.org/pdf/2511.00112.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u5f3a\u5316\u5b66\u4e60 / \u5b89\u5168\u6027\u5b66\u4e60", "reason": "The paper discusses the Real-DRL framework for safety-critical autonomous systems, focusing on deep reinforcement learning for action policies in real physical systems. While the methodology could be applied in the broader context of autonomous systems, the focus is not specifically on autonomous driving or related tasks such as perception, decision-making, or control, but rather on safety and learning in physical systems in general."}}, "694": {"title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "authors": ["Haoyuan Li, Yuanbo Tong, Yuchen Li, Zirui Wang, Chunhou Liu, Jiamou Liu"], "abstract": "arXiv:2511.00115v1 Announce Type: cross \nPersonality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00115", "pdf_url": "https://arxiv.org/pdf/2511.00115.pdf", "is_interesting": false}, "695": {"title": "LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers", "authors": ["Avisek Naug, Antonio Guillen, Vineet Kumar, Scott Greenwood, Wesley Brewer, Sahand Ghorbanpour, Ashwin Ramesh Babu, Vineet Gundecha, Ricardo Luna Gutierrez, Soumyendu Sarkar"], "abstract": "arXiv:2511.00116v1 Announce Type: cross \nLiquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00116", "pdf_url": "https://arxiv.org/pdf/2511.00116.pdf", "is_interesting": false}, "696": {"title": "DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads", "authors": ["Antonio Guillen-Perez, Avisek Naug, Vineet Gundecha, Sahand Ghorbanpour, Ricardo Luna Gutierrez, Ashwin Ramesh Babu, Munther Salim, Shubhanker Banerjee, Eoin H. Oude Essink, Damien Fay, Soumyendu Sarkar"], "abstract": "arXiv:2511.00117v1 Announce Type: cross \nThe increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00117", "pdf_url": "https://arxiv.org/pdf/2511.00117.pdf", "is_interesting": false}, "697": {"title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models", "authors": ["Sai Niranjan Ramachandran, Manish Krishan Lal, Suvrit Sra"], "abstract": "arXiv:2511.00124v1 Announce Type: cross \nWe analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00124", "pdf_url": "https://arxiv.org/pdf/2511.00124.pdf", "is_interesting": false}, "698": {"title": "Inferring multiple helper Dafny assertions with LLMs", "authors": ["\\'Alvaro Silva, Alexandra Mendes, Ruben Martins"], "abstract": "arXiv:2511.00125v1 Announce Type: cross \nThe Dafny verifier provides strong correctness guarantees but often requires numerous manual helper assertions, creating a significant barrier to adoption. We investigate the use of Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs, with a primary focus on cases involving multiple missing assertions. To support this study, we extend the DafnyBench benchmark with curated datasets where one, two, or all assertions are removed, and we introduce a taxonomy of assertion types to analyze inference difficulty. Our approach refines fault localization through a hybrid method that combines LLM predictions with error-message heuristics. We implement this approach in a new tool called DAISY (Dafny Assertion Inference SYstem). While our focus is on multiple missing assertions, we also evaluate DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. Notably, many programs can be verified with fewer assertions than originally present, highlighting that proofs often admit multiple valid repair strategies and that recovering every original assertion is unnecessary. These results demonstrate that automated assertion inference can substantially reduce proof engineering effort and represent a step toward more scalable and accessible formal verification.", "categories": ["cs.SE", "cs.AI", "cs.LO", "cs.PL"], "abs_url": "https://arxiv.org/abs/2511.00125", "pdf_url": "https://arxiv.org/pdf/2511.00125.pdf", "is_interesting": false}, "699": {"title": "Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features", "authors": ["Lu Bowen"], "abstract": "arXiv:2511.00126v1 Announce Type: cross \nRecent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing \"one-model-fits-all\" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis.\n  Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration.\n  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00126", "pdf_url": "https://arxiv.org/pdf/2511.00126.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u8f68\u8ff9\u9884\u6d4b", "reason": "The paper discusses dynamic model selection for trajectory prediction, a core task in autonomous driving, by improving the reliability of trajectory predictors in safety-critical urban contexts. The proposed method directly enhances trajectory prediction, which is highly relevant for autonomous driving systems."}}, "700": {"title": "Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells", "authors": ["Siyu Xiao, Xindi Zhao, Tianhao Mao, Yiwei Wang, Yuqiao Chen, Hongyun Zhang, Jian Wang, Junjie Wang, Shuang Liu, Tupei Chen, Yang Liu"], "abstract": "arXiv:2511.00129v1 Announce Type: cross \nAccurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our AlexNet-based neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "abs_url": "https://arxiv.org/abs/2511.00129", "pdf_url": "https://arxiv.org/pdf/2511.00129.pdf", "is_interesting": false}, "701": {"title": "Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning", "authors": ["Kowshik Balasubramanian, Andre Williams, Ismail Butun"], "abstract": "arXiv:2511.00133v1 Announce Type: cross \nThis paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00133", "pdf_url": "https://arxiv.org/pdf/2511.00133.pdf", "is_interesting": false}, "702": {"title": "A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control", "authors": ["Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Lin Zhang, Lei Li"], "abstract": "arXiv:2511.00136v1 Announce Type: cross \nLeveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00136", "pdf_url": "https://arxiv.org/pdf/2511.00136.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on traffic signal control using large language models (LLMs) for optimization. While traffic signal control is a component of intelligent transportation systems, the paper does not directly address autonomous driving systems or tasks like perception, prediction, or control for autonomous vehicles."}}, "703": {"title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection", "authors": ["Yu Cui, Yujian Zhang, Lina Tao, Yang Li, Xinyu Yi, Zhibin Li"], "abstract": "arXiv:2511.00139v1 Announce Type: cross \nAchieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00139", "pdf_url": "https://arxiv.org/pdf/2511.00139.pdf", "is_interesting": false}, "704": {"title": "What a diff makes: automating code migration with large language models", "authors": ["Katherine A. Rosenfeld, Cliff C. Kerr, Jessica Lundin"], "abstract": "arXiv:2511.00160v1 Announce Type: cross \nModern software programs are built on stacks that are often undergoing changes that introduce updates and improvements, but may also break any project that depends upon them. In this paper we explore the use of Large Language Models (LLMs) for code migration, specifically the problem of maintaining compatibility with a dependency as it undergoes major and minor semantic version changes. We demonstrate, using metrics such as test coverage and change comparisons, that contexts containing diffs can significantly improve performance against out of the box LLMs and, in some cases, perform better than using code. We provide a dataset to assist in further development of this problem area, as well as an open-source Python package, AIMigrate, that can be used to assist with migrating code bases. In a real-world migration of TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of required changes in a single run, increasing to 80% with multiple runs, with 47% of changes generated perfectly.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00160", "pdf_url": "https://arxiv.org/pdf/2511.00160.pdf", "is_interesting": false}, "705": {"title": "Effectiveness of LLMs in Temporal User Profiling for Recommendation", "authors": ["Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher"], "abstract": "arXiv:2511.00176v1 Announce Type: cross \nEffectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summaries of interaction histories. Our observations suggest that while LLMs tend to improve recommendation quality in domains with more active user engagement, their benefits appear less pronounced in sparser environments. This disparity likely stems from the varying distinguishability of short-term and long-term preferences across domains; the approach shows greater utility where these temporal interests are more clearly separable (e.g., Movies\\&amp;TV) compared to domains with more stable user profiles (e.g., Video Games). This highlights a critical trade-off between enhanced performance and computational costs, suggesting context-dependent LLM application. Beyond predictive capability, this LLM-driven approach inherently provides an intrinsic potential for interpretability through its natural language profiles and attention weights. This work contributes insights into the practical capability and inherent interpretability of LLM-driven temporal user profiling, outlining new research directions for developing adaptive and transparent recommender systems.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00176", "pdf_url": "https://arxiv.org/pdf/2511.00176.pdf", "is_interesting": false}, "706": {"title": "Generative Modeling Enables Molecular Structure Retrieval from Coulomb Explosion Imaging", "authors": ["Xiang Li, Till Jahnke, Rebecca Boll, Jiaqi Han, Minkai Xu, Michael Meyer, Maria Novella Piancastelli, Daniel Rolles, Artem Rudenko, Florian Trinter, Thomas J. A. Wolf, Jana B. Thayer, James P. Cryan, Stefano Ermon, Phay J. Ho"], "abstract": "arXiv:2511.00179v1 Announce Type: cross \nCapturing the structural changes that molecules undergo during chemical reactions in real space and time is a long-standing dream and an essential prerequisite for understanding and ultimately controlling femtochemistry. A key approach to tackle this challenging task is Coulomb explosion imaging, which benefited decisively from recently emerging high-repetition-rate X-ray free-electron laser sources. With this technique, information on the molecular structure is inferred from the momentum distributions of the ions produced by the rapid Coulomb explosion of molecules. Retrieving molecular structures from these distributions poses a highly non-linear inverse problem that remains unsolved for molecules consisting of more than a few atoms. Here, we address this challenge using a diffusion-based Transformer neural network. We show that the network reconstructs unknown molecular geometries from ion-momentum distributions with a mean absolute error below one Bohr radius, which is half the length of a typical chemical bond.", "categories": ["physics.chem-ph", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00179", "pdf_url": "https://arxiv.org/pdf/2511.00179.pdf", "is_interesting": false}, "707": {"title": "EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs", "authors": ["Ali Satvaty, Suzan Verberne, Fatih Turkmen"], "abstract": "arXiv:2511.00192v1 Announce Type: cross \nMembership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00192", "pdf_url": "https://arxiv.org/pdf/2511.00192.pdf", "is_interesting": false}, "708": {"title": "Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories", "authors": ["Oorja Majgaonkar, Zhiwei Fei, Xiang Li, Federica Sarro, He Ye"], "abstract": "arXiv:2511.00197v1 Announce Type: cross \nThe increasing deployment of Large Language Model (LLM) agents for complex software engineering tasks has created a need to understand their problem-solving behaviours beyond simple success metrics. While these agents demonstrate impressive capabilities in automated issue resolution, their decision-making processes remain largely opaque. This paper presents an empirical study of agent trajectories, namely the execution traces capturing the steps agents take when attempting to resolve software issues. We analyse trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus) on the SWE-Bench benchmark, examining both successful and failed attempts. Our investigation reveals several key insights into agent behaviour. First, we identify how distinct problem-solving strategies, such as defensive programming and context gathering, enable success in different scenarios. Second, we find that failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Third, our fault localisation analysis shows that while most trajectories correctly identify problematic files (72-81\\% even in failures), success depends more on achieving approximate rather than exact code modifications. These and other findings unveiled by our study, provide a foundation for understanding agent behaviour through trajectory analysis, contributing to the development of more robust and interpretable autonomous software engineering systems.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00197", "pdf_url": "https://arxiv.org/pdf/2511.00197.pdf", "is_interesting": false}, "709": {"title": "Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap", "authors": ["Chun-Hao Yang, Bo-Han Feng, Tzu-Yuan Lai, Yan Yu Chen, Yin-Kai Dean Huang, Shou-De Lin"], "abstract": "arXiv:2511.00198v1 Announce Type: cross \nOptimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00198", "pdf_url": "https://arxiv.org/pdf/2511.00198.pdf", "is_interesting": false}, "710": {"title": "Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides", "authors": ["Yiquan Wang, Yahui Ma, Yuhan Chang, Jiayao Yan, Jialin Zhang, Minnuo Cai, Kai Wei"], "abstract": "arXiv:2511.00209v1 Announce Type: cross \nDiffusion models have emerged as a leading framework in generative modeling, showing significant potential to accelerate and transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We analyze how a unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the need for more accurate scoring functions, the scarcity of high-quality experimental data, and the crucial requirement for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from chemical exploration to the targeted creation of novel therapeutics.", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2511.00209", "pdf_url": "https://arxiv.org/pdf/2511.00209.pdf", "is_interesting": false}, "711": {"title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "authors": ["Marwa Abdulhai, Ryan Cheng, Donovan Clay, Tim Althoff, Sergey Levine, Natasha Jaques"], "abstract": "arXiv:2511.00222v1 Announce Type: cross \nLarge Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&amp;A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00222", "pdf_url": "https://arxiv.org/pdf/2511.00222.pdf", "is_interesting": false}, "712": {"title": "Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI", "authors": ["Sheer Karny, Anthony Baez, Pat Pataranutaporn"], "abstract": "arXiv:2511.00230v1 Announce Type: cross \nMillions of users now design personalized LLM-based chatbots that shape their daily interactions, yet they can only loosely anticipate how their design choices will manifest as behaviors in deployment. This opacity is consequential: seemingly innocuous prompts can trigger excessive sycophancy, toxicity, or inconsistency, degrading utility and raising safety concerns. To address this issue, we introduce an interface that enables neural transparency by exposing language model internals during chatbot design. Our approach extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by computing differences in neural activations between contrastive system prompts that elicit opposing behaviors. We predict chatbot behaviors by projecting the system prompt's final token activations onto these trait vectors, normalizing for cross-trait comparability, and visualizing results via an interactive sunburst diagram. To evaluate this approach, we conducted an online user study using Prolific to compare our neural transparency interface against a baseline chatbot interface without any form of transparency. Our analyses suggest that users systematically miscalibrated AI behavior: participants misjudged trait activations for eleven of fifteen analyzable traits, motivating the need for transparency tools in everyday human-AI interaction. While our interface did not change design iteration patterns, it significantly increased user trust and was enthusiastically received. Qualitative analysis indicated that users' had nuanced experiences with the visualization that may enrich future work designing neurally transparent interfaces. This work offers a path for how mechanistic interpretability can be operationalized for non-technical users, establishing a foundation for safer, more aligned human-AI interactions.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00230", "pdf_url": "https://arxiv.org/pdf/2511.00230.pdf", "is_interesting": false}, "713": {"title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "authors": ["Shounak Paul, Dhananjay Ghumare, Pawan Goyal, Saptarshi Ghosh, Ashutosh Modi"], "abstract": "arXiv:2511.00268v1 Announce Type: cross \nIdentifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00268", "pdf_url": "https://arxiv.org/pdf/2511.00268.pdf", "is_interesting": false}, "714": {"title": "LongCat-Flash-Omni Technical Report", "authors": ["Meituan LongCat Team, Bairui Wang,  Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing, Xuezhi Cao, Xunliang Cai, Yang Yang, Yanli Tan, Yao Yao, Yerui Sun, Yi Chen, Yifan Lu, Yin Gong, Yining Zhang, Yitian Chen, Yiyang Gan, Yuchen Tang, Yuchen Xie, Yueqian Wang, Yuewen Zheng, Yufei Zhang, Yufeng Zhong, Yulei Qian, Yuqi Peng, Yuwei Jiang, Zeyang Hu, Zheng Zhang, Zhengkun Tian, Zhiqing Hong, Zhixiong Zeng, Zhuqi Mi, Ziran Li, Ziwen Wang, Ziyi Zhao, Ziyuan Zhuang, Zizhe Zhao"], "abstract": "arXiv:2511.00279v1 Announce Type: cross \nWe introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DC", "cs.LG", "cs.SD"], "abs_url": "https://arxiv.org/abs/2511.00279", "pdf_url": "https://arxiv.org/pdf/2511.00279.pdf", "is_interesting": false}, "715": {"title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs", "authors": ["Abhinav Joshi, Areeb Ahmad, Ashutosh Modi"], "abstract": "arXiv:2511.00280v1 Announce Type: cross \nLarge Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00280", "pdf_url": "https://arxiv.org/pdf/2511.00280.pdf", "is_interesting": false}, "716": {"title": "Language Modeling With Factorization Memory", "authors": ["Lee Xiong, Maksim Tkachenko, Johanes Effendi, Ting Cai"], "abstract": "arXiv:2511.00315v1 Announce Type: cross \nWe propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00315", "pdf_url": "https://arxiv.org/pdf/2511.00315.pdf", "is_interesting": false}, "717": {"title": "A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data", "authors": ["Dana Kim, Yichen Xu, Tiffany Lin"], "abstract": "arXiv:2511.00318v1 Announce Type: cross \nLarge Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00318", "pdf_url": "https://arxiv.org/pdf/2511.00318.pdf", "is_interesting": false}, "718": {"title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits", "authors": ["Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi"], "abstract": "arXiv:2511.00321v1 Announce Type: cross \nThe expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.", "categories": ["cs.AR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00321", "pdf_url": "https://arxiv.org/pdf/2511.00321.pdf", "is_interesting": false}, "719": {"title": "MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research", "authors": ["Hendrio Braganca, Diego Kreutz, Vanderson Rocha, Joner Assolin, and Eduardo Feitosa"], "abstract": "arXiv:2511.00342v1 Announce Type: cross \nWe present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset's invaluable role in understanding the evolving landscape of malware.", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PF"], "abs_url": "https://arxiv.org/abs/2511.00342", "pdf_url": "https://arxiv.org/pdf/2511.00342.pdf", "is_interesting": false}, "720": {"title": "Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks", "authors": ["Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, Weverton Cordeiro"], "abstract": "arXiv:2511.00346v1 Announce Type: cross \nThe rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00346", "pdf_url": "https://arxiv.org/pdf/2511.00346.pdf", "is_interesting": false}, "721": {"title": "Toward Unifying Group Fairness Evaluation from a Sparsity Perspective", "authors": ["Zhecheng Sheng, Jiawei Zhang, Enmao Diao"], "abstract": "arXiv:2511.00359v1 Announce Type: cross \nEnsuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. While numerous fairness criteria exist, they often lack generalizability across different machine learning problems. This paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. The framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. We demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. This work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications.", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00359", "pdf_url": "https://arxiv.org/pdf/2511.00359.pdf", "is_interesting": false}, "722": {"title": "Mind the Gap: Missing Cyber Threat Coverage in NIDS Datasets for the Energy Sector", "authors": ["Adrita Rahman Tory, Khondokar Fida Hasan, Md Saifur Rahman, Nickolaos Koroniotis, Mohammad Ali Moni"], "abstract": "arXiv:2511.00360v1 Announce Type: cross \nNetwork Intrusion Detection Systems (NIDS) developed using publicly available datasets predominantly focus on enterprise environments, raising concerns about their effectiveness for converged Information Technology (IT) and Operational Technology (OT) in energy infrastructures. This study evaluates the representativeness of five widely used datasets: CIC-IDS2017, SWaT, WADI, Sherlock, and CIC-Modbus2023 against network-detectable MITRE ATT&amp;CK techniques extracted from documented energy sector incidents. Using a structured five-step analytical approach, this article successfully developed and performed a gap analysis that identified 94 network observable techniques from an initial pool of 274 ATT&amp;CK techniques. Sherlock dataset exhibited the highest mean coverage (0.56), followed closely by CIC-IDS2017 (0.55), while SWaT and WADI recorded the lowest scores (0.38). Combining CIC-IDS2017, Sherlock, and CIC-Modbus2023 achieved an aggregate coverage of 92%, highlighting their complementary strengths. The analysis identifies critical gaps, particularly in lateral movement and industrial protocol manipulation, providing a clear pathway for dataset enhancement and more robust NIDS evaluation in hybrid IT/OT energy environments.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00360", "pdf_url": "https://arxiv.org/pdf/2511.00360.pdf", "is_interesting": false}, "723": {"title": "MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection", "authors": ["Kayua Oleques Paim, Angelo Gaspar Diniz Nogueira, Diego Kreutz, Weverton Cordeiro, Rodrigo Brandao Mansilha"], "abstract": "arXiv:2511.00361v1 Announce Type: cross \nHigh-quality data scarcity hinders malware detection, limiting ML performance. We introduce MalDataGen, an open-source modular framework for generating high-fidelity synthetic tabular data using modular deep learning models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR), seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like SDV while preserving data utility. Its flexible design enables seamless integration into detection pipelines, offering a practical solution for cybersecurity applications.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00361", "pdf_url": "https://arxiv.org/pdf/2511.00361.pdf", "is_interesting": false}, "724": {"title": "Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet", "authors": ["Farjana Aktar, Mohd Ruhul Ameen, Akif Islam, Md Ekramul Hamid"], "abstract": "arXiv:2511.00369v1 Announce Type: cross \nAchieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "abs_url": "https://arxiv.org/abs/2511.00369", "pdf_url": "https://arxiv.org/pdf/2511.00369.pdf", "is_interesting": false}, "725": {"title": "Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study", "authors": ["Lucky Onyekwelu-Udoka, Md Shafiqul Islam, Md Shahedul Hasan"], "abstract": "arXiv:2511.00402v1 Announce Type: cross \nEmotion recognition from speech plays a vital role in the development of empathetic human-computer interaction systems. This paper presents a comparative analysis of lightweight transformer-based models, DistilHuBERT and PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark their performance against a traditional CNN-LSTM baseline model using MFCC features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score (70.36%) while maintaining an exceptionally small model size (0.02 MB), outperforming both PaSST and the baseline. Furthermore, we conducted an ablation study on three variants of the PaSST, Linear, MLP, and Attentive Pooling heads, to understand the effect of classification head architecture on model performance. Our results indicate that PaSST with an MLP head yields the best performance among its variants but still falls short of DistilHuBERT. Among the emotion classes, angry is consistently the most accurately detected, while disgust remains the most challenging. These findings suggest that lightweight transformers like DistilHuBERT offer a compelling solution for real-time speech emotion recognition on edge devices. The code is available at: https://github.com/luckymaduabuchi/Emotion-detection-.", "categories": ["cs.SD", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00402", "pdf_url": "https://arxiv.org/pdf/2511.00402.pdf", "is_interesting": false}, "726": {"title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings", "authors": ["Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su"], "abstract": "arXiv:2511.00405v1 Announce Type: cross \nThe remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00405", "pdf_url": "https://arxiv.org/pdf/2511.00405.pdf", "is_interesting": false}, "727": {"title": "Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy", "authors": ["Thanveer Shaik, Xiaohui Tao, Haoran Xie, Robert Sang"], "abstract": "arXiv:2511.00406v1 Announce Type: cross \nQuantum Machine Unlearning has emerged as a foundational challenge at the intersection of quantum information theory privacypreserving computation and trustworthy artificial intelligence This paper advances QMU by establishing a formal framework that unifies physical constraints algorithmic mechanisms and ethical governance within a verifiable paradigm We define forgetting as a contraction of distinguishability between pre and postunlearning models under completely positive trace-preserving dynamics grounding data removal in the physics of quantum irreversibility Building on this foundation we present a fiveaxis taxonomy spanning scope guarantees mechanisms system context and hardware realization linking theoretical constructs to implementable strategies Within this structure we incorporate influence and quantum Fisher information weighted updates parameter reinitialization and kernel alignment as practical mechanisms compatible with noisy intermediatescale quantum NISQ devices The framework extends naturally to federated and privacyaware settings via quantum differential privacy homomorphic encryption and verifiable delegation enabling scalable auditable deletion across distributed quantum systems Beyond technical design we outline a forwardlooking research roadmap emphasizing formal proofs of forgetting scalable and secure architectures postunlearning interpretability and ethically auditable governance Together these contributions elevate QMU from a conceptual notion to a rigorously defined and ethically aligned discipline bridging physical feasibility algorithmic verifiability and societal accountability in the emerging era of quantum intelligence.", "categories": ["quant-ph", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00406", "pdf_url": "https://arxiv.org/pdf/2511.00406.pdf", "is_interesting": false}, "728": {"title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "authors": ["Yiwei Zha, Rui Min, Shanu Sushmita"], "abstract": "arXiv:2511.00416v1 Announce Type: cross \nWhile AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct LLM outputs, they fail catastrophically against iteratively-paraphrased content. We investigate why iteratively-paraphrased text -- itself AI-generated -- evades detection systems designed for AIGT identification. Through intrinsic mechanism analysis, we reveal that iterative paraphrasing creates an intermediate laundering region characterized by semantic displacement with preserved generation patterns, which brings up two attack categories: paraphrasing human-authored text (authorship obfuscation) and paraphrasing LLM-generated text (plagiarism evasion). To address these vulnerabilities, we introduce PADBen, the first benchmark systematically evaluating detector robustness against both paraphrase attack scenarios. PADBen comprises a five-type text taxonomy capturing the full trajectory from original content to deeply laundered text, and five progressive detection tasks across sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art detectors, revealing critical asymmetry: detectors successfully identify the plagiarism evasion problem but fail for the case of authorship obfuscation. Our findings demonstrate that current detection approaches cannot effectively handle the intermediate laundering region, necessitating fundamental advances in detection architectures beyond existing semantic and stylistic discrimination methods. For detailed code implementation, please see https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00416", "pdf_url": "https://arxiv.org/pdf/2511.00416.pdf", "is_interesting": false}, "729": {"title": "Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework", "authors": ["Marcel Valovy"], "abstract": "arXiv:2511.00417v1 Announce Type: cross \nAs artificial intelligence transforms software development, a critical question emerges: how can developers and AI systems collaborate most effectively? This dissertation optimizes human-AI programming roles through self-determination theory and personality psychology, introducing the Role Optimization Motivation Alignment (ROMA) framework.\n  Through Design Science Research spanning five cycles, this work establishes empirically-validated connections between personality traits, programming role preferences, and collaborative outcomes, engaging 200 experimental participants and 46 interview respondents.\n  Key findings demonstrate that personality-driven role optimization significantly enhances self-determination and team dynamics, yielding 23% average motivation increases among professionals and up to 65% among undergraduates. Five distinct personality archetypes emerge: The Explorer (high Openness/low Agreeableness), The Orchestrator (high Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low Extraversion), The Architect (high Conscientiousness), and The Adapter (balanced profile). Each exhibits distinct preferences for programming roles (Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for satisfaction.\n  The dissertation contributes: (1) an empirically-validated framework linking personality traits to role preferences and self-determination outcomes; (2) a taxonomy of AI collaboration modalities mapped to personality profiles while preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small Entities to implement personality-driven role optimization within established standards.\n  Keywords: artificial intelligence, human-computer interaction, behavioral software engineering, self-determination theory, personality psychology, phenomenology, intrinsic motivation, pair programming, design science research, ISO/IEC 29110", "categories": ["cs.SE", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.00417", "pdf_url": "https://arxiv.org/pdf/2511.00417.pdf", "is_interesting": false}, "730": {"title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "authors": ["Naoto Iwase, Hiroki Okuyama, Junichiro Iwasawa"], "abstract": "arXiv:2511.00421v1 Announce Type: cross \nLarge language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00421", "pdf_url": "https://arxiv.org/pdf/2511.00421.pdf", "is_interesting": false}, "731": {"title": "Bootstrap Off-policy with World Model", "authors": ["Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li"], "abstract": "arXiv:2511.00423v1 Announce Type: cross \nOnline planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.", "categories": ["cs.LG", "cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.00423", "pdf_url": "https://arxiv.org/pdf/2511.00423.pdf", "is_interesting": false}, "732": {"title": "LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026", "authors": ["Benjamin Clavi\\'e, Xianming Li, Antoine Chaffin, Omar Khattab, Tom Aarsen, Manuel Faysse, Jing Li"], "abstract": "arXiv:2511.00444v1 Announce Type: cross \nLate interaction retrieval methods, pioneered by ColBERT, have emerged as a powerful alternative to single-vector neural IR. By leveraging fine-grained, token-level representations, they have been demonstrated to deliver strong generalisation and robustness, particularly in out-of-domain settings. They have recently been shown to be particularly well-suited for novel use cases, such as reasoning-based or cross-modality retrieval. At the same time, these models pose significant challenges of efficiency, usability, and integrations into fully fledged systems; as well as the natural difficulties encountered while researching novel application domains. Recent years have seen rapid advances across many of these areas, but research efforts remain fragmented across communities and frequently exclude practitioners. The purpose of this workshop is to create an environment where all aspects of late interaction can be discussed, with a focus on early research explorations, real-world outcomes, and negative or puzzling results to be freely shared and discussed. The aim of LIR is to provide a highly-interactive environment for researchers from various backgrounds and practitioners to freely discuss their experience, fostering further collaboration.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00444", "pdf_url": "https://arxiv.org/pdf/2511.00444.pdf", "is_interesting": false}, "733": {"title": "DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture", "authors": ["Ruofan Liu, Yun Lin, Jin Song Dong"], "abstract": "arXiv:2511.00447v1 Announce Type: cross \nLarge language models (LLMs) have demonstrated impressive instruction-following capabilities. However, these capabilities also expose models to prompt injection attacks, where maliciously crafted inputs overwrite or distract from the intended instructions. A core vulnerability lies in the model's lack of semantic role understanding: it cannot distinguish directive intent from descriptive content, leading it to execute instruction-like phrases embedded in data.\n  We propose DRIP, a training-time defense grounded in a semantic modeling perspective, which enforces robust separation between instruction and data semantics without sacrificing utility. DRIP introduces two lightweight yet complementary mechanisms: (1) a token-wise de-instruction shift that performs semantic disentanglement, weakening directive semantics in data tokens while preserving content meaning; and (2) a residual fusion pathway that provides a persistent semantic anchor, reinforcing the influence of the true top-level instruction during generation. Experimental results on LLaMA-8B and Mistral-7B across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent) demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ, SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings underscore the power of lightweight representation edits and role-aware supervision in securing LLMs against adaptive prompt injection.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00447", "pdf_url": "https://arxiv.org/pdf/2511.00447.pdf", "is_interesting": false}, "734": {"title": "Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models", "authors": ["Mohammed N. Swileh, Shengli Zhang"], "abstract": "arXiv:2511.00460v1 Announce Type: cross \nCentralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local controllers, yet this architecture remains highly vulnerable to Distributed Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection and mitigation framework tailored for dSDN environments. The framework leverages lightweight port-level statistics combined with prompt engineering and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to classify traffic as benign or malicious without requiring fine-tuning or retraining. Once an anomaly is detected, mitigation is enforced directly at the attacker's port, ensuring that malicious traffic is blocked at their origin while normal traffic remains unaffected. An automatic recovery mechanism restores normal operation after the attack inactivity, ensuring both security and availability. Experimental evaluation under diverse DDoS attack scenarios demonstrates that the proposed approach achieves near-perfect detection, with 99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of 1.0. These results highlight the effectiveness of combining distributed monitoring with zero-training LLM inference, providing a proactive and scalable defense mechanism for securing dSDN infrastructures against DDoS threats.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00460", "pdf_url": "https://arxiv.org/pdf/2511.00460.pdf", "is_interesting": false}, "735": {"title": "Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima", "authors": ["Zhongxiang Lei, Qi Yang, Ping Qiu, Gang Zhang, Yuanchi Ma, Jinyan Liu"], "abstract": "arXiv:2511.00469v1 Announce Type: cross \nFederated optimization is a constrained form of distributed optimization that enables training a global model without directly sharing client data. Although existing algorithms can guarantee convergence in theory and often achieve stable training in practice, the reasons behind performance degradation under data heterogeneity remain unclear. To address this gap, the main contribution of this paper is to provide a theoretical perspective that explains why such degradation occurs. We introduce the assumption that heterogeneous client data lead to distinct local optima, and show that this assumption implies two key consequences: 1) the distance among clients' local optima raises the lower bound of the global objective, making perfect fitting of all client data impossible; and 2) in the final training stage, the global model oscillates within a region instead of converging to a single optimum, limiting its ability to fully fit the data. These results provide a principled explanation for performance degradation in non-iid settings, which we further validate through experiments across multiple tasks and neural network architectures. The framework used in this paper is open-sourced at: https://github.com/NPCLEI/fedtorch.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00469", "pdf_url": "https://arxiv.org/pdf/2511.00469.pdf", "is_interesting": false}, "736": {"title": "A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI", "authors": ["Ljupcho Milosheski, Kuon Akiyama, Bla\\v{z} Bertalani\\v{c}, Jernej Hribar, Ryoichi Shinkuma"], "abstract": "arXiv:2511.00494v1 Announce Type: cross \nThe growing number of smart devices supporting bandwidth-intensive and latency-sensitive applications, such as real-time video analytics, smart sensing, and Extended Reality (XR), necessitates reliable wireless connectivity in indoor environments. Therein, accurate estimation of Radio Environment Maps (REMs) enables adaptive wireless network planning and optimization of Access Point (AP) placement. However, generating realistic REMs remains challenging due to the complexity of indoor spaces. To overcome this challenge, this paper introduces a multimodal dataset that integrates high-resolution 3D LiDAR scans with Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected under 20 distinct AP configurations in a multi-room indoor environment. The dataset captures two measurement scenarios: the first without human presence in the environment, and the second with human presence. Thus, the presented dataset supports the study of dynamic environmental effects on wireless signal propagation. This resource is designed to facilitate research in data-driven wireless modeling, particularly in the context of emerging high-frequency standards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development of robust, high-capacity indoor communication systems.", "categories": ["eess.SP", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00494", "pdf_url": "https://arxiv.org/pdf/2511.00494.pdf", "is_interesting": false}, "737": {"title": "Reasoning Planning for Language Models", "authors": ["Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen"], "abstract": "arXiv:2511.00521v1 Announce Type: cross \nSelecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00521", "pdf_url": "https://arxiv.org/pdf/2511.00521.pdf", "is_interesting": false}, "738": {"title": "HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models", "authors": ["Robab Aghazadeh-Chakherlou, Qing Guo, Siddartha Khastgir, Peter Popov, Xiaoge Zhang, Xingyu Zhao"], "abstract": "arXiv:2511.00527v1 Announce Type: cross \nLarge Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00527", "pdf_url": "https://arxiv.org/pdf/2511.00527.pdf", "is_interesting": false}, "739": {"title": "On Improvisation and Open-Endedness: Insights for Experiential AI", "authors": ["Botao 'Amber' Hu"], "abstract": "arXiv:2511.00529v1 Announce Type: cross \nImprovisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless \"interestingness\"-is exemplified in natural or cultural evolution and has been considered \"the last grand challenge\" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a \"good\" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).", "categories": ["cs.HC", "cs.AI", "cs.NE", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00529", "pdf_url": "https://arxiv.org/pdf/2511.00529.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "740": {"title": "Air Pollution Forecasting in Bucharest", "authors": ["Drago\\c{s}-Andrei \\c{S}erban, R\\u{a}zvan-Alexandru Sm\\u{a}du, Dumitru-Clementin Cercel"], "abstract": "arXiv:2511.00532v1 Announce Type: cross \nAir pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.AP"], "abs_url": "https://arxiv.org/abs/2511.00532", "pdf_url": "https://arxiv.org/pdf/2511.00532.pdf", "is_interesting": false}, "741": {"title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations", "authors": ["Qiang Li, Jin Niu, Lina Yu"], "abstract": "arXiv:2511.00549v1 Announce Type: cross \nTraffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00549", "pdf_url": "https://arxiv.org/pdf/2511.00549.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on regional traffic signal control using reinforcement learning, which is related to traffic management but does not specifically address autonomous driving systems, vehicle perception, prediction, or control tasks."}}, "742": {"title": "Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales", "authors": ["Santhi Bharath Punati, Sandeep Kanta, Udaya Bhasker Cheerala, Madhusudan G Lanjewar, Praveen Damacharla"], "abstract": "arXiv:2511.00552v1 Announce Type: cross \nAccurate multi-horizon retail forecasts are critical for inventory and promotions. We present a novel study of weekly Walmart sales (45 stores, 2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store identifiers with time-varying exogenous signals (holidays, CPI, fuel price, temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via Quantile Loss, yielding calibrated 90\\% prediction intervals and interpretability through variable-selection networks, static enrichment, and temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of \\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold chronological cross-validation, the averages are RMSE = \\$64.6k USD and $R^2$ = 0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These results demonstrate practical value for inventory planning and holiday-period optimization, while maintaining model transparency.", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "abs_url": "https://arxiv.org/abs/2511.00552", "pdf_url": "https://arxiv.org/pdf/2511.00552.pdf", "is_interesting": false}, "743": {"title": "Red-teaming Activation Probes using Prompted LLMs", "authors": ["Phil Blandfort, Robert Graham"], "abstract": "arXiv:2511.00554v1 Announce Type: cross \nActivation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00554", "pdf_url": "https://arxiv.org/pdf/2511.00554.pdf", "is_interesting": false}, "744": {"title": "FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction", "authors": ["Varun Teja Chirukiri, Udaya Bhasker Cheerala, Sandeep Kanta, Abdul Karim, Praveen Damacharla"], "abstract": "arXiv:2511.00564v1 Announce Type: cross \nAccurate prediction of the remaining useful life (RUL) of industrial machinery is essential for reducing downtime and optimizing maintenance schedules. Existing approaches, such as long short-term memory (LSTM) networks and convolutional neural networks (CNNs), often struggle to model both global temporal dependencies and fine-grained degradation trends in multivariate sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal Transformer (FTT) -- a lightweight Transformer variant using linearized attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU) layer for sequential modeling. To the best of our knowledge, this is the first application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling simultaneous capture of global and local degradation patterns in a compact architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and $R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published deep baseline (TCN--Attention), it improves RMSE by 1.16\\% and MAE by 4.00\\%. Training curves averaged over $k=3$ runs show smooth convergence with narrow 95\\% confidence bands, and ablations (GRU-only, FTT-only) support the contribution of both components. These results demonstrate that a compact Transformer-RNN hybrid delivers accurate and efficient RUL predictions on CMAPSS, making it suitable for real-time industrial prognostics.", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.00564", "pdf_url": "https://arxiv.org/pdf/2511.00564.pdf", "is_interesting": false}, "745": {"title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "authors": ["Juan Gabriel Kostelec, Qinghai Guo"], "abstract": "arXiv:2511.00576v1 Announce Type: cross \nTransformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00576", "pdf_url": "https://arxiv.org/pdf/2511.00576.pdf", "is_interesting": false}, "746": {"title": "Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation", "authors": ["Dong Chen, Yanzhe Wei, Zonglin He, Guan-Ming Kuang, Canhua Ye, Meiru An, Huili Peng, Yong Hu, Huiren Tao, Kenneth MC Cheung"], "abstract": "arXiv:2511.00588v1 Announce Type: cross \nLarge language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\\pm$ 1.83 vs. 81.56 $\\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.00588", "pdf_url": "https://arxiv.org/pdf/2511.00588.pdf", "is_interesting": false}, "747": {"title": "EPARA: Parallelizing Categorized AI Inference in Edge Clouds", "authors": ["Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie"], "abstract": "arXiv:2511.00603v1 Announce Type: cross \nWith the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.", "categories": ["cs.DC", "cs.AI", "cs.NI"], "abs_url": "https://arxiv.org/abs/2511.00603", "pdf_url": "https://arxiv.org/pdf/2511.00603.pdf", "is_interesting": false}, "748": {"title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering", "authors": ["Eric Bigelow, Daniel Wurgaft, YingQiao Wang, Noah Goodman, Tomer Ullman, Hidenori Tanaka, Ekdeep Singh Lubana"], "abstract": "arXiv:2511.00617v1 Announce Type: cross \nLarge language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00617", "pdf_url": "https://arxiv.org/pdf/2511.00617.pdf", "is_interesting": false}, "749": {"title": "AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems", "authors": ["Yang Li, Siqi Ping, Xiyu Chen, Xiaojian Qi, Zigan Wang, Ye Luo, Xiaowei Zhang"], "abstract": "arXiv:2511.00628v1 Announce Type: cross \nWith the rapid progress of large language models (LLMs), LLM-powered multi-agent systems (MAS) are drawing increasing interest across academia and industry. However, many current MAS frameworks struggle with reliability and scalability, especially on complex tasks. We present AgentGit, a framework that brings Git-like rollback and branching to MAS workflows. Built as an infrastructure layer on top of LangGraph, AgentGit supports state commit, revert, and branching, allowing agents to traverse, compare, and explore multiple trajectories efficiently. To evaluate AgentGit, we designed an experiment that optimizes target agents by selecting better prompts. We ran a multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno -- on a real-world task: retrieving and analyzing paper abstracts. Results show that AgentGit significantly reduces redundant computation, lowers runtime and token usage, and supports parallel exploration across multiple branches, enhancing both reliability and scalability in MAS development. This work offers a practical path to more robust MAS design and enables error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems.", "categories": ["cs.MA", "cs.AI", "cs.SE"], "abs_url": "https://arxiv.org/abs/2511.00628", "pdf_url": "https://arxiv.org/pdf/2511.00628.pdf", "is_interesting": false}, "750": {"title": "Node Preservation and its Effect on Crossover in Cartesian Genetic Programming", "authors": ["Mark Kocherovsky, Illya Bakurov, Wolfgang Banzhaf"], "abstract": "arXiv:2511.00634v1 Announce Type: cross \nWhile crossover is a critical and often indispensable component in other forms of Genetic Programming, such as Linear- and Tree-based, it has consistently been claimed that it deteriorates search performance in CGP. As a result, a mutation-alone $(1+\\lambda)$ evolutionary strategy has become the canonical approach for CGP. Although several operators have been developed that demonstrate an increased performance over the canonical method, a general solution to the problem is still lacking. In this paper, we compare basic crossover methods, namely one-point and uniform, to variants in which nodes are ``preserved,'' including the subgraph crossover developed by Roman Kalkreuth, the difference being that when ``node preservation'' is active, crossover is not allowed to break apart instructions. We also compare a node mutation operator to the traditional point mutation; the former simply replaces an entire node with a new one. We find that node preservation in both mutation and crossover improves search using symbolic regression benchmark problems, moving the field towards a general solution to CGP crossover.", "categories": ["cs.NE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00634", "pdf_url": "https://arxiv.org/pdf/2511.00634.pdf", "is_interesting": false}, "751": {"title": "More Than A Shortcut: A Hyperbolic Approach To Early-Exit Networks", "authors": ["Swapnil Bhosale, Cosmin Frateanu, Camilla Clark, Arnoldas Jasonas, Chris Mitchell, Xiatian Zhu, Vamsi Krishna Ithapu, Giacomo Ferroni, Cagdas Bilen, Sanjeel Parekh"], "abstract": "arXiv:2511.00641v1 Announce Type: cross \nDeploying accurate event detection on resource-constrained devices is challenged by the trade-off between performance and computational cost. While Early-Exit (EE) networks offer a solution through adaptive computation, they often fail to enforce a coherent hierarchical structure, limiting the reliability of their early predictions. To address this, we propose Hyperbolic Early-Exit networks (HypEE), a novel framework that learns EE representations in the hyperbolic space. Our core contribution is a hierarchical training objective with a novel entailment loss, which enforces a partial-ordering constraint to ensure that deeper network layers geometrically refine the representations of shallower ones. Experiments on multiple audio event detection tasks and backbone architectures show that HypEE significantly outperforms standard Euclidean EE baselines, especially at the earliest, most computationally-critical exits. The learned geometry also provides a principled measure of uncertainty, enabling a novel triggering mechanism that makes the overall system both more efficient and more accurate than a conventional EE and standard backbone models without early-exits.", "categories": ["cs.SD", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00641", "pdf_url": "https://arxiv.org/pdf/2511.00641.pdf", "is_interesting": false}, "752": {"title": "Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare", "authors": ["Guilherme H. Travassos, Sabrina Rocha, Rodrigo Feitosa, Felipe Assis, Patricia Goncalves, Andre Gheventer, Larissa Galeno, Arthur Sasse, Julio Cesar Guimaraes, Carlos Brito, Joao Pedro Wieland"], "abstract": "arXiv:2511.00658v1 Announce Type: cross \nThe advances and availability of technologies involving Generative Artificial Intelligence (AI) are evolving clearly and explicitly, driving immediate changes in various work activities. Software Engineering (SE) is no exception and stands to benefit from these new technologies, enhancing productivity and quality in its software development processes. However, although the use of Generative AI in SE practices is still in its early stages, considering the lack of conclusive results from ongoing research and the limited technological maturity, we have chosen to incorporate these technologies in the development of a web-based software system to be used in clinical trials by a thoracic diseases research group at our university. For this reason, we decided to share this experience report documenting our development team's learning journey in using Generative AI during the software development process. Project management, requirements specification, design, development, and quality assurance activities form the scope of observation. Although we do not yet have definitive technological evidence to evolve our development process significantly, the results obtained and the suggestions shared here represent valuable insights for software organizations seeking to innovate their development practices to achieve software quality with generative AI.", "categories": ["cs.SE", "cs.AI", "cs.ET"], "abs_url": "https://arxiv.org/abs/2511.00658", "pdf_url": "https://arxiv.org/pdf/2511.00658.pdf", "is_interesting": false}, "753": {"title": "ShadowLogic: Backdoors in Any Whitebox LLM", "authors": ["Kasimir Schulz, Amelia Kawasaki, Leo Ring"], "abstract": "arXiv:2511.00664v1 Announce Type: cross \nLarge language models (LLMs) are widely deployed across various applications, often with safeguards to prevent the generation of harmful or restricted content. However, these safeguards can be covertly bypassed through adversarial modifications to the computational graph of a model. This work highlights a critical security vulnerability in computational graph-based LLM formats, demonstrating that widely used deployment pipelines may be susceptible to obscured backdoors. We introduce ShadowLogic, a method for creating a backdoor in a white-box LLM by injecting an uncensoring vector into its computational graph representation. We set a trigger phrase that, when added to the beginning of a prompt into the LLM, applies the uncensoring vector and removes the content generation safeguards in the model. We embed trigger logic directly into the computational graph which detects the trigger phrase in a prompt. To evade detection of our backdoor, we obfuscate this logic within the graph structure, making it similar to standard model functions. Our method requires minimal alterations to model parameters, making backdoored models appear benign while retaining the ability to generate uncensored responses when activated. We successfully implement ShadowLogic in Phi-3 and Llama 3.2, using ONNX for manipulating computational graphs. Implanting the uncensoring vector achieved a >60% attack success rate for further malicious queries.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00664", "pdf_url": "https://arxiv.org/pdf/2511.00664.pdf", "is_interesting": false}, "754": {"title": "Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?", "authors": ["Weijie Su"], "abstract": "arXiv:2511.00674v1 Announce Type: cross \nIn this paper, we introduce a model for analyzing deep learning optimization over a single iteration by leveraging the matrix structure of the weights. We derive the model by assuming isotropy of curvature, including the second-order Hessian and higher-order terms, of the loss function across all perturbation directions; hence, we call it the isotropic curvature model. This model is a convex optimization program amenable to analysis, which allows us to understand how an update on the weights in the form of a matrix relates to the change in the total loss function. As an application, we use the isotropic curvature model to analyze the recently introduced Muon optimizer and other matrix-gradient methods for training language models. First, we show that under a general growth condition on the curvature, the optimal update matrix is obtained by making the spectrum of the original gradient matrix more homogeneous -- that is, making its singular values closer in ratio -- which in particular improves the conditioning of the update matrix. Next, we show that the orthogonalized gradient becomes optimal for the isotropic curvature model when the curvature exhibits a phase transition in growth. Taken together, these results suggest that the gradient orthogonalization employed in Muon and other related methods is directionally correct but may not be strictly optimal. Finally, we discuss future research on how to leverage the isotropic curvature model for designing new optimization methods for training deep learning and language models.", "categories": ["math.OC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00674", "pdf_url": "https://arxiv.org/pdf/2511.00674.pdf", "is_interesting": false}, "755": {"title": "A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment", "authors": ["Veronica Bossio Botero, Vijay Yadav, Jacob Ouyang, Anzar Abbas, Michelle Worthington"], "abstract": "arXiv:2511.00709v1 Announce Type: cross \nTraining mental health clinicians to conduct standardized clinical assessments is challenging due to a lack of scalable, realistic practice opportunities, which can impact data quality in clinical trials. To address this gap, we introduce a voice-enabled virtual patient simulation system powered by a large language model (LLM). This study describes the system's development and validates its ability to generate virtual patients who accurately adhere to pre-defined clinical profiles, maintain coherent narratives, and produce realistic dialogue. We implemented a system using a LLM to simulate patients with specified symptom profiles, demographics, and communication styles. The system was evaluated by 5 experienced clinical raters who conducted 20 simulated structured MADRS interviews across 4 virtual patient personas. The virtual patients demonstrated strong adherence to their clinical profiles, with a mean item difference between rater-assigned MADRS scores and configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was 0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative realism and cohesiveness of the virtual patients favorably, giving average ratings between \"Agree\" and \"Strongly Agree.\" Our findings suggest that LLM-powered virtual patient simulations are a viable and scalable tool for training clinicians, capable of producing high-fidelity, clinically relevant practice scenarios.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00709", "pdf_url": "https://arxiv.org/pdf/2511.00709.pdf", "is_interesting": false}, "756": {"title": "TRISKELION-1: Unified Descriptive-Predictive-Generative AI", "authors": ["Nardeep Kumar, Arun Kanwar"], "abstract": "arXiv:2511.00711v1 Announce Type: cross \nTRISKELION-1 is a unified descriptive-predictive-generative architecture that integrates statistical, mechanistic, and generative reasoning within a single encoder-decoder framework. The model demonstrates how descriptive representation learning, predictive inference, and generative synthesis can be jointly optimized using variational objectives. Experiments on MNIST validate that descriptive reconstruction, predictive classification, and generative sampling can coexist stably within one model. The framework provides a blueprint toward universal intelligence architectures that connect interpretability, accuracy, and creativity.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00711", "pdf_url": "https://arxiv.org/pdf/2511.00711.pdf", "is_interesting": false}, "757": {"title": "FeNN-DMA: A RISC-V SoC for SNN acceleration", "authors": ["Zainab Aizaz, James C. Knight, Thomas Nowotny"], "abstract": "arXiv:2511.00732v1 Announce Type: cross \nSpiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than ANNs and are therefore not well-matched to standard accelerators like GPUs and TPUs. Field Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads and here we develop a novel, fully-programmable RISC-V-based system-on-chip (FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show that FeNN-DMA has comparable resource usage and energy requirements to state-of-the-art fixed-function SNN accelerators, yet it is capable of simulating much larger and more complex models. Using this functionality, we demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg Digits and Neuromorphic MNIST tasks.", "categories": ["cs.NE", "cs.AI", "cs.AR"], "abs_url": "https://arxiv.org/abs/2511.00732", "pdf_url": "https://arxiv.org/pdf/2511.00732.pdf", "is_interesting": false}, "758": {"title": "EP-HDC: Hyperdimensional Computing with Encrypted Parameters for High-Throughput Privacy-Preserving Inference", "authors": ["Jaewoo Park, Chenghao Quan, Jongeun Lee"], "abstract": "arXiv:2511.00737v1 Announce Type: cross \nWhile homomorphic encryption (HE) provides strong privacy protection, its high computational cost has restricted its application to simple tasks. Recently, hyperdimensional computing (HDC) applied to HE has shown promising performance for privacy-preserving machine learning (PPML). However, when applied to more realistic scenarios such as batch inference, the HDC-based HE has still very high compute time as well as high encryption and data transmission overheads. To address this problem, we propose HDC with encrypted parameters (EP-HDC), which is a novel PPML approach featuring client-side HE, i.e., inference is performed on a client using a homomorphically encrypted model. Our EP-HDC can effectively mitigate the encryption and data transmission overhead, as well as providing high scalability with many clients while providing strong protection for user data and model parameters. In addition to application examples for our client-side PPML, we also present design space exploration involving quantization, architecture, and HE-related parameters. Our experimental results using the BFV scheme and the Face/Emotion datasets demonstrate that our method can improve throughput and latency of batch inference by orders of magnitude over previous PPML methods (36.52~1068x and 6.45~733x, respectively) with less than 1% accuracy degradation.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00737", "pdf_url": "https://arxiv.org/pdf/2511.00737.pdf", "is_interesting": false}, "759": {"title": "Quantifying truth and authenticity in AI-assisted candidate evaluation: A multi-domain pilot analysis", "authors": ["Eldred Lee, Nicholas Worley, Koshu Takatsuji"], "abstract": "arXiv:2511.00774v1 Announce Type: cross \nThis paper presents a retrospective analysis of anonymized candidate-evaluation data collected during pilot hiring campaigns conducted through AlteraSF, an AI-native resume-verification platform. The system evaluates resume claims, generates context-sensitive verification questions, and measures performance along quantitative axes of factual validity and job fit, complemented by qualitative integrity detection. Across six job families and 1,700 applications, the platform achieved a 90-95% reduction in screening time and detected measurable linguistic patterns consistent with AI-assisted or copied responses. The analysis demonstrates that candidate truthfulness can be assessed not only through factual accuracy but also through patterns of linguistic authenticity. The results suggest that a multi-dimensional verification framework can improve both hiring efficiency and trust in AI-mediated evaluation systems.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00774", "pdf_url": "https://arxiv.org/pdf/2511.00774.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "760": {"title": "Fast PINN Eigensolvers via Biconvex Reformulation", "authors": ["Akshay Sai Banderwaar, Abhishek Gupta"], "abstract": "arXiv:2511.00792v1 Announce Type: cross \nEigenvalue problems have a distinctive forward-inverse structure and are fundamental to characterizing a system's thermal response, stability, and natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free alternative for solving such problems but are often orders of magnitude slower than classical numerical schemes. In this paper, we introduce a reformulated PINN approach that casts the search for eigenpairs as a biconvex optimization problem, enabling fast and provably convergent alternating convex search (ACS) over eigenvalues and eigenfunctions using analytically optimal updates. Numerical experiments show that PINN-ACS attains high accuracy with convergence speeds up to 500$\\times$ faster than gradient-based PINN training. We release our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "abs_url": "https://arxiv.org/abs/2511.00792", "pdf_url": "https://arxiv.org/pdf/2511.00792.pdf", "is_interesting": false}, "761": {"title": "Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration", "authors": ["Yan Sun, Jia Guo, Stanley Kok, Zihao Wang, Zujie Wen, Zhiqiang Zhang"], "abstract": "arXiv:2511.00794v1 Announce Type: cross \nReinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization, considering the amount of computation required. This study investigates how simply leveraging intrinsic data properties, almost free benefit during training, can improve data efficiency for RLVR. We propose PREPO with two complementary components. First, we adopt prompt perplexity as an indicator of model adaptability in learning, enabling the model to progress from well-understood contexts to more challenging ones. Second, we amplify the discrepancy among the rollouts by differentiating their relative entropy, and prioritize sequences that exhibit a higher degree of exploration. Together, these mechanisms reduce rollout demand while preserving competitive performance. On the Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3 times fewer rollouts than the baselines. Beyond empirical gains, we provide theoretical and in-depth analyses explaining the underlying rationale of our method to improve the data efficiency of RLVR.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00794", "pdf_url": "https://arxiv.org/pdf/2511.00794.pdf", "is_interesting": false}, "762": {"title": "Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation", "authors": ["Wang Zixian"], "abstract": "arXiv:2511.00797v1 Announce Type: cross \nPre-trained Transformers often exhibit over-confidence in source patterns and difficulty in forming new target-domain patterns during fine-tuning. We formalize the mechanism of output saturation leading to gradient suppression through standard cross-entropy and softmax analysis, showing that gradient suppression at inflection layers confines adaptation to high-level recombination of existing features while preventing low-level reconstruction. We introduce a set of layer-wise diagnostic metrics -- attention entropy (saturation proxy), activation gradient norm, parameter gradient norm, and Delta-CKA under a shared PCA basis -- to identify inflection layers characterized by both low attention entropy and steep gradient decay. Building on these findings, we propose a diagnose-first, inject-light fine-tuning strategy: selectively inserting LoRA adapters at inflection layers to restore suppressed backward signals with minimal parameter overhead. Experiments on BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and over-trained source regimes reveal that over-trained initialization benefits from inflection-layer LoRA injection, while under-trained initialization suffers performance degradation. When base features are strong, unblocking inflection layers facilitates high-level compositional adaptation; when base features are weak, full-pathway unblocking is required for low-level reconstruction, as supported by joint analysis of layer-wise activation gradients and Delta-CKA dynamics.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00797", "pdf_url": "https://arxiv.org/pdf/2511.00797.pdf", "is_interesting": false}, "763": {"title": "Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems", "authors": ["Guangxi Wan, Peng Zeng, Xiaoting Dong, Chunhe Song, Shijie Cui, Dong Li, Qingwei Dong, Yiyang Liu, Hongfei Bai"], "abstract": "arXiv:2511.00806v1 Announce Type: cross \nCyber-physical systems (CPS) require the joint optimization of discrete cyber actions and continuous physical parameters under stringent safety logic constraints. However, existing hierarchical approaches often compromise global optimality, whereas reinforcement learning (RL) in hybrid action spaces often relies on brittle reward penalties, masking, or shielding and struggles to guarantee constraint satisfaction. We present logic-informed reinforcement learning (LIRL), which equips standard policy-gradient algorithms with projection that maps a low-dimensional latent action onto the admissible hybrid manifold defined on-the-fly by first-order logic. This guarantees feasibility of every exploratory step without penalty tuning. Experimental evaluations have been conducted across multiple scenarios, including industrial manufacturing, electric vehicle charging stations, and traffic signal control, in all of which the proposed method outperforms existing hierarchical optimization approaches. Taking a robotic reducer assembly system in industrial manufacturing as an example, LIRL achieves a 36.47\\% to 44.33\\% reduction at most in the combined makespan-energy objective compared to conventional industrial hierarchical scheduling methods. Meanwhile, it consistently maintains zero constraint violations and significantly surpasses state-of-the-art hybrid-action reinforcement learning baselines. Thanks to its declarative logic-based constraint formulation, the framework can be seamlessly transferred to other domains such as smart transportation and smart grid, thereby paving the way for safe and real-time optimization in large-scale CPS.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00806", "pdf_url": "https://arxiv.org/pdf/2511.00806.pdf", "is_interesting": false}, "764": {"title": "Towards Ultra-Low Latency: Binarized Neural Network Architectures for In-Vehicle Network Intrusion Detection", "authors": ["Huiyao Dong, Igor Kotenko"], "abstract": "arXiv:2511.00828v1 Announce Type: cross \nThe Control Area Network (CAN) protocol is essential for in-vehicle communication, facilitating high-speed data exchange among Electronic Control Units (ECUs). However, its inherent design lacks robust security features, rendering vehicles susceptible to cyberattacks. While recent research has investigated machine learning and deep learning techniques to enhance network security, their practical applicability remains uncertain. This paper presents a lightweight intrusion detection technique based on Binarized Neural Networks (BNNs), which utilizes payload data, message IDs, and CAN message frequencies for effective intrusion detection. Additionally, we develop hybrid binary encoding techniques to integrate non-binary features, such as message IDs and frequencies. The proposed method, namely the BNN framework specifically optimized for in-vehicle intrusion detection combined with hybrid binary quantization techniques for non-payload attributes, demonstrates efficacy in both anomaly detection and multi-class network traffic classification. The system is well-suited for deployment on micro-controllers and Gateway ECUs, aligning with the real-time requirements of CAN bus safety applications.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00828", "pdf_url": "https://arxiv.org/pdf/2511.00828.pdf", "is_interesting": false}, "765": {"title": "CodeClash: Benchmarking Goal-Oriented Software Engineering", "authors": ["John Yang, Kilian Lieret, Joyce Yang, Carlos E. Jimenez, Ofir Press, Ludwig Schmidt, Diyi Yang"], "abstract": "arXiv:2511.00839v1 Announce Type: cross \nCurrent benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the study of autonomous, goal-oriented code development.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00839", "pdf_url": "https://arxiv.org/pdf/2511.00839.pdf", "is_interesting": false}, "766": {"title": "Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers", "authors": ["Yuhan Cao, Yu Wang, Sitong Liu, Miao Li, Yixin Tao, Tianxing He"], "abstract": "arXiv:2511.00847v2 Announce Type: cross \nThe widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers. This manipulation can manifest in various forms, such as secretly substituting a proclaimed high-performance model with a low-cost alternative, or inflating responses with meaningless tokens to increase billing. This work tackles the issue through the lens of algorithmic game theory and mechanism design. We are the first to propose a formal economic model for a realistic user-provider ecosystem, where a user can iteratively delegate $T$ queries to multiple model providers, and providers can engage in a range of strategic behaviors. As our central contribution, we prove that for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there exists an approximate incentive-compatible mechanism with an additive approximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear second-best user utility. We also prove an impossibility result, stating that no mechanism can guarantee an expected user utility that is asymptotically better than our mechanism. Furthermore, we demonstrate the effectiveness of our mechanism in simulation experiments with real-world API settings.", "categories": ["cs.GT", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00847", "pdf_url": "https://arxiv.org/pdf/2511.00847.pdf", "is_interesting": false}, "767": {"title": "MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models", "authors": ["Yayue Deng, Guoqiang Hu, Haiyang Sun, Xiangyu Zhang, Haoyang Zhang, Fei Tian, Xuerui Yang, Gang Yu, Eng Siong Chng"], "abstract": "arXiv:2511.00850v1 Announce Type: cross \nSpoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to sustain genuinely interactive multi-turn conversations remains underexplored, as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench, the first benchmark explicitly designed to evaluate SDMs in multi-turn interactive dialogue with an emphasis on emotional intelligence. Multi-Bench employs a hierarchical structure with a basic track for emotion understanding and reasoning and an advanced track for emotion support and application. It comprises five carefully designed tasks and about 3.2K samples, ranging from emotion recognition to complex reasoning and interactive dialogue, supported by a reproducible evaluation framework. We evaluate six representative SDMs on eight subsets of Multi-Bench. Results show that while current SDMs achieve good performance on basic understanding tasks, they still have room for improvement in advanced multi-turn interactive dialogue and reasoning-related tasks, particularly in emotion awareness and application.", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "abs_url": "https://arxiv.org/abs/2511.00850", "pdf_url": "https://arxiv.org/pdf/2511.00850.pdf", "is_interesting": false}, "768": {"title": "Fast Stochastic Greedy Algorithm for $k$-Submodular Cover Problem", "authors": ["Hue T. Nguyen, Tan D. Tran, Nguyen Long Giang, Canh V. Pham"], "abstract": "arXiv:2511.00869v1 Announce Type: cross \nWe study the $k$-Submodular Cover ($kSC$) problem, a natural generalization of the classical Submodular Cover problem that arises in artificial intelligence and combinatorial optimization tasks such as influence maximization, resource allocation, and sensor placement. Existing algorithms for $\\kSC$ often provide weak approximation guarantees or incur prohibitively high query complexity. To overcome these limitations, we propose a \\textit{Fast Stochastic Greedy} algorithm that achieves strong bicriteria approximation while substantially lowering query complexity compared to state-of-the-art methods. Our approach dramatically reduces the number of function evaluations, making it highly scalable and practical for large-scale real-world AI applications where efficiency is essential.", "categories": ["cs.DS", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00869", "pdf_url": "https://arxiv.org/pdf/2511.00869.pdf", "is_interesting": false}, "769": {"title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "authors": ["Hyeon Hwang, Yewon Cho, Chanwoong Yoon, Yein Park, Minju Song, Kyungjae Lee, Gangwoo Kim, Jaewoo Kang"], "abstract": "arXiv:2511.00879v1 Announce Type: cross \nStep-by-step reasoning has become a standard approach for large language models (LLMs) to tackle complex tasks. While this paradigm has proven effective, it raises a fundamental question: How can we verify that an LLM's reasoning is accurately grounded in knowledge? To address this question, we introduce a novel evaluation suite that systematically assesses the knowledge grounding of intermediate reasoning. Our framework comprises three key components. (1) Principal Knowledge Collection, a large-scale repository of atomic knowledge essential for reasoning. Based on the collection, we propose (2) knowledge-grounded evaluation metrics designed to measure how well models recall and apply prerequisite knowledge in reasoning. These metrics are computed by our (3) evaluator LLM, a lightweight model optimized for cost-effective and reliable metric computation. Our evaluation suite demonstrates remarkable effectiveness in identifying missing or misapplied knowledge elements, providing crucial insights for uncovering fundamental reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these metrics can be integrated into preference optimization, showcasing further applications of knowledge-grounded evaluation.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00879", "pdf_url": "https://arxiv.org/pdf/2511.00879.pdf", "is_interesting": false}, "770": {"title": "KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization", "authors": ["Joonyoung Lim, Younghwan Yoo"], "abstract": "arXiv:2511.00880v1 Announce Type: cross \nWe propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based second-order policy optimization with safety-aware gradient manipulation. KFCPO leverages K-FAC to perform efficient and stable natural gradient updates by approximating the Fisher Information Matrix (FIM) in a layerwise, closed form manner, avoiding iterative approximation overheads. To address the tradeoff between reward maximization and constraint satisfaction, we introduce a margin aware gradient manipulation mechanism that adaptively adjusts the influence of reward and cost gradients based on the agent's proximity to safety boundaries. This method blends gradients using a direction sensitive projection, eliminating harmful interference and avoiding abrupt changes caused by fixed hard thresholds. Additionally, a minibatch level KL rollback strategy is adopted to ensure trust region compliance and to prevent destabilizing policy shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves 10.3% to 50.2% higher average return across environments compared to the best baseline that respected the safety constraint, demonstrating superior balance of safety and performance.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00880", "pdf_url": "https://arxiv.org/pdf/2511.00880.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on Safe Reinforcement Learning (Safe RL) and the optimization of safety-aware policies, which is not directly related to autonomous driving tasks or systems. It discusses general reinforcement learning techniques without specific mention of autonomous driving applications or tasks like perception, prediction, or control."}}, "771": {"title": "Deep Generative Models for Enhanced Vitreous OCT Imaging", "authors": ["Simone Sarrocco, Philippe C. Cattin, Peter M. Maloca, Paul Friedrich, Philippe Valmaggia"], "abstract": "arXiv:2511.00881v2 Announce Type: cross \nPurpose: To evaluate deep learning (DL) models for enhancing vitreous optical coherence tomography (OCT) image quality and reducing acquisition time. Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs), Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised Generative Adversarial Network (VQ-GAN) were used to generate high-quality spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and outputs were compared to pseudoART100 images obtained by averaging ten ART10 images per eye location. Model performance was assessed using image quality metrics and Visual Turing Tests, where ophthalmologists ranked generated images and evaluated anatomical fidelity. The best model's performance was further tested within the manually segmented vitreous on newly acquired data. Results: U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM (0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest (3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and 85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous regions more similar in PSNR to the ART100 reference than true ART1 or ART10 B-scans and achieved higher PSNR on whole images when conditioned on ART1 than ART10. Conclusions: Results reveal discrepancies between quantitative metrics and clinical evaluation, highlighting the need for combined assessment. cDDPM showed strong potential for generating clinically meaningful vitreous OCT images while reducing acquisition time fourfold. Translational Relevance: cDDPMs show promise for clinical integration, supporting faster, higher-quality vitreous imaging. Dataset and code will be made publicly available.", "categories": ["eess.IV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00881", "pdf_url": "https://arxiv.org/pdf/2511.00881.pdf", "is_interesting": false}, "772": {"title": "Android Malware Detection: A Machine Leaning Approach", "authors": ["Hasan Abdulla"], "abstract": "arXiv:2511.00894v1 Announce Type: cross \nThis study examines machine learning techniques like Decision Trees, Support Vector Machines, Logistic Regression, Neural Networks, and ensemble methods to detect Android malware. The study evaluates these models on a dataset of Android applications and analyzes their accuracy, efficiency, and real-world applicability. Key findings show that ensemble methods demonstrate superior performance, but there are trade-offs between model interpretability, efficiency, and accuracy. Given its increasing threat, the insights guide future research and practical use of ML to combat Android malware.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.00894", "pdf_url": "https://arxiv.org/pdf/2511.00894.pdf", "is_interesting": false}, "773": {"title": "Dynamic Logic of Trust-Based Beliefs", "authors": ["Junli Jiang, Pavel Naumov, Wenxuan Zhang"], "abstract": "arXiv:2511.00899v1 Announce Type: cross \nTraditionally, an agent's beliefs would come from what the agent can see, hear, or sense. In the modern world, beliefs are often based on the data available to the agents. In this work, we investigate a dynamic logic of such beliefs that incorporates public announcements of data. The main technical contribution is a sound and complete axiomatisation of the interplay between data-informed beliefs and data announcement modalities. We also describe a non-trivial polynomial model checking algorithm for this logical system.", "categories": ["cs.LO", "cs.AI", "math.LO"], "abs_url": "https://arxiv.org/abs/2511.00899", "pdf_url": "https://arxiv.org/pdf/2511.00899.pdf", "is_interesting": false}, "774": {"title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots", "authors": ["Junyao Shi, Rujia Yang, Kaitian Chao, Selina Bingqing Wan, Yifei Shao, Jiahui Lei, Jianing Qian, Long Le, Pratik Chaudhari, Kostas Daniilidis, Chuan Wen, Dinesh Jayaraman"], "abstract": "arXiv:2511.00917v1 Announce Type: cross \nToday's best-explored routes towards generalist robots center on collecting ever larger \"observations-in actions-out\" robotics datasets to train large end-to-end models, copying a recipe that has worked for vision-language models (VLMs). We pursue a road less traveled: building generalist policies directly around VLMs by augmenting their general capabilities with specific robot capabilities encapsulated in a carefully curated set of perception, planning, and control modules. In Maestro, a VLM coding agent dynamically composes these modules into a programmatic policy for the current task and scenario. Maestro's architecture benefits from a streamlined closed-loop interface without many manually imposed structural constraints, and a comprehensive and diverse tool repertoire. As a result, it largely surpasses today's VLA models for zero-shot performance on challenging manipulation skills. Further, Maestro is easily extensible to incorporate new modules, easily editable to suit new embodiments such as a quadruped-mounted arm, and even easily adapts from minimal real-world experiences through local code edits.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00917", "pdf_url": "https://arxiv.org/pdf/2511.00917.pdf", "is_interesting": false}, "775": {"title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model", "authors": ["Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang"], "abstract": "arXiv:2511.00940v1 Announce Type: cross \nConstructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized $[SEG]$ token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00940", "pdf_url": "https://arxiv.org/pdf/2511.00940.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper primarily focuses on constructing digital twins for robotic simulation and kinematic parameter prediction using a multimodal language model. While it offers a method that could be applied to autonomous driving for simulation purposes, the paper does not explicitly discuss autonomous driving systems, vehicle perception, or control tasks. The connection to autonomous driving is indirect, mainly through its potential use in robotic simulations that may relate to AD scenarios."}}, "776": {"title": "The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks", "authors": ["Khoat Than"], "abstract": "arXiv:2511.00958v1 Announce Type: cross \nNormalization methods are fundamental components of modern deep neural networks (DNNs). Empirically, they are known to stabilize optimization dynamics and improve generalization. However, the underlying theoretical mechanism by which normalization contributes to both optimization and generalization remains largely unexplained, especially when using many normalization layers in a DNN architecture.\n  In this work, we develop a theoretical framework that elucidates the role of normalization through the lens of capacity control. We prove that an unnormalized DNN can exhibit exponentially large Lipschitz constants with respect to either its parameters or inputs, implying excessive functional capacity and potential overfitting. Such bad DNNs are uncountably many. In contrast, the insertion of normalization layers provably can reduce the Lipschitz constant at an exponential rate in the number of normalization operations. This exponential reduction yields two fundamental consequences: (1) it smooths the loss landscape at an exponential rate, facilitating faster and more stable optimization; and (2) it constrains the effective capacity of the network, thereby enhancing generalization guarantees on unseen data. Our results thus offer a principled explanation for the empirical success of normalization methods in deep learning.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.00958", "pdf_url": "https://arxiv.org/pdf/2511.00958.pdf", "is_interesting": false}, "777": {"title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "authors": ["Abhinav P M, Ojasva Saxena, Oswald C, Parameswari Krishnamurthy"], "abstract": "arXiv:2511.00960v2 Announce Type: cross \nThe extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a model's initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00960", "pdf_url": "https://arxiv.org/pdf/2511.00960.pdf", "is_interesting": false}, "778": {"title": "Using Synthetic Data to estimate the True Error is theoretically and practically doable", "authors": ["Hai Hoang Thanh, Duy-Tung Nguyen, Hung The Tran, Khoat Than"], "abstract": "arXiv:2511.00964v1 Announce Type: cross \nAccurately evaluating model performance is crucial for deploying machine learning systems in real-world applications. Traditional methods often require a sufficiently large labeled test set to ensure a reliable evaluation. However, in many contexts, a large labeled dataset is costly and labor-intensive. Therefore, we sometimes have to do evaluation by a few labeled samples, which is theoretically challenging. Recent advances in generative models offer a promising alternative by enabling the synthesis of high-quality data. In this work, we make a systematic investigation about the use of synthetic data to estimate the test error of a trained model under limited labeled data conditions. To this end, we develop novel generalization bounds that take synthetic data into account. Those bounds suggest novel ways to optimize synthetic samples for evaluation and theoretically reveal the significant role of the generator's quality. Inspired by those bounds, we propose a theoretically grounded method to generate optimized synthetic data for model evaluation. Experimental results on simulation and tabular datasets demonstrate that, compared to existing baselines, our method achieves accurate and more reliable estimates of the test error.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.00964", "pdf_url": "https://arxiv.org/pdf/2511.00964.pdf", "is_interesting": false}, "779": {"title": "Keys in the Weights: Transformer Authentication Using Model-Bound Latent Representations", "authors": ["Ay\\c{s}e S. Okatan, Mustafa \\.Ilhan Akba\\c{s}, Laxima Niure Kandel, Berker Pek\\\"oz"], "abstract": "arXiv:2511.00973v1 Announce Type: cross \nWe introduce Model-Bound Latent Exchange (MoBLE), a decoder-binding property in Transformer autoencoders formalized as Zero-Shot Decoder Non-Transferability (ZSDN). In identity tasks using iso-architectural models trained on identical data but differing in seeds, self-decoding achieves more than 0.91 exact match and 0.98 token accuracy, while zero-shot cross-decoding collapses to chance without exact matches. This separation arises without injected secrets or adversarial training, and is corroborated by weight-space distances and attention-divergence diagnostics. We interpret ZSDN as model binding, a latent-based authentication and access-control mechanism, even when the architecture and training recipe are public: encoder's hidden state representation deterministically reveals the plaintext, yet only the correctly keyed decoder reproduces it in zero-shot. We formally define ZSDN, a decoder-binding advantage metric, and outline deployment considerations for secure artificial intelligence (AI) pipelines. Finally, we discuss learnability risks (e.g., adapter alignment) and outline mitigations. MoBLE offers a lightweight, accelerator-friendly approach to secure AI deployment in safety-critical domains, including aviation and cyber-physical systems.", "categories": ["cs.CR", "cs.AI", "eess.SP"], "abs_url": "https://arxiv.org/abs/2511.00973", "pdf_url": "https://arxiv.org/pdf/2511.00973.pdf", "is_interesting": false}, "780": {"title": "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL", "authors": ["Yiwen Jiao, Tonghui Ren, Yuche Gao, Zhenying He, Yinan Jing, Kai Zhang, X. Sean Wang"], "abstract": "arXiv:2511.00985v2 Announce Type: cross \nLarge Language Models (LLMs) have demonstrated remarkable progress in translating natural language to SQL, but a significant semantic gap persists between their general knowledge and domain-specific semantics of databases. Historical translation logs constitute a rich source of this missing in-domain knowledge, where SQL queries inherently encapsulate real-world usage patterns of database schema. Existing methods primarily enhance the reasoning process for individual translations but fail to accumulate in-domain knowledge from past translations. We introduce ORANGE, an online self-evolutionary framework that constructs database-specific knowledge bases by parsing SQL queries from translation logs. By accumulating in-domain knowledge that contains schema and data semantics, ORANGE progressively reduces the semantic gap and enhances the accuracy of subsequent SQL translations. To ensure reliability, we propose a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking, which reduces semantic errors during knowledge generation. Experiments on multiple benchmarks confirm the practicality of ORANGE, demonstrating its effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries.", "categories": ["cs.DB", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.00985", "pdf_url": "https://arxiv.org/pdf/2511.00985.pdf", "is_interesting": false}, "781": {"title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "authors": ["Bowen Chen (DK), Jayesh Gajbhar (DK), Gregory Dusek (DK), Rob Redmon (DK), Patrick Hogan (DK), Paul Liu (DK), DelWayne Bohnenstiehl (DK),  Dongkuan (DK),  Xu, Ruoying He"], "abstract": "arXiv:2511.01019v1 Announce Type: cross \nArtificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified \"hallucinations\" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as \"What was Boston Harbor's highest water level in 2024?\" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "abs_url": "https://arxiv.org/abs/2511.01019", "pdf_url": "https://arxiv.org/pdf/2511.01019.pdf", "is_interesting": false}, "782": {"title": "Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs Subliminal Transfer", "authors": ["Ay\\c{s}e Selin Okatan, Mustafa \\.Ilhan Akba\\c{s}, Laxima Niure Kandel, Berker Pek\\\"oz"], "abstract": "arXiv:2511.01023v1 Announce Type: cross \nWe analyze subliminal transfer in Transformer models, where a teacher embeds hidden traits that can be linearly decoded by a student without degrading main-task performance. Prior work often attributes transferability to global representational similarity, typically quantified with Centered Kernel Alignment (CKA). Using synthetic corpora with disentangled public and private labels, we distill students under matched and independent random initializations. We find that transfer strength hinges on alignment within a trait-discriminative subspace: same-seed students inherit this alignment and show higher leakage {\\tau \\approx} 0.24, whereas different-seed students -- despite global CKA > 0.9 -- exhibit substantially reduced excess accuracy {\\tau \\approx} 0.12 - 0.13. We formalize this with subspace-level CKA diagnostic and residualized probes, showing that leakage tracks alignment within the trait-discriminative subspace rather than global representational similarity. Security controls (projection penalty, adversarial reversal, right-for-the-wrong-reasons regularization) reduce leakage in same-base models without impairing public-task fidelity. These results establish seed-induced uniqueness as a resilience property and argue for subspace-aware diagnostics for secure multi-model deployments.", "categories": ["eess.SP", "cs.AI", "cs.CR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01023", "pdf_url": "https://arxiv.org/pdf/2511.01023.pdf", "is_interesting": false}, "783": {"title": "HAFixAgent: History-Aware Automated Program Repair Agent", "authors": ["Yu Shi, Hao Li, Bram Adams, Ahmed E. Hassan"], "abstract": "arXiv:2511.01047v1 Announce Type: cross \nAutomated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01047", "pdf_url": "https://arxiv.org/pdf/2511.01047.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "784": {"title": "Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms", "authors": ["Przemys{\\l}aw Spyra, Witold Dzwinel"], "abstract": "arXiv:2511.01061v1 Announce Type: cross \nThe long-held assumption that backpropagation (BP) is essential for state-of-the-art performance is challenged by this work. We present rigorous, hardware-validated evidence that the Mono-Forward (MF) algorithm, a backpropagation-free method, consistently surpasses an optimally tuned BP baseline in classification accuracy on its native Multi-Layer Perceptron (MLP) architectures. This superior generalization is achieved with profound efficiency gains, including up to 41% less energy consumption and up to 34% faster training. Our analysis, which charts an evolutionary path from Geoffrey Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF, is grounded in a fair comparative framework using identical architectures and universal hyperparameter optimization. We further provide a critical re-evaluation of memory efficiency in BP-free methods, empirically demonstrating that practical overhead can offset theoretical gains. Ultimately, this work establishes MF as a practical, high-performance, and sustainable alternative to BP for MLPs.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01061", "pdf_url": "https://arxiv.org/pdf/2511.01061.pdf", "is_interesting": false}, "785": {"title": "Continual Learning, Not Training: Online Adaptation For Agents", "authors": ["Aman Jaglan, Jarrod Barnes"], "abstract": "arXiv:2511.01093v1 Announce Type: cross \nContinual Learning (CL) methods have traditionally focused on mitigating catastrophic forgetting through gradient-based retraining, an approach ill-suited for deployed agents that must adapt in real time. We introduce our Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that decouples reasoning (Teacher) from execution (Student) and incorporates a persistent learning memory that stores distilled guidance from experience. This informs the orchestration layer, enabling the system to dynamically adjust its operational strategies, such as supervision level or initial plan selection, at inference time. In doing so, ATLAS achieves gradient-free continual learning, shifting the locus of adaptation from model parameters to system-level orchestration. We formulate this as a system-centric paradigm for continual learning, where the objective is adaptive efficiency: maximizing task success while minimizing computational cost through inference-time orchestration rather than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1% success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High) by 13% while reducing cost by 86%. Cross-incident validation demonstrates generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to 41% with zero retraining, while shifting output composition from verbose exploration to structured reasoning. Together, these findings establish gradient-free continual learning as a viable path toward adaptive, deployable AI systems and provide causally annotated traces valuable for training explicit world models.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01093", "pdf_url": "https://arxiv.org/pdf/2511.01093.pdf", "is_interesting": false}, "786": {"title": "AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat Intelligence", "authors": ["Md Tanvirul Alam, Dipkamal Bhusal, Salman Ahmad, Nidhi Rastogi, Peter Worth"], "abstract": "arXiv:2511.01144v1 Announce Type: cross \nLarge Language Models (LLMs) have demonstrated strong capabilities in natural language reasoning, yet their application to Cyber Threat Intelligence (CTI) remains limited. CTI analysis involves distilling large volumes of unstructured reports into actionable knowledge, a process where LLMs could substantially reduce analyst workload. CTIBench introduced a comprehensive benchmark for evaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by developing AthenaBench, an enhanced benchmark that includes an improved dataset creation pipeline, duplicate removal, refined evaluation metrics, and a new task focused on risk mitigation strategies. We evaluate twelve LLMs, including state-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside seven open-source models from the LLaMA and Qwen families. While proprietary LLMs achieve stronger results overall, their performance remains subpar on reasoning-intensive tasks, such as threat actor attribution and risk mitigation, with open-source models trailing even further behind. These findings highlight fundamental limitations in the reasoning capabilities of current LLMs and underscore the need for models explicitly tailored to CTI workflows and automation.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01144", "pdf_url": "https://arxiv.org/pdf/2511.01144.pdf", "is_interesting": false}, "787": {"title": "A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation", "authors": ["Faquan Chen, Qingyang Tian, Ziren Wu, Rendong Ying, Fei Wen, Peilin Liu"], "abstract": "arXiv:2511.01158v1 Announce Type: cross \nSynaptic delay has attracted significant attention in neural network dynamics for integrating and processing complex spatiotemporal information. This paper introduces a high-throughput Spiking Neural Network (SNN) processor that supports synaptic delay-based emulation for edge applications. The processor leverages a multicore pipelined architecture with parallel compute engines, capable of real-time processing of the computational load associated with synaptic delays. We develop a SoC prototype of the proposed processor on PYNQ Z2 FPGA platform and evaluate its performance using the Spiking Heidelberg Digits (SHD) benchmark for low-power keyword spotting tasks. The processor achieves 93.4% accuracy in deployment and an average throughput of 104 samples/sec at a typical operating frequency of 125 MHz and 282 mW power consumption.", "categories": ["cs.NE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01158", "pdf_url": "https://arxiv.org/pdf/2511.01158.pdf", "is_interesting": false}, "788": {"title": "Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification", "authors": ["Ali Owfi, Amirmohammad Bamdad, Tolunay Seyfi, Fatemeh Afghah"], "abstract": "arXiv:2511.01172v1 Announce Type: cross \nDeep learning has emerged as a leading approach for Automatic Modulation Classification (AMC), demonstrating superior performance over traditional methods. However, vulnerability to adversarial attacks and susceptibility to data distribution shifts hinder their practical deployment in real-world, dynamic environments. To address these threats, we propose a novel, unified framework that integrates meta-learning with domain adaptation, making AMC systems resistant to both adversarial attacks and environmental changes. Our framework utilizes a two-phase strategy. First, in an offline phase, we employ a meta-learning approach to train the model on clean and adversarially perturbed samples from a single source domain. This method enables the model to generalize its defense, making it resistant to a combination of previously unseen attacks. Subsequently, in the online phase, we apply domain adaptation to align the model's features with a new target domain, allowing it to adapt without requiring substantial labeled data. As a result, our framework achieves a significant improvement in modulation classification accuracy against these combined threats, offering a critical solution to the deployment and operational challenges of modern AMC systems.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "abs_url": "https://arxiv.org/abs/2511.01172", "pdf_url": "https://arxiv.org/pdf/2511.01172.pdf", "is_interesting": false}, "789": {"title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "authors": ["Lvhua Wu, Xuefeng Jiang, Sheng Sun, Tian Wen, Yuwei Wang, Min Liu"], "abstract": "arXiv:2511.01188v1 Announce Type: cross \nThe rapid spread of fake news threatens social stability and public trust, rendering its detection an imperative research priority. Although large language models (LLMs) excel at numerous natural language processing tasks with their remarkable contextual understanding and extensive prior knowledge, the time-bounded knowledge coverage and tendency for generating hallucination content reduce their reliability when handling fast-evolving news streams. Furthermore, models trained on existing static datasets also often lack the generalization needed for emerging news topics. To address these challenges, we propose ZoFia, a novel two-stage zero-shot fake news detection framework. First, we introduce Hierarchical Salience to quantify the importance of entities in the news content, and propose the SC-MMR algorithm to effectively select an informative and diverse set of keywords that serve as queries for retrieving up-to-date external evidence. Subsequently, a multi LLM interactive system, in which each agent assumes a distinct role, performs multi-view collaborative analysis and adversarial debate over the news text and its related information, and finally produces an interpretable and robust judgment. Comprehensive experiments on two public datasets demonstrate that ZoFia obviously outperforms existing zero-shot baselines and most of few-shot methods. Our codes will be open-sourced to facilitate related communities.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01188", "pdf_url": "https://arxiv.org/pdf/2511.01188.pdf", "is_interesting": false}, "790": {"title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "authors": ["Ru Wang, Wei Huang, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo, Jiaxian Guo"], "abstract": "arXiv:2511.01191v1 Announce Type: cross \nTest-time reinforcement learning (TTRL) offers a label-free paradigm for adapting models using only synthetic signals at inference, but its success hinges on constructing reliable learning signals. Standard approaches such as majority voting often collapse to spurious yet popular answers. We introduce Self-Harmony, a framework built on a simple intuition: the correct answer should remain stable across both an original question and its paraphrase. Self-Harmony operationalizes this by employing a single model in two complementary roles: a Solver to produce answers and a Reframer to rephrase the input. Based on this, we further propose a pseudo-label method: instead of majority voting, it aggregates answer frequencies across these original and reframed views using the harmonic mean. This is a process that naturally selects for solutions stable under reframing, thereby avoiding the common trap of favoring view-dependent, spurious answers. Crucially, this requires no human supervision or auxiliary models. Across diverse reasoning benchmarks, Self-Harmony achieves state-of-the-art results at the label-free test-time setting, ranking first in 28 of 30 settings across multiple methods. Beyond accuracy, it demonstrates unprecedented robustness, with zero training failures in all experiments, underscoring its stability and reliability.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01191", "pdf_url": "https://arxiv.org/pdf/2511.01191.pdf", "is_interesting": false}, "791": {"title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation", "authors": ["Jicong Fan"], "abstract": "arXiv:2511.01196v1 Announce Type: cross \nMissing data is a fundamental challenge in data science, significantly hindering analysis and decision-making across a wide range of disciplines, including healthcare, bioinformatics, social science, e-commerce, and industrial monitoring. Despite decades of research and numerous imputation methods, the literature remains fragmented across fields, creating a critical need for a comprehensive synthesis that connects statistical foundations with modern machine learning advances. This work systematically reviews core concepts-including missingness mechanisms, single versus multiple imputation, and different imputation goals-and examines problem characteristics across various domains. It provides a thorough categorization of imputation methods, spanning classical techniques (e.g., regression, the EM algorithm) to modern approaches like low-rank and high-rank matrix completion, deep learning models (autoencoders, GANs, diffusion models, graph neural networks), and large language models. Special attention is given to methods for complex data types, such as tensors, time series, streaming data, graph-structured data, categorical data, and multimodal data. Beyond methodology, we investigate the crucial integration of imputation with downstream tasks like classification, clustering, and anomaly detection, examining both sequential pipelines and joint optimization frameworks. The review also assesses theoretical guarantees, benchmarking resources, and evaluation metrics. Finally, we identify critical challenges and future directions, emphasizing model selection and hyperparameter optimization, the growing importance of privacy-preserving imputation via federated learning, and the pursuit of generalizable models that can adapt across domains and data types, thereby outlining a roadmap for future research.", "categories": ["stat.ML", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01196", "pdf_url": "https://arxiv.org/pdf/2511.01196.pdf", "is_interesting": false}, "792": {"title": "Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs", "authors": ["Bo Bai"], "abstract": "arXiv:2511.01202v1 Announce Type: cross \nLarge language models (LLMs) have demonstrated remarkable capabilities in numerous real-world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate-distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research.", "categories": ["cs.IT", "cs.AI", "math.IT"], "abs_url": "https://arxiv.org/abs/2511.01202", "pdf_url": "https://arxiv.org/pdf/2511.01202.pdf", "is_interesting": false}, "793": {"title": "Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks", "authors": ["Jiahui Gao, Kuang Zhou, Yuchen Zhu"], "abstract": "arXiv:2511.01228v1 Announce Type: cross \nNode importance ranking is a fundamental problem in graph data analysis. Existing approaches typically rely on node features derived from either traditional centrality measures or advanced graph representation learning methods, which depend directly on the target network's topology. However, this reliance on structural information raises privacy concerns and often leads to poor generalization across different networks. In this work, we address a key question: Can we design a node importance ranking model trained exclusively on synthetic networks that is effectively appliable to real-world networks, eliminating the need to rely on the topology of target networks and improving both practicality and generalizability? We answer this question affirmatively by proposing the Influence-aware Causal Autoencoder Network (ICAN), a novel framework that leverages causal representation learning to get robust, invariant node embeddings for cross-network ranking tasks. Firstly, ICAN introduces an influence-aware causal representation learning module within an autoencoder architecture to extract node embeddings that are causally related to node importance. Moreover, we introduce a causal ranking loss and design a unified optimization framework that jointly optimizes the reconstruction and ranking objectives, enabling mutual reinforcement between node representation learning and ranking optimization. This design allows ICAN, trained on synthetic networks, to generalize effectively across diverse real-world graphs. Extensive experiments on multiple benchmark datasets demonstrate that ICAN consistently outperforms state-of-the-art baselines in terms of both ranking accuracy and generalization capability.", "categories": ["cs.SI", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01228", "pdf_url": "https://arxiv.org/pdf/2511.01228.pdf", "is_interesting": false}, "794": {"title": "Quantum Deep Learning Still Needs a Quantum Leap", "authors": ["Hans Gundlach, Hrvoje Kukina, Jayson Lynch, Neil Thompson"], "abstract": "arXiv:2511.01253v1 Announce Type: cross \nQuantum computing technology is advancing rapidly. Yet, even accounting for these trends, a quantum leap would be needed for quantum computers to meaningfully impact deep learning over the coming decade or two. We arrive at this conclusion based on a first-of-its-kind survey of quantum algorithms and how they match potential deep learning applications. This survey reveals three important areas where quantum computing could potentially accelerate deep learning, each of which faces a challenging roadblock to realizing its potential. First, quantum algorithms for matrix multiplication and other algorithms central to deep learning offer small theoretical improvements in the number of operations needed, but this advantage is overwhelmed on practical problem sizes by how slowly quantum computers do each operation. Second, some promising quantum algorithms depend on practical Quantum Random Access Memory (QRAM), which is underdeveloped. Finally, there are quantum algorithms that offer large theoretical advantages, but which are only applicable to special cases, limiting their practical benefits. In each of these areas, we support our arguments using quantitative forecasts of quantum advantage that build on the work by Choi et al. [2023] as well as new research on limitations and quantum hardware trends. Our analysis outlines the current scope of quantum deep learning and points to research directions that could lead to greater practical advances in the field.", "categories": ["quant-ph", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01253", "pdf_url": "https://arxiv.org/pdf/2511.01253.pdf", "is_interesting": false}, "795": {"title": "Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play", "authors": ["Jiatong Shi, Jionghao Han, Yichen Lu, Santiago Pascual, Pengfei Wu, Chenye Cui, Shinji Watanabe, Chao Weng, Cong Zhou"], "abstract": "arXiv:2511.01261v1 Announce Type: cross \nRole-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present Speech-DRAME, a unified framework that contributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: Archetype Evaluation, a top-down approach measuring adherence to broad role archetypes, and Realism Evaluation, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, DRAME-Eval achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.", "categories": ["cs.SD", "cs.AI", "eess.AS"], "abs_url": "https://arxiv.org/abs/2511.01261", "pdf_url": "https://arxiv.org/pdf/2511.01261.pdf", "is_interesting": false}, "796": {"title": "Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems", "authors": ["Minseok Kim, Hankook Lee, Hyungjoon Koo"], "abstract": "arXiv:2511.01268v1 Announce Type: cross \nLarge language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating responses grounded in external knowledge sources. A typical RAG system consists of i) a retriever that probes a group of relevant passages from a knowledge base and ii) a generator that formulates a response based on the retrieved content. However, as with other AI systems, recent studies demonstrate the vulnerability of RAG, such as knowledge corruption attacks by injecting misleading information. In response, several defense strategies have been proposed, including having LLMs inspect the retrieved passages individually or fine-tuning robust retrievers. While effective, such approaches often come with substantial computational costs.\n  In this work, we introduce RAGDefender, a resource-efficient defense mechanism against knowledge corruption (i.e., by data poisoning) attacks in practical RAG deployments. RAGDefender operates during the post-retrieval phase, leveraging lightweight machine learning techniques to detect and filter out adversarial content without requiring additional model training or inference. Our empirical evaluations show that RAGDefender consistently outperforms existing state-of-the-art defenses across multiple models and adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR) against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber legitimate ones by a factor of four (4x).", "categories": ["cs.CR", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.01268", "pdf_url": "https://arxiv.org/pdf/2511.01268.pdf", "is_interesting": false}, "797": {"title": "Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting", "authors": ["Zan Li, Kyongmin Yeo, Wesley Gifford, Lara Marcuse, Madeline Fields, B\\\"ulent Yener"], "abstract": "arXiv:2511.01275v1 Announce Type: cross \nForecasting epileptic seizures from multivariate EEG signals represents a critical challenge in healthcare time series prediction, requiring high sensitivity, low false alarm rates, and subject-specific adaptability. We present STAN, an Adversarial Spatio-Temporal Attention Network that jointly models spatial brain connectivity and temporal neural dynamics through cascaded attention blocks with alternating spatial and temporal modules. Unlike existing approaches that assume fixed preictal durations or separately process spatial and temporal features, STAN captures bidirectional dependencies between spatial and temporal patterns through a unified cascaded architecture. Adversarial training with gradient penalty enables robust discrimination between interictal and preictal states learned from clearly defined 15-minute preictal windows. Continuous 90-minute pre-seizure monitoring reveals that the learned spatio-temporal attention patterns enable early detection: reliable alarms trigger at subject-specific times (typically 15-45 minutes before onset), reflecting the model's capacity to capture subtle preictal dynamics without requiring individualized training. Experiments on two benchmark EEG datasets (CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14 events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011 false detections per hour and 94.2% sensitivity with 0.063 false detections per hour, respectively, while maintaining computational efficiency (2.3M parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond epilepsy, the proposed framework provides a general paradigm for spatio-temporal forecasting in healthcare and other time series domains where individual heterogeneity and interpretability are crucial.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01275", "pdf_url": "https://arxiv.org/pdf/2511.01275.pdf", "is_interesting": false}, "798": {"title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "authors": ["Min Fang, Zhihui Fu, Qibin Zhao, Jun Wang"], "abstract": "arXiv:2511.01282v1 Announce Type: cross \nSpeculative decoding (SD) has emerged as an effective technique to accelerate large language model (LLM) inference without compromising output quality. However, the achievable speedup largely depends on the effectiveness of the drafting model. While model-based methods like EAGLE-2 are accurate but costly, retrieval-enhanced methods like SAM-Decoding rely on heuristic switching strategies that often trigger unnecessary retrievals. To address this, we propose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a novel framework that transforms heuristic drafter switching into adaptive decision-making. ReSpec features three core innovations: 1) An \\textbf{entropy-guided adaptive trigger} quantifies contextual predictability to initiate retrieval only when uncertainty is low, avoiding costly low-quality speculations. 2) A \\textbf{feedback-driven candidate selection} leverages historical feedback to organize multiple high-quality candidates for parallel verification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed verification strategy} applies strict checks to model-generated drafts while using a relaxed verification for retrieved drafts, achieving a better balance between accuracy and efficiency. Extensive experiments on Spec-Bench demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming EAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while maintaining output quality.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01282", "pdf_url": "https://arxiv.org/pdf/2511.01282.pdf", "is_interesting": false}, "799": {"title": "LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping", "authors": ["Guanjie Cheng, Mengzhen Yang, Xinkui Zhao, Shuyi Yu, Tianyu Du, Yangyang Wu, Mengying Zhu, Shuiguang Deng"], "abstract": "arXiv:2511.01296v1 Announce Type: cross \nFederated learning (FL) enables collaborative model training across distributed nodes without exposing raw data, but its decentralized nature makes it vulnerable in trust-deficient environments. Inference attacks may recover sensitive information from gradient updates, while poisoning attacks can degrade model performance or induce malicious behaviors. Existing defenses often suffer from high communication and computation costs, or limited detection precision. To address these issues, we propose LSHFed, a robust and communication-efficient FL framework that simultaneously enhances aggregation robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a novel gradient verification mechanism that projects high-dimensional gradients into compact binary representations via multi-hyperplane locally-sensitive hashing. This enables accurate detection and filtering of malicious gradients using only their irreversible hash forms, thus mitigating privacy leakage risks and substantially reducing transmission overhead. Extensive experiments demonstrate that LSHFed maintains high model performance even when up to 50% of participants are collusive adversaries while achieving up to a 1000x reduction in gradient verification communication compared to full-gradient methods.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01296", "pdf_url": "https://arxiv.org/pdf/2511.01296.pdf", "is_interesting": false}, "800": {"title": "DeepSpecs: Expert-Level Questions Answering in 5G", "authors": ["Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu"], "abstract": "arXiv:2511.01305v1 Announce Type: cross \n5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.", "categories": ["cs.CL", "cs.AI", "cs.NI"], "abs_url": "https://arxiv.org/abs/2511.01305", "pdf_url": "https://arxiv.org/pdf/2511.01305.pdf", "is_interesting": false}, "801": {"title": "Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation", "authors": ["Chong Wang, Chen Zhang, Jiajun Wu, Wunan Guo, Jianfeng Qu, Yewen Tian, Yang Liu"], "abstract": "arXiv:2511.01316v1 Announce Type: cross \nContinuous Integration (CI) is a cornerstone of modern collaborative software development, and numerous CI platforms are available. Differences in maintenance overhead, reliability, and integration depth with code-hosting platforms make migration between CI platforms a common practice. A central step in migration is translating CI configurations, which is challenging due to the intrinsic complexity of CI configurations and the need to understand semantic differences and relationships across CI platforms.\n  With the advent of large language models (LLMs), recent advances in software engineering highlight their potential for CI configuration translation. In this paper, we present a study on LLM-based CI configuration translation, focusing on the migration from Travis CI to GitHub Actions. First, using 811 migration records, we quantify the effort involved and find that developers read an average of 38 lines of Travis configuration and write 58 lines of GitHub Actions configuration, with nearly half of the migrations requiring multiple commits. We further analyze translations produced by each of the four LLMs and identify 1,121 issues grouped into four categories: logic inconsistencies (38%), platform discrepancies (32%), environment errors (25%), and syntax errors (5%). Finally, we evaluate three enhancement strategies and show that combining guideline-based prompting with iterative refinement achieves the best performance, reaching a Build Success Rate of 75.5%-nearly a threefold improvement over GPT-4o with a basic prompt.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01316", "pdf_url": "https://arxiv.org/pdf/2511.01316.pdf", "is_interesting": false}, "802": {"title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "authors": ["Jiabao Ji, Min Li, Priyanshu Kumar, Shiyu Chang, Saloni Potdar"], "abstract": "arXiv:2511.01323v1 Announce Type: cross \nLarge language models (LLMs) with integrated search tools show strong promise in open-domain question answering (QA), yet they often struggle to produce complete answer set to complex questions such as Which actor from the film Heat won at least one Academy Award?, which requires (1) distinguishing between multiple films sharing the same title and (2) reasoning across a large set of actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate both challenges jointly. To address this, we introduce DeepAmbigQAGen, an automatic data generation pipeline that constructs QA tasks grounded in text corpora and linked knowledge graph, generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning. Based on this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop reasoning and half of them explicit name ambiguity resolving. Experiments reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions. These findings highlight the need for more robust QA systems aimed at information gathering and answer completeness.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01323", "pdf_url": "https://arxiv.org/pdf/2511.01323.pdf", "is_interesting": false}, "803": {"title": "AI for Requirements Engineering: Industry adoption and Practitioner perspectives", "authors": ["Lekshmi Murali Rani, Richard Berntsson Svensson, Robert Feldt"], "abstract": "arXiv:2511.01324v2 Announce Type: cross \nThe integration of AI for Requirements Engineering (RE) presents significant benefits but also poses real challenges. Although RE is fundamental to software engineering, limited research has examined AI adoption in RE. We surveyed 55 software practitioners to map AI usage across four RE phases: Elicitation, Analysis, Specification, and Validation, and four approaches for decision making: human-only decisions, AI validation, Human AI Collaboration (HAIC), and full AI automation. Participants also shared their perceptions, challenges, and opportunities when applying AI for RE tasks. Our data show that 58.2% of respondents already use AI in RE, and 69.1% view its impact as positive or very positive.HAIC dominates practice, accounting for 54.4% of all RE techniques, while full AI automation remains minimal at 5.4%. Passive AI validation (4.4 to 6.2%) lags even further behind, indicating that practitioners value AI's active support over passive oversight. These findings suggest that AI is most effective when positioned as a collaborative partner rather than a replacement for human expertise. It also highlights the need for RE-specific HAIC frameworks along with robust and responsible AI governance as AI adoption in RE grows.", "categories": ["cs.SE", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.01324", "pdf_url": "https://arxiv.org/pdf/2511.01324.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "804": {"title": "Embodied Cognition Augmented End2End Autonomous Driving", "authors": ["Ling Niu, Xiaoji Zheng, Han Wang, Chen Zheng, Ziyuan Yang, Bokui Chen, Jiangtao Gong"], "abstract": "arXiv:2511.01334v1 Announce Type: cross \nIn recent years, vision-based end-to-end autonomous driving has emerged as a new paradigm. However, popular end-to-end approaches typically rely on visual feature extraction networks trained under label supervision. This limited supervision framework restricts the generality and applicability of driving models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which advocates for comparative learning between visual feature extraction networks and the general EEG large model, in order to learn latent human driving cognition for enhancing end-to-end planning. In this work, we collected a cognitive dataset for the mentioned contrastive learning process. Subsequently, we investigated the methods and potential mechanisms for enhancing end-to-end planning with human driving cognition, using popular driving models as baselines on publicly available autonomous driving datasets. Both open-loop and closed-loop tests are conducted for a comprehensive evaluation of planning performance. Experimental results demonstrate that the $E^{3}AD$ paradigm significantly enhances the end-to-end planning performance of baseline models. Ablation studies further validate the contribution of driving cognition and the effectiveness of comparative learning process. To the best of our knowledge, this is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning. It represents an initial attempt to incorporate embodied cognitive data into end-to-end autonomous driving, providing valuable insights for future brain-inspired autonomous driving systems. Our code will be made available at Github", "categories": ["cs.RO", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.01334", "pdf_url": "https://arxiv.org/pdf/2511.01334.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "\u7aef\u5230\u7aef\u5b66\u4e60 / \u89c4\u5212\u63a7\u5236 / \u4eba\u7c7b\u8ba4\u77e5\u9a71\u52a8\u81ea\u52a8\u9a7e\u9a76", "reason": "The paper explicitly focuses on end-to-end autonomous driving, introducing a novel framework that integrates embodied human driving cognition into planning and control. It discusses learning paradigms, cognitive datasets, and evaluations on autonomous driving benchmarks, clearly within the autonomous driving domain."}}, "805": {"title": "Beyond Permissions: Investigating Mobile Personalization with Simulated Personas", "authors": ["Ibrahim Khalilov, Chaoran Chen, Ziang Xiao, Tianshi Li, Toby Jia-Jun Li, Yaxing Yao"], "abstract": "arXiv:2511.01336v1 Announce Type: cross \nMobile applications increasingly rely on sensor data to infer user context and deliver personalized experiences. Yet the mechanisms behind this personalization remain opaque to users and researchers alike. This paper presents a sandbox system that uses sensor spoofing and persona simulation to audit and visualize how mobile apps respond to inferred behaviors. Rather than treating spoofing as adversarial, we demonstrate its use as a tool for behavioral transparency and user empowerment. Our system injects multi-sensor profiles - generated from structured, lifestyle-based personas - into Android devices in real time, enabling users to observe app responses to contexts such as high activity, location shifts, or time-of-day changes. With automated screenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps document subtle personalization cues. Preliminary findings show measurable app adaptations across fitness, e-commerce, and everyday service apps such as weather and navigation. We offer this toolkit as a foundation for privacy-enhancing technologies and user-facing transparency interventions.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01336", "pdf_url": "https://arxiv.org/pdf/2511.01336.pdf", "is_interesting": false}, "806": {"title": "The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project", "authors": ["Robin Gr\\\"opler, Steffen Klepke, Jack Johns, Andreas Dreschinski, Klaus Schmid, Benedikt Dornauer, Eray T\\\"uz\\\"un, Joost Noppen, Mohammad Reza Mousavi, Yongjian Tang, Johannes Viehmann, Selin \\c{S}irin Aslang\\\"ul, Beum Seuk Lee, Adam Ziolkowski, Eric Zie"], "abstract": "arXiv:2511.01348v1 Announce Type: cross \nGenerative AI (GenAI) has recently emerged as a groundbreaking force in Software Engineering, capable of generating code, suggesting fixes, and supporting quality assurance. While its use in coding tasks shows considerable promise, applying GenAI across the entire Software Development Life Cycle (SDLC) has not yet been fully explored. Critical uncertainties in areas such as reliability, accountability, security, and data privacy demand deeper investigation and coordinated action. The GENIUS project, comprising over 30 European industrial and academic partners, aims to address these challenges by advancing AI integration across all SDLC phases. It focuses on GenAI's potential, the development of innovative tools, and emerging research challenges, actively shaping the future of software engineering. This vision paper presents a shared perspective on the future of GenAI-based software engineering, grounded in cross-sector dialogue and experience within the GENIUS consortium, supported by an exploratory literature review. The paper explores four central elements: (1) a structured overview of current challenges in GenAI adoption across the SDLC; (2) a forward-looking vision outlining key technological and methodological advances expected over the next five years; (3) anticipated shifts in the roles and required skill sets of software professionals; and (4) the contribution of GENIUS in realizing this transformation through practical tools and industrial validation. By aligning technical innovation with business relevance, this paper aims to inform both research agendas and industrial strategies, providing a foundation for reliable, scalable, and industry-ready GenAI solutions for software engineering teams.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01348", "pdf_url": "https://arxiv.org/pdf/2511.01348.pdf", "is_interesting": false}, "807": {"title": "AI Literacy in UAE Libraries: Assessing Competencies, Training Needs, and Ethical Considerations for the Digital Age", "authors": ["Zafar Imam Khan"], "abstract": "arXiv:2511.01353v1 Announce Type: cross \nThe study explores the current state of artificial intelligence (AI) literacy levels among library professionals employing a quantitative approach consisting of 92 surveys of LIS professionals in the United Arab Emirates (UAE). Findings of the study revealed the presence of strong cognitive competencies, while there were gaps observed in behavioral and normative competencies, especially related to AI biases, AI-powered learning, and ethical considerations. There was a disconnect observed between the perceived importance of AI skills and the effectiveness of the current training programs.", "categories": ["cs.DL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01353", "pdf_url": "https://arxiv.org/pdf/2511.01353.pdf", "is_interesting": false}, "808": {"title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "authors": ["Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang"], "abstract": "arXiv:2511.01354v1 Announce Type: cross \nRecently, the demand for small and efficient reasoning models to support real-world applications has driven the development of knowledge distillation techniques that balance reasoning performance and inference speed. In this paper, we further extend the DistilQwen model family, initialized from the Qwen models, by introducing four model series specifically designed to meet industrial requirements. The distilled model collection comprises: (1) slow-thinking models, optimized for reasoning tasks that require high accuracy; (2) two series of adaptive-thinking models, which dynamically adjust reasoning strategies based on input tasks to maximize efficiency across diverse scenarios; and (3) distilled reward models, which enable further reinforcement learning of reasoning models using distilled knowledge. Comprehensive evaluations across multiple benchmarks demonstrate both high inference efficiency and strong reasoning performance for these models, as well as the practical utility of distilled reward models. We further show that these models support industry practitioners by providing scalable training and inference functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence) platform.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01354", "pdf_url": "https://arxiv.org/pdf/2511.01354.pdf", "is_interesting": false}, "809": {"title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "authors": ["Sapir Harary, Eran Hirsch, Aviv Slobodkin, David Wan, Mohit Bansal, Ido Dagan"], "abstract": "arXiv:2511.01359v1 Announce Type: cross \nNatural Language Inference (NLI) models have been used in various ways to improve the factuality of LLM outputs. This is typically done by applying an NLI model to judge whether the model output is entailed from the supposed evidence, triggering some corrective actions, such as beam reranking at inference time or RL rewards during training. While NLI models are trained to detect factual inconsistencies over complete sentences, decisions in the common autoregressive generation architecture are made for each evolving text prefix, during decoding. Addressing this setting, we generalize the entailment detection task to apply over arbitrary text prefixes, and suggest its utility for improving generation faithfulness. Providing suitable evaluation and training datasets for this task, we train MiniTruePrefixes, a novel specialized model that better detects factual inconsistencies over text prefixes, outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level entailment. We further demonstrate that integrating MiniTruePrefixes into a controlled decoding framework substantially improves factual consistency in abstractive summarization. When guided by MiniTruePrefixes, LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from the same model family, while using only half the memory.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01359", "pdf_url": "https://arxiv.org/pdf/2511.01359.pdf", "is_interesting": false}, "810": {"title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "authors": ["Muhammed Yusuf Kartal (TOBB University of Economics and Technology), Suha Kagan Kose (Roketsan Inc), Korhan Sevin\\c{c} (TOBB University of Economics and Technology), Burak Aktas (Roketsan Inc)"], "abstract": "arXiv:2511.01386v1 Announce Type: cross \nRetrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8\\% on average (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in retrieval and +7.5\\% in generation. The search typically explores $\\approx 0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone -- vector retrieval plus post-generation reflection/revision -- augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual/long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.01386", "pdf_url": "https://arxiv.org/pdf/2511.01386.pdf", "is_interesting": false}, "811": {"title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths", "authors": ["Paolo Rabino, Gabriele Tiboni, Tatiana Tommasi"], "abstract": "arXiv:2511.01407v1 Announce Type: cross \nObject-Centric Motion Generation (OCMG) is instrumental in advancing automated manufacturing processes, particularly in domains requiring high-precision expert robotic motions, such as spray painting and welding. To realize effective automation, robust algorithms are essential for generating extended, object-aware trajectories across intricate 3D geometries. However, contemporary OCMG techniques are either based on ad-hoc heuristics or employ learning-based pipelines that are still reliant on sensitive post-processing steps to generate executable paths. We introduce FoldPath, a novel, end-to-end, neural field based method for OCMG. Unlike prior deep learning approaches that predict discrete sequences of end-effector waypoints, FoldPath learns the robot motion as a continuous function, thus implicitly encoding smooth output paths. This paradigm shift eliminates the need for brittle post-processing steps that concatenate and order the predicted discrete waypoints. Particularly, our approach demonstrates superior predictive performance compared to recently proposed learning-based methods, and attains generalization capabilities even in real industrial settings, where only a limited amount of 70 expert samples are provided. We validate FoldPath through comprehensive experiments in a realistic simulation environment and introduce new, rigorous metrics designed to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG task towards practical maturity.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01407", "pdf_url": "https://arxiv.org/pdf/2511.01407.pdf", "is_interesting": false}, "812": {"title": "DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation", "authors": ["Hao Wang, Zixuan Weng, Jindong Han, Wei Fan, Hao Liu"], "abstract": "arXiv:2511.01468v1 Announce Type: cross \nData Assimilation is a cornerstone of atmospheric system modeling, tasked with reconstructing system states by integrating sparse, noisy observations with prior estimation. While traditional approaches like variational and ensemble Kalman filtering have proven effective, recent advances in deep learning offer more scalable, efficient, and flexible alternatives better suited for complex, real-world data assimilation involving large-scale and multi-modal observations. However, existing deep learning-based DA research suffers from two critical limitations: (1) reliance on oversimplified scenarios with synthetically perturbed observations, and (2) the absence of standardized benchmarks for fair model comparison. To address these gaps, in this work, we introduce DAMBench, the first large-scale multi-modal benchmark designed to evaluate data-driven DA models under realistic atmospheric conditions. DAMBench integrates high-quality background states from state-of-the-art forecasting systems and real-world multi-modal observations (i.e., real-world weather stations and satellite imagery). All data are resampled to a common grid and temporally aligned to support systematic training, validation, and testing. We provide unified evaluation protocols and benchmark representative data assimilation approaches, including latent generative models and neural process frameworks. Additionally, we propose a lightweight multi-modal plugin to demonstrate how integrating realistic observations can enhance even simple baselines. Through comprehensive experiments, DAMBench establishes a rigorous foundation for future research, promoting reproducibility, fair comparison, and extensibility to real-world multi-modal scenarios. Our dataset and code are publicly available at https://github.com/figerhaowang/DAMBench.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01468", "pdf_url": "https://arxiv.org/pdf/2511.01468.pdf", "is_interesting": false}, "813": {"title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments", "authors": ["Cankut Bora Tuncer, Marc Toussaint, Ozgur S. Oguz"], "abstract": "arXiv:2511.01476v1 Announce Type: cross \nIn this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided Manipulation planner for highly constrained rearrangement problems. MO-SeGMan generates object placement sequences that minimize both replanning per object and robot travel distance while preserving critical dependency structures with a lazy evaluation method. To address highly cluttered, non-monotone scenarios, we propose a Selective Guided Forward Search (SGFS) that efficiently relocates only critical obstacles and to feasible relocation points. Furthermore, we adopt a refinement method for adaptive subgoal selection to eliminate unnecessary pick-and-place actions, thereby improving overall solution quality. Extensive evaluations on nine benchmark rearrangement tasks demonstrate that MO-SeGMan generates feasible motion plans in all cases, consistently achieving faster solution times and superior solution quality compared to the baselines. These results highlight the robustness and scalability of the proposed framework for complex rearrangement planning problems.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01476", "pdf_url": "https://arxiv.org/pdf/2511.01476.pdf", "is_interesting": false}, "814": {"title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "authors": ["Ayesha Afroza Mohsin, Mashrur Ahsan, Nafisa Maliyat, Shanta Maria, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan"], "abstract": "arXiv:2511.01512v1 Announce Type: cross \nToxic language in Bengali remains prevalent, especially in online environments, with few effective precautions against it. Although text detoxification has seen progress in high-resource languages, Bengali remains underexplored due to limited resources. In this paper, we propose a novel pipeline for Bengali text detoxification that combines Pareto class-optimized large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate detoxified sentences. To support this effort, we construct BanglaNirTox, an artificially generated parallel corpus of 68,041 toxic Bengali sentences with class-wise toxicity labels, reasonings, and detoxified paraphrases, using Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox dataset is used to fine-tune language models to produce better detoxified versions of Bengali sentences. Our findings show that Pareto-optimized LLMs with CoT prompting significantly enhance the quality and consistency of Bengali text detoxification.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01512", "pdf_url": "https://arxiv.org/pdf/2511.01512.pdf", "is_interesting": false}, "815": {"title": "Real-time Continual Learning on Intel Loihi 2", "authors": ["Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke H\\\"ullermeier, Yulia Sandamirskaya, Mike Davies"], "abstract": "arXiv:2511.01553v1 Announce Type: cross \nAI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without catastrophic forgetting--remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intel's Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\\times faster (0.33ms vs 23.2ms), and 5,600\\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems.", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE"], "abs_url": "https://arxiv.org/abs/2511.01553", "pdf_url": "https://arxiv.org/pdf/2511.01553.pdf", "is_interesting": false}, "816": {"title": "HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET", "authors": ["Wang Hao, Kuang Zhang, Hou Chengyu, Yuan Zhonghao, Tan Chenxing, Fu Weifeng, Zhu Yangying"], "abstract": "arXiv:2511.01572v1 Announce Type: cross \nTime series classification holds broad application value in communications, information countermeasures, finance, and medicine. However, state-of-the-art (SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high computational complexity, coupled with lengthy parameter tuning and training cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional Kernel Transform) offer greater efficiency but leave substantial room for improvement in kernel selection and computational overhead. To address these challenges, we propose a feature extraction approach based on Hadamard convolutional transform, utilizing column or row vectors of Hadamard matrices as convolution kernels with extended lengths of varying sizes. This enhancement maintains full compatibility with existing methods (e.g., ROCKET) while leveraging kernel orthogonality to boost computational efficiency, robustness, and adaptability. Comprehensive experiments on multi-domain datasets-focusing on the UCR time series dataset-demonstrate SOTA performance: F1-score improved by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET (fastest ROCKET variant) under identical hyperparameters, enabling deployment on ultra-low-power embedded devices. All code is available on GitHub.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01572", "pdf_url": "https://arxiv.org/pdf/2511.01572.pdf", "is_interesting": false}, "817": {"title": "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems", "authors": ["Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria"], "abstract": "arXiv:2511.01583v1 Announce Type: cross \nDetecting malware, especially ransomware, is essential to securing today's interconnected ecosystems, including cloud storage, enterprise file-sharing, and database services. Training high-performing artificial intelligence (AI) detectors requires diverse datasets, which are often distributed across multiple organizations, making centralization necessary. However, centralized learning is often impractical due to security, privacy regulations, data ownership issues, and legal barriers to cross-organizational sharing. Compounding this challenge, ransomware evolves rapidly, demanding models that are both robust and adaptable.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, which enables multiple organizations to collaboratively train a ransomware detection model while keeping raw data local and secure. This paradigm is particularly relevant for cybersecurity companies (including both software and hardware vendors) that deploy ransomware detection or firewall systems across millions of endpoints. In such environments, data cannot be transferred outside the customer's device due to strict security, privacy, or regulatory constraints. Although FL applies broadly to malware threats, we validate the approach using the Ransomware Storage Access Patterns (RanSAP) dataset.\n  Our experiments demonstrate that FL improves ransomware detection accuracy by a relative 9% over server-local models and achieves performance comparable to centralized training. These results indicate that FL offers a scalable, high-performing, and privacy-preserving framework for proactive ransomware detection across organizational and regulatory boundaries.", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01583", "pdf_url": "https://arxiv.org/pdf/2511.01583.pdf", "is_interesting": false}, "818": {"title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "authors": ["Francisco Portillo L\\'opez"], "abstract": "arXiv:2511.01615v1 Announce Type: cross \nLinguistic errors are not merely deviations from normative grammar; they offer a unique window into the cognitive architecture of language and expose the current limitations of artificial systems that seek to replicate them. This project proposes an interdisciplinary study of linguistic errors produced by native Spanish speakers, with the aim of analyzing how current large language models (LLM) interpret, reproduce, or correct them. The research integrates three core perspectives: theoretical linguistics, to classify and understand the nature of the errors; neurolinguistics, to contextualize them within real-time language processing in the brain; and natural language processing (NLP), to evaluate their interpretation against linguistic errors. A purpose-built corpus of authentic errors of native Spanish (+500) will serve as the foundation for empirical analysis. These errors will be tested against AI models such as GPT or Gemini to assess their interpretative accuracy and their ability to generalize patterns of human linguistic behavior. The project contributes not only to the understanding of Spanish as a native language but also to the development of NLP systems that are more cognitively informed and capable of engaging with the imperfect, variable, and often ambiguous nature of real human language.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01615", "pdf_url": "https://arxiv.org/pdf/2511.01615.pdf", "is_interesting": false}, "819": {"title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving", "authors": ["Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian"], "abstract": "arXiv:2511.01633v1 Announce Type: cross \nGraph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01633", "pdf_url": "https://arxiv.org/pdf/2511.01633.pdf", "is_interesting": false}, "820": {"title": "Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models", "authors": ["Daniyal Ganiuly, Assel Smaiyl"], "abstract": "arXiv:2511.01634v1 Announce Type: cross \nLarge Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01634", "pdf_url": "https://arxiv.org/pdf/2511.01634.pdf", "is_interesting": false}, "821": {"title": "A Graph-based RAG for Energy Efficiency Question Answering", "authors": ["Riccardo Campi, Nicol\\`o Oreste Pinciroli Vago, Mathyas Giudici, Pablo Barrachina Rodriguez-Guisado, Marco Brambilla, Piero Fraternali"], "abstract": "arXiv:2511.01643v1 Announce Type: cross \nIn this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.01643", "pdf_url": "https://arxiv.org/pdf/2511.01643.pdf", "is_interesting": false}, "822": {"title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "authors": ["Ayesha Gull, Muhammad Usman Safder, Rania Elbadry, Preslav Nakov, Zhuohan Xie"], "abstract": "arXiv:2511.01650v1 Announce Type: cross \nLarge Language Models (LLMs) are increasingly being applied to specialized, high-stakes domains like engineering, which demands rigorous evaluation of their complex reasoning capabilities. While current benchmarks assess language understanding, factual recall, mathematics or code generation, none capture the integrative reasoning central to engineering where scientific principles, quantitative modeling and practical constraints must converge. To address this gap, we introduce EngChain, a benchmark for verifiable multi-step engineering problem-solving. EngChain contains 90 problems spanning three engineering branches, organized into 9 domains and 20 distinct areas. The problems are generated from symbolic templates with a high degree of randomization to ensure diversity and eliminate the risk of contamination. With this benchmark, we move beyond final answer accuracy with a two-stage evaluation: we first quantitatively verify the numerical and semantic validity of each reasoning step and then introduce LLM-As-A-Judge, an automated system to qualitatively categorize the identified reasoning errors.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01650", "pdf_url": "https://arxiv.org/pdf/2511.01650.pdf", "is_interesting": false}, "823": {"title": "The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity", "authors": ["Louis Bradshaw, Alexander Spangher, Stella Biderman, Simon Colton"], "abstract": "arXiv:2511.01663v1 Announce Type: cross \nWhile generative models for music composition are increasingly capable, their adoption by musicians is hindered by text-prompting, an asynchronous workflow disconnected from the embodied, responsive nature of instrumental performance. To address this, we introduce Aria-Duet, an interactive system facilitating a real-time musical duet between a human pianist and Aria, a state-of-the-art generative model, using a Yamaha Disklavier as a shared physical interface. The framework enables a turn-taking collaboration: the user performs, signals a handover, and the model generates a coherent continuation performed acoustically on the piano. Beyond describing the technical architecture enabling this low-latency interaction, we analyze the system's output from a musicological perspective, finding the model can maintain stylistic semantics and develop coherent phrasal ideas, demonstrating that such embodied systems can engage in musically sophisticated dialogue and open a promising new path for human-AI co-creation.", "categories": ["cs.SD", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.01663", "pdf_url": "https://arxiv.org/pdf/2511.01663.pdf", "is_interesting": false}, "824": {"title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "authors": ["Chaoqun Liu, Mahani Aljunied, Guizhen Chen, Hou Pong Chan, Weiwen Xu, Yu Rong, Wenxuan Zhang"], "abstract": "arXiv:2511.01670v1 Announce Type: cross \nWe introduce SeaLLMs-Audio, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01670", "pdf_url": "https://arxiv.org/pdf/2511.01670.pdf", "is_interesting": false}, "825": {"title": "Spin-Adapted Neural Network Wavefunctions in Real Space", "authors": ["Ruichen Li, Yuzhi Liu, Du Jiang, Yixiao Chen, Xuelan Wen, Wenrui Li, Di He, Liwei Wang, Ji Chen, Weiluo Ren"], "abstract": "arXiv:2511.01671v1 Announce Type: cross \nSpin plays a fundamental role in understanding electronic structure, yet many real-space wavefunction methods fail to adequately consider it. We introduce the Spin-Adapted Antisymmetrization Method (SAAM), a general procedure that enforces exact total spin symmetry for antisymmetric many-electron wavefunctions in real space. In the context of neural network-based quantum Monte Carlo (NNQMC), SAAM leverages the expressiveness of deep neural networks to capture electron correlation while enforcing exact spin adaptation via group representation theory. This framework provides a principled route to embed physical priors into otherwise black-box neural network wavefunctions, yielding a compact representation of correlated system with neural network orbitals. Compared with existing treatments of spin in NNQMC, SAAM is more accurate and efficient, achieving exact spin purity without any additional tunable hyperparameters. To demonstrate its effectiveness, we apply SAAM to study the spin ladder of iron-sulfur clusters, a long-standing challenge for many-body methods due to their dense spectrum of nearly degenerate spin states. Our results reveal accurate resolution of low-lying spin states and spin gaps in [Fe$_2$S$_2$] and [Fe$_4$S$_4$] clusters, offering new insights into their electronic structures. In sum, these findings establish SAAM as a robust, hyperparameter-free standard for spin-adapted NNQMC, particularly for strongly correlated systems.", "categories": ["physics.chem-ph", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01671", "pdf_url": "https://arxiv.org/pdf/2511.01671.pdf", "is_interesting": false}, "826": {"title": "Student Engagement in AI Assisted Complex Problem Solving: A Pilot Study of Human AI Rubik's Cube Collaboration", "authors": ["Kirk Vanacore, Jaclyn Ocumpaugh, Forest Agostinelli, Dezhi Wu, Sai Vuruma, Matt Irvin"], "abstract": "arXiv:2511.01683v1 Announce Type: cross \nGames and puzzles play important pedagogical roles in STEM learning. New AI algorithms that can solve complex problems offer opportunities for scaffolded instruction in puzzle solving. This paper presents the ALLURE system, which uses an AI algorithm (DeepCubeA) to guide students in solving a common first step of the Rubik's Cube (i.e., the white cross). Using data from a pilot study we present preliminary findings about students' behaviors in the system, how these behaviors are associated with STEM skills - including spatial reasoning, critical thinking and algorithmic thinking. We discuss how data from ALLURE can be used in future educational data mining to understand how students benefit from AI assistance and collaboration when solving complex problems.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01683", "pdf_url": "https://arxiv.org/pdf/2511.01683.pdf", "is_interesting": false}, "827": {"title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "authors": ["Sharan Maiya, Henning Bartsch, Nathan Lambert, Evan Hubinger"], "abstract": "arXiv:2511.01689v1 Announce Type: cross \nThe character of the \"AI assistant\" persona generated by modern chatbot large language models influences both surface-level behavior and apparent values, beliefs, and ethics. These all affect interaction quality, perceived intelligence, and alignment with both developer and user intentions. The shaping of this persona, known as character training, is a critical component of industry post-training, yet remains effectively unstudied in the academic literature. We introduce the first open implementation of character training, leveraging Constitutional AI and a new data pipeline using synthetic introspective data to shape the assistant persona in a more effective and controlled manner than alternatives such as constraining system prompts or activation steering. Specifically, we fine-tune three popular open-weights models using 11 example personas, such as humorous, deeply caring, or even malevolent. To track the effects of our approach, we introduce a method which analyzes revealed preferences, uncovering clear and holistic changes in character. We find these changes are more robust to adversarial prompting than the above two alternatives, while also leading to more coherent and realistic generations. Finally, we demonstrate this fine-tuning has little to no effect on general capabilities as measured by common benchmarks. We describe and open-source our full post-training method, the implementation of which can be found at https://github.com/maiush/OpenCharacterTraining.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01689", "pdf_url": "https://arxiv.org/pdf/2511.01689.pdf", "is_interesting": false}, "828": {"title": "Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering", "authors": ["Hossein Abdi, Mingfei Sun, Wei Pan"], "abstract": "arXiv:2511.01694v1 Announce Type: cross \nVision-language pre-trained models, such as CLIP, have established new benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a major challenge to achieve optimal performance on both in-distribution (ID) and out-of-distribution (OOD) datasets, especially when labeled data is scarce. Most existing fine-tuning approaches rely on first-order gradient-based optimizers, which typically suffer from slow convergence, sensitivity to step-size hyperparameters, and poor generalization in OOD settings. In contrast, second-order methods utilize local curvature information of the loss landscape to adjust the update step size. This is particularly beneficial for CLIP models, whose non-convex loss functions often contain sharp critical points. In such cases, natural gradient direction can offer more substantial and efficient per-iteration updates when fine-tuning with limited data. Natural Gradient Descent (NGD) is obtained by preconditioning the standard gradient with the inverse Fisher Information Matrix (FIM), which is computationally expensive for large models. To address this, we propose a Bayesian approximation of NGD using a Kalman filter for CLIP models. Our method combines the benefits of second-order optimization with Bayesian inference, which enhances generalization while providing uncertainty quantification. Extensive experiments conducted on diverse image classification datasets demonstrate that our algorithm consistently achieves superior--or comparable--ID performance and improved OOD robustness compared to state-of-the-art baselines. To the best of our knowledge, this work represents the first successful application of Kalman filtering to fine-tuning CLIP-based models, which enables more robust and efficient learning in vision-language tasks.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01694", "pdf_url": "https://arxiv.org/pdf/2511.01694.pdf", "is_interesting": false}, "829": {"title": "Solution Space Topology Guides CMTS Search", "authors": ["Mirco A. Mannucci"], "abstract": "arXiv:2511.01701v1 Announce Type: cross \nA fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \\emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints.\n  Our method: (1) detect pattern rules automatically with 100\\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores.\n  We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \\emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.", "categories": ["cs.CE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01701", "pdf_url": "https://arxiv.org/pdf/2511.01701.pdf", "is_interesting": false}, "830": {"title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "authors": ["Sekh Mainul Islam, Pepa Atanasova, Isabelle Augenstein"], "abstract": "arXiv:2511.01706v1 Announce Type: cross \nNatural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01706", "pdf_url": "https://arxiv.org/pdf/2511.01706.pdf", "is_interesting": false}, "831": {"title": "A Proof of Learning Rate Transfer under $\\mu$P", "authors": ["Soufiane Hayou"], "abstract": "arXiv:2511.01734v1 Announce Type: cross \nWe provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\\mu P$, the optimal learning rate converges to a \\emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01734", "pdf_url": "https://arxiv.org/pdf/2511.01734.pdf", "is_interesting": false}, "832": {"title": "Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing", "authors": ["Song Gao, Shusen Jing, Shuai Zhang, Yue Wang, Xiangwei Zhou, Songyang Zhang"], "abstract": "arXiv:2511.01743v1 Announce Type: cross \nRecent advancements in large artificial intelligence models (LAMs) are driving significant innovations in mobile edge computing within next-generation wireless networks. However, the substantial demands for computational resources and large-scale training data required to train LAMs conflict with the limited storage and computational capacity of edge devices, posing significant challenges to training and deploying LAMs at the edge. In this work, we introduce the Networked Mixture-of-Experts (NMoE) system, in which clients infer collaboratively by distributing tasks to suitable neighbors based on their expertise and aggregate the returned results. For training the NMoE, we propose a federated learning framework that integrates both supervised and self-supervised learning to balance personalization and generalization, while preserving communication efficiency and data privacy. We conduct extensive experiments to demonstrate the efficacy of the proposed NMoE system, providing insights and benchmarks for the NMoE training algorithms.", "categories": ["cs.LG", "cs.AI", "cs.NI"], "abs_url": "https://arxiv.org/abs/2511.01743", "pdf_url": "https://arxiv.org/pdf/2511.01743.pdf", "is_interesting": false}, "833": {"title": "An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications", "authors": ["Mei-Chin Pang, Suraj Adhikari, Takuma Kasahara, Nagihiro Haba, Saneyuki Ohno"], "abstract": "arXiv:2511.01745v1 Announce Type: cross \nBattery safety is critical in applications ranging from consumer electronics to electric vehicles and aircraft, where undetected anomalies could trigger safety hazards or costly downtime. In this study, we present OSBAD as an open-source benchmark for anomaly detection frameworks in battery applications. By benchmarking 15 diverse algorithms encompassing statistical, distance-based, and unsupervised machine-learning methods, OSBAD enables a systematic comparison of anomaly detection methods across heterogeneous datasets. In addition, we demonstrate how a physics- and statistics-informed feature transformation workflow enhances anomaly separability by decomposing collective anomalies into point anomalies. To address a major bottleneck in unsupervised anomaly detection due to incomplete labels, we propose a Bayesian optimization pipeline that facilitates automated hyperparameter tuning based on transfer-learning and regression proxies. Through validation on datasets covering both liquid and solid-state chemistries, we further demonstrate the cross-chemistry generalization capability of OSBAD to identify irregularities across different electrochemical systems. By making benchmarking database with open-source reproducible anomaly detection workflows available to the community, OSBAD establishes a unified foundation for developing safe, scalable, and transferable anomaly detection tools in battery analytics. This research underscores the significance of physics- and statistics-informed feature engineering as well as model selection with probabilistic hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for safety-critical energy systems.", "categories": ["cs.LG", "cs.AI", "stat.ME"], "abs_url": "https://arxiv.org/abs/2511.01745", "pdf_url": "https://arxiv.org/pdf/2511.01745.pdf", "is_interesting": false}, "834": {"title": "Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks", "authors": ["Chen-Wei Chang, Shailik Sarkar, Hossein Salemi, Hyungmin Kim, Shutonu Mitra, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, Chang-Tien Lu"], "abstract": "arXiv:2511.01746v1 Announce Type: cross \nScam detection remains a critical challenge in cybersecurity as adversaries craft messages that evade automated filters. We propose a Hierarchical Scam Detection System (HSDS) that combines a lightweight multi-model voting front end with a fine-tuned LLaMA 3.1 8B Instruct back end to improve accuracy and robustness against adversarial attacks. An ensemble of four classifiers provides preliminary predictions through majority vote, and ambiguous cases are escalated to the fine-tuned model, which is optimized with adversarial training to reduce misclassification. Experiments show that this hierarchical design both improves adversarial scam detection and shortens inference time by routing most cases away from the LLM, outperforming traditional machine-learning baselines and proprietary LLM baselines. The findings highlight the effectiveness of a hybrid voting mechanism and adversarial fine-tuning in fortifying LLMs against evolving scam tactics, enhancing the resilience of automated scam detection systems.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01746", "pdf_url": "https://arxiv.org/pdf/2511.01746.pdf", "is_interesting": false}, "835": {"title": "SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic", "authors": ["Zachary Hansen, Yuliya Lierler"], "abstract": "arXiv:2511.01753v1 Announce Type: cross \nModern answer set programming solvers such as CLINGO support advanced language constructs that improve the expressivity and conciseness of logic programs. Conditional literals are one such construct. They form \"subformulas\" that behave as nested implications within the bodies of logic rules. Their inclusion brings the form of rules closer to the less restrictive syntax of first-order logic. These qualities make conditional literals useful tools for knowledge representation. In this paper, we propose a semantics for logic programs with conditional literals and arithmetic based on the SM operator. These semantics do not require grounding, unlike the established semantics for such programs that relies on a translation to infinitary propositional logic. The main result of this paper establishes the precise correspondence between the proposed and existing semantics.", "categories": ["cs.LO", "cs.AI", "cs.PL"], "abs_url": "https://arxiv.org/abs/2511.01753", "pdf_url": "https://arxiv.org/pdf/2511.01753.pdf", "is_interesting": false}, "836": {"title": "RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks", "authors": ["Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, Aviral Kumar"], "abstract": "arXiv:2511.01758v1 Announce Type: cross \nOpen-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic's error detection and the generator's output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.01758", "pdf_url": "https://arxiv.org/pdf/2511.01758.pdf", "is_interesting": false}, "837": {"title": "Context-Guided Decompilation: A Step Towards Re-executability", "authors": ["Xiaohan Wang, Yuxin Hu, Kevin Leach"], "abstract": "arXiv:2511.01763v1 Announce Type: cross \nBinary decompilation plays an important role in software security analysis, reverse engineering, and malware understanding when source code is unavailable. However, existing decompilation techniques often fail to produce source code that can be successfully recompiled and re-executed, particularly for optimized binaries. Recent advances in large language models (LLMs) have enabled neural approaches to decompilation, but the generated code is typically only semantically plausible rather than truly executable, limiting their practical reliability. These shortcomings arise from compiler optimizations and the loss of semantic cues in compiled code, which LLMs struggle to recover without contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid decompilation framework that leverages in-context learning (ICL) to guide LLMs toward generating re-executable source code. We evaluate our method across multiple datasets, optimization levels, and compilers, demonstrating around 40\\% improvement in re-executability over state-of-the-art decompilation methods while maintaining robustness.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01763", "pdf_url": "https://arxiv.org/pdf/2511.01763.pdf", "is_interesting": false}, "838": {"title": "GenDexHand: Generative Simulation for Dexterous Hands", "authors": ["Feng Chen, Zhuxiu Xu, Tianzhe Chu, Xunzhe Zhou, Li Sun, Zewen Wu, Shenghua Gao, Zhongyu Li, Yanchao Yang, Yi Ma"], "abstract": "arXiv:2511.01791v1 Announce Type: cross \nData scarcity remains a fundamental bottleneck for embodied intelligence. Existing approaches use large language models (LLMs) to automate gripper-based simulation generation, but they transfer poorly to dexterous manipulation, which demands more specialized environment design. Meanwhile, dexterous manipulation tasks are inherently more difficult due to their higher degrees of freedom. Massively generating feasible and trainable dexterous hand tasks remains an open challenge. To this end, we present GenDexHand, a generative simulation pipeline that autonomously produces diverse robotic tasks and environments for dexterous manipulation. GenDexHand introduces a closed-loop refinement process that adjusts object placements and scales based on vision-language model (VLM) feedback, substantially improving the average quality of generated environments. Each task is further decomposed into sub-tasks to enable sequential reinforcement learning, reducing training time and increasing success rates. Our work provides a viable path toward scalable training of diverse dexterous hand behaviors in embodied intelligence by offering a simulation-based solution to synthetic data generation. Our website: https://winniechen2002.github.io/GenDexHand/.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01791", "pdf_url": "https://arxiv.org/pdf/2511.01791.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on generative simulation for dexterous manipulation in robotic tasks, which is not directly or indirectly related to autonomous driving. It discusses methods for generating robotic tasks and environments but does not address driving systems, vehicle perception, or control, making it irrelevant to autonomous driving."}}, "839": {"title": "Random Initialization of Gated Sparse Adapters", "authors": ["Vi Retault, Yoha\\\"i-Eliel Berreby"], "abstract": "arXiv:2511.01794v1 Announce Type: cross \nWhen fine-tuning language models on new tasks, catastrophic forgetting -- performance degradation on previously-learned tasks -- is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn't impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.01794", "pdf_url": "https://arxiv.org/pdf/2511.01794.pdf", "is_interesting": false}, "840": {"title": "Accumulating Context Changes the Beliefs of Language Models", "authors": ["Jiayi Geng, Howard Chen, Ryan Liu, Manoel Horta Ribeiro, Robb Willer, Graham Neubig, Thomas L. Griffiths"], "abstract": "arXiv:2511.01805v2 Announce Type: cross \nLanguage model (LM) assistants are increasingly used in applications such as brainstorming and research. Improvements in memory and context size have allowed these models to become more autonomous, which has also resulted in more text accumulation in their context windows without explicit user intervention. This comes with a latent risk: the belief profiles of models -- their understanding of the world as manifested in their responses or actions -- may silently change as context accumulates. This can lead to subtly inconsistent user experiences, or shifts in behavior that deviate from the original alignment of the models. In this paper, we explore how accumulating context by engaging in interactions and processing text -- talking and reading -- can change the beliefs of language models, as manifested in their responses and behaviors. Our results reveal that models' belief profiles are highly malleable: GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of discussion about moral dilemmas and queries about safety, while Grok 4 shows a 27.2% shift on political issues after reading texts from the opposing position. We also examine models' behavioral changes by designing tasks that require tool use, where each tool selection corresponds to an implicit belief. We find that these changes align with stated belief shifts, suggesting that belief shifts will be reflected in actual behavior in agentic systems. Our analysis exposes the hidden risk of belief shift as models undergo extended sessions of talking or reading, rendering their opinions and actions unreliable.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01805", "pdf_url": "https://arxiv.org/pdf/2511.01805.pdf", "is_interesting": false}, "841": {"title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "authors": ["Adewale Akinfaderin, Shreyas Subramanian, Akarsha Sehwag"], "abstract": "arXiv:2511.01807v1 Announce Type: cross \nLength control in Large Language Models (LLMs) is a crucial but under-addressed challenge, with applications ranging from voice interfaces requiring concise responses to research summaries needing comprehensive outputs. Current approaches to length control, including Regularized DPO, Length-Instruction Fine Tuning, and tool-augmented methods, typically require expensive model retraining or complex inference-time tooling. This paper presents a prompt engineering methodology that enables precise length control without model retraining. Our structure-guided approach implements deliberate planning and word counting mechanisms within the prompt, encouraging the model to carefully track and adhere to specified length constraints. Comprehensive evaluations across six state-of-the-art LLMs demonstrate that our method significantly improves length fidelity for several models compared to standard prompting when applied to document summarization tasks, particularly for shorter-to-medium length constraints. The proposed technique shows varying benefits across different model architectures, with some models demonstrating up to 37.6% improvement in length adherence. Quality evaluations further reveal that our approach maintains or enhances overall output quality compared to standard prompting techniques. Our approach provides an immediately deployable solution for applications requiring precise length control, particularly valuable for production environments where model retraining is impractical or cost-prohibitive.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01807", "pdf_url": "https://arxiv.org/pdf/2511.01807.pdf", "is_interesting": false}, "842": {"title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "authors": ["Konrad Staniszewski, Adrian {\\L}a\\'ncucki"], "abstract": "arXiv:2511.01815v1 Announce Type: cross \nServing large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\\times$ compression while maintaining reasoning and long-context accuracy, and 40$\\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.01815", "pdf_url": "https://arxiv.org/pdf/2511.01815.pdf", "is_interesting": false}, "843": {"title": "Machine and Deep Learning for Indoor UWB Jammer Localization", "authors": ["Hamed Fard, Mahsa Kholghi, Benedikt Gro{\\ss}, Gerhard Wunder"], "abstract": "arXiv:2511.01819v1 Announce Type: cross \nUltra-wideband (UWB) localization delivers centimeter-scale accuracy but is vulnerable to jamming attacks, creating security risks for asset tracking and intrusion detection in smart buildings. Although machine learning (ML) and deep learning (DL) methods have improved tag localization, localizing malicious jammers within a single room and across changing indoor layouts remains largely unexplored. Two novel UWB datasets, collected under original and modified room configurations, are introduced to establish comprehensive ML/DL baselines. Performance is rigorously evaluated using a variety of classification and regression metrics. On the source dataset with the collected UWB features, Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves the lowest mean Euclidean error of 20.16 cm. However, deploying these source-trained models in the modified room layout led to severe performance degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99 cm, demonstrating significant domain shift. To mitigate this degradation, a domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a gradient-reversal layer to align CIR-derived features across domains. The A-CNT framework restores localization performance by reducing the mean Euclidean error to 34.67 cm. This represents a 77 percent improvement over non-adversarial transfer learning and an 83 percent improvement over the best baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the results demonstrate that adversarial feature alignment enables robust and transferable indoor jammer localization despite environmental changes. Code and dataset available at https://github.com/afbf4c8996f/Jammer-Loc", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01819", "pdf_url": "https://arxiv.org/pdf/2511.01819.pdf", "is_interesting": false}, "844": {"title": "Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models", "authors": ["Jay Mohta, Kenan Emir Ak, Dimitrios Dimitriadis, Yan Xu, Mingwei Shen"], "abstract": "arXiv:2511.01831v2 Announce Type: cross \nVision-Language Models (VLMs) suffer from catastrophic forgetting when sequentially fine-tuned on new tasks, degrading performance on previously learned foundational and task-specific capabilities. While multi-task learning can mitigate forgetting, it requires simultaneous access to all datasets and imposes computational overhead that scales linearly with the number of tasks. In this work, we introduce a routing-based approach that enables the integration of new tasks while preserving the foundational knowledge acquired during pretraining. We evaluate our method using InternVL-2 models (2B and 8B parameters) and demonstrate that routing preserves the model's foundational capabilities by maintaining performance on general-purpose benchmarks such as ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on specialized tasks. Importantly, our approach achieves this without requiring concurrent access to data from all tasks, avoiding the significant computational and data overhead associated with traditional multi-task learning. We further conduct extensive ablation studies to evaluate the scalability and robustness of routing-based learning, showing that the approach is resilient to a growing number of tasks and performs particularly well when new tasks are semantically related. Finally, we show that the routing mechanism enables superior cross-modal transfer between language and vision capabilities, allowing knowledge learned in one modality to enhance performance in another capability not achieved by existing continual learning methods.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01831", "pdf_url": "https://arxiv.org/pdf/2511.01831.pdf", "is_interesting": false}, "845": {"title": "Efficient Vector Symbolic Architectures from Histogram Recovery", "authors": ["Zirui Deng, Netanel Raviv"], "abstract": "arXiv:2511.01838v1 Announce Type: cross \nVector symbolic architectures (VSAs) are a family of information representation techniques which enable composition, i.e., creating complex information structures from atomic vectors via binding and superposition, and have recently found wide ranging applications in various neurosymbolic artificial intelligence (AI) systems. Recently, Raviv proposed the use of random linear codes in VSAs, suggesting that their subcode structure enables efficient binding, while preserving the quasi-orthogonality that is necessary for neural processing. Yet, random linear codes are difficult to decode under noise, which severely limits the resulting VSA's ability to support recovery, i.e., the retrieval of information objects and their attributes from a noisy compositional representation.\n  In this work we bridge this gap by utilizing coding theoretic tools. First, we argue that the concatenation of Reed-Solomon and Hadamard codes is suitable for VSA, due to the mutual quasi-orthogonality of the resulting codewords (a folklore result). Second, we show that recovery of the resulting compositional representations can be done by solving a problem we call histogram recovery. In histogram recovery, a collection of $N$ histograms over a finite field is given as input, and one must find a collection of Reed-Solomon codewords of length $N$ whose entry-wise symbol frequencies obey those histograms. We present an optimal solution to the histogram recovery problem by using algorithms related to list-decoding, and analyze the resulting noise resilience. Our results give rise to a noise-resilient VSA with formal guarantees regarding efficient encoding, quasi-orthogonality, and recovery, without relying on any heuristics or training, and while operating at improved parameters relative to similar solutions such as the Hadamard code.", "categories": ["cs.IT", "cs.AI", "cs.NE", "math.IT"], "abs_url": "https://arxiv.org/abs/2511.01838", "pdf_url": "https://arxiv.org/pdf/2511.01838.pdf", "is_interesting": false}, "846": {"title": "A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains", "authors": ["Greta Ontrup, Annika Bush, Markus Pauly, Meltem Aksoy"], "abstract": "arXiv:2511.01840v1 Announce Type: cross \nOrganizations increasingly use Large Language Models (LLMs) to improve supply chain processes and reduce environmental impacts. However, LLMs have been shown to reproduce biases regarding the prioritization of sustainable business strategies. Thus, it is important to identify underlying training data biases that LLMs pertain regarding the importance and role of sustainable business and supply chain practices. This study investigates how different LLMs respond to validated surveys about the role of ethics and responsibility for businesses, and the importance of sustainable practices and relations with suppliers and customers. Using standardized questionnaires, we systematically analyze responses generated by state-of-the-art LLMs to identify variations. We further evaluate whether differences are augmented by four organizational culture types, thereby evaluating the practical relevance of identified biases. The findings reveal significant systematic differences between models and demonstrate that organizational culture prompts substantially modify LLM responses. The study holds important implications for LLM-assisted decision-making in sustainability contexts.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01840", "pdf_url": "https://arxiv.org/pdf/2511.01840.pdf", "is_interesting": false}, "847": {"title": "Towards Robust Mathematical Reasoning", "authors": ["Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, Junehyuk Jung"], "abstract": "arXiv:2511.01846v1 Announce Type: cross \nFinding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01846", "pdf_url": "https://arxiv.org/pdf/2511.01846.pdf", "is_interesting": false}, "848": {"title": "SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring", "authors": ["Jiawei Jin, Yingxin Su, Xiaotong Zhu"], "abstract": "arXiv:2511.01850v1 Announce Type: cross \nThe rapid expansion of artificial intelligence and machine learning (ML) applications has intensified the demand for integrated environments that unify model development, deployment, and monitoring. Traditional Integrated Development Environments (IDEs) focus primarily on code authoring, lacking intelligent support for the full ML lifecycle, while existing MLOps platforms remain detached from the coding workflow. To address this gap, this study proposes the design of an LLM-Integrated IDE with automated MLOps pipelines that enables continuous model development and monitoring within a single environment. The proposed system embeds a Large Language Model (LLM) assistant capable of code generation, debugging recommendation, and automatic pipeline configuration. The backend incorporates automated data validation, feature storage, drift detection, retraining triggers, and CI/CD deployment orchestration. This framework was implemented in a prototype named SmartMLOps Studio and evaluated using classification and forecasting tasks on the UCI Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio reduces pipeline configuration time by 61%, improves experiment reproducibility by 45%, and increases drift detection accuracy by 14% compared to traditional workflows. By bridging intelligent code assistance and automated operational pipelines, this research establishes a novel paradigm for AI engineering - transforming the IDE from a static coding tool into a dynamic, lifecycle-aware intelligent platform for scalable and efficient model development.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01850", "pdf_url": "https://arxiv.org/pdf/2511.01850.pdf", "is_interesting": false}, "849": {"title": "Trove: A Flexible Toolkit for Dense Retrieval", "authors": ["Reza Esfandiarpoor, Max Zuo, Stephen H. Bach"], "abstract": "arXiv:2511.01857v1 Announce Type: cross \nWe introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.01857", "pdf_url": "https://arxiv.org/pdf/2511.01857.pdf", "is_interesting": false}, "850": {"title": "Learning Complementary Policies for Human-AI Teams", "authors": ["Ruijiang Gao, Maytal Saar-Tsechansky, Maria De-Arteaga"], "abstract": "arXiv:2302.02944v2 Announce Type: replace \nThis paper tackles the critical challenge of human-AI complementarity in decision-making. Departing from the traditional focus on algorithmic performance in favor of performance of the human-AI team, and moving past the framing of collaboration as classification to focus on decision-making tasks, we introduce a novel approach to policy learning. Specifically, we develop a robust solution for human-AI collaboration when outcomes are only observed under assigned actions. We propose a deferral collaboration approach that maximizes decision rewards by exploiting the distinct strengths of humans and AI, strategically allocating instances among them. Critically, our method is robust to misspecifications in both the human behavior and reward models. Leveraging the insight that performance gains stem from divergent human and AI behavioral patterns, we demonstrate, using synthetic and real human responses, that our proposed method significantly outperforms independent human and algorithmic decision-making. Moreover, we show that substantial performance improvements are achievable by routing only a small fraction of instances to human decision-makers, highlighting the potential for efficient and effective human-AI collaboration in complex management settings.", "categories": ["cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2302.02944", "pdf_url": "https://arxiv.org/pdf/2302.02944.pdf", "is_interesting": false}, "851": {"title": "Memory-Enhanced Neural Solvers for Routing Problems", "authors": ["Felix Chalumeau, Refiloe Shabe, Noah De Nicola, Arnu Pretorius, Thomas D. Barrett, Nathan Grinsztajn"], "abstract": "arXiv:2406.16424v3 Announce Type: replace \nRouting Problems are central to many real-world applications, yet remain challenging due to their (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. Current best methods either rely on a collection of pre-trained policies, or on RL fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an approach that leverages memory to improve the search of neural solvers at inference. MEMENTO leverages online data collected across repeated attempts to dynamically adjust the action distribution based on the outcome of previous decisions. We validate its effectiveness on the Traveling Salesman and Capacitated Vehicle Routing problems, demonstrating its superiority over tree-search and policy-gradient fine-tuning; and showing that it can be zero-shot combined with diversity-based solvers. We successfully train all RL auto-regressive solvers on large instances, and verify MEMENTO's scalability and data-efficiency: pushing the state-of-the-art on 11 out of 12 evaluated tasks.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2406.16424", "pdf_url": "https://arxiv.org/pdf/2406.16424.pdf", "is_interesting": false}, "852": {"title": "Multi-Step Reasoning with Large Language Models, a Survey", "authors": ["Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back"], "abstract": "arXiv:2407.11511v3 Announce Type: replace \nLarge language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2407.11511", "pdf_url": "https://arxiv.org/pdf/2407.11511.pdf", "is_interesting": false}, "853": {"title": "STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with FeedBack", "authors": ["Shashank Kirtania, Naman Gupta, Priyanshu Gupta, Krishna Kariya, Sumit Gulwani, Arun Iyer, Suresh Parthasarathy, Arjun Radhakrishna, Sriram K. Rajamani, Gustavo Soares"], "abstract": "arXiv:2410.10584v2 Announce Type: replace \nLarge Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base editing with FEEDback approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. STACKFEED defines a ReACT actor agent on each document to perform structured edits based on document specific targeted instructions. Experimental results showcase that STACKFEED significantly improves KB quality and performance of the RAG system. We evaluate STACKFEED on low-resource programming problems, modified python packaged and factual question-answering tasks.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2410.10584", "pdf_url": "https://arxiv.org/pdf/2410.10584.pdf", "is_interesting": false}, "854": {"title": "Interpretable end-to-end Neurosymbolic Reinforcement Learning agents", "authors": ["Nils Grandien, Quentin Delfosse, Kristian Kersting"], "abstract": "arXiv:2410.14371v2 Announce Type: replace \nDeep reinforcement learning (RL) agents rely on shortcut learning, preventing them from generalizing to slightly different environments. To address this problem, symbolic method, that use object-centric states, have been developed. However, comparing these methods to deep agents is not fair, as these last operate from raw pixel-based states. In this work, we instantiate the symbolic SCoBots framework. SCoBots decompose RL tasks into intermediate, interpretable representations, culminating in action decisions based on a comprehensible set of object-centric relational concepts. This architecture aids in demystifying agent decisions. By explicitly learning to extract object-centric representations from raw states, object-centric RL, and policy distillation via rule extraction, this work places itself within the neurosymbolic AI paradigm, blending the strengths of neural networks with symbolic AI. We present the first implementation of an end-to-end trained SCoBot, separately evaluate of its components, on different Atari games. The results demonstrate the framework's potential to create interpretable and performing RL systems, and pave the way for future research directions in obtaining end-to-end interpretable RL agents.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2410.14371", "pdf_url": "https://arxiv.org/pdf/2410.14371.pdf", "is_interesting": false}, "855": {"title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration", "authors": ["Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang"], "abstract": "arXiv:2410.18032v5 Announce Type: replace \nGraphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.", "categories": ["cs.AI", "cs.CL", "cs.MA"], "abs_url": "https://arxiv.org/abs/2410.18032", "pdf_url": "https://arxiv.org/pdf/2410.18032.pdf", "is_interesting": false}, "856": {"title": "The Digital Ecosystem of Beliefs: does evolution favour AI over humans?", "authors": ["David M. Bossens, Shanshan Feng, Yew-Soon Ong"], "abstract": "arXiv:2412.14500v3 Announce Type: replace \nAs AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on beliefs. To understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. Following a Universal Darwinism approach, the framework models a population of agents which change their messaging strategies due to evolutionary updates. They interact via messages, update their beliefs following a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with Digico implement two types of agents, which are modelled to represent AIs vs humans based on higher rates of communication, higher rates of evolution, seeding fixed beliefs with propaganda aims, and higher influence on the recommendation algorithm. These experiments show that: a) when AIs have faster messaging, evolution, and more influence on the recommendation algorithm, they get 80% to 95% of the views; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness up to 8%. We further discuss Digico as a tool for systematic experimentation across multi-agent configurations, the implications for legislation, personal use, and platform design, and the use of Digico for studying evolutionary principles.", "categories": ["cs.AI", "cs.MA", "cs.NE"], "abs_url": "https://arxiv.org/abs/2412.14500", "pdf_url": "https://arxiv.org/pdf/2412.14500.pdf", "is_interesting": false}, "857": {"title": "Survey Transfer Learning: Recycling Data with Silicon Responses", "authors": ["Ali Amini"], "abstract": "arXiv:2501.06577v2 Announce Type: replace \nAs researchers increasingly turn to large language models (LLMs) to generate synthetic survey data, less attention has been paid to alternative AI paradigms given environmental costs of LLMs. This paper introduces Survey Transfer Learning (STL), which develops transfer learning paradigms from computer science for survey research to recycle existing survey data and generate empirically grounded silicon responses. Inspired by political behavior theory, STL leverages shared demographic variables with high predictive power in a polarized American context to transfer knowledge across surveys. Using a neural network pre-trained on the Cooperative Election Study (CES) 2020, freezing early layers to preserve learned structure, and fine-tuning top layers on the American National Election Studies (ANES) 2020, STL generates silicon responses CES 2022 and in held-out ANES 2020 data with accuracy rates of up to 93 percent. Results show that STL outperforms LLMs, especially on sensitive measures such as racial resentment. While LLMs silicon samples are costly and opaque, STL generates empirically grounded silicon responses with high individual-level accuracy, potentially helping to mitigate key challenges in social science and the polling industry.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2501.06577", "pdf_url": "https://arxiv.org/pdf/2501.06577.pdf", "is_interesting": false}, "858": {"title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals", "authors": ["Linda Zeng, Rithwik Gupta, Divij Motwani, Yi Zhang, Diji Yang"], "abstract": "arXiv:2502.16101v4 Announce Type: replace \nRetrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to maintain consistent reasoning when exposed to misleading or conflicting evidence, especially in real-world domains such as politics, where information is polarized or selectively framed. Mainstream RAG benchmarks evaluate models under clean retrieval settings, where systems generate answers from gold-standard documents, or under synthetically perturbed settings, where documents are artificially injected with noise. These assumptions fail to reflect real-world conditions, often leading to an overestimation of RAG system performance. To address this gap, we introduce RAGuard, the first benchmark to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our fact-checking dataset captures naturally occurring misinformation by constructing its retrieval corpus from Reddit discussions. It categorizes retrieved evidence into three types: supporting, misleading, and unrelated, providing a realistic and challenging testbed for assessing how well RAG systems navigate different types of evidence. Our experiments reveal that, when exposed to potentially misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), while human annotators consistently perform better, highlighting LLMs' susceptibility to noisy environments. To our knowledge, RAGuard is the first benchmark to systematically assess the robustness of the RAG against misleading evidence. We expect this benchmark to drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications. The dataset is available at https://huggingface.co/datasets/UCSC-IRKM/RAGuard.", "categories": ["cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2502.16101", "pdf_url": "https://arxiv.org/pdf/2502.16101.pdf", "is_interesting": false}, "859": {"title": "LLM Strategic Reasoning: Agentic Study through Behavioral Game Theory", "authors": ["Jingru Jia, Zehua Yuan, Junhao Pan, Paul E. McNamara, Deming Chen"], "abstract": "arXiv:2502.20432v3 Announce Type: replace \nStrategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness.", "categories": ["cs.AI", "cs.CY", "cs.GT", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.20432", "pdf_url": "https://arxiv.org/pdf/2502.20432.pdf", "is_interesting": false}, "860": {"title": "Damper-B-PINN: Damper Characteristics-Based Bayesian Physics-Informed Neural Network for Vehicle State Estimation", "authors": ["Tianyi Zeng, Tianyi Wang, Zimo Zeng, Feiyang Zhang, Jiseop Byeon, Yujin Wang, Yajie Zou, Yangyang Wang, Junfeng Jiao, Christian Claudel, Xinbo Chen"], "abstract": "arXiv:2502.20772v2 Announce Type: replace \nAccurate state estimation is fundamental to intelligent vehicles. Wheel load, one of the most important chassis states, serves as an essential input for advanced driver assistance systems (ADAS) and exerts a direct influence on vehicle stability and safety. However, wheel load estimation remains challenging due to the complexity of chassis modeling and the susceptibility of nonlinear systems to noise. To address these issues, this paper first introduces a refined suspension linkage-level modeling approach that constructs a nonlinear instantaneous dynamic model by explicitly considering the complex geometric structure of the suspension. Building upon this, we propose a damper characteristics-based Bayesian physics-informed neural network (Damper-B-PINN) framework to estimate dynamic wheel load, which leverages the suspension dynamics as physical guidance of PINN while employing Bayesian inference to mitigate the effects of system noise and uncertainty. Moreover, a damper-characteristic physics conditioning (DPC) module is designed for embedding physical prior. The proposed Damper-B-PINN is evaluated using both high-fidelity simulation datasets generated by CarSim software and real-world datasets collected from a Formula Student race car. Experimental results demonstrate that our Damper-B-PINN consistently outperforms existing methods across various test conditions, particularly extreme ones. These findings highlight the potential of the proposed Damper-B-PINN framework to enhance the accuracy and robustness of dynamic wheel load estimation, thereby improving the reliability and safety of ADAS applications.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.20772", "pdf_url": "https://arxiv.org/pdf/2502.20772.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5 / \u667a\u80fd\u8f66\u8f86\u72b6\u6001\u4f30\u8ba1", "reason": "The paper focuses on dynamic wheel load estimation, which is crucial for advanced driver assistance systems (ADAS) and vehicle stability, directly impacting the safety and reliability of autonomous driving systems. Although it does not explicitly discuss autonomous driving systems, the proposed method has direct relevance to vehicle state estimation, a key component for autonomous vehicles."}}, "861": {"title": "Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning", "authors": ["Leon Keller, Daniel Tanneberg, Jan Peters"], "abstract": "arXiv:2503.21406v2 Announce Type: replace \nImitation learning is a popular method for teaching robots new behaviors. However, most existing methods focus on teaching short, isolated skills rather than long, multi-step tasks. To bridge this gap, imitation learning algorithms must not only learn individual skills but also an abstract understanding of how to sequence these skills to perform extended tasks effectively. This paper addresses this challenge by proposing a neuro-symbolic imitation learning framework. Using task demonstrations, the system first learns a symbolic representation that abstracts the low-level state-action space. The learned representation decomposes a task into easier subtasks and allows the system to leverage symbolic planning to generate abstract plans. Subsequently, the system utilizes this task decomposition to learn a set of neural skills capable of refining abstract plans into actionable robot commands. Experimental results in three simulated robotic environments demonstrate that, compared to baselines, our neuro-symbolic approach increases data efficiency, improves generalization capabilities, and facilitates interpretability.", "categories": ["cs.AI", "cs.LG", "cs.RO"], "abs_url": "https://arxiv.org/abs/2503.21406", "pdf_url": "https://arxiv.org/pdf/2503.21406.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0\u5173", "reason": "The paper focuses on neuro-symbolic imitation learning for task decomposition and skill learning in robots, with no direct or indirect relation to autonomous driving tasks such as perception, prediction, or control."}}, "862": {"title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "authors": ["Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, Xiaowen Guo, Jiecao Chen"], "abstract": "arXiv:2504.00509v3 Announce Type: replace \nThe rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60 percent performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2504.00509", "pdf_url": "https://arxiv.org/pdf/2504.00509.pdf", "is_interesting": false}, "863": {"title": "Computational Basis of LLM's Decision Making in Social Simulation", "authors": ["Ji Ma"], "abstract": "arXiv:2504.11671v3 Announce Type: replace \nLarge language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract \"vectors of variable variations\" (e.g., \"male\" to \"female\") from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.GN", "q-fin.EC"], "abs_url": "https://arxiv.org/abs/2504.11671", "pdf_url": "https://arxiv.org/pdf/2504.11671.pdf", "is_interesting": false}, "864": {"title": "The Limits of AI Explainability: An Algorithmic Information Theory Approach", "authors": ["Shrisha Rao"], "abstract": "arXiv:2504.20676v2 Announce Type: replace \nThis paper establishes a theoretical foundation for understanding the fundamental limits of AI explainability through algorithmic information theory. We formalize explainability as the approximation of complex models by simpler ones, quantifying both approximation error and explanation complexity using Kolmogorov complexity. Our key theoretical contributions include: (1) a complexity gap theorem proving that any explanation significantly simpler than the original model must differ from it on some inputs; (2) precise bounds showing that explanation complexity grows exponentially with input dimension but polynomially with error tolerance for Lipschitz functions; and (3) a characterization of the gap between local and global explainability, demonstrating that local explanations can be significantly simpler while maintaining accuracy in relevant regions. We further establish a regulatory impossibility theorem proving that no governance framework can simultaneously pursue unrestricted AI capabilities, human-interpretable explanations, and negligible error. These results highlight considerations likely to be relevant to the design, evaluation, and oversight of explainable AI systems.", "categories": ["cs.AI", "cs.CY", "cs.IT", "math.IT"], "abs_url": "https://arxiv.org/abs/2504.20676", "pdf_url": "https://arxiv.org/pdf/2504.20676.pdf", "is_interesting": false}, "865": {"title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS", "authors": ["Kai Mei, Xi Zhu, Hang Gao, Shuhang Lin, Yongfeng Zhang"], "abstract": "arXiv:2505.18829v2 Announce Type: replace \nWe present AIOS 1.0, a novel platform designed to advance computer-use agent (CUA) capabilities through environmental contextualization. While existing approaches primarily focus on building more powerful agent frameworks or enhancing agent models, we identify a fundamental limitation: the semantic disconnect between how language models understand the world and how computer interfaces are structured. AIOS 1.0 addresses this challenge by transforming computers into contextual environments that language models can natively comprehend, implementing a Model Context Protocol (MCP) server architecture to abstract computer states and actions. This approach effectively decouples interface complexity from decision complexity, enabling agents to reason more effectively about computing environments. To demonstrate our platform's effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark, outperforming several specialized agent frameworks despite its simple architecture. Our results suggest that contextualizing computer environments for language models represents a promising direction for developing more capable computer-use agents and advancing toward AI that can interact with digital systems.", "categories": ["cs.AI", "cs.HC", "cs.OS"], "abs_url": "https://arxiv.org/abs/2505.18829", "pdf_url": "https://arxiv.org/pdf/2505.18829.pdf", "is_interesting": false}, "866": {"title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "authors": ["Chen Xiong, Pin-Yu Chen, Tsung-Yi Ho"], "abstract": "arXiv:2506.00781v2 Announce Type: replace \nRecent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2506.00781", "pdf_url": "https://arxiv.org/pdf/2506.00781.pdf", "is_interesting": false}, "867": {"title": "Language-Driven Coordination and Learning in Multi-Agent Simulation Environments", "authors": ["Zhengyang Li, Sawyer Campos, Nana Wang"], "abstract": "arXiv:2506.04251v4 Announce Type: replace \nThis paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2506.04251", "pdf_url": "https://arxiv.org/pdf/2506.04251.pdf", "is_interesting": false}, "868": {"title": "Solving Inequality Proofs with Large Language Models", "authors": ["Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu"], "abstract": "arXiv:2506.07927v2 Announce Type: replace \nInequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.07927", "pdf_url": "https://arxiv.org/pdf/2506.07927.pdf", "is_interesting": false}, "869": {"title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou, Yiheng Wang, Xuming He, Ao Shen, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Manning Wang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai"], "abstract": "arXiv:2506.10521v5 Announce Type: replace \nScientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2506.10521", "pdf_url": "https://arxiv.org/pdf/2506.10521.pdf", "is_interesting": false}, "870": {"title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction", "authors": ["Song Wang, Zhen Tan, Zihan Chen, Shuang Zhou, Tianlong Chen, Jundong Li"], "abstract": "arXiv:2506.17784v2 Announce Type: replace \nRecent progress in large language model (LLM)-based multi-agent collaboration highlights the power of structured communication in enabling collective intelligence. However, existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication. In this work, we propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, offering a significantly larger topology space for multi-agent communication. Our method focuses on two key directions: (1) Next-Agent Prediction, which selects the most suitable agent role at each step, and (2) Next-Context Selection (NCS), which enables each agent to selectively access relevant information from any previous step. Together, these components construct task-adaptive communication pipelines that support both role flexibility and global information flow. Extensive evaluations across multiple benchmarks demonstrate that our approach achieves superior performance while substantially reducing communication overhead.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2506.17784", "pdf_url": "https://arxiv.org/pdf/2506.17784.pdf", "is_interesting": false}, "871": {"title": "Limits of Safe AI Deployment: Differentiating Oversight and Control", "authors": ["David Manheim, Aidan Homewood"], "abstract": "arXiv:2507.03525v2 Announce Type: replace \nOversight and control, which we collectively call supervision, are often discussed as ways to ensure that AI systems are accountable, reliable, and able to fulfill governance and management requirements. However, the requirements for \"human oversight\" risk codifying vague or inconsistent interpretations of key concepts like oversight and control. This ambiguous terminology could undermine efforts to design or evaluate systems that must operate under meaningful human supervision. This matters because the term is used by regulatory texts such as the EU AI Act.\n  This paper undertakes a targeted critical review of literature on supervision outside of AI, along with a brief summary of past work on the topic related to AI. We next differentiate control as ex-ante or real-time and operational rather than policy or governance, and oversight as performed ex-post, or a policy and governance function. Control aims to prevent failures, while oversight focuses on detection, remediation, or incentives for future prevention. Building on this, we make three contributions. 1) We propose a framework to align regulatory expectations with what is technically and organizationally plausible, articulating the conditions under which each mechanism is possible, where they fall short, and what is required to make them meaningful in practice. 2) We outline how supervision methods should be documented and integrated into risk management, and drawing on the Microsoft Responsible AI Maturity Model, we outline a maturity model for AI supervision. 3) We explicitly highlight boundaries of these mechanisms, including where they apply, where they fail, and where it is clear that no existing methods suffice. This foregrounds the question of whether meaningful supervision is possible in a given deployment context, and can support regulators, auditors, and practitioners in identifying both present and future limitations.", "categories": ["cs.AI", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2507.03525", "pdf_url": "https://arxiv.org/pdf/2507.03525.pdf", "is_interesting": false}, "872": {"title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "authors": ["Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Mu\\~noz-M\\'armol, Sahar Omidi Shayegan, Stefania Raimondo, Xue Liu, Alexandre Drouin, Laurent Charlin, Alexandre Pich\\'e, Alexandre Lacoste, Massimo Caccia"], "abstract": "arXiv:2507.04103v3 Announce Type: replace \nLLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2507.04103", "pdf_url": "https://arxiv.org/pdf/2507.04103.pdf", "is_interesting": false}, "873": {"title": "Agentic Large Language Models for Conceptual Systems Engineering and Design", "authors": ["Soheyl Massoudi, Mark Fuge"], "abstract": "arXiv:2507.08619v2 Announce Type: replace \nEarly-stage engineering design involves complex, iterative reasoning, yet existing large language model (LLM) workflows struggle to maintain task continuity and generate executable models. We evaluate whether a structured multi-agent system (MAS) can more effectively manage requirements extraction, functional decomposition, and simulator code generation than a simpler two-agent system (2AS). The target application is a solar-powered water filtration system as described in a cahier des charges. We introduce the Design-State Graph (DSG), a JSON-serializable representation that bundles requirements, physical embodiments, and Python-based physics models into graph nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS collapses the process to a Generator-Reflector loop. Both systems run a total of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1 70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON validity, requirement coverage, embodiment presence, code compatibility, workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS maintained perfect JSON integrity and embodiment tagging. Requirement coverage remained minimal (less than 20%). Code compatibility peaked at 100% under specific 2AS settings but averaged below 50% for MAS. Only the reasoning-distilled model reliably flagged workflow completion. Powered by DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes) whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced design detail. Reasoning-distilled LLM improved completion rates, yet low requirements and fidelity gaps in coding persisted.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2507.08619", "pdf_url": "https://arxiv.org/pdf/2507.08619.pdf", "is_interesting": false}, "874": {"title": "Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization", "authors": ["No\\'e Lallouet, Tristan Cazenave, Cyrille Enderli"], "abstract": "arXiv:2507.19109v3 Announce Type: replace \nWe introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for multi-objective optimization problems over discrete search spaces. Extending the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for single-objective problems, Pareto-NRPA generalizes the nested search and policy update mechanism to multi-objective optimization. The algorithm uses a set of policies to concurrently explore different regions of the solution space and maintains non-dominated fronts at each level of search. Policy adaptation is performed with respect to the diversity and isolation of sequences within the Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel bi-objective variant of the Traveling Salesman Problem with Time Windows problem (MO-TSPTW), and a neural architecture search task on well-known benchmarks. Results demonstrate that Pareto-NRPA achieves competitive performance against state-of-the-art multi-objective algorithms, both in terms of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly outperforms state-of-the-art evolutionary multi-objective algorithms on constrained search spaces. To our knowledge, this work constitutes the first adaptation of NRPA to the multi-objective setting.", "categories": ["cs.AI", "cs.NE"], "abs_url": "https://arxiv.org/abs/2507.19109", "pdf_url": "https://arxiv.org/pdf/2507.19109.pdf", "is_interesting": false}, "875": {"title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting", "authors": ["Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma, Emily Martin, Marisa Eisenberg"], "abstract": "arXiv:2508.12260v3 Announce Type: replace \nInfectious disease forecasting in novel outbreaks or low-resource settings is hampered by the need for disease-specific data, bespoke training, and expert tuning. We introduce Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. We evaluated Mantis against 48 forecasting models across six diseases with diverse transmission modes, assessing both point forecast accuracy (mean absolute error) and probabilistic performance (weighted interval score and coverage). Despite using no real-world data during training, Mantis achieved lower mean absolute error than all models in the CDC's COVID-19 Forecast Hub when backtested on early pandemic forecasts. Across all other diseases tested, including respiratory, vector-borne, and waterborne pathogens, Mantis consistently ranked in the top two models across all evaluation metrics. Notably, Mantis generalized to diseases with transmission mechanisms not represented in its training data, demonstrating that it captures fundamental contagion dynamics rather than memorizing disease-specific patterns. These capabilities position Mantis as a practical foundation for disease forecasting: general-purpose, accurate, and deployable where traditional models fail.", "categories": ["cs.AI", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2508.12260", "pdf_url": "https://arxiv.org/pdf/2508.12260.pdf", "is_interesting": false}, "876": {"title": "CausalARC: Abstract Reasoning with Causal World Models", "authors": ["Jacqueline Maasch, John Kalantari, Kia Khezeli"], "abstract": "arXiv:2509.03636v2 Announce Type: replace \nOn-the-fly reasoning often requires adaptation to novel problems under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning. Within- and between-model performance varied heavily across tasks, indicating room for significant improvement in language model reasoning.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.03636", "pdf_url": "https://arxiv.org/pdf/2509.03636.pdf", "is_interesting": false}, "877": {"title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models", "authors": ["Ching Chang, Yidan Shi, Defu Cao, Wei Yang, Jeehyun Hwang, Haixin Wang, Jiacheng Pang, Wei Wang, Yan Liu, Wen-Chih Peng, Tien-Fu Chen"], "abstract": "arXiv:2509.11575v2 Announce Type: replace \nTime series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2509.11575", "pdf_url": "https://arxiv.org/pdf/2509.11575.pdf", "is_interesting": false}, "878": {"title": "Neuromorphic Intelligence", "authors": ["Marcel van Gerven"], "abstract": "arXiv:2509.11940v4 Announce Type: replace \nNeuromorphic computing seeks to replicate the remarkable efficiency, flexibility, and adaptability of the human brain in artificial systems. Unlike conventional digital approaches, which suffer from the Von Neumann bottleneck and depend on massive computational and energy resources, neuromorphic systems exploit brain-inspired principles of computation to achieve orders of magnitude greater energy efficiency. By drawing on insights from a wide range of disciplines -- including artificial intelligence, physics, chemistry, biology, neuroscience, cognitive science and materials science -- neuromorphic computing promises to deliver intelligent systems that are sustainable, transparent, and widely accessible. A central challenge, however, is to identify a unifying theoretical framework capable of bridging these diverse disciplines. We argue that dynamical systems theory provides such a foundation. Rooted in differential calculus, it offers a principled language for modeling inference, learning, and control in both natural and artificial substrates. Within this framework, noise can be harnessed as a resource for learning, while differential genetic programming enables the discovery of dynamical systems that implement adaptive behaviors. Embracing this perspective paves the way toward emergent neuromorphic intelligence, where intelligent behavior arises from the dynamics of physical substrates, advancing both the science and sustainability of AI.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2509.11940", "pdf_url": "https://arxiv.org/pdf/2509.11940.pdf", "is_interesting": false}, "879": {"title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "authors": ["Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy"], "abstract": "arXiv:2509.16648v3 Announce Type: replace \nThe accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.16648", "pdf_url": "https://arxiv.org/pdf/2509.16648.pdf", "is_interesting": false}, "880": {"title": "Combinatorial Creativity: A New Frontier in Generalization Abilities", "authors": ["Samuel Schapiro, Sumuk Shashidhar, Alexi Gladstone, Jonah Black, Royce Moon, Dilek Hakkani-Tur, Lav R. Varshney"], "abstract": "arXiv:2509.21043v4 Announce Type: replace \nArtificial intelligence (AI) systems, and Large Language Models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Despite its similarities to compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Importantly, this tradeoff remains persistent even at scale, casting doubt on the long-term creative potential of LLMs in their current form. Together, our conceptual framework and empirical findings provide a foundation for understanding and improving creativity in modern AI models, bridging the gap between human and machine intelligence.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.21043", "pdf_url": "https://arxiv.org/pdf/2509.21043.pdf", "is_interesting": false}, "881": {"title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild", "authors": ["Siyang Wu, Honglin Bao, Sida Li, Ari Holtzman, James A. Evans"], "abstract": "arXiv:2509.23488v3 Announce Type: replace \nWe develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures probe the capacity required for benchmark performance. We formally define them as a set of salient tokens drawn from in-the-wild, naturally authored corpora, where LLM token perplexity, reflecting more or less pre-training exposure, becomes highly predictive of LLM benchmark performance. Through a large-scale meta-evaluation, we extract benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 benchmarks spanning diverse knowledge, coding, logic, instruction following, math, language, reasoning, and world modeling. Our analysis situates signatures in relation to both the semantic similarity of benchmark questions and the correlation of model performance. While performance overlaps are universally high and semantic overlaps remain confined to a narrow mid-range, benchmark signatures prove highly informative in capturing variation, overlap, and divergence. We observe overlap in knowledge and reasoning subtasks, whereas multilingual and cultural benchmarks exhibit less similarity, even compared to cross-task overlap. Notably, performance-level results are strongly influenced by benchmark-orthogonal factors such as question format, highlighting limitations in LLM generalization, the conflation of performance with ability, and issues inherent in current mainstream benchmark agreement studies. Benchmark signatures, however, remain robust to such effects. Ultimately, we identify cross-functional overlaps across logic, math, language, instruction following, and world modeling, with coding emerging as the least overlapping domain. Together, these findings provide mechanistic insights into benchmark validity and LLM sensitivities, and sketch the underlying landscape of interconnected LLM capabilities.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2509.23488", "pdf_url": "https://arxiv.org/pdf/2509.23488.pdf", "is_interesting": false}, "882": {"title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning", "authors": ["Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu"], "abstract": "arXiv:2510.02091v3 Announce Type: replace \nRecent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.02091", "pdf_url": "https://arxiv.org/pdf/2510.02091.pdf", "is_interesting": false}, "883": {"title": "Open Agent Specification (Agent Spec) Technical Report", "authors": ["Yassine Benajiba, Cesare Bernardis, Vladislav Blinov, Paul Cayet, Hassan Chafi, Abderrahim Fathan, Louis Faucon, Damien Hilloulin, Sungpack Hong, Ingo Kossyk, Rhicheek Patra, Sujith Ravi, Jonas Schweizer, Jyotika Singh, Shailender Singh, Xuelin Situ, Weiyi Sun, Kartik Talamadupula, Jerry Xu, Ying Xu"], "abstract": "arXiv:2510.04173v3 Announce Type: replace \nOpen Agent Specification (Agent Spec) is a declarative language for defining AI agents and workflows in a way that is compatible across different AI frameworks, promoting portability and interoperability within AI Agent frameworks. Agent Spec aims to resolve the challenges of fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, improving interoperability and reusability, while reducing redundant efforts. Additionally, Agent Spec facilitates development tools and portability, allowing AI agents to be defined independently of their execution environment and enabling teams to exchange solutions without implementation-specific limitations. Agent Spec benefits four key groups: (i) Agent developers, who gain a superset of reusable components and design patterns, enabling them to leverage a broader range of functionalities; (ii) Agent framework and tool developers, who can use Agent Spec as an interchange format and therefore benefit from cross-framework and tool support; (iii) Researchers, who can achieve reproducible results and comparability, facilitating more reliable and consistent outcomes; (iv) Enterprises, which see faster prototype-to-deployment, increased productivity, and greater scalability and maintainability for their AI agent solutions. This technical report provides an overview of the technical foundations of Agent Spec, including motivation, benefits, and future work. We also introduce a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes (LangGraph, CrewAI, AutoGen, and WayFlow), using three different benchmarks (SimpleQA Verified, $\\tau^2$-Bench and BIRD-SQL) - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.04173", "pdf_url": "https://arxiv.org/pdf/2510.04173.pdf", "is_interesting": false}, "884": {"title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare", "authors": ["Siqi Zhu, David Zhang, Pedro Cisneros-Velarde, Jiaxuan You"], "abstract": "arXiv:2510.08872v3 Announce Type: replace \nLarge Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a social welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and social welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .", "categories": ["cs.AI", "cs.GT", "cs.HC", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2510.08872", "pdf_url": "https://arxiv.org/pdf/2510.08872.pdf", "is_interesting": false}, "885": {"title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control", "authors": ["Joachim Diederich"], "abstract": "arXiv:2510.09338v2 Announce Type: replace \nWe present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovation is a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, and dynamic rule injection. We provide rigorous mathematical proofs establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks, with exponential bounds on attention entropy and pointer fidelity. Specifically, we prove that when group sparsity penalties exceed certain threshold values, the model's attention mechanisms concentrate on semantically relevant blocks, achieving low entropy and high fidelity with negligible error. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes, supporting applications in regulated domains requiring both transparency and capability.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.09338", "pdf_url": "https://arxiv.org/pdf/2510.09338.pdf", "is_interesting": false}, "886": {"title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "authors": ["Zhuo-Yang Song"], "abstract": "arXiv:2510.14846v3 Announce Type: replace \nThe generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via two instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.", "categories": ["cs.AI", "cs.CL", "cs.LO"], "abs_url": "https://arxiv.org/abs/2510.14846", "pdf_url": "https://arxiv.org/pdf/2510.14846.pdf", "is_interesting": false}, "887": {"title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models", "authors": ["Akira Okutomi"], "abstract": "arXiv:2510.14925v2 Announce Type: replace \nWe reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we observe preliminary correlations between internal fragility and miscalibration or hallucination (confabulation), and find that lightweight critique prompts may modestly improve or worsen calibration in small-scale tests. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens to diagnose and potentially mitigate overconfidence in reasoning systems.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.14925", "pdf_url": "https://arxiv.org/pdf/2510.14925.pdf", "is_interesting": false}, "888": {"title": "Experience-Driven Exploration for Efficient API-Free AI Agents", "authors": ["Chenwei Tang, Jingyu Xing, Xinyu Liu, Zizhou Wang, Jiawei Du, Liangli Zhen, Jiancheng Lv"], "abstract": "arXiv:2510.15259v2 Announce Type: replace \nMost existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.15259", "pdf_url": "https://arxiv.org/pdf/2510.15259.pdf", "is_interesting": false}, "889": {"title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval", "authors": ["Binxiao Xu, Junyu Feng, Shaolin Lu, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang"], "abstract": "arXiv:2510.22765v2 Announce Type: replace \nThe rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22765", "pdf_url": "https://arxiv.org/pdf/2510.22765.pdf", "is_interesting": false}, "890": {"title": "Will Humanity Be Rendered Obsolete by AI?", "authors": ["Mohamed El Louadi, Emna Ben Romdhane"], "abstract": "arXiv:2510.22814v2 Announce Type: replace \nThis article analyzes the existential risks artificial intelligence (AI) poses to humanity, tracing the trajectory from current AI to ultraintelligence. Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and superintelligence. Considering machines' exponentially growing cognitive power and hypothetical IQs, it addresses the ethical and existential implications of an intelligence vastly exceeding humanity's, fundamentally alien. Human extinction may result not from malice, but from uncontrollable, indifferent cognitive superiority.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22814", "pdf_url": "https://arxiv.org/pdf/2510.22814.pdf", "is_interesting": false}, "891": {"title": "Mixed-Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution", "authors": ["Crimson Stambaugh, Rajesh P. N. Rao"], "abstract": "arXiv:2510.23026v2 Announce Type: replace \nRecent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional or memory computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a temporal horizon and that certain parts of a planned trajectory should be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL task domains.", "categories": ["cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2510.23026", "pdf_url": "https://arxiv.org/pdf/2510.23026.pdf", "is_interesting": false}, "892": {"title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "authors": ["Tingyue Pan, Mingyue Cheng, Shilong Zhang, Zhiding Liu, Xiaoyu Tao, Yucong Luo, Jintao Zhang, Qi Liu"], "abstract": "arXiv:2510.24028v2 Announce Type: replace \nCross-domain time series forecasting is a valuable task in various web applications. Despite its rapid advancement, achieving effective generalization across heterogeneous time series data remains a significant challenge. Existing methods have made progress by extending single-domain models, yet often fall short when facing domain-specific trend shifts and inconsistent periodic patterns. We argue that a key limitation lies in treating temporal series as undifferentiated sequence, without explicitly decoupling their inherent structural components. To address this, we propose OneCast, a structured and modular forecasting framework that decomposes time series into seasonal and trend components, each modeled through tailored generative pathways. Specifically, the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions. In parallel, the trend component is encoded into discrete tokens at segment level via a semantic-aware tokenizer, and subsequently inferred through a masked discrete diffusion mechanism. The outputs from both branches are combined to produce a final forecast that captures seasonal patterns while tracking domain-specific trends. Extensive experiments across eight domains demonstrate that OneCast mostly outperforms state-of-the-art baselines.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.24028", "pdf_url": "https://arxiv.org/pdf/2510.24028.pdf", "is_interesting": false}, "893": {"title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "authors": ["Wenhao Wang, Peizhi Niu, Zhao Xu, Zhaoyu Chen, Jian Du, Yaxin Du, Xianghe Pang, Keduan Huang, Yanfeng Wang, Qiang Yan, Siheng Chen"], "abstract": "arXiv:2510.24284v2 Announce Type: replace \nLarge Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.24284", "pdf_url": "https://arxiv.org/pdf/2510.24284.pdf", "is_interesting": false}, "894": {"title": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys", "authors": ["Siyi Wu, Chiaxin Liang, Ziqian Bi, Leyi Zhao, Tianyang Wang, Junhao Song, Yichao Zhang, Keyu Chen, Xinyuan Song"], "abstract": "arXiv:2510.26012v2 Announce Type: replace \nThe rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at https://github.com/annihi1ation/auto_research.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26012", "pdf_url": "https://arxiv.org/pdf/2510.26012.pdf", "is_interesting": false}, "895": {"title": "Neighboring State-based Exploration for Reinforcement Learning", "authors": ["Yu-Teng Li, Justin Lin, Jeffery Cheng, Pedro Pachuca"], "abstract": "arXiv:2212.10712v3 Announce Type: replace-cross \nReinforcement Learning is a powerful tool to model decision-making processes. However, it relies on an exploration-exploitation trade-off that remains an open challenge for many tasks. In this work, we study neighboring state-based, model-free exploration led by the intuition that, for an early-stage agent, considering actions derived from a bounded region of nearby states may lead to better actions when exploring. We propose two algorithms that choose exploratory actions based on a survey of nearby states, and find that one of our methods, ${\\rho}$-explore, consistently outperforms the Double DQN baseline in an discrete environment by 49% in terms of Eval Reward Return.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2212.10712", "pdf_url": "https://arxiv.org/pdf/2212.10712.pdf", "is_interesting": false}, "896": {"title": "Complex QA and language models hybrid architectures, Survey", "authors": ["Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco"], "abstract": "arXiv:2302.09051v5 Announce Type: replace-cross \nThis paper reviews the state-of-the-art of large language models (LLM) architectures and strategies for \"complex\" question-answering with a focus on hybrid architectures. LLM based chatbot services have allowed anyone to grasp the potential of LLM to solve many common problems, but soon discovered their limitations for complex questions. Addressing more specific, complex questions (e.g., \"What is the best mix of power-generation methods to reduce climate change ?\") often requires specialized architectures, domain knowledge, new skills, decomposition and multi-step resolution, deep reasoning, sensitive data protection, explainability, and human-in-the-loop processes. Therefore, we review: (1) necessary skills and tasks for handling complex questions and common LLM limits to overcome; (2) dataset, cost functions and evaluation metrics for measuring and improving (e.g. accuracy, explainability, fairness, robustness, groundedness, faithfulness, toxicity...); (3) family of solutions to overcome LLM limitations by (a) training and reinforcement (b) hybridization, (c) prompting, (d) agentic-architectures (agents, tools) and extended reasoning.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2302.09051", "pdf_url": "https://arxiv.org/pdf/2302.09051.pdf", "is_interesting": false}, "897": {"title": "Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints", "authors": ["Barathi Subramanian, Rathinaraja Jeyaraj, Anand Paul"], "abstract": "arXiv:2402.09034v2 Announce Type: replace-cross \nActivation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such as sign language recognition, regression, and time-series classification tasks, where the dataset is limited. Our experiments demonstrate that SST models consistently outperform RNN-based models with baseline activations, exhibiting improved test accuracy.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2402.09034", "pdf_url": "https://arxiv.org/pdf/2402.09034.pdf", "is_interesting": false}, "898": {"title": "Calibrating Bayesian Learning via Regularization, Confidence Minimization, and Selective Inference", "authors": ["Jiayi Huang, Sangwoo Park, Osvaldo Simeone"], "abstract": "arXiv:2404.11350v3 Announce Type: replace-cross \nThe application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision. A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs. A conventional approach to improve calibration is the application of Bayesian ensembling. However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration. This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization. The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs for which the calibration performance is expected to be insufficient. Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods. Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs.", "categories": ["cs.LG", "cs.AI", "eess.SP"], "abs_url": "https://arxiv.org/abs/2404.11350", "pdf_url": "https://arxiv.org/pdf/2404.11350.pdf", "is_interesting": false}, "899": {"title": "SST: Multi-Scale Hybrid Mamba-Transformer Experts for Time Series Forecasting", "authors": ["Xiongxiao Xu, Canyu Chen, Yueqing Liang, Baixiang Huang, Guangji Bai, Liang Zhao, Kai Shu"], "abstract": "arXiv:2404.14757v3 Announce Type: replace-cross \nTime series forecasting has made significant advances, including with Transformer-based models. The attention mechanism in Transformer effectively captures temporal dependencies by attending to all past inputs simultaneously. However, its quadratic complexity with respect to sequence length limits the scalability for long-range modeling. Recent state space models (SSMs) such as Mamba offer a promising alternative by achieving linear complexity without attention. Yet, Mamba compresses historical information into a fixed-size latent state, potentially causing information loss and limiting representational effectiveness. This raises a key research question: Can we design a hybrid Mamba-Transformer architecture that is both effective and efficient for time series forecasting? To address it, we adapt a hybrid Mamba-Transformer architecture Mambaformer, originally proposed for language modeling, to the time series domain. Preliminary experiments reveal that naively stacking Mamba and Transformer layers in Mambaformer is suboptimal for time series forecasting, due to an information interference problem. To mitigate this issue, we introduce a new time series decomposition strategy that separates time series into long-range patterns and short-range variations. Then we show that Mamba excels at capturing long-term structures, while Transformer is more effective at modeling short-term dynamics. Building on this insight, we propose State Space Transformer (SST), a multi-scale hybrid model with expert modules: a Mamba expert for long-range patterns and a Transformer expert for short-term variations. SST also employs a multi-scale patching mechanism to adaptively adjust time series resolution: low resolution for long-term patterns and high resolution for short-term variations. Experiments show that SST obtains SOTA performance with linear scalability. The code is at https://github.com/XiongxiaoXu/SST.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2404.14757", "pdf_url": "https://arxiv.org/pdf/2404.14757.pdf", "is_interesting": false}, "900": {"title": "Ocean Wave Forecasting with Deep Learning as Alternative to Conventional Models", "authors": ["Ziliang Zhang, Huaming Yu, Danqin Ren, Chenyu Zhang, Minghua Sun, Xin Qi"], "abstract": "arXiv:2406.03848v4 Announce Type: replace-cross \nThis study presents OceanCastNet (OCN), a machine learning approach for wave forecasting that incorporates wind and wave fields to predict significant wave height, mean wave period, and mean wave direction.We evaluate OCN's performance against the operational ECWAM model using two independent datasets: NDBC buoy and Jason-3 satellite observations. NDBC station validation indicates OCN performs better at 24 stations compared to ECWAM's 10 stations, and Jason-3 satellite validation confirms similar accuracy across 228-hour forecasts. OCN successfully captures wave patterns during extreme weather conditions, demonstrated through Typhoon Goni with prediction errors typically within $\\pm$0.5 m. The approach also offers computational efficiency advantages. The results suggest that machine learning approaches can achieve performance comparable to conventional wave forecasting systems for operational wave prediction applications.", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2406.03848", "pdf_url": "https://arxiv.org/pdf/2406.03848.pdf", "is_interesting": false}, "901": {"title": "Augmenting learning in neuro-embodied systems through neurobiological first principles", "authors": ["Alejandro Rodriguez-Garcia, Anindya Ghosh, Jie Mei, Srikanth Ramaswamy"], "abstract": "arXiv:2407.04525v5 Announce Type: replace-cross \nRecent progress in artificial intelligence (AI) has been driven by insights from physics and neuroscience, particularly through the development of artificial neural networks (ANNs) capable of complex cognitive tasks such as vision and language processing. Despite these advances, they struggle with continual learning, adaptable knowledge transfer, robustness, and resource efficiency -- capabilities that biological systems handle seamlessly. Specifically, neuromorphic systems and artificial neural networks often overlook two key biophysical properties of neural circuits: neuronal diversity and cell-specific neuromodulation. These mechanisms, essential for regulating dynamic learning across brain scales, allow neuromodulators to introduce degeneracy in biological neural networks, ensuring stability and adaptability under changing conditions. In this article, we summarize recent bioinspired models, learning rules, and architectures, and propose a framework for augmenting ANNs, which has the potential to bridge the gap between neuroscience and AI through neurobiological first principles. Our proposed dual-framework approach leverages spiking neural networks to emulate diverse spiking behaviors and dendritic compartmental dynamics, thereby simulating the morphological and functional diversity of neuronal computations. Finally, we outline how integrating these biophysical principles into task-driven spiking neural networks and neuromorphic systems provides scalable solutions for continual learning, adaptability, robustness, and resource-efficiency. Additionally, this approach will not only provide insights into how emergent behaviors arise in neural networks but also catalyze the development of more efficient, reliable, and intelligent neuromorphic systems and robotic agents.", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2407.04525", "pdf_url": "https://arxiv.org/pdf/2407.04525.pdf", "is_interesting": false}, "902": {"title": "Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT", "authors": ["Tianheng Ling, Chao Qian, Gregor Schiele"], "abstract": "arXiv:2407.11041v5 Announce Type: replace-cross \nThis paper presents the design of a hardware accelerator for Transformers, optimized for on-device time-series forecasting in AIoT systems. It integrates integer-only quantization and Quantization-Aware Training with optimized hardware designs to realize 6-bit and 4-bit quantized Transformer models, which achieved precision comparable to 8-bit quantized models from related research. Utilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7 XC7S15), we examine the feasibility of deploying Transformer models on embedded IoT devices. This includes a thorough analysis of achievable precision, resource utilization, timing, power, and energy consumption for on-device inference. Our results indicate that while sufficient performance can be attained, the optimization process is not trivial. For instance, reducing the quantization bitwidth does not consistently result in decreased latency or energy consumption, underscoring the necessity of systematically exploring various optimization combinations. Compared to an 8-bit quantized Transformer model in related studies, our 4-bit quantized Transformer model increases test loss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less energy. Relevant source code is provided in the accompanying GitHub repository\\footnote{https://github.com/tianheng-ling/TinyTransformer4TS}.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2407.11041", "pdf_url": "https://arxiv.org/pdf/2407.11041.pdf", "is_interesting": false}, "903": {"title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships", "authors": ["Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan"], "abstract": "arXiv:2407.12543v3 Announce Type: replace-cross \nWhile interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "abs_url": "https://arxiv.org/abs/2407.12543", "pdf_url": "https://arxiv.org/pdf/2407.12543.pdf", "is_interesting": false}, "904": {"title": "Dataset Distillation for Offline Reinforcement Learning", "authors": ["Jonathan Light, Yuanzhe Liu, Ziniu Hu"], "abstract": "arXiv:2407.20299v3 Announce Type: replace-cross \nOffline reinforcement learning often requires a quality dataset that we can train a policy on. However, in many situations, it is not possible to get such a dataset, nor is it easy to train a policy to perform well in the actual environment given the offline data. We propose using data distillation to train and distill a better dataset which can then be used for training a better policy model. We show that our method is able to synthesize a dataset where a model trained on it achieves similar performance to a model trained on the full dataset or a model trained using percentile behavioral cloning. Our project site is available at https://datasetdistillation4rl.github.io . We also provide our implementation at https://github.com/ggflow123/DDRL .", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2407.20299", "pdf_url": "https://arxiv.org/pdf/2407.20299.pdf", "is_interesting": false}, "905": {"title": "DeepHQ: Learned Hierarchical Quantizer for Progressive Deep Image Coding", "authors": ["Jooyoung Lee, Se Yoon Jeong, Munchurl Kim"], "abstract": "arXiv:2408.12150v2 Announce Type: replace-cross \nUnlike fixed- or variable-rate image coding, progressive image coding (PIC) aims to compress various qualities of images into a single bitstream, increasing the versatility of bitstream utilization and providing high compression efficiency compared to simulcast compression. Research on neural network (NN)-based PIC is in its early stages, mainly focusing on applying varying quantization step sizes to the transformed latent representations in a hierarchical manner. These approaches are designed to compress only the progressively added information as the quality improves, considering that a wider quantization interval for lower-quality compression includes multiple narrower sub-intervals for higher-quality compression. However, the existing methods are based on handcrafted quantization hierarchies, resulting in sub-optimal compression efficiency. In this paper, we propose an NN-based progressive coding method that firstly utilizes learned quantization step sizes via learning for each quantization layer. We also incorporate selective compression with which only the essential representation components are compressed for each quantization layer. We demonstrate that our method achieves significantly higher coding efficiency than the existing approaches with decreased decoding time and reduced model size. The source code is publicly available at https://github.com/JooyoungLeeETRI/DeepHQ", "categories": ["eess.IV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2408.12150", "pdf_url": "https://arxiv.org/pdf/2408.12150.pdf", "is_interesting": false}, "906": {"title": "AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems", "authors": ["Mohamed Dhouioui, Jonathan Barnoud, Rhoslyn Roebuck Williams, Harry J. Stroud, Phil Bates, David R. Glowacki"], "abstract": "arXiv:2409.07189v2 Announce Type: replace-cross \nMolecular dynamics (MD) simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently emerged as a \"human-in-the-loop\" strategy for efficiently navigating hyper-dimensional molecular systems. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular simulations running on high-performance computing architectures, iMD-VR enables researchers to reach out and guide molecular conformational dynamics, in order to efficiently explore complex, high-dimensional molecular systems. Moreover, iMD-VR simulations generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the use of researcher-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL enables agents to mimic complex behaviours from expert demonstrations, circumventing the need for explicit programming or intricate reward design. In this article, we review IL across robotics and Multi-agents systems domains which are comparable to iMD-VR, and discuss how iMD-VR recordings could be used to train IL models to interact with MD simulations. We then illustrate the applications of these ideas through a proof-of-principle study where iMD-VR data was used to train a CNN network on a simple molecular manipulation task; namely, threading a small molecule through a nanotube pore. Finally, we outline future research directions and potential challenges of using AI agents to augment human expertise in navigating vast molecular conformational spaces.", "categories": ["cs.LG", "cs.AI", "cs.HC", "q-bio.BM"], "abs_url": "https://arxiv.org/abs/2409.07189", "pdf_url": "https://arxiv.org/pdf/2409.07189.pdf", "is_interesting": false}, "907": {"title": "Prevailing Research Areas for Music AI in the Era of Foundation Models", "authors": ["Megan Wei, Mateusz Modrzejewski, Aswin Sivaraman, Dorien Herremans"], "abstract": "arXiv:2409.09378v3 Announce Type: replace-cross \nParallel to rapid advancements in foundation model research, the past few years have witnessed a surge in music AI applications. As AI-generated and AI-augmented music become increasingly mainstream, many researchers in the music AI community may wonder: what research frontiers remain unexplored? This paper outlines several key areas within music AI research that present significant opportunities for further investigation. We begin by examining foundational representation models and highlight emerging efforts toward explainability and interpretability. We then discuss the evolution toward multimodal systems, provide an overview of the current landscape of music datasets and their limitations, and address the growing importance of model efficiency in both training and deployment. Next, we explore applied directions, focusing first on generative models. We review recent systems, their computational constraints, and persistent challenges related to evaluation and controllability. We then examine extensions of these generative approaches to multimodal settings and their integration into artists' workflows, including applications in music editing, captioning, production, transcription, source separation, performance, discovery, and education. Finally, we explore copyright implications of generative music and propose strategies to safeguard artist rights. While not exhaustive, this survey aims to illuminate promising research directions enabled by recent developments in music foundation models.", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "abs_url": "https://arxiv.org/abs/2409.09378", "pdf_url": "https://arxiv.org/pdf/2409.09378.pdf", "is_interesting": false}, "908": {"title": "Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models", "authors": ["Xin Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Zhiyuan Liu, Chuan Shi, Maosong Sun, Cheng Yang"], "abstract": "arXiv:2409.19667v4 Announce Type: replace-cross \nThe need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis. The benchmark, datasets and enhanced open-source models are available at https://github.com/BUPT-GAMMA/ProGraph.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2409.19667", "pdf_url": "https://arxiv.org/pdf/2409.19667.pdf", "is_interesting": false}, "909": {"title": "IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?", "authors": ["Akhilesh Aravapalli, Mounika Marreddy, Radhika Mamidi, Manish Gupta, Subba Reddy Oota"], "abstract": "arXiv:2410.02611v2 Announce Type: replace-cross \nTransformer-based models have revolutionized the field of natural language processing. To understand why they perform so well and to assess their reliability, several studies have focused on questions such as: Which linguistic properties are encoded by these models, and to what extent? How robust are these models in encoding linguistic properties when faced with perturbations in the input text? However, these studies have mainly focused on BERT and the English language. In this paper, we investigate similar questions regarding encoding capability and robustness for 8 linguistic properties across 13 different perturbations in 6 Indic languages, using 9 multilingual Transformer models (7 universal and 2 Indic-specific). To conduct this study, we introduce a novel multilingual benchmark dataset, IndicSentEval, containing approximately $\\sim$47K sentences. Surprisingly, our probing analysis of surface, syntactic, and semantic properties reveals that while almost all multilingual models demonstrate consistent encoding performance for English, they show mixed results for Indic languages. As expected, Indic-specific multilingual models capture linguistic properties in Indic languages better than universal models. Intriguingly, universal models broadly exhibit better robustness compared to Indic-specific models, particularly under perturbations such as dropping both nouns and verbs, dropping only verbs, or keeping only nouns. Overall, this study provides valuable insights into probing and perturbation-specific strengths and weaknesses of popular multilingual Transformer-based models for different Indic languages. We make our code and dataset publicly available [https://github.com/aforakhilesh/IndicBertology].", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2410.02611", "pdf_url": "https://arxiv.org/pdf/2410.02611.pdf", "is_interesting": false}, "910": {"title": "Exploring Large Language Models for Detecting Mental Disorders", "authors": ["Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Natalia Chudova, Artem Shelmanov, Ivan Smirnov"], "abstract": "arXiv:2410.07129v3 Announce Type: replace-cross \nThis paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five Russian-language datasets were considered, each differing in format and in the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.07129", "pdf_url": "https://arxiv.org/pdf/2410.07129.pdf", "is_interesting": false}, "911": {"title": "Federated Vision-Language-Recommendation with Personalized Fusion", "authors": ["Zhiwei Li, Guodong Long, Jing Jiang, Chengqi Zhang, Qiang Yang"], "abstract": "arXiv:2410.08478v4 Announce Type: replace-cross \nApplying large pre-trained Vision-Language Models to recommendation is a burgeoning field, a direction we term Vision-Language-Recommendation (VLR). Bringing VLR to user-oriented on-device intelligence within a federated learning framework is a crucial step for enhancing user privacy and delivering personalized experiences. This paper introduces FedVLR, a federated VLR framework specially designed for user-specific personalized fusion of vision-language representations. At its core is a novel bi-level fusion mechanism: The server-side multi-view fusion module first generates a diverse set of pre-fused multimodal views. Subsequently, each client employs a user-specific mixture-of-expert mechanism to adaptively integrate these views based on individual user interaction history. This designed lightweight personalized fusion module provides an efficient solution to implement a federated VLR system. The effectiveness of our proposed FedVLR has been validated on seven benchmark datasets.", "categories": ["cs.IR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2410.08478", "pdf_url": "https://arxiv.org/pdf/2410.08478.pdf", "is_interesting": false}, "912": {"title": "FTSmartAudit: A Knowledge Distillation-Enhanced Framework for Automated Smart Contract Auditing Using Fine-Tuned LLMs", "authors": ["Zhiyuan Wei, Jing Sun, Zijian Zhang, Xianhao Zhang, Zhe Hou"], "abstract": "arXiv:2410.13918v3 Announce Type: replace-cross \nThe rapid growth of blockchain technology has driven the widespread adoption of smart contracts. However, their inherent vulnerabilities have led to significant financial losses. Traditional auditing methods, while essential, struggle to keep pace with the increasing complexity and scale of smart contracts. Large Language Models (LLMs) offer promising capabilities for automating vulnerability detection, but their adoption is often limited by high computational costs. Although prior work has explored leveraging large models through agents or workflows, relatively little attention has been given to improving the performance of smaller, fine-tuned models--a critical factor for achieving both efficiency and data privacy. In this paper, we introduce HKT-SmartAudit, a framework for developing lightweight models optimized for smart contract auditing. It features a multi-stage knowledge distillation pipeline that integrates classical distillation, external domain knowledge, and reward-guided learning to transfer high-quality insights from large teacher models. A single-task learning strategy is employed to train compact student models that maintain high accuracy and robustness while significantly reducing computational overhead. Experimental results show that our distilled models outperform both commercial tools and larger models in detecting complex vulnerabilities and logical flaws, offering a practical, secure, and scalable solution for smart contract auditing. The source code is available at Github repository.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.13918", "pdf_url": "https://arxiv.org/pdf/2410.13918.pdf", "is_interesting": false}, "913": {"title": "A Comprehensive Evaluation of Cognitive Biases in LLMs", "authors": ["Simon Malberg, Roman Poletukhin, Carolin M. Schuster, Georg Groh"], "abstract": "arXiv:2410.15413v2 Announce Type: replace-cross \nWe present a large-scale evaluation of 30 cognitive biases in 20 state-of-the-art large language models (LLMs) under various decision-making scenarios. Our contributions include a novel general-purpose test framework for reliable and large-scale generation of tests for LLMs, a benchmark dataset with 30,000 tests for detecting cognitive biases in LLMs, and a comprehensive assessment of the biases found in the 20 evaluated LLMs. Our work confirms and broadens previous findings suggesting the presence of cognitive biases in LLMs by reporting evidence of all 30 tested biases in at least some of the 20 LLMs. We publish our framework code to encourage future research on biases in LLMs: https://github.com/simonmalberg/cognitive-biases-in-llms", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.15413", "pdf_url": "https://arxiv.org/pdf/2410.15413.pdf", "is_interesting": false}, "914": {"title": "High Resolution Seismic Waveform Generation using Denoising Diffusion", "authors": ["Kadek Hendrawan Palgunadi, Andreas Bergmeister, Andrea Bosisio, Laura Ermert, Maria Koroni, Nathana\\\"el Perraudin, Simon Dirmeier, Men-Andrin Meier"], "abstract": "arXiv:2410.19343v2 Announce Type: replace-cross \nAccurate prediction and synthesis of seismic waveforms are crucial for seismic-hazard assessment and earthquake-resistant infrastructure design. Existing prediction methods, such as ground-motion models and physics-based wave-field simulations, often fail to capture the full complexity of seismic wavefields, particularly at higher frequencies. This study introduces HighFEM, a novel, computationally efficient, and scalable (i.e., capable of generating many seismograms simultaneously) generative model for high-frequency seismic-waveform generation. Our approach leverages a spectrogram representation of the seismic-waveform data, which is reduced to a lower-dimensional manifold via an autoencoder. A state-of-the-art diffusion model is trained to generate this latent representation conditioned on key input parameters: earthquake magnitude, recording distance, site conditions, hypocenter depth, and azimuthal gap. The model generates waveforms with frequency content up to 50 Hz. Any scalar ground-motion statistic, such as peak ground-motion amplitudes and spectral accelerations, can be readily derived from the synthesized waveforms. We validate our model using commonly employed seismological metrics and performance metrics from image-generation studies. Our results demonstrate that the openly available model can generate realistic high-frequency seismic waveforms across a wide range of input parameters, even in data-sparse regions. For the scalar ground-motion statistics commonly used in seismic-hazard and earthquake-engineering studies, we show that our model accurately reproduces both the median trends of the real data and their variability. To evaluate and compare the growing number of these and similar Generative Waveform Models (GWMs), we argue that they should be openly available and included in community ground-motion-model evaluation efforts.", "categories": ["physics.geo-ph", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2410.19343", "pdf_url": "https://arxiv.org/pdf/2410.19343.pdf", "is_interesting": false}, "915": {"title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks", "authors": ["Nathalie Kirch, Constantin Weisser, Severin Field, Helen Yannakoudakis, Stephen Casper"], "abstract": "arXiv:2411.03343v3 Announce Type: replace-cross \nJailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train linear and non-linear probes on hidden states of open-weight LLMs to predict jailbreak success. Probes achieve strong in-distribution accuracy but transfer is attack-family-specific, revealing that different jailbreaks are supported by distinct internal mechanisms rather than a single universal direction. To establish causal relevance, we construct probe-guided latent interventions that systematically shift compliance in the predicted direction. Interventions derived from non-linear probes produce larger and more reliable effects than those from linear probes, indicating that features linked to jailbreak success are encoded non-linearly in prompt representations. Overall, the results surface heterogeneous, non-linear structure in jailbreak mechanisms and provide a prompt-side methodology for recovering and testing the features that drive jailbreak outcomes.", "categories": ["cs.CR", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2411.03343", "pdf_url": "https://arxiv.org/pdf/2411.03343.pdf", "is_interesting": false}, "916": {"title": "DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation", "authors": ["Yingxu Wang, Nan Yin, Mingyan Xiao, Xinhao Yi, Siwei Liu, Shangsong Liang"], "abstract": "arXiv:2411.10000v3 Announce Type: replace-cross \nGraph Neural Networks (GNNs) with equivariant properties have achieved significant success in modeling complex dynamic systems and molecular properties. However, their expressiveness ability is limited by: (1) Existing methods often overlook the over-smoothing issue caused by traditional GNN models, as well as the gradient explosion or vanishing problems in deep GNNs. (2) Most models operate on first-order information, neglecting that the real world often consists of second-order systems, which further limits the model's representation capabilities. To address these issues, we propose the \\textbf{Du}al \\textbf{S}econd-order \\textbf{E}quivariant \\textbf{G}raph \\textbf{O}rdinary Differential Equation (\\method{}) for equivariant representation. Specifically, \\method{} apply the dual second-order equivariant graph ordinary differential equations (Graph ODEs) on graph embeddings and node coordinates, simultaneously. Theoretically, we first prove that \\method{} maintains the equivariant property. Furthermore, we provide theoretical insights showing that \\method{} effectively alleviates the over-smoothing problem in both feature representation and coordinate update. Additionally, we demonstrate that the proposed \\method{} mitigates the exploding and vanishing gradients problem, facilitating the training of deep multi-layer GNNs. Extensive experiments on benchmark datasets validate the superiority of the proposed \\method{} compared to baselines.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2411.10000", "pdf_url": "https://arxiv.org/pdf/2411.10000.pdf", "is_interesting": false}, "917": {"title": "Incivility and Rigidity: Evaluating the Risks of Fine-Tuning LLMs for Political Argumentation", "authors": ["Svetlana Churina, Kokil Jaidka"], "abstract": "arXiv:2411.16813v4 Announce Type: replace-cross \nIncivility on platforms such as Twitter (now X) and Reddit complicates the development of AI systems that can support productive, rhetorically sound political argumentation. We present experiments with \\textit{GPT-3.5 Turbo} fine-tuned on two contrasting datasets of political discourse: high-incivility Twitter replies to U.S. Congress and low-incivility posts from Reddit's \\textit{r/ChangeMyView}. Our evaluation examines how data composition and prompting strategies affect the rhetorical framing and deliberative quality of model-generated arguments. Results show that Reddit-finetuned models generate safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies adversarial tone and toxicity. Prompt-based steering reduces overt toxicity (e.g., personal attacks) but cannot fully offset the influence of noisy training data. We introduce a rhetorical evaluation rubric - covering justification, reciprocity, alignment, and authority - and provide implementation guidelines for authoring, moderation, and deliberation-support systems.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2411.16813", "pdf_url": "https://arxiv.org/pdf/2411.16813.pdf", "is_interesting": false}, "918": {"title": "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research", "authors": ["A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Kevin Klyman, Matthew Jagielski, Katja Filippova, Ken Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Eleni Triantafillou, Peter Kairouz, Nicole Elyse Mitchell, Niloofar Mireshghallah, Abigail Z. Jacobs, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Ilia Shumailov, Andreas Terzis, Solon Barocas, Jennifer Wortman Vaughan, Danah Boyd, Yejin Choi, Sanmi Koyejo, Fernando Delgado, Percy Liang, Daniel E. Ho, Pamela Samuelson, Miles Brundage, David Bau, Seth Neel, Hanna Wallach, Amy B. Cyphert, Mark A. Lemley, Nicolas Papernot, Katherine Lee"], "abstract": "arXiv:2412.06966v2 Announce Type: replace-cross \n\"Machine unlearning\" is a popular proposed solution for mitigating the existence of content in an AI model that is problematic for legal or moral reasons, including privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of specific information from a generative-AI model's parameters, e.g., a particular individual's personal data or the inclusion of copyrighted content in the model's training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual's data or reflect the concept of \"Spiderman.\" Both of these goals--the targeted removal of information from a model and the targeted suppression of information from a model's outputs--present various technical and substantive challenges. We provide a framework for ML researchers and policymakers to think rigorously about these challenges, identifying several mismatches between the goals of unlearning and feasible implementations. These mismatches explain why unlearning is not a general-purpose solution for circumscribing generative-AI model behavior in service of broader positive impact.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2412.06966", "pdf_url": "https://arxiv.org/pdf/2412.06966.pdf", "is_interesting": false}, "919": {"title": "Low-Rank Adaptation for Foundation Models: A Comprehensive Review", "authors": ["Menglin Yang, Jialin Chen, Jinkai Tao, Yifei Zhang, Jiahong Liu, Jiasheng Zhang, Qiyao Ma, Harshit Verma, Regina Zhang, Min Zhou, Irwin King, Rex Ying"], "abstract": "arXiv:2501.00365v2 Announce Type: replace-cross \nThe rapid advancement of foundation modelslarge-scale neural networks trained on diverse, extensive datasetshas revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2501.00365", "pdf_url": "https://arxiv.org/pdf/2501.00365.pdf", "is_interesting": false}, "920": {"title": "AnyEnhance: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement", "authors": ["Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu"], "abstract": "arXiv:2501.15417v3 Announce Type: replace-cross \nWe introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at https://amphionspace.github.io/anyenhance. An open-source implementation is provided at https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc.", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "abs_url": "https://arxiv.org/abs/2501.15417", "pdf_url": "https://arxiv.org/pdf/2501.15417.pdf", "is_interesting": false}, "921": {"title": "Runtime Analysis of Evolutionary Algorithms for Multi-party Multi-objective Optimization", "authors": ["Yuetong Sun, Peilan Xu, Wenjian Luo"], "abstract": "arXiv:2501.16336v3 Announce Type: replace-cross \nIn scenarios where multiple decision-makers operate within a common decision space, each focusing on their own multi-objective optimization problem (e.g., bargaining games), the problem can be modeled as a multi-party multi-objective optimization problem (MPMOP). While numerous evolutionary algorithms have been proposed to solve MPMOPs, most results remain empirical. This paper presents the first theoretical analysis of the expected runtime of evolutionary algorithms on bi-party multi-objective optimization problems (BPMOPs). Our findings demonstrate that employing traditional multi-objective optimization algorithms to solve MPMOPs is both time-consuming and inefficient, as the resulting population contains many solutions that fail to achieve consensus among decision-makers. An alternative approach involves decision-makers individually solving their respective optimization problems and seeking consensus only in the final stage. While feasible for pseudo-Boolean optimization problems, this method may fail to guarantee approximate performance for one party in NP-hard problems. Finally, we propose evolutionary multi-party multi-objective optimizers (EMPMO) for pseudo-Boolean optimization and shortest path problems within a multi-party multi-objective context, maintain a common solution set among all parties. Theoretical and experimental results demonstrate that the proposed \\( \\text{EMPMO}_{\\text{random}} \\) outperforms previous algorithms in terms of the lower bound on the expected runtime for pseudo-Boolean optimization problems. Additionally, the consensus-based evolutionary multi-party multi-objective optimizer( \\( \\text{EMPMO}_{\\text{cons}}^{\\text{SP}} \\) ) achieves better efficiency and precision in solving shortest path problems compared to existing algorithms.", "categories": ["cs.NE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2501.16336", "pdf_url": "https://arxiv.org/pdf/2501.16336.pdf", "is_interesting": false}, "922": {"title": "Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds", "authors": ["Fan Wang, Pengtao Shao, Yiming Zhang, Bo Yu, Shaoshan Liu, Ning Ding, Yang Cao, Yu Kang, Haifeng Wang"], "abstract": "arXiv:2502.02869v4 Announce Type: replace-cross \nIn-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce decoupled policy distillation and induce prior information in the ICRL framework. Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set through versatile in-context learning paradigms. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.02869", "pdf_url": "https://arxiv.org/pdf/2502.02869.pdf", "is_interesting": false}, "923": {"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning", "authors": ["Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Xiaolong Ma, Jaewoo Lee, Jin Lu, Geng Yuan"], "abstract": "arXiv:2502.03304v4 Announce Type: replace-cross \nLarge language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at https://github.com/Skilteee/DiZO.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2502.03304", "pdf_url": "https://arxiv.org/pdf/2502.03304.pdf", "is_interesting": false}, "924": {"title": "Generative AI and Empirical Software Engineering: A Paradigm Shift", "authors": ["Christoph Treude, Margaret-Anne Storey"], "abstract": "arXiv:2502.08108v2 Announce Type: replace-cross \nThe adoption of large language models (LLMs) and autonomous agents in software engineering marks an enduring paradigm shift. These systems create new opportunities for tool design, workflow orchestration, and empirical observation, while fundamentally reshaping the roles of developers and the artifacts they produce. Although traditional empirical methods remain central to software engineering research, the rapid evolution of AI introduces new data modalities, alters causal assumptions, and challenges foundational constructs such as \"developer\", \"artifact\", and \"interaction\". As humans and AI agents increasingly co-create, the boundaries between social and technical actors blur, and the reproducibility of findings becomes contingent on model updates and prompt contexts. This vision paper examines how the integration of LLMs into software engineering disrupts established research paradigms. We discuss how it transforms the phenomena we study, the methods and theories we rely on, the data we analyze, and the threats to validity that arise in dynamic AI-mediated environments. Our aim is to help the empirical software engineering community adapt its questions, instruments, and validation standards to a future in which AI systems are not merely tools, but active collaborators shaping software engineering and its study.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.08108", "pdf_url": "https://arxiv.org/pdf/2502.08108.pdf", "is_interesting": false}, "925": {"title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks", "authors": ["Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, Junlan Feng"], "abstract": "arXiv:2502.11090v3 Announce Type: replace-cross \nWith the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.11090", "pdf_url": "https://arxiv.org/pdf/2502.11090.pdf", "is_interesting": false}, "926": {"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models", "authors": ["Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang"], "abstract": "arXiv:2502.11559v3 Announce Type: replace-cross \nPre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textbf{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that FaIRMaker automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.11559", "pdf_url": "https://arxiv.org/pdf/2502.11559.pdf", "is_interesting": false}, "927": {"title": "THFlow: A Temporally Hierarchical Flow Matching Framework for 3D Peptide Design", "authors": ["Dengdeng Huang, Shikui Tu"], "abstract": "arXiv:2502.15855v3 Announce Type: replace-cross \nDeep generative models provide a promising approach to de novo 3D peptide design. Most of them jointly model the distributions of peptide's position, orientation, and conformation, attempting to simultaneously converge to the target pocket. However, in the early stage of docking, optimizing conformation-only modalities such as rotation and torsion can be physically meaningless, as the peptide is initialized far from the protein pocket and no interaction field is present. We define this problem as the multimodal temporal inconsistency problem and claim it is a key factor contributing to low binding affinity in generated peptides. To address this challenge, we propose THFlow, a novel flow matching-based multimodal generative model that explicitly models the temporal hierarchy between peptide position and conformation. It employs a polynomial based conditional flow to accelerate positional convergence early on, and later aligns it with rotation and torsion for coordinated conformation refinement under the emerging interaction field. Additionally, we incorporate interaction-related features, such as polarity, to further enhance the model's understanding of peptide-protein binding. Extensive experiments demonstrate that THFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering an effective and accurate solution for advancing peptide-based therapeutic development.", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.15855", "pdf_url": "https://arxiv.org/pdf/2502.15855.pdf", "is_interesting": false}, "928": {"title": "Tool and Tutor? Experimental evidence from AI deployment in cancer diagnosis", "authors": ["Vivianna Fang He, Sihan Li, Phanish Puranam, Feng Lin"], "abstract": "arXiv:2502.16411v4 Announce Type: replace-cross \nNumerous countries globally face shortages of medical experts, deepening inequalities in access to healthcare. Artificial Intelligence (AI)-based diagnostic tools hold considerable promise to tackle this challenge by enabling even novices to deliver expert-level medical services. However, reliance on AI for task completion may hinder the learning required for novices to develop expertise. We thus explore whether AI-based diagnostic tools can be used to enhance not only performance but also learning in the context of lung cancer diagnosis. We examine the distinct effects of AI input during training (i.e., learning how to diagnose) versus in practice (i.e., completing diagnostic tasks) on novice medical professionals' performance. In two field experiments, 576 medical students were randomly assigned across conditions, manipulating the access to AI input during their training, during a test of their diagnostic capabilities, or both. During practice, participants diagnosed potential lung cancer cases using chest CT scans, and their diagnoses were evaluated against the ground truth obtained through histopathological examinations. Study 1 (N = 336) revealed that AI input in training alone improved human diagnostic accuracy by 3.2 percentage points over the control, while AI input during practice alone increased human accuracy by 7.9 percentage points. Combined deployment in both training and practice yielded an improvement of 13.7 percentage points--significantly exceeding either approach alone. Study 2 (N = 240) showed that AI input in practice alone improved accuracy in subsequent practice, unaided by AI, by 9.9 percentage points over the control. Even minimally informative AI input in training improved diagnostic accuracy by 5.3 percentage points over the control. These results reveal AI's dual role: As a tool, it could rapidly improve novices' performance.", "categories": ["cs.HC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.16411", "pdf_url": "https://arxiv.org/pdf/2502.16411.pdf", "is_interesting": false}, "929": {"title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding", "authors": ["Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, Rui Wang"], "abstract": "arXiv:2503.01422v3 Announce Type: replace-cross \nTest-time scaling enhances large language model performance by allocating additional compute resources during inference. Best-of-N (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost-performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating N full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose Self-Truncation Best-of-N (ST-BoN), a decoding method that avoids fully generating all N samples and eliminates the need for reward models. It leverages early sampling consistency in the model's internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost-performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%-80%, and under the same cost, it can improve accuracy by 3-4 points.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.01422", "pdf_url": "https://arxiv.org/pdf/2503.01422.pdf", "is_interesting": false}, "930": {"title": "Medical Hallucinations in Foundation Models and Their Impact on Healthcare", "authors": ["Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Chanwoo Park, Mingyu Lu, Kumail Alhamoud, Jimin Mun, Cristina Grau, Minseok Jung, Rodrigo Gameiro, Lizhou Fan, Eugene Park, Tristan Lin, Joonsik Yoon, Wonjin Yoon, Maarten Sap, Yulia Tsvetkov, Paul Liang, Xuhai Xu, Xin Liu, Chunjong Park, Hyeonhoon Lee, Hae Won Park, Daniel McDuff, Samir Tulebaev, Cynthia Breazeal"], "abstract": "arXiv:2503.05777v2 Announce Type: replace-cross \nHallucinations in foundation models arise from autoregressive training objectives that prioritize token-likelihood optimization over epistemic accuracy, fostering overconfidence and poorly calibrated uncertainty. We define medical hallucination as any model-generated output that is factually incorrect, logically inconsistent, or unsupported by authoritative clinical evidence in ways that could alter clinical decisions. We evaluated 11 foundation models (7 general-purpose, 4 medical-specialized) across seven medical hallucination tasks spanning medical reasoning and biomedical information retrieval. General-purpose models achieved significantly higher proportions of hallucination-free responses than medical-specialized models (median: 76.6% vs 51.3%, difference = 25.2%, 95% CI: 18.7-31.3%, Mann-Whitney U = 27.0, p = 0.012, rank-biserial r = -0.64). Top-performing models such as Gemini-2.5 Pro exceeded 97% accuracy when augmented with chain-of-thought prompting (base: 87.6%), while medical-specialized models like MedGemma ranged from 28.6-61.9% despite explicit training on medical corpora. Chain-of-thought reasoning significantly reduced hallucinations in 86.4% of tested comparisons after FDR correction (q < 0.05), demonstrating that explicit reasoning traces enable self-verification and error detection. Physician audits confirmed that 64-72% of residual hallucinations stemmed from causal or temporal reasoning failures rather than knowledge gaps. A global survey of clinicians (n = 70) validated real-world impact: 91.8% had encountered medical hallucinations, and 84.7% considered them capable of causing patient harm. The underperformance of medical-specialized models despite domain training indicates that safety emerges from sophisticated reasoning capabilities and broad knowledge integration developed during large-scale pre-training, not from narrow optimization.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2503.05777", "pdf_url": "https://arxiv.org/pdf/2503.05777.pdf", "is_interesting": false}, "931": {"title": "Understanding Endogenous Data Drift in Adaptive Models with Recourse-Seeking Users", "authors": ["Bo-Yi Liu, Zhi-Xuan Liu, Kuan Lun Chen, Shih-Yu Tsai, Jie Gao, Hao-Tsung Yang"], "abstract": "arXiv:2503.09658v2 Announce Type: replace-cross \nDeep learning models are widely used in decision-making and recommendation systems, where they typically rely on the assumption of a static data distribution between training and deployment. However, real-world deployment environments often violate this assumption. Users who receive negative outcomes may adapt their features to meet model criteria, i.e., recourse action. These adaptive behaviors create shifts in the data distribution and when models are retrained on this shifted data, a feedback loop emerges: user behavior influences the model, and the updated model in turn reshapes future user behavior. Despite its importance, this bidirectional interaction between users and models has received limited attention. In this work, we develop a general framework to model user strategic behaviors and their interactions with decision-making systems under resource constraints and competitive dynamics. Both the theoretical and empirical analyses show that user recourse behavior tends to push logistic and MLP models toward increasingly higher decision standards, resulting in higher recourse costs and less reliable recourse actions over time. To mitigate these challenges, we propose two methods--Fair-top-k and Dynamic Continual Learning (DCL)--which significantly reduce recourse cost and improve model robustness. Our findings draw connections to economic theories, highlighting how algorithmic decision-making can unintentionally reinforce a higher standard and generate endogenous barriers to entry.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.09658", "pdf_url": "https://arxiv.org/pdf/2503.09658.pdf", "is_interesting": false}, "932": {"title": "JudgeLRM: Large Reasoning Models as a Judge", "authors": ["Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He"], "abstract": "arXiv:2504.00050v3 Announce Type: replace-cross \nLarge Language Models (LLMs) are increasingly adopted as evaluators, offering a scalable alternative to human annotation. However, existing supervised fine-tuning (SFT) approaches often fall short in domains that demand complex reasoning. Judgment is inherently reasoning-intensive: beyond surface-level scoring, it requires verifying evidence, identifying errors, and justifying decisions. Through the analysis of evaluation tasks, we find a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples, revealing the limits of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs, trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards to activate reasoning capabilities. JudgeLRM consistently outperform SFT-tuned baselines in the same size, as well as other RL and SFT variants, and even surpass state-of-the-art reasoning models: notably, JudgeLRM-3B/4B exceeds GPT-4, while JudgeLRM-7B/8B/14B outperforms DeepSeek-R1 by over 2% in F1 score, with particularly strong gains on reasoning-heavy tasks. Our findings underscore the value of RL in unlocking reasoning-aligned LLM judges.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.00050", "pdf_url": "https://arxiv.org/pdf/2504.00050.pdf", "is_interesting": false}, "933": {"title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation", "authors": ["Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang"], "abstract": "arXiv:2504.03546v2 Announce Type: replace-cross \nMultilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified/Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the field's history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https://github.com/leduckhai/MultiMed-ST", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "abs_url": "https://arxiv.org/abs/2504.03546", "pdf_url": "https://arxiv.org/pdf/2504.03546.pdf", "is_interesting": false}, "934": {"title": "DMol: A Highly Efficient and Chemical Motif-Preserving Molecule Generation Platform", "authors": ["Peizhi Niu, Yu-Hsiang Wang, Vishal Rana, Chetan Rupakheti, Abhishek Pandey, Olgica Milenkovic"], "abstract": "arXiv:2504.06312v3 Announce Type: replace-cross \nWe introduce a new graph diffusion model for small molecule generation, DMol, which outperforms the state-of-the-art DiGress model in terms of validity by roughly 1.5% across all benchmarking datasets while reducing the number of diffusion steps by at least 10-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a graph noise scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly 2%, increases the novelty of the method, and further improves the running time due to reductions in the graph size.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.06312", "pdf_url": "https://arxiv.org/pdf/2504.06312.pdf", "is_interesting": false}, "935": {"title": "Trustworthy AI Must Account for Interactions", "authors": ["Jesse C. Cresswell"], "abstract": "arXiv:2504.07170v2 Announce Type: replace-cross \nTrustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. Ultimately the goal of Trustworthy AI research is to achieve all aspects simultaneously. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases, undermining fairness. Drawing on these findings, we take the position that current research practices of improving one or two aspects in isolation are insufficient. Instead, research on Trustworthy AI must account for interactions between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how practitioners can work towards integrated trust, examples of how interactions affect the financial industry, and alternative views.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.07170", "pdf_url": "https://arxiv.org/pdf/2504.07170.pdf", "is_interesting": false}, "936": {"title": "Enhancing Reasoning Abilities of Small LLMs with Cognitive Alignment", "authors": ["Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang"], "abstract": "arXiv:2504.09802v2 Announce Type: replace-cross \nThe reasoning capabilities of large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need for training effective small reasoning models. A critical challenge is that small models possess different reasoning capacities and cognitive trajectories compared with their larger counterparts. Hence, directly distilling chain-of-thought (CoT) rationales from large LRMs to smaller ones can sometimes be ineffective and often requires a substantial amount of annotated data. In this paper, we first introduce a novel Critique-Rethink-Verify (CRV) system, designed for training smaller yet powerful LRMs. Our CRV system consists of multiple LLM agents, each specializing in unique tasks: (i) critiquing the CoT rationales according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. Building on the CRV system, we further propose the Cognitive Preference Optimization (CogPO) algorithm to continuously enhance the reasoning abilities of smaller models by aligning their reasoning processes with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of our CRV+CogPO framework, which outperforms other methods by a large margin.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.09802", "pdf_url": "https://arxiv.org/pdf/2504.09802.pdf", "is_interesting": false}, "937": {"title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "authors": ["Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang"], "abstract": "arXiv:2504.16129v4 Announce Type: replace-cross \nLLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new MG called Flex-MG, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "abs_url": "https://arxiv.org/abs/2504.16129", "pdf_url": "https://arxiv.org/pdf/2504.16129.pdf", "is_interesting": false}, "938": {"title": "Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access", "authors": ["Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu"], "abstract": "arXiv:2504.16795v2 Announce Type: replace-cross \nA key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose Hierarchical Sparse Attention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selects the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.16795", "pdf_url": "https://arxiv.org/pdf/2504.16795.pdf", "is_interesting": false}, "939": {"title": "HCT-QA: A Benchmark for Question Answering on Human-Centric Tables", "authors": ["Mohammad S. Ahmad, Zan A. Naeem, Micha\\\"el Aupetit, Ahmed Elmagarmid, Mohamed Eltabakh, Xiasong Ma, Mourad Ouzzani, Chaoyi Ruan"], "abstract": "arXiv:2504.20047v2 Announce Type: replace-cross \nTabular data embedded within PDF files, web pages, and other document formats are prevalent across numerous sectors such as government, engineering, science, and business. These human-centric tables (HCTs) possess a unique combination of high business value, intricate layouts, limited operational power at scale, and sometimes serve as the only data source for critical insights. However, their complexity poses significant challenges to traditional data extraction, processing, and querying methods. While current solutions focus on transforming these tables into relational formats for SQL queries, they fall short in handling the diverse and complex layouts of HCTs and hence being amenable to querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural language queries, and related answers on thousands of tables. Our dataset includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables with 67.5K QA pairs. While HCTs can be potentially processed by different type of query engines, in this paper, we focus on Large Language Models as potential engines and assess their ability in processing and querying such tables.", "categories": ["cs.IR", "cs.AI", "cs.DB"], "abs_url": "https://arxiv.org/abs/2504.20047", "pdf_url": "https://arxiv.org/pdf/2504.20047.pdf", "is_interesting": false}, "940": {"title": "Memory Assisted LLM for Personalized Recommendation System", "authors": ["Jiarui Chen"], "abstract": "arXiv:2505.03824v2 Announce Type: replace-cross \nLarge language models (LLMs) have demonstrated significant potential in solving recommendation tasks. With proven capabilities in understanding user preferences, LLM personalization has emerged as a critical area for providing tailored responses to individuals. Current studies explore personalization through prompt design and fine-tuning, paving the way for further research in personalized LLMs. However, existing approaches are either costly and inefficient in capturing diverse user preferences or fail to account for timely updates to user history. To address these gaps, we propose the Memory-Assisted Personalized LLM (MAP). Through user interactions, we first create a history profile for each user, capturing their preferences, such as ratings for historical items. During recommendation, we extract relevant memory based on similarity, which is then incorporated into the prompts to enhance personalized recommendations. In our experiments, we define a new task that enables testing with varying memory size under two scenarios: single domain where memory and tasks are from the same category and cross-domain (e.g. memory from movies and recommendation tasks in books). The results show that MAP outperforms regular LLM-based recommenders that integrate user history directly through prompt design. Moreover, as user history grows, MAP's advantage increases in both scenarios, making it more suitable for addressing successive personalized user requests.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.03824", "pdf_url": "https://arxiv.org/pdf/2505.03824.pdf", "is_interesting": false}, "941": {"title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions", "authors": ["Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, Hongyang Li"], "abstract": "arXiv:2505.06111v3 Announce Type: replace-cross \nA generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.06111", "pdf_url": "https://arxiv.org/pdf/2505.06111.pdf", "is_interesting": false}, "942": {"title": "Evaluating Simplification Algorithms for Interpretability of Time Series Classification", "authors": ["Brigt H{\\aa}vardstun, Felix Marti-Perez, C\\`esar Ferri, Jan Arne Telle"], "abstract": "arXiv:2505.08846v2 Announce Type: replace-cross \nIn this work, we introduce metrics to evaluate the use of simplified time series in the context of interpretability of a TSC -- a Time Series Classifier. Such simplifications are important because time series data, in contrast to text and image data, are not intuitively under- standable to humans. These metrics are related to the complexity of the simplifications -- how many segments they contain -- and to their loyalty -- how likely they are to maintain the classification of the original time series. We focus on simplifications that select a subset of the original data points, and show that these typically have high Shapley value, thereby aiding interpretability. We employ these metrics to experimentally evaluate four distinct simplification algorithms, across several TSC algorithms and across datasets of varying characteristics, from seasonal or stationary to short or long. We subsequently perform a human-grounded evaluation with forward simulation, that confirms also the practical utility of the introduced metrics to evaluate the use of simplifications in the context of interpretability of TSC. Our findings are summarized in a framework for deciding, for a given TSC, if the various simplifications are likely to aid in its interpretability.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.08846", "pdf_url": "https://arxiv.org/pdf/2505.08846.pdf", "is_interesting": false}, "943": {"title": "Learning Repetition-Invariant Representations for Polymer Informatics", "authors": ["Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang"], "abstract": "arXiv:2505.10726v2 Announce Type: replace-cross \nPolymers are large macromolecules composed of repeating structural units known as monomers and are widely applied in fields such as energy storage, construction, medicine, and aerospace. However, existing graph neural network methods, though effective for small molecules, only model the single unit of polymers and fail to produce consistent vector representations for the true polymer structure with varying numbers of units. To address this challenge, we introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer representations that are invariant to the number of repeating units in their graph representations. GRIN integrates a graph-based maximum spanning tree alignment with repeat-unit augmentation to ensure structural consistency. We provide theoretical guarantees for repetition-invariance from both model and data perspectives, demonstrating that three repeating units are the minimal augmentation required for optimal invariant representation learning. GRIN outperforms state-of-the-art baselines on both homopolymer and copolymer benchmarks, learning stable, repetition-invariant representations that generalize effectively to polymer chains of unseen sizes.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.10726", "pdf_url": "https://arxiv.org/pdf/2505.10726.pdf", "is_interesting": false}, "944": {"title": "New Encoders for German Trained from Scratch: Comparing ModernGBERT with Converted LLM2Vec Models", "authors": ["Julia Wunderle, Anton Ehrmanntraut, Jan Pfister, Fotis Jannidis, Andreas Hotho"], "abstract": "arXiv:2505.13136v2 Announce Type: replace-cross \nEncoders remain essential for efficient German NLP and NLU scenarios despite the rise of decoder-only LLMs. This work studies two routes to high-quality German encoders under identical data and training constraints: 1) training from scratch and 2) converting decoders via LLM2Vec. We introduce two resources: ModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT style, and LL\\\"aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions trained with masked next-token prediction, both undergoing a context extension to 8.192 tokens.\n  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808), surpassing GBERT Large (+4%) and the seven-times larger converted 7B model (0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551) approaches the converted 7B model (0.557).\n  We release all models, checkpoints, datasets, and full training records, and introduce an encoder-adapted QA-NIAH evaluation. All in all, our results provide actionable guidance: when parameter efficiency and latency matter, from-scratch encoders dominate. When a pre-trained decoder exists and compute is a limited, conversion offers an effective alternative. ModernGBERT and LL\\\"aMmleinVec, including all code, data and intermediary checkpoints are published under a research-only RAIL license.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.13136", "pdf_url": "https://arxiv.org/pdf/2505.13136.pdf", "is_interesting": false}, "945": {"title": "Multi-head Temporal Latent Attention", "authors": ["Keqi Deng, Philip C. Woodland"], "abstract": "arXiv:2505.13544v3 Announce Type: replace-cross \nWhile Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.13544", "pdf_url": "https://arxiv.org/pdf/2505.13544.pdf", "is_interesting": false}, "946": {"title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "authors": ["Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Rutwik Routu, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava"], "abstract": "arXiv:2505.17048v2 Announce Type: replace-cross \nCentral banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle \"the whole is greater than the sum of its parts.\" Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "abs_url": "https://arxiv.org/abs/2505.17048", "pdf_url": "https://arxiv.org/pdf/2505.17048.pdf", "is_interesting": false}, "947": {"title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "authors": ["Xinyi Wu, Yanhao Jia, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao"], "abstract": "arXiv:2505.17050v2 Announce Type: replace-cross \nProject-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "abs_url": "https://arxiv.org/abs/2505.17050", "pdf_url": "https://arxiv.org/pdf/2505.17050.pdf", "is_interesting": false}, "948": {"title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation", "authors": ["C\\'ecile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno"], "abstract": "arXiv:2505.17103v2 Announce Type: replace-cross \nSDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. The model is open-sourced at https://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.17103", "pdf_url": "https://arxiv.org/pdf/2505.17103.pdf", "is_interesting": false}, "949": {"title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models", "authors": ["Louis B\\'ethune, David Vigouroux, Yilun Du, Rufin VanRullen, Thomas Serre, Victor Boutin"], "abstract": "arXiv:2505.18230v3 Announce Type: replace-cross \nWhat is the shortest path between two data points lying in a high-dimensional space? While the answer is trivial in Euclidean geometry, it becomes significantly more complex when the data lies on a curved manifold -- requiring a Riemannian metric to describe the space's local curvature. Estimating such a metric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly from pretrained Energy-Based Models (EBMs) -- a class of generative models that assign low energy to high-density regions. These metrics define spatially varying distances, enabling the computation of geodesics -- shortest paths that follow the data manifold's intrinsic geometry. We introduce two novel metrics derived from EBMs and show that they produce geodesics that remain closer to the data manifold and exhibit lower curvature distortion, as measured by alignment with ground-truth trajectories. We evaluate our approach on increasingly complex datasets: synthetic datasets with known data density, rotated character images with interpretable geometry, and high-resolution natural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established baselines, especially in high-dimensional settings. Our work is the first to derive Riemannian metrics from EBMs, enabling data-aware geodesics and unlocking scalable, geometry-driven learning for generative modeling and simulation.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.18230", "pdf_url": "https://arxiv.org/pdf/2505.18230.pdf", "is_interesting": false}, "950": {"title": "Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators", "authors": ["Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao"], "abstract": "arXiv:2505.18574v4 Announce Type: replace-cross \nHardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.", "categories": ["cs.PL", "cs.AI", "cs.AR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.18574", "pdf_url": "https://arxiv.org/pdf/2505.18574.pdf", "is_interesting": false}, "951": {"title": "Exploring the limits of strong membership inference attacks on large language models", "authors": ["Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Milad Nasr, Sahra Ghalebikesabi, Meenatchi Sundaram Mutu Selva Annamalai, Niloofar Mireshghallah, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Katherine Lee, Franziska Boenisch, Adam Dziedzic, A. Feder Cooper"], "abstract": "arXiv:2505.18773v2 Announce Type: replace-cross \nState-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training references (e.g., fine-tuning attacks), or on stronger attacks applied to small models and datasets. However, weaker attacks have been shown to be brittle and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges prompt an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA--one of the strongest MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training references on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g., AUC<0.7) in practical settings. (3) Even when strong MIAs achieve better-than-random AUC, aggregate metrics can conceal substantial per-sample MIA decision instability: due to training randomness, many decisions are so unstable that they are statistically indistinguishable from a coin flip. Finally, (4) the relationship between MIA success and related LLM privacy metrics is not as straightforward as prior work has suggested.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.18773", "pdf_url": "https://arxiv.org/pdf/2505.18773.pdf", "is_interesting": false}, "952": {"title": "PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models", "authors": ["Xiaoyan Hu, Lauren Pick, Ho-fung Leung, Farzan Farnia"], "abstract": "arXiv:2505.18901v2 Announce Type: replace-cross \nThe rapid advancement of generative AI has provided users with a wide range of well-trained models to address diverse prompts. When selecting a model for a given prompt, users should weigh not only its performance but also its service cost. However, existing model-selection methods typically emphasize performance while overlooking cost differences. In this paper, we introduce PromptWise, an online learning framework that assigns prompts to generative models in a cost-aware manner. PromptWise estimates prompt-model compatibility to select the least expensive model expected to deliver satisfactory outputs. Unlike standard contextual bandits that make a one-shot decision per prompt, PromptWise employs a cost-aware bandit structure that allows sequential model assignments per prompt to reduce total service cost. Through numerical experiments on tasks such as code generation and translation, we demonstrate that PromptWise can achieve performance comparable to baseline selection methods while incurring substantially lower costs. The code is available at: github.com/yannxiaoyanhu/PromptWise.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.18901", "pdf_url": "https://arxiv.org/pdf/2505.18901.pdf", "is_interesting": false}, "953": {"title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "authors": ["Zhaolin Li, Yining Liu, Danni Liu, Tuan Nam Nguyen, Enes Yavuz Ugan, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues"], "abstract": "arXiv:2505.19679v2 Announce Type: replace-cross \nThis paper presents KIT's submissions to the IWSLT 2025 low-resource track. We develop both cascaded systems, consisting of Automatic Speech Recognition (ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech Translation (ST) systems for three language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we fine-tune our systems with different strategies to utilize resources efficiently. This study further explores system enhancement with synthetic data and model regularization. Specifically, we investigate MT-augmented ST by generating translations from ASR data using MT models. For North Levantine, which lacks parallel ST training data, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. We also explore augmentation using text-to-speech models by generating synthetic speech from MT data, demonstrating the benefits of synthetic data in improving both ASR and ST performance for Bemba. Additionally, we apply intra-distillation to enhance model performance. Our experiments show that this approach consistently improves results across ASR, MT, and ST tasks, as well as across different pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine the cascaded and end-to-end systems, achieving an improvement of approximately 1.5 BLEU points.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.19679", "pdf_url": "https://arxiv.org/pdf/2505.19679.pdf", "is_interesting": false}, "954": {"title": "Exploring the Hidden Capacity of LLMs for One-Step Text Generation", "authors": ["Gleb Mezentsev, Ivan Oseledets"], "abstract": "arXiv:2505.21189v2 Announce Type: replace-cross \nA recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space - suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.21189", "pdf_url": "https://arxiv.org/pdf/2505.21189.pdf", "is_interesting": false}, "955": {"title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies", "authors": ["Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan de Kock, Claude Formanek, Sasha Abramowitz, Oumayma Mahjoub, Wiem Khlifi, Simon Du Toit, Louay Ben Nessir, Refiloe Shabe, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius"], "abstract": "arXiv:2505.21236v2 Announce Type: replace-cross \nReinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2505.21236", "pdf_url": "https://arxiv.org/pdf/2505.21236.pdf", "is_interesting": false}, "956": {"title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge", "authors": ["Eleni Vasilaki"], "abstract": "arXiv:2505.22767v3 Announce Type: replace-cross \nLarge Language Models (LLMs) can be understood as Collective Knowledge (CK): a condensation of human cultural and technical output, whose apparent intelligence emerges in dialogue. This perspective article, drawing on extended interaction with ChatGPT-4, postulates differential response modes that plausibly trace their origin to distinct model subnetworks. It argues that CK has no persistent internal state or ``spine'': it drifts, it complies, and its behaviour is shaped by the user and by fine-tuning. It develops the notion of co-augmentation, in which human judgement and CK's representational reach jointly produce forms of analysis that neither could generate alone. Finally, it suggests that CK offers a tractable object for neuroscience: unlike biological brains, these systems expose their architecture, training history, and activation dynamics, making the human--CK loop itself an experimental target.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.22767", "pdf_url": "https://arxiv.org/pdf/2505.22767.pdf", "is_interesting": false}, "957": {"title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios", "authors": ["Zhongzhen Huang, Linjie Mu, Yakun Zhu, Xiangyu Zhao, Shaoting Zhang, Xiaofan Zhang"], "abstract": "arXiv:2505.23118v3 Announce Type: replace-cross \nEffective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \\textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.23118", "pdf_url": "https://arxiv.org/pdf/2505.23118.pdf", "is_interesting": false}, "958": {"title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models", "authors": ["Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi"], "abstract": "arXiv:2505.23945v2 Announce Type: replace-cross \nChain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.23945", "pdf_url": "https://arxiv.org/pdf/2505.23945.pdf", "is_interesting": false}, "959": {"title": "A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models", "authors": ["YuQing Xie, Tess Smidt"], "abstract": "arXiv:2506.02269v2 Announce Type: replace-cross \nEquivariant neural networks have proven to be effective for tasks with known underlying symmetries. However, optimizing equivariant networks can be tricky and best training practices are less established than for standard networks. In particular, recent works have found small training benefits from relaxing equivariance constraints. This raises the question: do equivariance constraints introduce fundamental obstacles to optimization? Or do they simply require different hyperparameter tuning? In this work, we investigate this question through a theoretical analysis of the loss landscape geometry. We focus on networks built using permutation representations, which we can view as a subset of unconstrained MLPs. Importantly, we show that the parameter symmetries of the unconstrained model has nontrivial effects on the loss landscape of the equivariant subspace and under certain conditions can provably prevent learning of the global minima. Further, we empirically demonstrate in such cases, relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly, the weights eventually found via relaxation corresponds to a different choice of group representation in the hidden layer. From this, we draw 3 key takeaways. (1) By viewing the unconstrained version of an architecture, we can uncover hidden parameter symmetries which were broken by choice of constraint enforcement (2) Hidden symmetries give important insights on loss landscapes and can induce critical points and even minima (3) Hidden symmetry induced minima can sometimes be escaped by constraint relaxation and we observe the network jumps to a different choice of constraint enforcement. Effective equivariance relaxation may require rethinking the fixed choice of group representation in the hidden layers.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.02269", "pdf_url": "https://arxiv.org/pdf/2506.02269.pdf", "is_interesting": false}, "960": {"title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "authors": ["Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, Shuiwang Ji"], "abstract": "arXiv:2506.06632v2 Announce Type: replace-cross \nWe aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method. Our code can be found on https://github.com/divelab/E2H-Reasoning.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2506.06632", "pdf_url": "https://arxiv.org/pdf/2506.06632.pdf", "is_interesting": false}, "961": {"title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments", "authors": ["Jin Huang, Yuchao Jin, Le An, Josh Park"], "abstract": "arXiv:2506.07416v2 Announce Type: replace-cross \nThis paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.07416", "pdf_url": "https://arxiv.org/pdf/2506.07416.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "\u7aef\u5230\u7aef\u63a7\u5236 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses the development of a low-latency Vision-Language Model (VLM) pipeline optimized for resource-constrained environments, with a specific evaluation on the NVIDIA DRIVE Thor platform, which is used for autonomous driving applications. The optimization and performance improvements directly impact real-time autonomous driving systems, making it highly relevant to the field."}}, "962": {"title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "authors": ["Iustin Sirbu, Robert-Adrian Popovici, Cornelia Caragea, Stefan Trausan-Matu, Traian Rebedea"], "abstract": "arXiv:2506.07801v3 Announce Type: replace-cross \nWe introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a pseudo-label weighting module designed for selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, i.e., MultiMatch achieves state-of-the-art results on 8 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 21 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%, a critical advantage for real-world text classification tasks. Our code is available on GitHub.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.07801", "pdf_url": "https://arxiv.org/pdf/2506.07801.pdf", "is_interesting": false}, "963": {"title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "authors": ["Marco Spinaci, Marek Polewczyk, Maximilian Schambach, Sam Thelin"], "abstract": "arXiv:2506.10707v4 Announce Type: replace-cross \nTabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/sap-rpt-1-oss.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.10707", "pdf_url": "https://arxiv.org/pdf/2506.10707.pdf", "is_interesting": false}, "964": {"title": "Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi, Keran Wang, Dong Whi Yoo, Ravi Karkar, Koustuv Saha"], "abstract": "arXiv:2506.14196v2 Announce Type: replace-cross \nAlzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.14196", "pdf_url": "https://arxiv.org/pdf/2506.14196.pdf", "is_interesting": false}, "965": {"title": "Flat Channels to Infinity in Neural Loss Landscapes", "authors": ["Flavio Martinelli, Alexander Van Meegen, Berfin \\c{S}im\\c{s}ek, Wulfram Gerstner, Johanni Brea"], "abstract": "arXiv:2506.14951v2 Announce Type: replace-cross \nThe loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight vectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot \\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot \\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.", "categories": ["cs.LG", "cs.AI", "cs.NE"], "abs_url": "https://arxiv.org/abs/2506.14951", "pdf_url": "https://arxiv.org/pdf/2506.14951.pdf", "is_interesting": false}, "966": {"title": "Over-squashing in Spatiotemporal Graph Neural Networks", "authors": ["Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein"], "abstract": "arXiv:2506.15507v2 Announce Type: replace-cross \nGraph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that, counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.15507", "pdf_url": "https://arxiv.org/pdf/2506.15507.pdf", "is_interesting": false}, "967": {"title": "TabArena: A Living Benchmark for Machine Learning on Tabular Data", "authors": ["Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzm\\\"uller, Prateek Mutalik Desai, David Salinas, Frank Hutter"], "abstract": "arXiv:2506.16791v4 Announce Type: replace-cross \nWith the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.16791", "pdf_url": "https://arxiv.org/pdf/2506.16791.pdf", "is_interesting": false}, "968": {"title": "PPMI: Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases", "authors": ["Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah"], "abstract": "arXiv:2506.17336v3 Announce Type: replace-cross \nLarge language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.17336", "pdf_url": "https://arxiv.org/pdf/2506.17336.pdf", "is_interesting": false}, "969": {"title": "Context Tuning for In-Context Optimization", "authors": ["Jack Lu, Ryan Teehan, Zhenbang Yang, Mengye Ren"], "abstract": "arXiv:2507.04221v2 Announce Type: replace-cross \nWe introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2507.04221", "pdf_url": "https://arxiv.org/pdf/2507.04221.pdf", "is_interesting": false}, "970": {"title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "authors": ["Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti"], "abstract": "arXiv:2507.05362v2 Announce Type: replace-cross \nRecent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2507.05362", "pdf_url": "https://arxiv.org/pdf/2507.05362.pdf", "is_interesting": false}, "971": {"title": "A Collectivist, Economic Perspective on AI", "authors": ["Michael I. Jordan"], "abstract": "arXiv:2507.06268v2 Announce Type: replace-cross \nInformation technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word \"intelligence\" is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.", "categories": ["cs.CY", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2507.06268", "pdf_url": "https://arxiv.org/pdf/2507.06268.pdf", "is_interesting": false}, "972": {"title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training", "authors": ["Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun"], "abstract": "arXiv:2507.09846v4 Announce Type: replace-cross \nAs both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the \"river\" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abs_url": "https://arxiv.org/abs/2507.09846", "pdf_url": "https://arxiv.org/pdf/2507.09846.pdf", "is_interesting": false}, "973": {"title": "Chain of Retrieval: Multi-Aspect Iterative Search Expansion and Post-Order Search Aggregation for Full Paper Retrieval", "authors": ["Sangwoo Park, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang"], "abstract": "arXiv:2507.10057v2 Announce Type: replace-cross \nScientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused exclusively on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity between them. Yet, abstracts offer only sparse and high-level summaries, and such methods primarily optimize one-to-one similarity, overlooking the dynamic relations that emerge among relevant papers during the retrieval process. To address this, we propose Chain of Retrieval(COR), a novel iterative framework for full-paper retrieval. Specifically, CoR decomposes each query paper into multiple aspect-specific views, matches them against segmented candidate papers, and iteratively expands the search by promoting top-ranked results as new queries, thereby forming a tree-structured retrieval process. The resulting retrieval tree is then aggregated in a post-order manner: descendants are first combined at the query level, then recursively merged with their parent nodes, to capture hierarchical relations across iterations. To validate this, we present SCIFULLBENCH, a large-scale benchmark providing both complete and segmented contexts of full papers for queries and candidates, and results show that CoR significantly outperforms existing retrieval baselines. Our code and dataset is available at https://github.com/psw0021/Chain-of-Retrieval.git.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2507.10057", "pdf_url": "https://arxiv.org/pdf/2507.10057.pdf", "is_interesting": false}, "974": {"title": "H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance", "authors": ["Shijun Guo, Haoran Xu, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyi Zhang, Yishan Song"], "abstract": "arXiv:2507.13370v3 Announce Type: replace-cross \nThe openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.", "categories": ["cs.SI", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2507.13370", "pdf_url": "https://arxiv.org/pdf/2507.13370.pdf", "is_interesting": false}, "975": {"title": "A Self-Evolving AI Agent System for Climate Science", "authors": ["Zijie Guo, Jiong Wang, Fenghua Ling, Wangxu Wei, Xiaoyu Yue, Zhe Jiang, Wanghan Xu, Jing-Jia Luo, Lijing Cheng, Yoo-Geun Ham, Fengfei Song, Pierre Gentine, Toshio Yamagata, Ben Fei, Wenlong Zhang, Xinyu Gu, Chao Li, Yaqiang Wang, Tao Chen, Wanli Ouyang, Bowen Zhou, Lei Bai"], "abstract": "arXiv:2507.17311v3 Announce Type: replace-cross \nScientific progress in Earth science depends on integrating data across the planet's interconnected spheres. However, the accelerating volume and fragmentation of multi-sphere knowledge and data have surpassed human analytical capacity. This creates a major bottleneck for discovery, especially in climate science. To address this challenge, we introduce EarthLink, the first self-evolving AI agent system designed as an interactive \"copilot\" for Earth scientists. Through natural language interaction, EarthLink automates the entire research workflow by integrating planning, code execution, data analysis, and physical reasoning into a unified process that directly addresses this limitation. Beyond efficiency, it exhibits human-like cross-disciplinary analytical ability and achieves proficiency comparable to a junior researcher in expert evaluations on core large-scale climate tasks, including model-observation comparison and climate change understanding. When tasked with an open scientific problem, specifically the discovery of precursors of the Atlantic Ni\\~no, EarthLink autonomously developed a research strategy, identified sources of predictability, verified its hypotheses with available data, and proposed a physically consistent mechanism. These emerging capabilities enable a new human-AI research paradigm. Scientists can focus on value and result judgments, while AI systems handle complex data analysis and knowledge integration. This accelerates the pace and breadth of discovery in Earth sciences. The system is accessible at our website https://earthlink.intern-ai.org.cn.", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "abs_url": "https://arxiv.org/abs/2507.17311", "pdf_url": "https://arxiv.org/pdf/2507.17311.pdf", "is_interesting": false}, "976": {"title": "Music Arena: Live Evaluation for Text-to-Music", "authors": ["Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue"], "abstract": "arXiv:2507.20900v2 Announce Type: replace-cross \nWe present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org . Preference data is available at: https://huggingface.co/music-arena .", "categories": ["cs.SD", "cs.AI", "cs.MM"], "abs_url": "https://arxiv.org/abs/2507.20900", "pdf_url": "https://arxiv.org/pdf/2507.20900.pdf", "is_interesting": false}, "977": {"title": "Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research", "authors": ["Raluca Rilla, Tobias Werner, Hiromu Yakura, Iyad Rahwan, Anne-Marie Nussberger"], "abstract": "arXiv:2508.01390v2 Announce Type: replace-cross \nOnline behavioural research faces an emerging threat as participants increasingly turn to large language models (LLMs) for advice, translation, or task delegation: LLM Pollution. We identify three interacting variants through which LLM Pollution threatens the validity and integrity of online behavioural research. First, Partial LLM Mediation occurs when participants make selective use of LLMs for specific aspects of a task, such as translation or wording support, leading researchers to (mis)interpret LLM-shaped outputs as human ones. Second, Full LLM Delegation arises when agentic LLMs complete studies with little to no human oversight, undermining the central premise of human-subject research at a more foundational level. Third, LLM Spillover signifies human participants altering their behaviour as they begin to anticipate LLM presence in online studies, even when none are involved. While Partial Mediation and Full Delegation form a continuum of increasing automation, LLM Spillover reflects second-order reactivity effects. Together, these variants interact and generate cascading distortions that compromise sample authenticity, introduce biases that are difficult to detect post hoc, and ultimately undermine the epistemic grounding of online research on human cognition and behaviour. Crucially, the threat of LLM Pollution is already co-evolving with advances in generative AI, creating an escalating methodological arms race. To address this, we propose a multi-layered response spanning researcher practices, platform accountability, and community efforts. As the challenge evolves, coordinated adaptation will be essential to safeguard methodological integrity and preserve the validity of online behavioural research.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.01390", "pdf_url": "https://arxiv.org/pdf/2508.01390.pdf", "is_interesting": false}, "978": {"title": "Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation", "authors": ["Zhaoyu Hu, Jianyang Wang, Hao Guo, Yuan Tian, Erpeng Xue, Xianyang Qi, Hongxiang Lin, Lei Wang, Sheng Chen"], "abstract": "arXiv:2508.02451v2 Announce Type: replace-cross \nIn the context of the booming digital economy, recommendation systems, as a key link connecting users and numerous services, face challenges in modeling user behavior sequences on local-life service platforms, including the sparsity of long sequences and strong spatio-temporal dependence. Such challenges can be addressed by drawing an analogy to the forgetting process in human memory. This is because users' responses to recommended content follow the recency effect and the cyclicality of memory. By exploring this, this paper introduces the forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM) with long sequences for local-life service recommendation. STIM integrates three key components: a dynamic masking module based on the forgetting curve, which is used to extract both recent spatiotemporal features and periodic spatiotemporal features; a query-based mixture of experts (MoE) approach that can adaptively activate expert networks under different dynamic masks, enabling the collaborative modeling of time, location, and items; and a hierarchical multi-interest network unit, which captures multi-interest representations by modeling the hierarchical interactions between the shallow and deep semantics of users' recent behaviors. By introducing the STIM method, we conducted online A/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV). In addition, extended offline experiments also showed improvements. STIM has been deployed in a large-scale local-life service recommendation system, serving hundreds of millions of daily active users in core application scenarios.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.02451", "pdf_url": "https://arxiv.org/pdf/2508.02451.pdf", "is_interesting": false}, "979": {"title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design", "authors": ["Claudiu Leoveanu-Condrei"], "abstract": "arXiv:2508.03665v4 Announce Type: replace-cross \nGenerative models, particularly Large Language Models (LLMs), produce fluent outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and type-theoretic principles to introduce a contract layer that mediates every LLM call. Contracts stipulate semantic and type requirements on inputs and outputs, coupled with probabilistic remediation to steer generation toward compliance. The layer exposes the dual view of LLMs as semantic parsers and probabilistic black-box components. Contract satisfaction is probabilistic and semantic validation is operationally defined through programmer-specified conditions on well-typed data structures. More broadly, this work postulates that any two agents satisfying the same contracts are \\emph{functionally equivalent} with respect to those contracts.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.03665", "pdf_url": "https://arxiv.org/pdf/2508.03665.pdf", "is_interesting": false}, "980": {"title": "Why Attention Fails: A Taxonomy of Faults in Attention-Based Neural Networks", "authors": ["Sigma Jahan, Saurabh Singh Rajput, Tushar Sharma, Mohammad Masudur Rahman"], "abstract": "arXiv:2508.04925v2 Announce Type: replace-cross \nAttention mechanisms are at the core of modern neural architectures, powering systems ranging from ChatGPT to autonomous vehicles and driving a major economic impact. However, high-profile failures, such as ChatGPT's nonsensical outputs or Google's suspension of Gemini's image generation due to attention weight errors, highlight a critical gap: existing deep learning fault taxonomies might not adequately capture the unique failures introduced by attention mechanisms. This gap leaves practitioners without actionable diagnostic guidance. To address this gap, we present the first comprehensive empirical study of faults in attention-based neural networks (ABNNs). Our work is based on a systematic analysis of 555 real-world faults collected from 96 projects across ten frameworks, including GitHub, Hugging Face, and Stack Overflow. Through our analysis, we develop a novel taxonomy comprising seven attention-specific fault categories, not captured by existing work. Our results show that over half of the ABNN faults arise from mechanisms unique to attention architectures. We further analyze the root causes and manifestations of these faults through various symptoms. Finally, by analyzing symptom-root cause associations, we identify four evidence-based diagnostic heuristics that explain 33.0% of attention-specific faults, offering the first systematic diagnostic guidance for attention-based models.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.04925", "pdf_url": "https://arxiv.org/pdf/2508.04925.pdf", "is_interesting": false}, "981": {"title": "Identity Increases Stability in Neural Cellular Automata", "authors": ["James Stovold"], "abstract": "arXiv:2508.06389v2 Announce Type: replace-cross \nNeural Cellular Automata (NCAs) offer a way to study the growth of two-dimensional artificial organisms from a single seed cell. From the outset, NCA-grown organisms have had issues with stability, their natural boundary often breaking down and exhibiting tumour-like growth or failing to maintain the expected shape. In this paper, we present a method for improving the stability of NCA-grown organisms by introducing an 'identity' layer with simple constraints during training.\n  Results show that NCAs grown in close proximity are more stable compared with the original NCA model. Moreover, only a single identity value is required to achieve this increase in stability. We observe emergent movement from the stable organisms, with increasing prevalence for models with multiple identity values.\n  This work lays the foundation for further study of the interaction between NCA-grown organisms, paving the way for studying social interaction at a cellular level in artificial organisms.\n  Code/Videos available at: https://github.com/jstovold/ALIFE2025", "categories": ["cs.NE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.06389", "pdf_url": "https://arxiv.org/pdf/2508.06389.pdf", "is_interesting": false}, "982": {"title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "authors": ["Aman Goel, Daniel Schwartz, Yanjun Qi"], "abstract": "arXiv:2508.14314v2 Announce Type: replace-cross \nLarge language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages fine-grained cross-model consistency to detect and mitigate hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves up to 9 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation on multiple datasets demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2508.14314", "pdf_url": "https://arxiv.org/pdf/2508.14314.pdf", "is_interesting": false}, "983": {"title": "Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing", "authors": ["Rodrigo Tertulino, Ricardo Almeida"], "abstract": "arXiv:2508.18316v2 Announce Type: replace-cross \nThis study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.", "categories": ["cs.LG", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2508.18316", "pdf_url": "https://arxiv.org/pdf/2508.18316.pdf", "is_interesting": false}, "984": {"title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "authors": ["Mir Tafseer Nayeem, Davood Rafiei"], "abstract": "arXiv:2509.00285v2 Announce Type: replace-cross \nWe study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2509.00285", "pdf_url": "https://arxiv.org/pdf/2509.00285.pdf", "is_interesting": false}, "985": {"title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "authors": ["Yuze Liu, Zhaoyuan Zhang, Xiangsheng Zeng, Yihe Zhang, Leping Yu, Lejia Wang, Xi Yu"], "abstract": "arXiv:2509.06093v2 Announce Type: replace-cross \nThe preparation procedures of materials are often embedded narratively in experimental protocols, research articles, patents, and laboratory notes, and are structured around procedural sequences, causal relationships, and conditional logic. The synthesis of boron nitride nanosheet (BNNS) polymer composites exemplifies this linguistically encoded decision-making system, where the practical experiments involve interdependent multistage and path-dependent processes such as exfoliation, functionalization, and dispersion, each governed by heterogeneous parameters and contextual contingencies, challenging conventional numerical optimization paradigms for experiment design. We reformulate this challenge into a text-reasoning problem through a framework centered on a text-first, lightly structured materials database and large language models (LLMs) as text reasoning engines. We constructed a database that captures evidence-linked narrative excerpts from the literature while normalizing only the minimum necessary entities, attributes, and relations to enable composite retrieval that unifies semantic matching, lexical cues, and explicit value filters. Building on this language-native, provenance-preserving foundation, the LLM operates in two complementary modes: retrieval-augmented generation (RAG), grounding outputs in retrieved evidence modules from the database, and experience-augmented reasoning (EAR), which leverages iteratively trained text guides derived from multi-source literature-based narrative data as external references to inform reasoning and decision-making. Applying this integration-and-reasoning framework, we demonstrate rapid, laboratory-scale optimization of BNNS preparation, highlighting how language-native data combined with LLM-based reasoning can significantly accelerate practical material preparation.", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2509.06093", "pdf_url": "https://arxiv.org/pdf/2509.06093.pdf", "is_interesting": false}, "986": {"title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "authors": ["Chengze Li, Yitong Zhang, Jia Li, Liyi Cai, Ge Li"], "abstract": "arXiv:2509.11252v2 Announce Type: replace-cross \nLLMs have become the mainstream approaches to code generation. Existing LLMs mainly employ autoregressive generation, i.e. generating code token-by-token from left to right. However, the underlying autoregressive generation has two limitations in code generation. First, autoregressive LLMs only generate a token at each step, showing low efficiency in practice. Second, programming is a non-sequential process involving back-and-forth editing, while autoregressive LLMs only employ the left-to-right generation order. These two intrinsic limitations hinder the further development of LLMs in code generation. Recently, diffusion LLMs have emerged as a promising alternative. Diffusion LLMs address the above limitations with two advances, including multi-token prediction (i.e. generating multiple tokens at each step) and flexible generation order (i.e. flexibly determining which positions to generate tokens). However, there is no systematic study exploring diffusion LLMs in code generation. To bridge the knowledge gap, we present the first empirical study of diffusion LLMs for code generation. Our study involves 9 representative diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on the results, we summarize the following findings. (1) Existing diffusion LLMs are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs have a stronger length extrapolation ability than autoregressive LLMs and perform better in long code understanding. (3) We explore factors impacting the effectiveness and efficiency of diffusion LLMs, and provide practical guidance. (4) We discuss several promising further directions to improve diffusion LLMs on code generation. We open-source all source code, data, and results to facilitate the following research. The code is publicly available at https://github.com/zhangyitonggg/dllm4code.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.11252", "pdf_url": "https://arxiv.org/pdf/2509.11252.pdf", "is_interesting": false}, "987": {"title": "RL Fine-Tuning Heals OOD Forgetting in SFT", "authors": ["Hangzhan Jin, Sitao Luan, Sicheng Lyu, Guillaume Rabusseau, Reihaneh Rabbany, Doina Precup, Mohammad Hamdaqa"], "abstract": "arXiv:2509.12235v2 Announce Type: replace-cross \nThe two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. In our study, we find the well-known claim \"SFT memorizes, RL generalizes\" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an \\textbf{OOD restoration} role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, \\ie{} \\textbf{if SFT trains for too short or too long, RL cannot recover the lost OOD ability;} (4) To uncover the underlying mechanisms behind the forgetting and restoration process, we employ SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, we find that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the \\textbf{rotation of singular vectors}. Our findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. %reversing the rotations induced by SFT, which shows recovery from forgetting, whereas imposing the SFT parameter directions onto a RL-tuned model results in performance degradation. Code is available at https://github.com/xiaodanguoguo/RL_Heals_SFT", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.12235", "pdf_url": "https://arxiv.org/pdf/2509.12235.pdf", "is_interesting": false}, "988": {"title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "authors": ["Yangning Li, Tingwei Lu, Yinghui Li, Yankai Chen, Wei-Chieh Huang, Wenhao Jiang, Hui Wang, Hai-Tao Zheng, Philip S. Yu"], "abstract": "arXiv:2509.13790v2 Announce Type: replace-cross \nEfficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.13790", "pdf_url": "https://arxiv.org/pdf/2509.13790.pdf", "is_interesting": false}, "989": {"title": "DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models", "authors": ["Kevin Wilkinghoff, Zheng-Hua Tan"], "abstract": "arXiv:2509.13927v2 Announce Type: replace-cross \nReasoning about spatial audio with large language models requires a spatial audio encoder as an acoustic front-end to obtain audio embeddings for further processing. Such an encoder needs to capture all information required to detect the type of sound events, as well as the direction and distance of their corresponding sources. Accomplishing this with a single audio encoder is demanding as the information required for each of these tasks is mostly independent of each other. As a result, the performance obtained with a single encoder is often worse than when using task-specific audio encoders. In this work, we present DSpAST, a novel audio encoder based on SpatialAST that learns disentangled representations of spatial audio while having only 0.2% additional parameters. Experiments on SpatialSoundQA with the spatial audio reasoning system BAT demonstrate that DSpAST significantly outperforms SpatialAST.", "categories": ["eess.AS", "cs.AI", "cs.SD"], "abs_url": "https://arxiv.org/abs/2509.13927", "pdf_url": "https://arxiv.org/pdf/2509.13927.pdf", "is_interesting": false}, "990": {"title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection", "authors": ["Yihao Guo, Haocheng Bian, Liutong Zhou, Ze Wang, Zhaoyi Zhang, Francois Kawala, Milan Dean, Ian Fischer, Yuantao Peng, Noyan Tokgozoglu, Ivan Barrientos, Riyaaz Shaik, Rachel Li, Chandru Venkataraman, Reza Shifteh Far, Moses Pawar, Venkat Sundaranatha, Michael Xu, Frank Chu"], "abstract": "arXiv:2509.14622v3 Announce Type: replace-cross \nWith the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.", "categories": ["cs.CR", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.14622", "pdf_url": "https://arxiv.org/pdf/2509.14622.pdf", "is_interesting": false}, "991": {"title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses", "authors": ["Fangyi Yu, Nabeel Seedat, Dasha Herrmannova, Frank Schilder, Jonathan Richard Schwarz"], "abstract": "arXiv:2509.16093v2 Announce Type: replace-cross \nEvaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ($r=0.78$), compared to traditional metrics ($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.16093", "pdf_url": "https://arxiv.org/pdf/2509.16093.pdf", "is_interesting": false}, "992": {"title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications", "authors": ["Zhenyu Tao, Wei Xu, Xiaohu You"], "abstract": "arXiv:2509.18714v3 Announce Type: replace-cross \nThe bisimulation metric (BSM) is a powerful tool for computing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to multiple-MDP scenarios, such as policy transfer, remains challenging. Prior work has attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis of its mathematical properties has limited further theoretical progress. In this work, we formally establish a generalized bisimulation metric (GBSM) between pairs of MDPs, which is rigorously proven with the three fundamental properties: GBSM symmetry, inter-MDP triangle inequality, and the distance bound on identical state spaces. Leveraging these properties, we theoretically analyse policy transfer, state aggregation, and sampling-based estimation in MDPs, obtaining explicit bounds that are strictly tighter than those derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.18714", "pdf_url": "https://arxiv.org/pdf/2509.18714.pdf", "is_interesting": false}, "993": {"title": "EmbeddingGemma: Powerful and Lightweight Text Representations", "authors": ["Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divyashree Sreepathihalli, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hern\\'andez \\'Abrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, Ga\\\"el Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon, Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Qin Yin, Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig, Mojtaba Seyedhosseini"], "abstract": "arXiv:2509.20354v3 Announce Type: replace-cross \nWe introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.20354", "pdf_url": "https://arxiv.org/pdf/2509.20354.pdf", "is_interesting": false}, "994": {"title": "From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education", "authors": ["Iris Delikoura, Yi. R Fung, Pan Hui"], "abstract": "arXiv:2509.21972v2 Announce Type: replace-cross \nLarge Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.21972", "pdf_url": "https://arxiv.org/pdf/2509.21972.pdf", "is_interesting": false}, "995": {"title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning", "authors": ["Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, Zhi Zhang"], "abstract": "arXiv:2510.00192v2 Announce Type: replace-cross \nLow-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.00192", "pdf_url": "https://arxiv.org/pdf/2510.00192.pdf", "is_interesting": false}, "996": {"title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models", "authors": ["Shutong Wu, Jiawei Zhang"], "abstract": "arXiv:2510.00294v2 Announce Type: replace-cross \nDiffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous \"reversal curse\" or learning under data-constrained scenarios. In addition, taking advantage of their inherent modeling foundations, DLLMs have the great potential of efficient inference with parallel decoding algorithms, which enable multi-token prediction per step. However, the high generation quality often requires the number of decoding steps equal to the sequence length, which performs a one-token-per-step decoding, and existing parallel decoding algorithms, which yield suboptimal decoding paths, bring inference speedup at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (FreeDave), a novel fast decoding algorithm tailored for DLLMs that achieves lossless parallel decoding without any model modification or extra modules. Specifically, we propose an algorithm of parallel-decoded candidate generation and verification, which is theoretically guaranteed to use the fewest model forward calls to reproduce the same sequence generated by static decoding when enough computation and memory budget is provided. By extensive evaluations on math reasoning and code generation benchmarks across different DLLMs, FreeDave is proven to boost the inference throughput up to $3.78\\times$ without performance degradation.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.00294", "pdf_url": "https://arxiv.org/pdf/2510.00294.pdf", "is_interesting": false}, "997": {"title": "Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse", "authors": ["Nathan Junzi Chen"], "abstract": "arXiv:2510.01258v2 Announce Type: replace-cross \nAmidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information media. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague this novel technology. This study employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing one of the aforementioned metrics. The results show an amplified liberal-authoritarian alignment across the six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on the region's pre-existing socio-political structures.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.01258", "pdf_url": "https://arxiv.org/pdf/2510.01258.pdf", "is_interesting": false}, "998": {"title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time", "authors": ["Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Rich\\'e, David Demitri Africa, Mia Taylor"], "abstract": "arXiv:2510.04340v4 Announce Type: replace-cross \nLanguage model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.04340", "pdf_url": "https://arxiv.org/pdf/2510.04340.pdf", "is_interesting": false}, "999": {"title": "Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models", "authors": ["Fabrizio Dimino, Krati Saxena, Bhaskarjit Sarmah, Stefano Pasquali"], "abstract": "arXiv:2510.05702v2 Announce Type: replace-cross \nLarge Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.", "categories": ["q-fin.CP", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.05702", "pdf_url": "https://arxiv.org/pdf/2510.05702.pdf", "is_interesting": false}, "1000": {"title": "What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)", "authors": ["Zixuan Gong, Jiaye Teng, Yong Liu"], "abstract": "arXiv:2510.10089v2 Announce Type: replace-cross \nWhile looped transformers (termed as Looped-Attn) often outperform standard transformers (termed as Single-Attn) on complex reasoning tasks, the theoretical basis for this advantage remains underexplored. In this paper, we explain this phenomenon through the lens of loss landscape geometry, inspired by empirical observations of their distinct dynamics at both sample and Hessian levels. To formalize this, we extend the River-Valley landscape model by distinguishing between U-shaped valleys (flat) and V-shaped valleys (steep). Based on empirical observations, we conjecture that the recursive architecture of Looped-Attn induces a landscape-level inductive bias towards River-V-Valley. Theoretical derivations based on this inductive bias guarantee a better loss convergence along the river due to valley hopping, and further encourage learning about complex patterns compared to the River-U-Valley induced by Single-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical Framework for Progressive Training), a staged training framework that accelerates the training process of Looped-Attn while achieving comparable performances.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.10089", "pdf_url": "https://arxiv.org/pdf/2510.10089.pdf", "is_interesting": false}, "1001": {"title": "Debiasing LLMs by Masking Unfairness-Driving Attention Heads", "authors": ["Tingxu Han, Wei Song, Ziqi Ding, Ziming Li, Chunrong Fang, Yuekang Li, Dongfang Liu, Zhenyu Chen, Zhenting Wang"], "abstract": "arXiv:2510.10142v3 Announce Type: replace-cross \nLarge language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable. Existing work probes when biased outputs appear, but gives little insight into the mechanisms that generate them, leaving existing mitigations largely fragile. In this paper, we conduct a systematic investigation LLM unfairness and propose DiffHeads, a lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting across eight representative open- and closed-source LLMs. DA will trigger the nature bias part of LLM and improve measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues. Next, we define a token-to-head contribution score that traces each token's influence back to individual attention heads. This reveals a small cluster of bias heads that activate under DA but stay largely dormant with CoT, providing the first causal link between prompting strategy and bias emergence. Finally, building on this insight, we propose DiffHeads that identifies bias heads through differential activation analysis between DA and CoT, and selectively masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under DA and CoT, respectively, without harming model utility.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.10142", "pdf_url": "https://arxiv.org/pdf/2510.10142.pdf", "is_interesting": false}, "1002": {"title": "Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models", "authors": ["Di Wu, Shuaidong Pan"], "abstract": "arXiv:2510.10613v2 Announce Type: replace-cross \nThis paper proposes a modeling framework for dynamic topic evolution based on temporal large language models. The method first uses a large language model to obtain contextual embeddings of text and then introduces a temporal decay function and an attention mechanism. These components allow the model to adjust the importance of semantic units according to time intervals and capture topic variations across different periods. The temporal representations are then mapped into a latent topic space, where a state transition matrix is applied to describe the dynamic evolution of topics. A joint optimization objective constrains both semantic modeling and temporal consistency, ensuring diversity and smoothness in topic generation. The design emphasizes the unified modeling of semantic representation and temporal evolution, which improves topic coherence and diversity while enhancing stability and interpretability over time. Experiments on real-world corpora show that the framework effectively captures the generation, expansion, and decline of topics and outperforms existing models across multiple metrics. Overall, the proposed method provides a systematic solution for understanding dynamic semantic patterns in large-scale text, enriches the research paradigm of topic modeling, and supports complex text analysis tasks in multiple domains.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.10613", "pdf_url": "https://arxiv.org/pdf/2510.10613.pdf", "is_interesting": false}, "1003": {"title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation", "authors": ["Hyeseon Ahn, Shinwoo Park, Suyeon Woo, Yo-Sub Han"], "abstract": "arXiv:2510.10987v2 Announce Type: replace-cross \nThe promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://github.com/hsannn/ditto.git.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.10987", "pdf_url": "https://arxiv.org/pdf/2510.10987.pdf", "is_interesting": false}, "1004": {"title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems", "authors": ["Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen"], "abstract": "arXiv:2510.12872v2 Announce Type: replace-cross \nMulti-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.", "categories": ["cs.MA", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.12872", "pdf_url": "https://arxiv.org/pdf/2510.12872.pdf", "is_interesting": false}, "1005": {"title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts", "authors": ["Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li"], "abstract": "arXiv:2510.13500v2 Announce Type: replace-cross \nLLMs hold great promise for healthcare applications, but the rapid evolution of medical knowledge and errors in training data often cause them to generate outdated or inaccurate information, limiting their applicability in high-stakes clinical practice. Model editing has emerged as a potential remedy without full retraining. While parameter-based editing often compromises locality and is thus ill-suited for the medical domain, retrieval-based editing offers a more viable alternative. However, it still faces two critical challenges: (1) representation overlap within the medical knowledge space often causes inaccurate retrieval and reduces editing accuracy; (2) existing methods are restricted to single-sample edits, while batch-editing remains largely unexplored despite its importance for real-world medical applications. To address these challenges, we first construct MedVersa, an enhanced benchmark with broader coverage of medical subjects, designed to evaluate both single and batch edits under strict locality constraints. We then propose MedREK, a retrieval-based editing framework that integrates a shared query-key module for precise matching with an attention-based prompt encoder for informative guidance. Experimental results on various medical benchmarks demonstrate that our MedREK achieves superior performance across different core metrics and provides the first validated solution for batch-editing in medical LLMs. Our code and dataset are available at https://github.com/mylittleriver/MedREK.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.13500", "pdf_url": "https://arxiv.org/pdf/2510.13500.pdf", "is_interesting": false}, "1006": {"title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers", "authors": ["Tuhin Chakrabarty, Jane C. Ginsburg, Paramveer Dhillon"], "abstract": "arXiv:2510.13939v3 Announce Type: replace-cross \nThe use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative content. Yet it's unclear if these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^-8) & writing quality (OR=0.13, p<10^-7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^-13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the \"effect upon the potential market or value\" of the source works.", "categories": ["cs.CL", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2510.13939", "pdf_url": "https://arxiv.org/pdf/2510.13939.pdf", "is_interesting": false}, "1007": {"title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "authors": ["Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu"], "abstract": "arXiv:2510.14830v2 Announce Type: replace-cross \nReal-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained by supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.14830", "pdf_url": "https://arxiv.org/pdf/2510.14830.pdf", "is_interesting": false}, "1008": {"title": "Automotive Crash Dynamics Modeling Accelerated with Machine Learning", "authors": ["Mohammad Amin Nabian, Sudeep Chavare, Deepak Akhare, Rishikesh Ranade, Ram Cherukuri, Srinivas Tadepalli"], "abstract": "arXiv:2510.15201v3 Announce Type: replace-cross \nCrashworthiness assessment is a critical aspect of automotive design, traditionally relying on high-fidelity finite element (FE) simulations that are computationally expensive and time-consuming. This work presents an exploratory comparative study on developing machine learning-based surrogate models for efficient prediction of structural deformation in crash scenarios using the NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine learning to structural crash dynamics, the primary contribution lies in demonstrating the feasibility and engineering utility of the various modeling approaches explored in this work. We investigate two state-of-the-art neural network architectures for modeling crash dynamics: MeshGraphNet, and Transolver. Additionally, we examine three strategies for modeling transient dynamics: time-conditional, the standard Autoregressive approach, and a stability-enhanced Autoregressive scheme incorporating rollout-based training. The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a structurally rich vehicle assembly with over 200 components, including 38 key components featuring variable thickness distributions to capture realistic manufacturing variability. Each model utilizes the undeformed mesh geometry and component characteristics as inputs to predict the spatiotemporal evolution of the deformed mesh during the crash sequence. Evaluation results show that the models capture the overall deformation trends with reasonable fidelity, demonstrating the feasibility of applying machine learning to structural crash dynamics. Although not yet matching full FE accuracy, the models achieve orders-of-magnitude reductions in computational cost, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation.", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.app-ph", "physics.comp-ph"], "abs_url": "https://arxiv.org/abs/2510.15201", "pdf_url": "https://arxiv.org/pdf/2510.15201.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0", "reason": "The paper focuses on machine learning-based models for automotive crash dynamics and structural deformation prediction, which is related to vehicle safety but not directly relevant to autonomous driving tasks like perception, prediction, or control."}}, "1009": {"title": "Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction", "authors": ["Tian Guo, Emmanuel Hauptmann"], "abstract": "arXiv:2510.15691v2 Announce Type: replace-cross \nIn quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured financial data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three representative methods: representation combination, representation summation, and attentive representations. Next, building on empirical observations from fusion learning, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability observed in the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.15691", "pdf_url": "https://arxiv.org/pdf/2510.15691.pdf", "is_interesting": false}, "1010": {"title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness", "authors": ["Longwei Wang, Ifrat Ikhtear Uddin, KC Santosh, Chaowei Zhang, Xiao Qin, Yang Zhou"], "abstract": "arXiv:2510.16171v3 Announce Type: replace-cross \nAdversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions-specifically, rotation- and scale-equivariant layers-into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.16171", "pdf_url": "https://arxiv.org/pdf/2510.16171.pdf", "is_interesting": false}, "1011": {"title": "ADPO: Anchored Direct Preference Optimization", "authors": ["Wang Zixian"], "abstract": "arXiv:2510.18913v3 Announce Type: replace-cross \nDirect Preference Optimization (DPO) has become a standard for aligning models with human feedback, yet its reliance on hard, pairwise preferences makes it brittle to annotator noise and distribution shift. We propose Anchored Direct Preference Optimization (ADPO), a generalized framework that learns from soft, listwise supervision by anchoring policy updates to a reference model. Our key theoretical contribution is to show that this anchoring mechanism imposes an implicit trust region on the policy update, enforced by the softmax Fisher information metric. This provides a robust geometric interpretation for both fixed and dynamic anchor strategies. Our central empirical finding is a task-dependent tradeoff between anchor update strategies. Through controlled experiments across twelve scenarios and two MuJoCo environments, we demonstrate that (1) for online exploration in noisy environments, a dynamic anchor that tracks the learning policy is superior, improving performance by 5 to 11 percent over a fixed anchor; and (2) for offline distillation, a fixed anchor pointing to the teacher policy is dramatically more effective, achieving returns of 206.7 on HalfCheetah-v5 (387 percent of teacher) and 65.4 on Hopper-v5 (61 percent of teacher), while reducing KL divergence to the teacher by up to 5000 times compared with standard knowledge distillation. These findings offer clear, practical guidance for selecting anchor strategies and establish ADPO as a robust, unified framework for preference learning. Larger models further amplify ADPO's benefits (0.718 vs. 0.416 at hidden dimension 256), suggesting that anchoring acts as an effective trust-region regularizer. We release code and configurations to facilitate reproducibility.", "categories": ["cs.LG", "cs.AI", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.18913", "pdf_url": "https://arxiv.org/pdf/2510.18913.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1012": {"title": "Knowledge-guided Continual Learning for Behavioral Analytics Systems", "authors": ["Yasas Senarath, Hemant Purohit"], "abstract": "arXiv:2510.22405v2 Announce Type: replace-cross \nUser behavior on online platforms is evolving, reflecting real-world changes in how people post, whether it's helpful messages or hate speech. Models that learn to capture this content can experience a decrease in performance over time due to data drift, which can lead to ineffective behavioral analytics systems. However, fine-tuning such a model over time with new data can be detrimental due to catastrophic forgetting. Replay-based approaches in continual learning offer a simple yet efficient method to update such models, minimizing forgetting by maintaining a buffer of important training instances from past learned tasks. However, the main limitation of this approach is the fixed size of the buffer. External knowledge bases can be utilized to overcome this limitation through data augmentation. We propose a novel augmentation-based approach to incorporate external knowledge in the replay-based continual learning framework. We evaluate several strategies with three datasets from prior studies related to deviant behavior classification to assess the integration of external knowledge in continual learning and demonstrate that augmentation helps outperform baseline replay-based approaches.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22405", "pdf_url": "https://arxiv.org/pdf/2510.22405.pdf", "is_interesting": false}, "1013": {"title": "GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction", "authors": ["Minjoo Kim, Jinwoong Kim, Sangjin Park"], "abstract": "arXiv:2510.23112v3 Announce Type: replace-cross \nRecent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.", "categories": ["cs.CE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.23112", "pdf_url": "https://arxiv.org/pdf/2510.23112.pdf", "is_interesting": false}, "1014": {"title": "Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation", "authors": ["Thaweerath Phisannupawong, Joshua Julian Damanik, Han-Lim Choi"], "abstract": "arXiv:2510.23636v2 Announce Type: replace-cross \nFlight delay prediction has become a key focus in air traffic management, as delays highlight inefficiencies that impact overall network performance. This paper presents a lightweight large language model-based multimodal flight delay prediction, formulated from the perspective of air traffic controllers monitoring aircraft delay after entering the terminal area. The approach integrates trajectory representations with textual aeronautical information, including flight information, weather reports, and aerodrome notices, by adapting trajectory data into the language modality to capture airspace conditions. The experiments show that the model consistently achieves sub-minute prediction error by effectively leveraging contextual information related to the sources of delay, fulfilling the operational standard for minute-level precision. The framework demonstrates that linguistic understanding, when combined with cross-modality adaptation of trajectory data, enhances delay prediction. Moreover, the approach shows practicality and potential scalability for real-world operations, supporting real-time updates that refine predictions upon receiving new operational information.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.23636", "pdf_url": "https://arxiv.org/pdf/2510.23636.pdf", "is_interesting": false}, "1015": {"title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers", "authors": ["Marko Karbevski, Antonij Mijoski"], "abstract": "arXiv:2510.23912v2 Announce Type: replace-cross \nThe Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. We theoretically investigate whether this triplet can be reduced, proving under simplifying assumptions that the Query weights are redundant, thereby reducing the number of non-embedding/lm-head parameters by over 8%. We validate the theory on full-complexity GPT-3 small architectures (with layer normalization, skip connections, and weight decay) trained from scratch, demonstrating that the reduced model achieves comparable validation loss to standard baselines. These findings motivate the investigation of the Query weight redundancy at scale.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.23912", "pdf_url": "https://arxiv.org/pdf/2510.23912.pdf", "is_interesting": false}, "1016": {"title": "The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems", "authors": ["Stefano Natangelo"], "abstract": "arXiv:2510.24831v2 Announce Type: replace-cross \nArtificial intelligence systems based on large language models (LLMs) can now generate coherent text, music, and images, yet they operate without a persistent state: each inference reconstructs context from scratch. This paper introduces the Narrative Continuity Test (NCT) -- a conceptual framework for evaluating identity persistence and diachronic coherence in AI systems. Unlike capability benchmarks that assess task performance, the NCT examines whether an LLM remains the same interlocutor across time and interaction gaps. The framework defines five necessary axes -- Situated Memory, Goal Persistence, Autonomous Self-Correction, Stylistic & Semantic Stability, and Persona/Role Continuity -- and explains why current architectures systematically fail to support them. Case analyses (Character.\\,AI, Grok, Replit, Air Canada) show predictable continuity failures under stateless inference. The NCT reframes AI evaluation from performance to persistence, outlining conceptual requirements for future benchmarks and architectural designs that could sustain long-term identity and goal coherence in generative models.", "categories": ["cs.CY", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2510.24831", "pdf_url": "https://arxiv.org/pdf/2510.24831.pdf", "is_interesting": false}, "1017": {"title": "Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime", "authors": ["Beomhan Baek, Minhak Song, Chulhee Yun"], "abstract": "arXiv:2510.26303v2 Announce Type: replace-cross \nAdam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\\ell_\\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\\ell_2$-max-margin classifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abs_url": "https://arxiv.org/abs/2510.26303", "pdf_url": "https://arxiv.org/pdf/2510.26303.pdf", "is_interesting": false}, "1018": {"title": "iFlyBot-VLA Technical Report", "authors": ["Yuan Zhang, Chenyu Xue, Wenjie Xu, Chao Ji, Jiajia wu, Jia Pan"], "abstract": "arXiv:2511.01914v1 Announce Type: new \nWe introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community", "categories": ["cs.CV", "cs.AI", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.01914", "pdf_url": "https://arxiv.org/pdf/2511.01914.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on a Vision-Language-Action (VLA) model for robotic manipulation tasks and does not discuss autonomous driving, vehicle perception, or related tasks like 3D detection, trajectory prediction, or vehicle control. It is more relevant to general robotics and manipulation, with no direct connection to autonomous driving systems."}}, "1019": {"title": "Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound", "authors": ["Edoardo Conti, Riccardo Rosati, Lorenzo Federici, Adriano Mancini, Maria Chiara Fiorentin"], "abstract": "arXiv:2511.01915v1 Announce Type: new \nPurpose: This study provides the first comprehensive evaluation of foundation models in fetal ultrasound (US) imaging under low inter-class variability conditions. While recent vision foundation models such as DINOv3 have shown remarkable transferability across medical domains, their ability to discriminate anatomically similar structures has not been systematically investigated. We address this gap by focusing on fetal brain standard planes--transthalamic (TT), transventricular (TV), and transcerebellar (TC)--which exhibit highly overlapping anatomical features and pose a critical challenge for reliable biometric assessment.\n  Methods: To ensure a fair and reproducible evaluation, all publicly available fetal ultrasound datasets were curated and aggregated into a unified multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated images from heterogeneous acquisition settings. DINOv3 was pretrained in a self-supervised manner to learn ultrasound-aware representations. The learned features were then evaluated through standardized adaptation protocols, including linear probing with frozen backbone and full fine-tuning, under two initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization from natural-image DINOv3 weights.\n  Results: Models pretrained on fetal ultrasound data consistently outperformed those initialized on natural images, with weighted F1-score improvements of up to 20 percent. Domain-adaptive pretraining enabled the network to preserve subtle echogenic and structural cues crucial for distinguishing intermediate planes such as TV.\n  Conclusion: Results demonstrate that generic foundation models fail to generalize under low inter-class variability, whereas domain-specific pretraining is essential to achieve robust and clinically reliable representations in fetal brain ultrasound imaging.", "categories": ["cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.01915", "pdf_url": "https://arxiv.org/pdf/2511.01915.pdf", "is_interesting": false}, "1020": {"title": "Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users", "authors": ["Saurabh Kaushik, Lalit Maurya, Elizabeth Tellman, ZhiJie Zhang"], "abstract": "arXiv:2511.01990v1 Announce Type: new \nGeo-Foundational Models (GFMs) enable fast and reliable extraction of spatiotemporal information from satellite imagery, improving flood inundation mapping by leveraging location and time embeddings. Despite their potential, it remains unclear whether GFMs outperform traditional models like U-Net. A systematic comparison across sensors and data availability scenarios is still lacking, which is an essential step to guide end-users in model selection. To address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a Prithvi variant), against TransNorm, U-Net, and Attention U-Net using PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance among all GFMs, with only 2-5% variation between the best and worst models across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In leave-one-region-out cross-validation across five regions, Clay shows slightly better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07), 0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA (0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and Sentinel-1, respectively. Across all 19 sites, leave-one-region-out cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual inspection highlights Clay's superior ability to retain fine details. Few-shot experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational time, Clay is a better choice due to its smaller model size (26M parameters), making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M). Contrary to previous findings, our results suggest GFMs offer small to moderate improvements in flood mapping accuracy at lower computational cost and labeling effort compared to traditional U-Net.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.01990", "pdf_url": "https://arxiv.org/pdf/2511.01990.pdf", "is_interesting": false}, "1021": {"title": "Locally-Supervised Global Image Restoration", "authors": ["Benjamin Walder, Daniel Toader, Robert Nuster, G\\\"unther Paltauf, Peter Burgholzer, Gregor Langer, Lukas Krainer, Markus Haltmeier"], "abstract": "arXiv:2511.01998v1 Announce Type: new \nWe address the problem of image reconstruction from incomplete measurements, encompassing both upsampling and inpainting, within a learning-based framework. Conventional supervised approaches require fully sampled ground truth data, while self-supervised methods allow incomplete ground truth but typically rely on random sampling that, in expectation, covers the entire image. In contrast, we consider fixed, deterministic sampling patterns with inherently incomplete coverage, even in expectation. To overcome this limitation, we exploit multiple invariances of the underlying image distribution, which theoretically allows us to achieve the same reconstruction performance as fully supervised approaches. We validate our method on optical-resolution image upsampling in photoacoustic microscopy (PAM), demonstrating competitive or superior results while requiring substantially less ground truth data.", "categories": ["cs.CV", "cs.NA", "math.NA"], "abs_url": "https://arxiv.org/abs/2511.01998", "pdf_url": "https://arxiv.org/pdf/2511.01998.pdf", "is_interesting": false}, "1022": {"title": "Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images", "authors": ["Tuan Truong, Guillermo Jimenez Perez, Pedro Osorio, Matthias Lenga"], "abstract": "arXiv:2511.02014v1 Announce Type: new \nThe detection of Protected Health Information (PHI) in medical imaging is critical for safeguarding patient privacy and ensuring compliance with regulatory frameworks. Traditional detection methodologies predominantly utilize Optical Character Recognition (OCR) models in conjunction with named entity recognition. However, recent advancements in Large Multimodal Model (LMM) present new opportunities for enhanced text extraction and semantic analysis. In this study, we systematically benchmark three prominent closed and open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing two distinct pipeline configurations: one dedicated to text analysis alone and another integrating both OCR and semantic analysis. Our results indicate that LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to conventional models like EasyOCR. However, this improvement in OCR performance does not consistently correlate with enhanced overall PHI detection accuracy. The strongest performance gains are observed on test cases with complex imprint patterns. In scenarios where text regions are well readable with sufficient contrast, and strong LMMs are employed for text analysis after OCR, different pipeline configurations yield similar results. Furthermore, we provide empirically grounded recommendations for LMM selection tailored to specific operational constraints and propose a deployment strategy that leverages scalable and modular infrastructure.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02014", "pdf_url": "https://arxiv.org/pdf/2511.02014.pdf", "is_interesting": false}, "1023": {"title": "StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities", "authors": ["Zeyu Yang, Clayton Souza Leite, Yu Xiao"], "abstract": "arXiv:2511.02027v1 Announce Type: new \nTracking strength-demanding activities with wearable sensors like IMUs is crucial for monitoring muscular strength, endurance, and power. However, there is a lack of comprehensive datasets capturing these activities. To fill this gap, we introduce \\textit{StrengthSense}, an open dataset that encompasses IMU signals capturing 11 strength-demanding activities, such as sit-to-stand, climbing stairs, and mopping. For comparative purposes, the dataset also includes 2 non-strength demanding activities. The dataset was collected from 29 healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was annotated using video recordings as references. This paper provides a comprehensive overview of the data collection, pre-processing, and technical validation. We conducted a comparative analysis between the joint angles estimated by IMUs and those directly extracted from video to verify the accuracy and reliability of the sensor data. Researchers and developers can utilize \\textit{StrengthSense} to advance the development of human activity recognition algorithms, create fitness and health monitoring applications, and more.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02027", "pdf_url": "https://arxiv.org/pdf/2511.02027.pdf", "is_interesting": false}, "1024": {"title": "Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis", "authors": ["Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan"], "abstract": "arXiv:2511.02046v1 Announce Type: new \nCreation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02046", "pdf_url": "https://arxiv.org/pdf/2511.02046.pdf", "is_interesting": false}, "1025": {"title": "Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study", "authors": ["Yue Yang, Fabian Necker, Christoph Leuze, Michelle Chen, Andrey Finegersh, Jake Lee, Vasu Divi, Bruce Daniel, Brian Hargreaves, Jie Ying Wu, Fred M Baik"], "abstract": "arXiv:2511.02086v1 Announce Type: new \nPurpose: In this paper, we develop and clinically evaluate a depth-only, markerless augmented reality (AR) registration pipeline on a head-mounted display, and assess accuracy across small or low-curvature anatomies in real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i) depth-bias correction, (ii) brief human-in-the-loop initialization, (iii) global and local registration. We validated the surface-tracing error metric by comparing \"skin-to-bone\" relative distances to CT ground truth on leg and foot models, using an AR-tracked tool. We then performed seven intraoperative target trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap harvest and mandibular reconstruction surgery, and collected 500+ data per trial. Results: Preclinical validation showed tight agreement between AR-traced and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm, 1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed significantly (Delta median ~1.1 mm; p < 0.001). Conclusion: A depth-only, markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear, and lower leg in live surgical settings without fiducials, approaching typical clinical error thresholds for moderate-risk tasks. Human-guided initialization plus global-to-local registration enabled accurate alignment on small or low-curvature targets, improving the clinical readiness of markerless AR guidance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02086", "pdf_url": "https://arxiv.org/pdf/2511.02086.pdf", "is_interesting": false}, "1026": {"title": "From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera", "authors": ["Huahua Lin, Xiaohao Cai, Mark Nixon, James M. Mulqueeney, Thomas H. G. Ezard"], "abstract": "arXiv:2511.02142v1 Announce Type: new \nPlanktonic foraminifera, marine protists characterized by their intricate chambered shells, serve as valuable indicators of past and present environmental conditions. Understanding their chamber growth trajectory provides crucial insights into organismal development and ecological adaptation under changing environments. However, automated tracing of chamber growth from imaging data remains largely unexplored, with existing approaches relying heavily on manual segmentation of each chamber, which is time-consuming and subjective. In this study, we propose an end-to-end pipeline that integrates instance segmentation, a computer vision technique not extensively explored in foraminifera, with a dedicated chamber ordering algorithm to automatically reconstruct three-dimensional growth trajectories from high-resolution computed tomography scans. We quantitatively and qualitatively evaluate multiple instance segmentation methods, each optimized for distinct spatial features of the chambers, and examine their downstream influence on growth-order reconstruction accuracy. Experimental results on expert-annotated datasets demonstrate that the proposed pipeline substantially reduces manual effort while maintaining biologically meaningful accuracy. Although segmentation models exhibit under-segmentation in smaller chambers due to reduced voxel fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm remains robust, achieving consistent reconstruction of developmental trajectories even under partial segmentation. This work provides the first fully automated and reproducible pipeline for digital foraminiferal growth analysis, establishing a foundation for large-scale, data-driven ecological studies.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02142", "pdf_url": "https://arxiv.org/pdf/2511.02142.pdf", "is_interesting": false}, "1027": {"title": "Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis", "authors": ["Zhicheng Wang, Junbiao Pang"], "abstract": "arXiv:2511.02144v1 Announce Type: new \nAccurate quantification of pavement crack width plays a pivotal role in assessing structural integrity and guiding maintenance interventions. However, achieving precise crack width measurements presents significant challenges due to: (1) the complex, non-uniform morphology of crack boundaries, which limits the efficacy of conventional approaches, and (2) the demand for rapid measurement capabilities from arbitrary pixel locations to facilitate comprehensive pavement condition evaluation. To overcome these limitations, this study introduces a cascaded framework integrating Principal Component Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from digital images. The proposed methodology comprises three sequential stages: (1) initial crack segmentation using established detection algorithms to generate a binary representation, (2) determination of the primary orientation axis for quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations were conducted across three publicly available datasets, demonstrating that the proposed approach achieves superior performance in both computational efficiency and measurement accuracy compared to existing state-of-the-art techniques.", "categories": ["cs.CV", "stat.AP"], "abs_url": "https://arxiv.org/abs/2511.02144", "pdf_url": "https://arxiv.org/pdf/2511.02144.pdf", "is_interesting": false}, "1028": {"title": "Autobiasing Event Cameras for Flickering Mitigation", "authors": ["Mehdi Sefidgar Dilmaghani, Waseem Shariff, Cian Ryan, Joe Lemley, Peter Corcoran"], "abstract": "arXiv:2511.02180v1 Announce Type: new \nUnderstanding and mitigating flicker effects caused by rapid variations in light intensity is critical for enhancing the performance of event cameras in diverse environments. This paper introduces an innovative autonomous mechanism for tuning the biases of event cameras, effectively addressing flicker across a wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on additional hardware or software for flicker filtering, our approach leverages the event cameras inherent bias settings. Utilizing a simple Convolutional Neural Networks -CNNs, the system identifies instances of flicker in a spatial space and dynamically adjusts specific biases to minimize its impact. The efficacy of this autobiasing system was robustly tested using a face detector framework under both well-lit and low-light conditions, as well as across various frequencies. The results demonstrated significant improvements: enhanced YOLO confidence metrics for face detection, and an increased percentage of frames capturing detected faces. Moreover, the average gradient, which serves as an indicator of flicker presence through edge detection, decreased by 38.2 percent in well-lit conditions and by 53.6 percent in low-light conditions. These findings underscore the potential of our approach to significantly improve the functionality of event cameras in a range of adverse lighting scenarios.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02180", "pdf_url": "https://arxiv.org/pdf/2511.02180.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on mitigating flicker effects in event cameras, which is a general computer vision task. Although the technology could have potential applications in autonomous driving, the paper does not specifically address autonomous driving systems, vehicle perception, or related tasks such as 3D detection, trajectory prediction, or planning and control."}}, "1029": {"title": "Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models", "authors": ["Jinhwan Seo, Yoonki Cho, Junhyug Noh, Sung-eui Yoon"], "abstract": "arXiv:2511.02182v1 Announce Type: new \nIn this technical report, we introduce a framework to address Grounded Video Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The GVQA task demands robust multimodal models capable of complex reasoning over video content, grounding the resulting answers visually, and tracking the referenced objects temporally. To achieve this capability, our proposed approach decomposes the GVQA task into a three-stage pipeline: (1) Video Reasoning \\& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key contribution is the introduction of a trigger moment, derived from our proposed CORTEX prompt, which pinpoints the single most visible frame of a target object to serve as a robust anchor for grounding and tracking. To this end, we achieve the HOTA score of 0.4968, which marks a significant improvement over the previous year's winning score of 0.2704 on GVQA task.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02182", "pdf_url": "https://arxiv.org/pdf/2511.02182.pdf", "is_interesting": false}, "1030": {"title": "MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation", "authors": ["Jiawen Liu, Yuanbo Zeng, Jiaming Liang, Yizhen Yang, Yiheng Zhang, Enhui Cai, Xiaoqi Sheng, Hongmin Cai"], "abstract": "arXiv:2511.02193v1 Announce Type: new \nAccurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $\\%$ on DRIVE and 1.25 $\\%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via https://github.com/liujiawen-jpg/MM-UNet.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02193", "pdf_url": "https://arxiv.org/pdf/2511.02193.pdf", "is_interesting": false}, "1031": {"title": "Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers", "authors": ["Zhengjie Zhang, Xiaoxie Mao, Qihao Guo, Shaoting Zhang, Qi Huang, Mu Zhou, Fang Xie, Mianxin Liu"], "abstract": "arXiv:2511.02206v1 Announce Type: new \nBackground: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM = 0.920 +/- 0.003) and regional patterns (Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC = 0.78) outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC = 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimer's disease.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02206", "pdf_url": "https://arxiv.org/pdf/2511.02206.pdf", "is_interesting": false}, "1032": {"title": "Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping", "authors": ["Jiajia Li, Keyi Zhu, Qianwen Zhang, Dong Chen, Qi Sun, Zhaojian Li"], "abstract": "arXiv:2511.02207v1 Announce Type: new \nStrawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02207", "pdf_url": "https://arxiv.org/pdf/2511.02207.pdf", "is_interesting": false}, "1033": {"title": "Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning", "authors": ["Anders Austlid Task\\'en, Thierry Judge, Erik Andreas Rye Berg, Jinyang Yu, Bj{\\o}rnar Grenne, Frank Lindseth, Svend Aakhus, Pierre-Marc Jodoin, Nicolas Duchateau, Olivier Bernard, Gabriel Kiss"], "abstract": "arXiv:2511.02210v1 Announce Type: new \nSegmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.\n  As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.\n  Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95\\% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.", "categories": ["cs.CV", "cs.AI", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.02210", "pdf_url": "https://arxiv.org/pdf/2511.02210.pdf", "is_interesting": false}, "1034": {"title": "Can Foundation Models Revolutionize Mobile AR Sparse Sensing?", "authors": ["Yiqin Zhao, Tian Guo"], "abstract": "arXiv:2511.02215v1 Announce Type: new \nMobile sensing systems have long faced a fundamental trade-off between sensing quality and efficiency due to constraints in computation, power, and other limitations. Sparse sensing, which aims to acquire and process only a subset of sensor data, has been a key strategy for maintaining performance under such constraints. However, existing sparse sensing methods often suffer from reduced accuracy, as missing information across space and time introduces uncertainty into many sensing systems. In this work, we investigate whether foundation models can change the landscape of mobile sparse sensing. Using real-world mobile AR data, our evaluations demonstrate that foundation models offer significant improvements in geometry-aware image warping, a central technique for enabling accurate reuse of cross-frame information. Furthermore, our study demonstrates the scalability of foundation model-based sparse sensing and shows its leading performance in 3D scene reconstruction. Collectively, our study reveals critical aspects of the promises and the open challenges of integrating foundation models into mobile sparse sensing systems.", "categories": ["cs.CV", "cs.ET"], "abs_url": "https://arxiv.org/abs/2511.02215", "pdf_url": "https://arxiv.org/pdf/2511.02215.pdf", "is_interesting": false}, "1035": {"title": "Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis", "authors": ["Delin Ma, Menghui Zhou, Jun Qi, Yun Yang, Po Yang"], "abstract": "arXiv:2511.02228v1 Announce Type: new \nAlzheimer's disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02228", "pdf_url": "https://arxiv.org/pdf/2511.02228.pdf", "is_interesting": false}, "1036": {"title": "Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency", "authors": ["Hao Li, Daiwei Lu, Jesse d'Almeida, Dilara Isik, Ehsan Khodapanah Aghdam, Nick DiSanto, Ayberk Acar, Susheela Sharma, Jie Ying Wu, Robert J. Webster III, Ipek Oguz"], "abstract": "arXiv:2511.02247v1 Announce Type: new \nMonocular depth estimation (MDE) is a critical task to guide autonomous medical robots. However, obtaining absolute (metric) depth from an endoscopy camera in surgical scenes is difficult, which limits supervised learning of depth on real endoscopic images. Current image-level unsupervised domain adaptation methods translate synthetic images with known depth maps into the style of real endoscopic frames and train depth networks using these translated images with their corresponding depth maps. However a domain gap often remains between real and translated synthetic images. In this paper, we present a latent feature alignment method to improve absolute depth estimation by reducing this domain gap in the context of endoscopic videos of the central airway. Our methods are agnostic to the image translation process and focus on the depth estimation itself. Specifically, the depth network takes translated synthetic and real endoscopic frames as input and learns latent domain-invariant features via adversarial learning and directional feature consistency. The evaluation is conducted on endoscopic videos of central airway phantoms with manually aligned absolute depth maps. Compared to state-of-the-art MDE methods, our approach achieves superior performance on both absolute and relative depth metrics, and consistently improves results across various backbones and pretrained weights. Our code is available at https://github.com/MedICL-VU/MDE.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02247", "pdf_url": "https://arxiv.org/pdf/2511.02247.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on monocular depth estimation for autonomous medical robots in endoscopic settings, which is unrelated to autonomous driving. It discusses methods for improving depth estimation in medical applications rather than tasks related to vehicle perception, control, or planning."}}, "1037": {"title": "Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework", "authors": ["Yucheng Song, Yifan Ge, Junhao Li, Zhining Liao, Zhifang Liao"], "abstract": "arXiv:2511.02271v1 Announce Type: new \nMedical Report Generation (MRG) is a key part of modern medical diagnostics, as it automatically generates reports from radiological images to reduce radiologists' burden. However, reliable MRG models for lesion description face three main challenges: insufficient domain knowledge understanding, poor text-visual entity embedding alignment, and spurious correlations from cross-modal biases. Previous work only addresses single challenges, while this paper tackles all three via a novel hierarchical task decomposition approach, proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into low-, mid-, and high-level tasks: 1) Low-level: align medical entity features with spatial locations to enhance domain knowledge for visual encoders; 2) Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling (images) to boost cross-modal alignment via mutual guidance; 3) High-level: a cross-modal causal intervention module (via front-door intervention) to reduce confounders and improve interpretability. Extensive experiments confirm HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA) MRG methods. Code will be made public upon paper acceptance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02271", "pdf_url": "https://arxiv.org/pdf/2511.02271.pdf", "is_interesting": false}, "1038": {"title": "Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?", "authors": ["Giorgos Sfikas, Konstantina Nikolaidou, Foteini Papadopoulou, George Retsinas, Anastasios L. Kesidis"], "abstract": "arXiv:2511.02277v1 Announce Type: new \nObject pose estimation is a task that is of central importance in 3D Computer Vision. Given a target image and a canonical pose, a single point estimate may very often be sufficient; however, a probabilistic pose output is related to a number of benefits when pose is not unambiguous due to sensor and projection constraints or inherent object symmetries. With this paper, we explore the usefulness of using the well-known Euler angles parameterisation as a basis for a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation, 3D pose has been parameterized in a number of ways, either in or out of the context of parameter estimation. We explore the idea that Euler angles, despite their shortcomings, may lead to useful models in a number of aspects, compared to a model built on a more complex parameterisation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02277", "pdf_url": "https://arxiv.org/pdf/2511.02277.pdf", "is_interesting": false}, "1039": {"title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning", "authors": ["Fangxun Shu, Yongjie Ye, Yue Liao, Zijian Kang, Weijie Yin, Jiacong Wang, Xiao Liang, Shuicheng Yan, Chao Feng"], "abstract": "arXiv:2511.02280v1 Announce Type: new \nWe introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02280", "pdf_url": "https://arxiv.org/pdf/2511.02280.pdf", "is_interesting": false}, "1040": {"title": "Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions", "authors": ["Cuong Tuan Nguyen, Ngoc Tuan Nguyen, Triet Hoang Minh Dao, Huy Minh Nhat, Huy Truong Dinh"], "abstract": "arXiv:2511.02288v1 Announce Type: new \nWe propose a Graph Neural Network (GNN)-based approach for Handwritten Mathematical Expression (HME) recognition by modeling HMEs as graphs, where nodes represent symbols and edges capture spatial dependencies. A deep BLSTM network is used for symbol segmentation, recognition, and spatial relation classification, forming an initial primitive graph. A 2D-CFG parser then generates all possible spatial relations, while the GNN-based link prediction model refines the structure by removing unnecessary connections, ultimately forming the Symbol Label Graph. Experimental results demonstrate the effectiveness of our approach, showing promising performance in HME structure recognition.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02288", "pdf_url": "https://arxiv.org/pdf/2511.02288.pdf", "is_interesting": false}, "1041": {"title": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization", "authors": ["Shaohan Li, Yunpeng Shi, Gilad Lerman"], "abstract": "arXiv:2511.02329v1 Announce Type: new \nWe introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) -- originally developed for group synchronization -- to camera location estimation. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, showing that cycle consistency alone -- without access to inter-camera distances -- suffices to achieve the lowest sample complexity currently known. To further enhance robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation synchronization. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment.", "categories": ["cs.CV", "cs.NA", "cs.RO", "math.NA", "stat.ME"], "abs_url": "https://arxiv.org/abs/2511.02329", "pdf_url": "https://arxiv.org/pdf/2511.02329.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper focuses on camera pose estimation and synchronization using a cycle-consistent method, which is a general computer vision task. While this method could be applied to autonomous driving for camera-based localization, the paper does not specifically address autonomous driving contexts like vehicle control or perception tasks."}}, "1042": {"title": "GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection", "authors": ["Kun Zou, Yongheng Xu, Jianxing Yu, Yan Pan, Jian Yin, Hanjiang Lai"], "abstract": "arXiv:2511.02335v1 Announce Type: new \nOut-of-distribution (OOD) detection is paramount to ensuring the reliability and robustness of learning models in real-world applications. Existing post-hoc OOD detection methods detect OOD samples by leveraging their features and logits information without retraining. However, they often overlook the inherent correlation between features and logits, which is crucial for effective OOD detection. To address this limitation, we propose Global-Aware Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to refine decision boundaries and increase discriminative performance. Firstly, it performs global-aware feature decoupling guided by classification weights. This involves aligning features with the direction of global classification weights to decouple them. From this, GAFD-CC extracts two types of critical information: positively correlated features that promote in-distribution (ID)/OOD boundary refinement and negatively correlated features that suppress false positives and tighten these boundaries. Secondly, it adaptively fuses these decoupled features with multi-scale logit-based confidence for comprehensive and robust OOD detection. Extensive experiments on large-scale benchmarks demonstrate GAFD-CC's competitive performance and strong generalization ability compared to those of state-of-the-art methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02335", "pdf_url": "https://arxiv.org/pdf/2511.02335.pdf", "is_interesting": false}, "1043": {"title": "M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings", "authors": ["Jiankai Tang, Tao Zhang, Jia Li, Yiru Zhang, Mingyu Zhang, Kegang Wang, Yuming Hao, Bolin Wang, Haiyang Li, Xingyao Wang, Yuanchun Shi, Yuntao Wang, Sichong Qian"], "abstract": "arXiv:2511.02349v1 Announce Type: new \nPortable physiological monitoring is essential for early detection and management of cardiovascular disease, but current methods often require specialized equipment that limits accessibility or impose impractical postures that patients cannot maintain. Video-based photoplethysmography on smartphones offers a convenient noninvasive alternative, yet it still faces reliability challenges caused by motion artifacts, lighting variations, and single-view constraints. Few studies have demonstrated reliable application to cardiovascular patients, and no widely used open datasets exist for cross-device accuracy. To address these limitations, we introduce the M3PD dataset, the first publicly available dual-view mobile photoplethysmography dataset, comprising synchronized facial and fingertip videos captured simultaneously via front and rear smartphone cameras from 60 participants (including 47 cardiovascular patients). Building on this dual-view setting, we further propose F3Mamba, which fuses the facial and fingertip views through Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to 30.2 percent over existing single-view baselines while improving robustness in challenging real-world scenarios. Data and code: https://github.com/Health-HCI-Group/F3Mamba.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02349", "pdf_url": "https://arxiv.org/pdf/2511.02349.pdf", "is_interesting": false}, "1044": {"title": "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning", "authors": ["Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan"], "abstract": "arXiv:2511.02360v1 Announce Type: new \nIn human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02360", "pdf_url": "https://arxiv.org/pdf/2511.02360.pdf", "is_interesting": false}, "1045": {"title": "RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning", "authors": ["Jiahe Song, Chuang Wang, Bowen Jiang, Yinfan Wang, Hao Zheng, Xingjian Wei, Chengjin Liu, Junyuan Gao, Yubin Wang, Lijun Wu, Jiang Wu, Qian Yu, Conghui He"], "abstract": "arXiv:2511.02384v1 Announce Type: new \nLarge-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed \"BBox and Index as Visual Prompt\" (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02384", "pdf_url": "https://arxiv.org/pdf/2511.02384.pdf", "is_interesting": false}, "1046": {"title": "Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds", "authors": ["Leon Schwarzer, Matthias Zeller, Daniel Casado Herraez, Simon Dierl, Michael Heidingsfeld, Cyrill Stachniss"], "abstract": "arXiv:2511.02395v1 Announce Type: new \nMoving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point's Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02395", "pdf_url": "https://arxiv.org/pdf/2511.02395.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u591a\u4f20\u611f\u5668\u878d\u5408 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper addresses self-supervised moving object segmentation for radar point clouds, which is directly relevant to autonomous driving systems. Moving object segmentation is crucial for tasks such as SLAM, path planning, and perception, and radar sensors are commonly used in autonomous vehicles to enhance object detection in challenging conditions like sparse or noisy data."}}, "1047": {"title": "A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds", "authors": ["Kuo-Liang Chung, Ting-Chung Tang"], "abstract": "arXiv:2511.02397v1 Announce Type: new \nColor consistency correction for color point clouds is a fundamental yet important task in 3D rendering and compression applications. In the past, most previous color correction methods aimed at correcting color for color images. The purpose of this paper is to propose a grouping-based hybrid color correction algorithm for color point clouds. Our algorithm begins by estimating the overlapping rate between the aligned source and target point clouds, and then adaptively partitions the target points into two groups, namely the close proximity group Gcl and the moderate proximity group Gmod, or three groups, namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated overlapping rate is low or high, respectively. To correct color for target points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method is proposed. To correct color for target points in Gmod, a joint KBI and the histogram equalization (JKHE) method is proposed. For target points in Gdist, a histogram equalization (HE) method is proposed for color correction. Finally, we discuss the grouping-effect free property and the ablation study in our algorithm. The desired color consistency correction benefit of our algorithm has been justified through 1086 testing color point cloud pairs against the state-of-the-art methods. The C++ source code of our algorithm can be accessed from the website: https://github.com/ivpml84079/Point-cloud-color-correction.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02397", "pdf_url": "https://arxiv.org/pdf/2511.02397.pdf", "is_interesting": false}, "1048": {"title": "Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs", "authors": ["Arya Shah, Vaibhav Tripathi"], "abstract": "arXiv:2511.02404v1 Announce Type: new \nCats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF $\\approx0.814$, mean CKA-linear $\\approx0.745$, mean RSA $\\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B/16 RSA $\\approx0.53$ at block8; ViT-L/16 $\\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02404", "pdf_url": "https://arxiv.org/pdf/2511.02404.pdf", "is_interesting": false}, "1049": {"title": "IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition", "authors": ["Wenyang Wei, Yang yang, Xixi Jia, Xiangchu Feng, Weiwei Wang, Renzhen Wang"], "abstract": "arXiv:2511.02411v1 Announce Type: new \nWe present IllumFlow, a novel framework that synergizes conditional Rectified Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our model addresses low-light enhancement through separate optimization of illumination and reflectance components, effectively handling both lighting variations and noise. Specifically, we first decompose an input image into reflectance and illumination components following Retinex theory. To model the wide dynamic range of illumination variations in low-light images, we propose a conditional rectified flow framework that represents illumination changes as a continuous flow field. While complex noise primarily resides in the reflectance component, we introduce a denoising network, enhanced by flow-derived data augmentation, to remove reflectance noise and chromatic aberration while preserving color fidelity. IllumFlow enables precise illumination adaptation across lighting conditions while naturally supporting customizable brightness enhancement. Extensive experiments on low-light enhancement and exposure correction demonstrate superior quantitative and qualitative performance over existing methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02411", "pdf_url": "https://arxiv.org/pdf/2511.02411.pdf", "is_interesting": false}, "1050": {"title": "ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension", "authors": ["Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang"], "abstract": "arXiv:2511.02415v1 Announce Type: new \nComplex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&amp;A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02415", "pdf_url": "https://arxiv.org/pdf/2511.02415.pdf", "is_interesting": false}, "1051": {"title": "Synthetic Crop-Weed Image Generation and its Impact on Model Generalization", "authors": ["Garen Boyadjian (INRAE), Cyrille Pierre (INRAE), Johann Laconte (INRAE, UR TSCF), Riccardo Bertoglio (INRAE)"], "abstract": "arXiv:2511.02417v1 Announce Type: new \nPrecise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.02417", "pdf_url": "https://arxiv.org/pdf/2511.02417.pdf", "is_interesting": false}, "1052": {"title": "From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics", "authors": ["Nicolas Schuler, Lea Dewald, Nick Baldig, J\\\"urgen Graf"], "abstract": "arXiv:2511.02427v1 Announce Type: new \nVideo Understanding, Scene Interpretation and Commonsense Reasoning are highly challenging tasks enabling the interpretation of visual information, allowing agents to perceive, interact with and make rational decisions in its environment. Large Language Models (LLMs) and Visual Language Models (VLMs) have shown remarkable advancements in these areas in recent years, enabling domain-specific applications as well as zero-shot open vocabulary tasks, combining multiple domains. However, the required computational complexity poses challenges for their application on edge devices and in the context of Mobile Robotics, especially considering the trade-off between accuracy and inference time. In this paper, we investigate the capabilities of state-of-the-art VLMs for the task of Scene Interpretation and Action Recognition, with special regard to small VLMs capable of being deployed to edge devices in the context of Mobile Robotics. The proposed pipeline is evaluated on a diverse dataset consisting of various real-world cityscape, on-campus and indoor scenarios. The experimental evaluation discusses the potential of these small models on edge devices, with particular emphasis on challenges, weaknesses, inherent model biases and the application of the gained information. Supplementary material is provided via the following repository: https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.02427", "pdf_url": "https://arxiv.org/pdf/2511.02427.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper primarily focuses on video understanding, scene interpretation, and action recognition using Visual Language Models for mobile robotics, with an emphasis on edge devices. While these tasks can be relevant for autonomous driving, the paper does not specifically discuss autonomous driving systems or tasks such as vehicle perception, decision-making, or control, and the focus is more on general robotics applications."}}, "1053": {"title": "KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image", "authors": ["Teerapong Panboonyuen"], "abstract": "arXiv:2511.02462v1 Announce Type: new \nSatellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02462", "pdf_url": "https://arxiv.org/pdf/2511.02462.pdf", "is_interesting": false}, "1054": {"title": "MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer", "authors": ["Taiga Yamane, Satoshi Suzuki, Ryo Masumura, Shotaro Tora"], "abstract": "arXiv:2511.02473v1 Announce Type: new \nMulti-view action recognition aims to recognize human actions using multiple camera views and deals with occlusion caused by obstacles or crowds. In this task, cooperation among views, which generates a joint representation by combining multiple views, is vital. Previous studies have explored promising cooperation methods for improving performance. However, since their methods focus only on the task setting of recognizing a single action from an entire video, they are not applicable to the recently popular spatio-temporal action recognition~(STAR) setting, in which each person's action is recognized sequentially. To address this problem, this paper proposes a multi-view action recognition method for the STAR setting, called MVAFormer. In MVAFormer, we introduce a novel transformer-based cooperation module among views. In contrast to previous studies, which utilize embedding vectors with lost spatial information, our module utilizes the feature map for effective cooperation in the STAR setting, which preserves the spatial information. Furthermore, in our module, we divide the self-attention for the same and different views to model the relationship between multiple views effectively. The results of experiments using a newly collected dataset demonstrate that MVAFormer outperforms the comparison baselines by approximately $4.4$ points on the F-measure.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02473", "pdf_url": "https://arxiv.org/pdf/2511.02473.pdf", "is_interesting": false}, "1055": {"title": "OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control", "authors": ["Xilong Zhou, Jianchun Chen, Pramod Rao, Timo Teufel, Linjie Lyu, Tigran Minasian, Oleksandr Sotnychenko, Xiaoxiao Long, Marc Habermann, Christian Theobalt"], "abstract": "arXiv:2511.02483v1 Announce Type: new \nWe introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2511.02483", "pdf_url": "https://arxiv.org/pdf/2511.02483.pdf", "is_interesting": false}, "1056": {"title": "Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization", "authors": ["Tao Liu, Kan Ren, Qian Chen"], "abstract": "arXiv:2511.02489v1 Announce Type: new \nWith the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems. However, in GNSS-denied areas, satellite-based localization methods are prone to failure. This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching. In typical pipelines, UAV visual localization is formulated as an image-retrieval problem: features are extracted to build a localization map, and the pose of a query image is estimated by matching it to a reference database with known poses. Because publicly available UAV localization datasets are limited, many approaches recast localization as a classification task and rely on scene labels in these datasets to ensure accuracy. Other methods seek to reduce cross-domain differences using polar-coordinate reprojection, perspective transformations, or generative adversarial networks; however, they can suffer from misalignment, content loss, and limited realism. In contrast, we leverage modern object detection to accurately extract salient instances from UAV and satellite images, and integrate a graph neural network to reason about inter-image and intra-image node relationships. Using a fine-grained, graph-based node-similarity metric, our method achieves strong retrieval and localization performance. Extensive experiments on public and real-world datasets show that our approach handles heterogeneous appearance differences effectively and generalizes well, making it applicable to scenarios with larger modality gaps, such as infrared-visible image matching. Our dataset will be publicly available at the following URL: https://github.com/liutao23/ODGNNLoc.git.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02489", "pdf_url": "https://arxiv.org/pdf/2511.02489.pdf", "is_interesting": false}, "1057": {"title": "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding", "authors": ["Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang"], "abstract": "arXiv:2511.02495v1 Announce Type: new \nRecent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02495", "pdf_url": "https://arxiv.org/pdf/2511.02495.pdf", "is_interesting": false}, "1058": {"title": "Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes", "authors": ["Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang"], "abstract": "arXiv:2511.02503v1 Announce Type: new \nThe automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02503", "pdf_url": "https://arxiv.org/pdf/2511.02503.pdf", "is_interesting": false}, "1059": {"title": "ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing", "authors": ["Yaosen Chen, Wei Wang, Xuming Wen, Han Yang, Yanru Zhang"], "abstract": "arXiv:2511.02505v1 Announce Type: new \nShot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly.To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02505", "pdf_url": "https://arxiv.org/pdf/2511.02505.pdf", "is_interesting": false}, "1060": {"title": "Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems", "authors": ["Nicolas Schuler, Lea Dewald, J\\\"urgen Graf"], "abstract": "arXiv:2511.02507v1 Announce Type: new \nRecent advancements in Deep Learning enable hardware-based cognitive systems, that is, mechatronic systems in general and robotics in particular with integrated Artificial Intelligence, to interact with dynamic and unstructured environments. While the results are impressive, the application of such systems to critical tasks like autonomous driving as well as service and care robotics necessitate the evaluation of large amount of heterogeneous data. Automated report generation for Mobile Robotics can play a crucial role in facilitating the evaluation and acceptance of such systems in various domains. In this paper, we propose a pipeline for generating automated reports in natural language utilizing various multi-modal sensors that solely relies on local models capable of being deployed on edge computing devices, thus preserving the privacy of all actors involved and eliminating the need for external services. In particular, we evaluate our implementation on a diverse dataset spanning multiple domains including indoor, outdoor and urban environments, providing quantitative as well as qualitative evaluation results. Various generated example reports and other supplementary materials are available via a public repository.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.02507", "pdf_url": "https://arxiv.org/pdf/2511.02507.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.4, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses mobile robotics and edge computing for cognitive systems, which are applicable to autonomous driving, especially in terms of multi-modal sensor data processing and environment interaction. While the paper does not focus directly on autonomous driving tasks like perception or planning, it highlights relevant technologies like data evaluation in dynamic environments, which could benefit autonomous driving systems."}}, "1061": {"title": "LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization", "authors": ["Jee Won Lee, Jongseong Brad Choi"], "abstract": "arXiv:2511.02510v1 Announce Type: new \nSparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter. Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning logic on maximum blending weight, stabilized by EMA-hysteresis guards and refines structure through ray-footprint-based, priority-driven subdivision under an explicit growth budget. Ablations and full-system results across Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation of errors in low-frequency regions and boundary instability while keeping PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline. Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency detail that prior setups miss, enabling more predictable, memory-efficient training without sacrificing perceptual quality.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02510", "pdf_url": "https://arxiv.org/pdf/2511.02510.pdf", "is_interesting": false}, "1062": {"title": "Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data", "authors": ["Jessica Plassmann, Nicolas Schuler, Georg von Freymann, Michael Schuth"], "abstract": "arXiv:2511.02541v1 Announce Type: new \nShearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02541", "pdf_url": "https://arxiv.org/pdf/2511.02541.pdf", "is_interesting": false}, "1063": {"title": "Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction", "authors": ["Ali Farki, Elaheh Moradi, Deepika Koundal, Jussi Tohka"], "abstract": "arXiv:2511.02558v1 Announce Type: new \nPredicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "abs_url": "https://arxiv.org/abs/2511.02558", "pdf_url": "https://arxiv.org/pdf/2511.02558.pdf", "is_interesting": false}, "1064": {"title": "The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic", "authors": ["Akash Sharma, Chinmay Mhatre, Sankalp Gawali, Ruthvik Bokkasam, Brij Kishore, Vishwajeet Pattanaik, Tarun Rambha, Abdul R. Pinjari, Vijay Kovvali, Anirban Chakraborty, Punit Rathore, Raghu Krishnapuram, Yogesh Simmhan"], "abstract": "arXiv:2511.02563v1 Announce Type: new \nThis report describes the UVH-26 dataset, the first public release by AIM@IISc of a large-scale dataset of annotated traffic-camera images from India. The dataset comprises 26,646 high-resolution (1080p) images sampled from 2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently annotated through a crowdsourced hackathon involving 565 college students from across India. In total, 1.8 million bounding boxes were labeled across 14 vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler (Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller, Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k consensus ground truth bounding boxes and labels were derived for distinct objects in the 26k images using Majority Voting and STAPLE algorithms. Further, we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X, and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50, mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in mAP50:95 over equivalent baseline models trained on COCO dataset, with RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40 for COCO-trained weights for common classes (Car, Bus, and Truck). This demonstrates the benefits of domain-specific training data for Indian traffic scenarios. The release package provides the 26k images with consensus annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the 6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the heterogeneity of Indian urban mobility directly from operational traffic-camera streams, UVH-26 addresses a critical gap in existing global benchmarks, and offers a foundation for advancing detection, classification, and deployment of intelligent transportation systems in emerging nations with complex traffic conditions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02563", "pdf_url": "https://arxiv.org/pdf/2511.02563.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses the UVH-26 dataset, which focuses on annotated traffic-camera images from India for vehicle detection and classification. While the dataset can be beneficial for applications like traffic monitoring and intelligent transportation systems, it does not directly address autonomous driving tasks like perception, prediction, or decision-making, though it may be applicable in a broader context for autonomous driving systems in terms of traffic scene understanding."}}, "1065": {"title": "Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification", "authors": ["Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, Hugo Proenca"], "abstract": "arXiv:2511.02564v1 Announce Type: new \nVideo-based person re-identification (ReID) in cross-view domains (for example, aerial-ground surveillance) remains an open problem because of extreme viewpoint shifts, scale disparities, and temporal inconsistencies. To address these challenges, we propose MTF-CVReID, a parameter-efficient framework that introduces seven complementary modules over a ViT-B/16 backbone. Specifically, we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment (IVFA) for perspective-invariant representation alignment; (6) Hierarchical Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities; and (7) Multi-View Identity Consistency Learning (MVICL) that enforces cross-view identity coherence using a contrastive learning paradigm. Despite adding only about 2 million parameters and 0.7 GFLOPs over the baseline, MTF-CVReID maintains real-time efficiency (189 FPS) and achieves state-of-the-art performance on the AG-VPReID benchmark across all altitude levels, with strong cross-dataset generalization to G2A-VReID and MARS datasets. These results show that carefully designed adapter-based modules can substantially enhance cross-view robustness and temporal consistency without compromising computational efficiency. The source code is available at https://github.com/MdRashidunnabi/MTF-CVReID", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02564", "pdf_url": "https://arxiv.org/pdf/2511.02564.pdf", "is_interesting": false}, "1066": {"title": "A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding", "authors": ["Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li"], "abstract": "arXiv:2511.02565v1 Announce Type: new \nSubject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02565", "pdf_url": "https://arxiv.org/pdf/2511.02565.pdf", "is_interesting": false}, "1067": {"title": "TAUE: Training-free Noise Transplant and Cultivation Diffusion Model", "authors": ["Daichi Nagai, Ryugo Morita, Shunsuke Kitada, Hitoshi Iyatomi"], "abstract": "arXiv:2511.02580v1 Announce Type: new \nDespite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02580", "pdf_url": "https://arxiv.org/pdf/2511.02580.pdf", "is_interesting": false}, "1068": {"title": "Zero-Shot Multi-Animal Tracking in the Wild", "authors": ["Jan Frederik Meier, Timo L\\\"uddecke"], "abstract": "arXiv:2511.02591v1 Announce Type: new \nMulti-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot multi-animal tracking. By combining a Grounding Dino object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully designed heuristics, we develop a tracking framework that can be applied to new datasets without any retraining or hyperparameter adaptation. Evaluations on ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate strong and consistent performance across diverse species and environments. The code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02591", "pdf_url": "https://arxiv.org/pdf/2511.02591.pdf", "is_interesting": false}, "1069": {"title": "UniChange: Unifying Change Detection with Multimodal Large Language Model", "authors": ["Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li"], "abstract": "arXiv:2511.02607v1 Announce Type: new \nChange detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02607", "pdf_url": "https://arxiv.org/pdf/2511.02607.pdf", "is_interesting": false}, "1070": {"title": "Robust Face Liveness Detection for Biometric Authentication using Single Image", "authors": ["Poulami Raha, Yeongnam Chae"], "abstract": "arXiv:2511.02645v1 Announce Type: new \nBiometric technologies are widely adopted in security, legal, and financial systems. Face recognition can authenticate a person based on the unique facial features such as shape and texture. However, recent works have demonstrated the vulnerability of Face Recognition Systems (FRS) towards presentation attacks. Using spoofing (aka.,presentation attacks), a malicious actor can get illegitimate access to secure systems. This paper proposes a novel light-weight CNN framework to identify print/display, video and wrap attacks. The proposed robust architecture provides seamless liveness detection ensuring faster biometric authentication (1-2 seconds on CPU). Further, this also presents a newly created 2D spoof attack dataset consisting of more than 500 videos collected from 60 subjects. To validate the effectiveness of this architecture, we provide a demonstration video depicting print/display, video and wrap attack detection approaches. The demo can be viewed in the following link: https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02645", "pdf_url": "https://arxiv.org/pdf/2511.02645.pdf", "is_interesting": false}, "1071": {"title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models", "authors": ["Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui"], "abstract": "arXiv:2511.02650v1 Announce Type: new \nLarge multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02650", "pdf_url": "https://arxiv.org/pdf/2511.02650.pdf", "is_interesting": false}, "1072": {"title": "Differentiable Hierarchical Visual Tokenization", "authors": ["Marius Aasan, Martine Hjelkrem-Tan, Nico Catalano, Changkyu Choi, Ad\\'in Ram\\'irez Rivera"], "abstract": "arXiv:2511.02652v1 Announce Type: new \nVision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02652", "pdf_url": "https://arxiv.org/pdf/2511.02652.pdf", "is_interesting": false}, "1073": {"title": "Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification", "authors": ["Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li"], "abstract": "arXiv:2511.02685v1 Announce Type: new \nVisible-infrared person re-identification (VI-ReID) technique could associate the pedestrian images across visible and infrared modalities in the practical scenarios of background illumination changes. However, a substantial gap inherently exists between these two modalities. Besides, existing methods primarily rely on intermediate representations to align cross-modal features of the same person. The intermediate feature representations are usually create by generating intermediate images (kind of data enhancement), or fusing intermediate features (more parameters, lack of interpretability), and they do not make good use of the intermediate features. Thus, we propose a novel VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a middle generated image as a transmitter from visible to infrared modals, which are fully aligned with the original visible images and similar to the infrared modality. After that, using a modality-transition contrastive loss and a modality-query regularization loss for training, which could align the cross-modal features more effectively. Notably, our proposed framework does not need any additional parameters, which achieves the same inference speed to the backbone while improving its performance on VI-ReID task. Extensive experimental results illustrate that our model significantly and consistently outperforms existing SOTAs on three typical VI-ReID datasets.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02685", "pdf_url": "https://arxiv.org/pdf/2511.02685.pdf", "is_interesting": false}, "1074": {"title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models", "authors": ["Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang"], "abstract": "arXiv:2511.02712v1 Announce Type: new \nUnderstanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02712", "pdf_url": "https://arxiv.org/pdf/2511.02712.pdf", "is_interesting": false}, "1075": {"title": "LLEXICORP: End-user Explainability of Convolutional Neural Networks", "authors": ["Vojt\\v{e}ch K\\r{u}r, Adam Bajger, Adam Kuku\\v{c}ka, Marek Hradil, V\\'it Musil, Tom\\'a\\v{s} Br\\'azdil"], "abstract": "arXiv:2511.02720v1 Announce Type: new \nConvolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.\n  To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.\n  We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02720", "pdf_url": "https://arxiv.org/pdf/2511.02720.pdf", "is_interesting": false}, "1076": {"title": "Dynamic Reflections: Probing Video Representations with Text Alignment", "authors": ["Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica P\\u{a}tr\\u{a}ucean, Maks Ovsjanikov"], "abstract": "arXiv:2511.02767v1 Announce Type: new \nThe alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02767", "pdf_url": "https://arxiv.org/pdf/2511.02767.pdf", "is_interesting": false}, "1077": {"title": "PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing", "authors": ["Antonio Oroz, Matthias Nie{\\ss}ner, Tobias Kirschstein"], "abstract": "arXiv:2511.02777v1 Announce Type: new \nWe present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.\n  Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02777", "pdf_url": "https://arxiv.org/pdf/2511.02777.pdf", "is_interesting": false}, "1078": {"title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation", "authors": ["Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang"], "abstract": "arXiv:2511.02778v1 Announce Type: new \nCode has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02778", "pdf_url": "https://arxiv.org/pdf/2511.02778.pdf", "is_interesting": false}, "1079": {"title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought", "authors": ["Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye"], "abstract": "arXiv:2511.02779v1 Announce Type: new \nWe propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02779", "pdf_url": "https://arxiv.org/pdf/2511.02779.pdf", "is_interesting": false}, "1080": {"title": "AI-Generated Image Detection: An Empirical Study and Future Research Directions", "authors": ["Nusrat Tasnim, Kutub Uddin, Khalid Mahmood Malik"], "abstract": "arXiv:2511.02791v1 Announce Type: new \nThe threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.", "categories": ["cs.CV", "cs.GT"], "abs_url": "https://arxiv.org/abs/2511.02791", "pdf_url": "https://arxiv.org/pdf/2511.02791.pdf", "is_interesting": false}, "1081": {"title": "PLUTO-4: Frontier Pathology Foundation Models", "authors": ["Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed"], "abstract": "arXiv:2511.02826v1 Announce Type: new \nFoundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02826", "pdf_url": "https://arxiv.org/pdf/2511.02826.pdf", "is_interesting": false}, "1082": {"title": "Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks", "authors": ["Dmitrii Pozdeev, Alexey Artemov, Ananta R. Bhattarai, Artem Sevastopolsky"], "abstract": "arXiv:2511.02830v1 Announce Type: new \nWe propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face/head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02830", "pdf_url": "https://arxiv.org/pdf/2511.02830.pdf", "is_interesting": false}, "1083": {"title": "Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models", "authors": ["Haoming Wang, Wei Gao"], "abstract": "arXiv:2511.01932v1 Announce Type: cross \nImage generation models are usually personalized in practical uses in order to better meet the individual users' heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \\textbf{FineXL}, towards \\textbf{Fine}-grained e\\textbf{X}plainability in natural \\textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56\\%, when different personalization scenarios are applied to multiple types of image generation models.", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.01932", "pdf_url": "https://arxiv.org/pdf/2511.01932.pdf", "is_interesting": false}, "1084": {"title": "Opto-Electronic Convolutional Neural Network Design Via Direct Kernel Optimization", "authors": ["Ali Almuallem, Harshana Weligampola, Abhiram Gnanasambandam, Wei Xu, Dilshan Godaliyadda, Hamid R. Sheikh, Stanley H. Chan, Qi Guo"], "abstract": "arXiv:2511.02065v1 Announce Type: cross \nOpto-electronic neural networks integrate optical front-ends with electronic back-ends to enable fast and energy-efficient vision. However, conventional end-to-end optimization of both the optical and electronic modules is limited by costly simulations and large parameter spaces. We introduce a two-stage strategy for designing opto-electronic convolutional neural networks (CNNs): first, train a standard electronic CNN, then realize the optical front-end implemented as a metasurface array through direct kernel optimization of its first convolutional layer. This approach reduces computational and memory demands by hundreds of times and improves training stability compared to end-to-end optimization. On monocular depth estimation, the proposed two-stage design achieves twice the accuracy of end-to-end training under the same training time and resource constraints.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02065", "pdf_url": "https://arxiv.org/pdf/2511.02065.pdf", "is_interesting": false}, "1085": {"title": "A Step Toward World Models: A Survey on Robotic Manipulation", "authors": ["Peng-Fei Zhang, Ying Cheng, Xiaofan Sun, Shijie Wang, Lei Zhu, Heng Tao Shen"], "abstract": "arXiv:2511.02097v1 Announce Type: cross \nAutonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond purely reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and enable prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, rather than directly imposing a fixed definition and limiting our scope to methods explicitly labeled as world models, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a real world model should possess. Building on this analysis, we aim to outline a roadmap for developing generalizable and practical world models for robotics.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02097", "pdf_url": "https://arxiv.org/pdf/2511.02097.pdf", "is_interesting": false}, "1086": {"title": "OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning", "authors": ["Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, David Keetae Park"], "abstract": "arXiv:2511.02205v1 Announce Type: cross \nMultimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02205", "pdf_url": "https://arxiv.org/pdf/2511.02205.pdf", "is_interesting": false}, "1087": {"title": "High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network", "authors": ["Abuobaida M. Khair, Wenjing Jiang, Yousuf Babiker M. Osman, Wenjun Xia, Xiaopeng Ma"], "abstract": "arXiv:2511.02212v1 Announce Type: cross \nThis study presents a hybrid deep learning framework, the Vision Transformer with Residual Feature Network (VRF-Net), for recovering high-resolution system matrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from downsampling and coil sensitivity variations. VRF-Net addresses these challenges by combining transformer-based global attention with residual convolutional refinement, enabling recovery of both large-scale structures and fine details. To reflect realistic MPI conditions, the system matrix is degraded using a dual-stage downsampling strategy. Training employed paired-image super-resolution on the public Open MPI dataset and a simulated dataset incorporating variable coil sensitivity profiles. For system matrix recovery on the Open MPI dataset, VRF-Net achieved nRMSE = 0.403, pSNR = 39.08 dB, and SSIM = 0.835 at 2x scaling, and maintained strong performance even at challenging scale 8x (pSNR = 31.06 dB, SSIM = 0.717). For the simulated dataset, VRF-Net achieved nRMSE = 4.44, pSNR = 28.52 dB, and SSIM = 0.771 at 2x scaling, with stable performance at higher scales. On average, it reduced nRMSE by 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over interpolation and CNN-based methods. In image reconstruction of Open MPI phantoms, VRF-Net further reduced reconstruction error to nRMSE = 1.79 at 2x scaling, while preserving structural fidelity (pSNR = 41.58 dB, SSIM = 0.960), outperforming existing methods. These findings demonstrate that VRF-Net enables sharper, artifact-free system matrix recovery and robust image reconstruction across multiple scales, offering a promising direction for future in vivo applications.", "categories": ["physics.med-ph", "cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.02212", "pdf_url": "https://arxiv.org/pdf/2511.02212.pdf", "is_interesting": false}, "1088": {"title": "3D Point Cloud Object Detection on Edge Devices for Split Computing", "authors": ["Taisuke Noguchi, Takuya Azumi"], "abstract": "arXiv:2511.02293v1 Announce Type: cross \nThe field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.", "categories": ["cs.DC", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02293", "pdf_url": "https://arxiv.org/pdf/2511.02293.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses the use of 3D point cloud data from LiDAR for object detection, which is a key task in autonomous driving. The focus on optimizing processing time and power consumption for edge devices is particularly relevant for autonomous vehicles that rely on efficient real-time data processing, thus directly linked to autonomous driving technology."}}, "1089": {"title": "MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization", "authors": ["Yalda Zafari, Hongyi Pan, Gorkem Durak, Ulas Bagci, Essam A. Rashed, Mohamed Mabrok"], "abstract": "arXiv:2511.02400v1 Announce Type: cross \nThe development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: https://github.com/Minds-R-Lab/MammoClean.", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02400", "pdf_url": "https://arxiv.org/pdf/2511.02400.pdf", "is_interesting": false}, "1090": {"title": "A Kullback-Leibler divergence method for input-system-state identification", "authors": ["Marios Impraimakis"], "abstract": "arXiv:2511.02426v1 Announce Type: cross \nThe capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "abs_url": "https://arxiv.org/abs/2511.02426", "pdf_url": "https://arxiv.org/pdf/2511.02426.pdf", "is_interesting": false}, "1091": {"title": "HAGI++: Head-Assisted Gaze Imputation and Generation", "authors": ["Chuhan Jiao, Zhiming Hu, Andreas Bulling"], "abstract": "arXiv:2511.02468v1 Announce Type: cross \nMobile eye tracking plays a vital role in capturing human visual attention across both real-world and extended reality (XR) environments, making it an essential tool for applications ranging from behavioural research to human-computer interaction. However, missing values due to blinks, pupil detection errors, or illumination changes pose significant challenges for further gaze data analysis. To address this challenge, we introduce HAGI++ - a multi-modal diffusion-based approach for gaze data imputation that, for the first time, uses the integrated head orientation sensors to exploit the inherent correlation between head and eye movements. HAGI++ employs a transformer-based diffusion model to learn cross-modal dependencies between eye and head representations and can be readily extended to incorporate additional body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D, and HOT3D datasets demonstrate that HAGI++ consistently outperforms conventional interpolation methods and deep learning-based time-series imputation baselines in gaze imputation. Furthermore, statistical analyses confirm that HAGI++ produces gaze velocity distributions that closely match actual human gaze behaviour, ensuring more realistic gaze imputations. Moreover, by incorporating wrist motion captured from commercial wearable devices, HAGI++ surpasses prior methods that rely on full-body motion capture in the extreme case of 100% missing gaze data (pure gaze generation). Our method paves the way for more complete and accurate eye gaze recordings in real-world settings and has significant potential for enhancing gaze-based analysis and interaction across various application domains.", "categories": ["cs.HC", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02468", "pdf_url": "https://arxiv.org/pdf/2511.02468.pdf", "is_interesting": false}, "1092": {"title": "SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration", "authors": ["Dan Bohus, Sean Andrist, Ann Paradiso, Nick Saw, Tim Schoonbeek, Maia Stiber"], "abstract": "arXiv:2511.02560v1 Announce Type: cross \nWe introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at https://github.com/microsoft/SigmaCollab.", "categories": ["cs.HC", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02560", "pdf_url": "https://arxiv.org/pdf/2511.02560.pdf", "is_interesting": false}, "1093": {"title": "Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback", "authors": ["Alix de Langlais, Benjamin Billot, Th\\'eo Aguilar Vidal, Marc-Olivier Gauci, Herv\\'e Delingette"], "abstract": "arXiv:2511.02576v1 Announce Type: cross \nDelineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02576", "pdf_url": "https://arxiv.org/pdf/2511.02576.pdf", "is_interesting": false}, "1094": {"title": "An unscented Kalman filter method for real time input-parameter-state estimation", "authors": ["Marios Impraimakis, Andrew W. Smyth"], "abstract": "arXiv:2511.02717v1 Announce Type: cross \nThe input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.SY", "eess.AS", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.02717", "pdf_url": "https://arxiv.org/pdf/2511.02717.pdf", "is_interesting": false}, "1095": {"title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System", "authors": ["Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu"], "abstract": "arXiv:2511.02832v1 Announce Type: cross \nLarge-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .", "categories": ["cs.RO", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02832", "pdf_url": "https://arxiv.org/pdf/2511.02832.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper discusses a humanoid robotics system for teleoperation and data collection, focusing on motion capture and visuomotor policies for humanoid robots. It does not address autonomous driving or related tasks such as vehicle perception, prediction, decision-making, or control."}}, "1096": {"title": "Robust Identity Perceptual Watermark Against Deepfake Face Swapping", "authors": ["Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang"], "abstract": "arXiv:2311.01357v3 Announce Type: replace \nNotwithstanding offering convenience and entertainment to society, Deepfake face swapping has caused critical privacy issues with the rapid development of deep generative models. Due to imperceptible artifacts in high-quality synthetic images, passive detection models against face swapping in recent years usually suffer performance damping regarding the generalizability issue in cross-domain scenarios. Therefore, several studies have been attempted to proactively protect the original images against malicious manipulations by inserting invisible signals in advance. However, existing proactive defense approaches demonstrate unsatisfactory results with respect to visual quality, detection accuracy, and source tracing ability. In this study, to fulfill the research gap, we propose a robust identity perceptual watermarking framework that concurrently performs detection and source tracing against Deepfake face swapping proactively. We innovatively assign identity semantics regarding the image contents to the watermarks and devise an unpredictable and nonreversible chaotic encryption system to ensure watermark confidentiality. The watermarks are robustly encoded and recovered by jointly training an encoder-decoder framework along with adversarial image manipulations. For a suspect image, falsification is accomplished by justifying the consistency between the content-matched identity perceptual watermark and the recovered robust watermark, without requiring the ground-truth. Moreover, source tracing can be accomplished based on the identity semantics that the recovered watermark carries. Extensive experiments demonstrate state-of-the-art detection and source tracing performance against Deepfake face swapping with promising watermark robustness for both cross-dataset and cross-manipulation settings.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2311.01357", "pdf_url": "https://arxiv.org/pdf/2311.01357.pdf", "is_interesting": false}, "1097": {"title": "Training Convolutional Neural Networks with the Forward-Forward algorithm", "authors": ["Riccardo Scodellaro, Ajinkya Kulkarni, Frauke Alves, Matthias Schr\\\"oter"], "abstract": "arXiv:2312.14924v4 Announce Type: replace \nRecent successes in image analysis with deep neural networks are achieved almost exclusively with Convolutional Neural Networks (CNNs), typically trained using the backpropagation (BP) algorithm. In a 2022 preprint, Geoffrey Hinton proposed the Forward-Forward (FF) algorithm as a biologically inspired alternative, where positive and negative examples are jointly presented to the network and training is guided by a locally defined goodness function. Here, we extend the FF paradigm to CNNs. We introduce two spatially extended labeling strategies, based on Fourier patterns and morphological transformations, that enable convolutional layers to access label information across all spatial positions. On CIFAR10, we show that deeper FF-trained CNNs can be optimized successfully and that morphology-based labels prevent shortcut solutions on dataset with more complex and fine features. On CIFAR100, carefully designed label sets scale effectively to 100 classes. Class Activation Maps reveal that FF-trained CNNs learn meaningful and complementary features across layers. Together, these results demonstrate that FF training is feasible beyond fully connected networks, provide new insights into its learning dynamics and stability, and highlight its potential for neuromorphic computing and biologically inspired learning.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2312.14924", "pdf_url": "https://arxiv.org/pdf/2312.14924.pdf", "is_interesting": false}, "1098": {"title": "Deep Fourier-embedded Network for RGB and Thermal Salient Object Detection", "authors": ["Pengfei Lyu, Xiaosheng Yu, Pak-Hei Yeung, Chengdong Wu, Jagath C. Rajapakse"], "abstract": "arXiv:2411.18409v3 Announce Type: replace \nThe rapid development of deep learning has significantly improved salient object detection (SOD) combining both RGB and thermal (RGB-T) images. However, existing Transformer-based RGB-T SOD models with quadratic complexity are memory-intensive, limiting their application in high-resolution bimodal feature fusion. To overcome this limitation, we propose a purely Fourier Transform-based model, namely Deep Fourier-embedded Network (FreqSal), for accurate RGB-T SOD. Specifically, we leverage the efficiency of Fast Fourier Transform with linear complexity to design three key components: (1) To fuse RGB and thermal modalities, we propose Modal-coordinated Perception Attention, which aligns and enhances bimodal Fourier representation in multiple dimensions; (2) To clarify object edges and suppress noise, we design Frequency-decomposed Edge-aware Block, which deeply decomposes and filters Fourier components of low-level features; (3) To accurately decode features, we propose Fourier Residual Channel Attention Block, which prioritizes high-frequency information while aligning channel-wise global relationships. Additionally, even when converged, existing deep learning-based SOD models' predictions still exhibit frequency gaps relative to ground-truth. To address this problem, we propose Co-focus Frequency Loss, which dynamically weights hard frequencies during edge frequency reconstruction by cross-referencing bimodal edge information in the Fourier domain. Extensive experiments on ten bimodal SOD benchmark datasets demonstrate that FreqSal outperforms twenty-nine existing state-of-the-art bimodal SOD models. Comprehensive ablation studies further validate the value and effectiveness of our newly proposed components. The code is available at https://github.com/JoshuaLPF/FreqSal.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2411.18409", "pdf_url": "https://arxiv.org/pdf/2411.18409.pdf", "is_interesting": false}, "1099": {"title": "Visual Program Distillation with Template-Based Augmentation", "authors": ["Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem"], "abstract": "arXiv:2412.08564v4 Announce Type: replace \nAdapting visual programming or prompting large language models (LLMs) to generate executable code for visual tasks like visual question answering (VQA) for specialized tasks or domains remains challenging due to high annotation and inference costs. We propose a low-cost visual program distillation method that can be used for models with at most 1 billion parameters and requires no human-generated program annotations. We achieve this through synthetic data augmentation based on decoupling programs into higher-level skills, called templates, and their corresponding arguments. Experimental results show that, with a relatively small amount of question/answer data, small language models can generate high-quality specialized visual programs with the added benefit of much faster inference", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2412.08564", "pdf_url": "https://arxiv.org/pdf/2412.08564.pdf", "is_interesting": false}, "1100": {"title": "Image Super-Resolution with Guarantees via Conformalized Generative Models", "authors": ["Eduardo Adame, Daniel Csillag, Guilherme Tegoni Goedert"], "abstract": "arXiv:2502.09664v3 Announce Type: replace \nThe increasing use of generative ML foundation models for image restoration tasks such as super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a 'confidence mask' capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.", "categories": ["cs.CV", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2502.09664", "pdf_url": "https://arxiv.org/pdf/2502.09664.pdf", "is_interesting": false}, "1101": {"title": "Mobile Robotic Multi-View Photometric Stereo", "authors": ["Suryansh Kumar"], "abstract": "arXiv:2502.10842v2 Announce Type: replace \nMulti-View Photometric Stereo (MVPS) is a popular method for fine-detailed 3D acquisition of an object from images. Despite its outstanding results on diverse material objects, a typical MVPS experimental setup requires a well-calibrated light source and a monocular camera installed on an immovable base. This restricts the use of MVPS on a movable platform, limiting us from taking MVPS benefits in 3D acquisition for mobile robotics applications. To this end, we introduce a new mobile robotic system for MVPS. While the proposed system brings advantages, it introduces additional algorithmic challenges. Addressing them, in this paper, we further propose an incremental approach for mobile robotic MVPS. Our approach leverages a supervised learning setup to predict per-view surface normal, object depth, and per-pixel uncertainty in model-predicted results. A refined depth map per view is obtained by solving an MVPS-driven optimization problem proposed in this paper. Later, we fuse the refined depth map while tracking the camera pose w.r.t the reference frame to recover globally consistent object 3D geometry. Experimental results show the advantages of our robotic system and algorithm, featuring the local high-frequency surface detail recovery with globally consistent object shape. Our work is beyond any MVPS system yet presented, providing encouraging results on objects with unknown reflectance properties using fewer frames without a tiring calibration and installation process, enabling computationally efficient robotic automation approach to photogrammetry. The proposed approach is nearly 100 times computationally faster than the state-of-the-art MVPS methods such as [1, 2] while maintaining the similar results when tested on subjects taken from the benchmark DiLiGenT MV dataset [3].", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2502.10842", "pdf_url": "https://arxiv.org/pdf/2502.10842.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0", "reason": "The paper discusses a mobile robotic system for Multi-View Photometric Stereo (MVPS) applied to 3D acquisition of objects, with a focus on improving efficiency and reducing the need for calibration. While it involves mobile robotics and 3D geometry recovery, it does not directly discuss autonomous driving tasks such as perception, prediction, decision-making, or control."}}, "1102": {"title": "Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms", "authors": ["Kangning Cui, Rongkun Zhu, Manqi Wang, Wei Tang, Gregory D. Larsen, Victor P. Pauca, Sarra Alqahtani, Fan Yang, David Segurado, David Lutz, Jean-Michel Morel, Miles R. Silman"], "abstract": "arXiv:2502.13023v2 Announce Type: replace \nPalms are ecologically and economically indicators of tropical forest health, biodiversity, and human impact that support local economies and global forest product supply chains. While palm detection in plantations is well-studied, efforts to map naturally occurring palms in dense forests remain limited by overlapping crowns, uneven shading, and heterogeneous landscapes. We develop PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline for detecting and localizing palms in dense tropical forests using large orthomosaic images. Orthomosaics are created from thousands of aerial images and spanning several to hundreds of gigabytes. Our contributions are threefold. First, we construct a large UAV-derived orthomosaic dataset collected across 21 ecologically diverse sites in western Ecuador, annotated with 8,830 bounding boxes and 5,026 palm center points. Second, we evaluate multiple state-of-the-art object detectors based on efficiency and performance, integrating zero-shot SAM 2 as the segmentation backbone, and refining the results for precise geographic mapping. Third, we apply calibration methods to align confidence scores with IoU and explore saliency maps for feature explainability. Though optimized for palms, PRISM is adaptable for identifying other natural objects, such as eastern white pines. Future work will explore transfer learning for lower-resolution datasets (0.5 to 1m).", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.13023", "pdf_url": "https://arxiv.org/pdf/2502.13023.pdf", "is_interesting": false}, "1103": {"title": "Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment", "authors": ["Zhihao Zhan, Wang Pang, Xiang Zhu, Yechao Bai"], "abstract": "arXiv:2503.03355v5 Announce Type: replace \nIn this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.", "categories": ["cs.CV", "cs.LG", "eess.IV"], "abs_url": "https://arxiv.org/abs/2503.03355", "pdf_url": "https://arxiv.org/pdf/2503.03355.pdf", "is_interesting": false}, "1104": {"title": "Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal Adverse Weather Removal", "authors": ["Rongxin Liao, Feng Li, Yanyan Wei, Zenglin Shi, Le Zhang, Huihui Bai, Meng Wang"], "abstract": "arXiv:2503.09013v2 Announce Type: replace \nUniversal adverse weather removal (UAWR) seeks to address various weather degradations within a unified framework. Recent methods are inspired by prompt learning using pre-trained vision-language models (e.g., CLIP), leveraging degradation-aware prompts to facilitate weather-free image restoration, yielding significant improvements. In this work, we propose CyclicPrompt, an innovative cyclic prompt approach designed to enhance the effectiveness, adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key components: 1) a composite context prompt that integrates weather-related information and context-aware representations into the network to guide restoration. This prompt differs from previous methods by marrying learnable input-conditional vectors with weather-specific knowledge, thereby improving adaptability across various degradations. 2) The erase-and-paste mechanism, after the initial guided restoration, substitutes weather-specific knowledge with constrained restoration priors, inducing high-quality weather-free concepts into the composite prompt to further fine-tune the restoration process. Therefore, we can form a cyclic \"Prompt-Restore-Prompt\" pipeline that adeptly harnesses weather-specific knowledge, textual contexts, and reliable textures. Extensive experiments on synthetic and real-world datasets validate the superior performance of CyclicPrompt. The code is available at: https://github.com/RongxinL/CyclicPrompt.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2503.09013", "pdf_url": "https://arxiv.org/pdf/2503.09013.pdf", "is_interesting": false}, "1105": {"title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing", "authors": ["Fengxiang Wang, Yulin Wang, Mingshuo Chen, Haiyan Zhao, Yangang Sun, Shuo Wang, Hongzhen Wang, Di Wang, Long Lan, Wenjing Yang, Jing Zhang"], "abstract": "arXiv:2503.10392v2 Announce Type: replace \nRecent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models will be released at https://github.com/MiliLab/RoMA.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2503.10392", "pdf_url": "https://arxiv.org/pdf/2503.10392.pdf", "is_interesting": false}, "1106": {"title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation", "authors": ["Ziming Wei, Bingqian Lin, Yunshuang Nie, Jiaqi Chen, Shikui Ma, Hang Xu, Xiaodan Liang"], "abstract": "arXiv:2503.18065v3 Announce Type: replace \nData scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "abs_url": "https://arxiv.org/abs/2503.18065", "pdf_url": "https://arxiv.org/pdf/2503.18065.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on Vision-Language Navigation (VLN) and a data augmentation technique for improving generalization in navigation tasks. It does not discuss autonomous driving, vehicle perception, or any direct or indirect applications to autonomous driving systems."}}, "1107": {"title": "The Coralscapes Dataset: Semantic Scene Understanding in Coral Reefs", "authors": ["Jonathan Sauder, Viktor Domazetoski, Guilhem Banc-Prandi, Gabriela Perna, Anders Meibom, Devis Tuia"], "abstract": "arXiv:2503.20000v2 Announce Type: replace \nCoral reefs are declining worldwide due to climate change and local stressors. To inform effective conservation or restoration, monitoring at the highest possible spatial and temporal resolution is necessary. Conventional coral reef surveying methods are limited in scalability due to their reliance on expert labor time, motivating the use of computer vision tools to automate the identification and abundance estimation of live corals from images. However, the design and evaluation of such tools has been impeded by the lack of large high quality datasets. We release the Coralscapes dataset, the first general-purpose dense semantic segmentation dataset for coral reefs, covering 2075 images, 39 benthic classes, and 174k segmentation masks annotated by experts. Coralscapes has a similar scope and the same structure as the widely used Cityscapes dataset for urban scene segmentation, allowing benchmarking of semantic segmentation models in a new challenging domain which requires expert knowledge to annotate. We benchmark a wide range of semantic segmentation models, and find that transfer learning from Coralscapes to existing smaller datasets consistently leads to state-of-the-art performance. Coralscapes will catalyze research on efficient, scalable, and standardized coral reef surveying methods based on computer vision, and holds the potential to streamline the development of underwater ecological robotics.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.20000", "pdf_url": "https://arxiv.org/pdf/2503.20000.pdf", "is_interesting": false}, "1108": {"title": "3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting", "authors": ["Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong"], "abstract": "arXiv:2504.01619v2 Announce Type: replace \nRecent advancements in text-to-3D generation have shown remarkable results by leveraging 3D priors in combination with 2D diffusion. However, previous methods utilize 3D priors that lack detailed and complex structural information, limiting them to generating simple objects and presenting challenges for creating intricate structures such as bonsai. In this paper, we propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with complex structures. Technically, we first design a trainable 3D space colonization algorithm to produce bonsai structures, which are then enhanced through random sampling and point cloud augmentation to serve as the 3D Gaussian priors. We introduce two bonsai generation pipelines with distinct structural levels: fine structure conditioned generation, which initializes 3D Gaussians using a 3D structure prior to produce detailed and complex bonsai, and coarse structure conditioned generation, which employs a multi-view structure consistency module to align 2D and 3D structures. Moreover, we have compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental results demonstrate that 3DBonsai significantly outperforms existing methods, providing a new benchmark for structure-aware 3D bonsai generation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.01619", "pdf_url": "https://arxiv.org/pdf/2504.01619.pdf", "is_interesting": false}, "1109": {"title": "FractalForensics: Proactive Deepfake Detection and Localization via Fractal Watermarks", "authors": ["Tianyi Wang, Harry Cheng, Ming-Hui Liu, Mohan Kankanhalli"], "abstract": "arXiv:2504.09451v2 Announce Type: replace \nProactive Deepfake detection via robust watermarks has seen interest ever since passive Deepfake detectors encountered challenges in identifying high-quality synthetic images. However, while demonstrating reasonable detection performance, they lack localization functionality and explainability in detection results. Additionally, the unstable robustness of watermarks can significantly affect the detection performance. In this study, we propose novel fractal watermarks for proactive Deepfake detection and localization, namely FractalForensics. Benefiting from the characteristics of fractals, we devise a parameter-driven watermark generation pipeline that derives fractal-based watermarks and performs one-way encryption of the selected parameters. Subsequently, we propose a semi-fragile watermarking framework for watermark embedding and recovery, trained to be robust against benign image processing operations and fragile when facing Deepfake manipulations in a black-box setting. Moreover, we introduce an entry-to-patch strategy that implicitly embeds the watermark matrix entries into image patches at corresponding positions, achieving localization of Deepfake manipulations. Extensive experiments demonstrate satisfactory robustness and fragility of our approach against common image processing operations and Deepfake manipulations, outperforming state-of-the-art semi-fragile watermarking algorithms and passive detectors for Deepfake detection. Furthermore, by highlighting the areas manipulated, our method provides explainability for the proactive Deepfake detection results.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.09451", "pdf_url": "https://arxiv.org/pdf/2504.09451.pdf", "is_interesting": false}, "1110": {"title": "Breaking Down Monocular Ambiguity: Exploiting Temporal Evolution for 3D Lane Detection", "authors": ["Huan Zheng, Wencheng Han, Tianyi Yan, Cheng-zhong Xu, Jianbing Shen"], "abstract": "arXiv:2504.20525v2 Announce Type: replace \nMonocular 3D lane detection aims to estimate the 3D position of lanes from frontal-view (FV) images. However, existing methods are fundamentally constrained by the inherent ambiguity of single-frame input, which leads to inaccurate geometric predictions and poor lane integrity, especially for distant lanes.To overcome this, we propose to unlock the rich information embedded in the temporal evolution of the scene as the vehicle moves. Our proposed Geometry-aware Temporal Aggregation Network (GTA-Net) systematically leverages the temporal information from complementary perspectives.First, Temporal Geometry Enhancement Module (TGEM) learns geometric consistency across consecutive frames, effectively recovering depth information from motion to build a reliable 3D scene representation.Second, to enhance lane integrity, Temporal Instance-aware Query Generation (TIQG) module aggregates instance cues from past and present frames. Crucially, for lanes that are ambiguous in the current view, TIQG innovatively synthesizes a pseudo future perspective to generate queries that reveal lanes which would otherwise be missed.The experiments demonstrate that GTA-Net achieves new SoTA results, significantly outperforming existing monocular 3D lane detection solutions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2504.20525", "pdf_url": "https://arxiv.org/pdf/2504.20525.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "3D\u611f\u77e5 / \u8f66\u9053\u7ebf\u68c0\u6d4b / \u65f6\u5e8f\u5efa\u6a21", "reason": "The paper focuses on monocular 3D lane detection, a core perception task in autonomous driving. It introduces a temporal aggregation network (GTA-Net) to enhance geometric accuracy and lane integrity across consecutive frames, addressing challenges directly tied to vehicle perception and scene understanding in autonomous driving systems."}}, "1111": {"title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "authors": ["Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang"], "abstract": "arXiv:2505.21375v2 Announce Type: replace \nUltra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: Background Token Pruning and Anchored Token Selection, to reduce the memory footprint while preserving key semantics.Integrating these techniques, we introduce GeoLLaVA-8K, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.21375", "pdf_url": "https://arxiv.org/pdf/2505.21375.pdf", "is_interesting": false}, "1112": {"title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data", "authors": ["Fengxiang Wang, Mingshuo Chen, Xuming He, Yueying Li, YiFan Zhang, Feng Liu, Zijie Guo, Zhenghao Hu, Jiong Wang, Jingyi Xu, Zhangrui Li, Fenghua Ling, Ben Fei, Weijia Li, Long Lan, Wenjing Yang, Wenlong Zhang, Lei Bai"], "abstract": "arXiv:2505.23522v2 Announce Type: replace \nExisting benchmarks for multimodal learning in Earth science offer limited, siloed coverage of Earth's spheres and their cross-sphere interactions, typically restricting evaluation to the human-activity sphere of atmosphere and to at most 16 tasks. These limitations: \\textit{narrow-source heterogeneity (single/few data sources), constrained scientific granularity, and limited-sphere extensibility}. Therefore, we introduce \\textbf{OmniEarth-Bench}, the first multimodal benchmark that systematically spans all six spheres: atmosphere, lithosphere, oceanosphere, cryosphere, biosphere, and human-activity sphere, and cross-spheres. Built with a scalable, modular-topology data inference framework and native multi-observation sources and expert-in-the-loop curation, OmniEarth-Bench produces 29,855 standardized, expert-curated annotations. All annotations are organized into a four-level hierarchy (Sphere, Scenario, Ability, Task), encompassing 109 expert-curated evaluation tasks. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\\% accuracy, revealing systematic gaps in Earth-system cognitive ability. The dataset and evaluation code were released at OmniEarth-Bench (https://anonymous.4open.science/r/OmniEarth-Bench-B1BD).", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.23522", "pdf_url": "https://arxiv.org/pdf/2505.23522.pdf", "is_interesting": false}, "1113": {"title": "Towards Predicting Any Human Trajectory In Context", "authors": ["Ryo Fujii, Hideo Saito, Ryo Hachiuma"], "abstract": "arXiv:2506.00871v3 Announce Type: replace \nPredicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "abs_url": "https://arxiv.org/abs/2506.00871", "pdf_url": "https://arxiv.org/pdf/2506.00871.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u8f68\u8ff9\u9884\u6d4b", "reason": "The paper addresses pedestrian trajectory prediction, a key task for autonomous systems to ensure safe interaction with pedestrians. The proposed approach aims to improve trajectory prediction by adapting to various scenarios, which directly relates to the autonomous driving field, specifically in trajectory prediction for human agents in driving environments."}}, "1114": {"title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "authors": ["Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas"], "abstract": "arXiv:2506.09024v2 Announce Type: replace \nSafe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard the training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping the training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code: https://github.com/FelixWag/DIsoN", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.09024", "pdf_url": "https://arxiv.org/pdf/2506.09024.pdf", "is_interesting": false}, "1115": {"title": "GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field", "authors": ["Chengrui Zhang, Maizhen Ning, Tianyi Liu, Zihao Zhou, Jie Sun, Qiufeng Wang, Kaizhu Huang"], "abstract": "arXiv:2506.13492v2 Announce Type: replace \nPlane Geometry Diagram Synthesis has been a crucial task in computer graphics, with applications ranging from educational tools to AI-driven mathematical reasoning. Traditionally, we rely on manual tools (e.g., Matplotlib and GeoGebra) to generate precise diagrams, but this usually requires huge, complicated calculations. Recently, researchers start to work on model-based methods (e.g., Stable Diffusion and GPT5) to automatically generate diagrams, saving operational cost but usually suffering from limited realism and insufficient accuracy. In this paper, we propose a novel framework GeoSDF, to automatically generate diagrams efficiently and accurately with Signed Distance Field (SDF). Specifically, we first represent geometric elements (e.g., points, segments, and circles) in the SDF, then construct a series of constraint functions to represent geometric relationships. Next, we optimize those constructed constraint functions to get an optimized field of both elements and constraints. Finally, by rendering the optimized field, we can obtain the synthesized diagram. In our GeoSDF, we define a symbolic language to represent geometric elements and constraints, and our synthesized geometry diagrams can be self-verified in the SDF, ensuring both mathematical accuracy and visual plausibility. In experiments, through both qualitative and quantitative analysis, GeoSDF synthesized both normal high-school level and IMO-level geometry diagrams. We achieve 88.67\\% synthesis accuracy by human evaluation in the IMO problem set. Furthermore, we obtain a very high accuracy of solving geometry problems (over 95\\% while the current SOTA accuracy is around 75%) by leveraging our self-verification property. All of these demonstrate the advantage of GeoSDF, paving the way for more sophisticated, accurate, and flexible generation of geometric diagrams for a wide array of applications.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.13492", "pdf_url": "https://arxiv.org/pdf/2506.13492.pdf", "is_interesting": false}, "1116": {"title": "MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image Generation", "authors": ["Qingyue Jiao, Yongcan Tang, Jun Zhuang, Jason Cong, Yiyu Shi"], "abstract": "arXiv:2506.21015v2 Announce Type: replace \nMachine learning-assisted diagnosis shows promise, yet medical imaging datasets are often scarce, imbalanced, and constrained by privacy, making data augmentation essential. Classical generative models typically demand extensive computational and sample resources. Quantum computing offers a promising alternative, but existing quantum-based image generation methods remain limited in scale and often face barren plateaus. We present MediQ-GAN, a quantum-inspired GAN with prototype-guided skip connections and a dual-stream generator that fuses classical and quantum-inspired branches. Its variational quantum circuits inherently preserve full-rank mappings, avoid rank collapse, and are theory-guided to balance expressivity with trainability. Beyond generation quality, we provide the first latent-geometry and rank-based analysis of quantum-inspired GANs, offering theoretical insight into their performance. Across three medical imaging datasets, MediQ-GAN outperforms state-of-the-art GANs and diffusion models. While validated on IBM hardware for robustness, our contribution is hardware-agnostic, offering a scalable and data-efficient framework for medical image generation and augmentation.", "categories": ["cs.CV", "cs.LG", "quant-ph"], "abs_url": "https://arxiv.org/abs/2506.21015", "pdf_url": "https://arxiv.org/pdf/2506.21015.pdf", "is_interesting": false}, "1117": {"title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "authors": ["Hassan Baker, Matthew S. Emigh, Austin J. Brockmeier"], "abstract": "arXiv:2506.22505v2 Announce Type: replace \nAs a computer vision task, automatic object segmentation remains challenging in specialized image domains without massive labeled data, such as synthetic aperture sonar images, remote sensing, biomedical imaging, etc. In any domain, obtaining pixel-wise segmentation masks is expensive. In this work, we propose a method for training a masking network to perform binary object segmentation using weak supervision in the form of image-wise presence or absence of an object of interest, which provides less information but may be obtained more quickly from manual or automatic labeling. A key step in our method is that the segmented objects can be placed into background-only images to create realistic images of the objects with counterfactual backgrounds. To create a contrast between the original and counterfactual background images, we propose to first cluster the background-only images and then, during learning, create counterfactual images that blend objects segmented from their original source backgrounds to backgrounds chosen from a targeted cluster. One term in the training loss is the divergence between these counterfactual images and the real object images with backgrounds of the target cluster. The other term is a supervised loss for background-only images. While an adversarial critic could provide the divergence, we use sample-based divergences. We conduct experiments on side-scan and synthetic aperture sonar in which our approach succeeds compared to previous unsupervised segmentation baselines that were only tested on natural images. Furthermore, to show generality we extend our experiments to natural images, obtaining reasonable performance with our method that avoids pretrained networks, generative networks, and adversarial critics. The code for this work can be found at \\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.22505", "pdf_url": "https://arxiv.org/pdf/2506.22505.pdf", "is_interesting": false}, "1118": {"title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "authors": ["Siyue Yao, Mingjie Sun, Eng Gee Lim, Ran Yi, Baojiang Zhong, Moncef Gabbouj"], "abstract": "arXiv:2507.09915v2 Announce Type: replace \nThe scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide \"crucial information\" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code is publicly available at https://github.com/JJessicaYao/Crucial-diff.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.09915", "pdf_url": "https://arxiv.org/pdf/2507.09915.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.4, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper addresses the issue of data scarcity in various domains, including autonomous driving, and proposes a generative model to synthesize crucial samples for training, which could be applied to tasks like object detection and segmentation in autonomous driving systems. While it is not exclusively focused on autonomous driving, the method has clear potential for improving training datasets for AD-related visual tasks."}}, "1119": {"title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "authors": ["Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Kaichen Zhou, Paul Pu Liang, Shijian Lu, Fangneng Zhan"], "abstract": "arXiv:2507.14501v4 Announce Type: replace \n3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.14501", "pdf_url": "https://arxiv.org/pdf/2507.14501.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.5, "subfield": "3D\u8868\u5f81 / SLAM / \u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper surveys feed-forward 3D reconstruction and view synthesis methods such as NeRF and 3D Gaussian Splatting, which are general 3D vision techniques. While not directly focused on autonomous driving, these methods can be applied to vehicle perception, SLAM, and 3D scene understanding tasks in autonomous driving systems."}}, "1120": {"title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "authors": ["Zesen Zhong, Duomin Zhang, Yijia Li"], "abstract": "arXiv:2507.14809v2 Announce Type: replace \nPredicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.", "categories": ["cs.CV", "cs.MM", "cs.RO"], "abs_url": "https://arxiv.org/abs/2507.14809", "pdf_url": "https://arxiv.org/pdf/2507.14809.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u8f68\u8ff9\u9884\u6d4b / \u591a\u6a21\u6001\u878d\u5408", "reason": "The paper discusses a novel approach to predicting future motion trajectories using a multimodal framework that combines visual and textual inputs, which is relevant to autonomous driving systems where motion prediction is critical for safe decision-making. The method focuses on forecasting future frames, which can be applied to autonomous vehicles for motion trajectory prediction and decision-making, particularly in dynamic environments."}}, "1121": {"title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "authors": ["Guoxuan Xia, Harleen Hanspal, Petru-Daniel Tudosiu, Shifeng Zhang, Sarah Parisot"], "abstract": "arXiv:2507.15724v2 Announce Type: replace \nEnabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g. edge maps, poses. Although this task has seen impressive improvements in recent times, a focus on rapidly producing stronger models has come at the cost of detailed and fair scientific comparison. Differing training data, model architectures and generation paradigms make it difficult to disentangle the factors contributing to performance. Meanwhile, the motivations and nuances of certain approaches become lost in the literature. In this work, we aim to provide clear takeaways across generation paradigms for practitioners wishing to develop transformer-based systems for spatially-controlled generation, clarifying the literature and addressing knowledge gaps. We perform controlled experiments on ImageNet across diffusion-based/flow-based and autoregressive (AR) models. First, we establish control token prefilling as a simple, general and performant baseline approach for transformers. We then investigate previously underexplored sampling time enhancements, showing that extending classifier-free guidance to control, as well as softmax truncation, have a strong impact on control-generation consistency. Finally, we re-clarify the motivation of adapter-based approaches, demonstrating that they mitigate \"forgetting\" and maintain generation quality when trained on limited downstream data, but underperform full training in terms of generation-control consistency.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.15724", "pdf_url": "https://arxiv.org/pdf/2507.15724.pdf", "is_interesting": false}, "1122": {"title": "Label tree semantic losses for rich multi-class medical image segmentation", "authors": ["Junwen Wang, Oscar MacCormac, William Rochford, Aaron Kujawa, Jonathan Shapey, Tom Vercauteren"], "abstract": "arXiv:2507.15777v2 Announce Type: replace \nRich and accurate medical image segmentation is poised to underpin the next generation of AI-defined clinical practice by delineating critical anatomy for pre-operative planning, guiding real-time intra-operative navigation, and supporting precise post-operative assessment. However, commonly used learning methods for medical and surgical imaging segmentation tasks penalise all errors equivalently and thus fail to exploit any inter-class semantics in the labels space. This becomes particularly problematic as the cardinality and richness of labels increases to include subtly different classes. In this work, we propose two tree-based semantic loss functions which take advantage of a hierarchical organisation of the labels. We further incorporate our losses in a recently proposed approach for training with sparse, background-free annotations to extend the applicability of our proposed losses. Extensive experiments are reported on two medical and surgical image segmentation tasks, namely head MRI for whole brain parcellation (WBP) with full supervision and neurosurgical hyperspectral imaging (HSI) for scene understanding with sparse annotations. Results demonstrate that our proposed method reaches state-of-the-art performance in both cases.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2507.15777", "pdf_url": "https://arxiv.org/pdf/2507.15777.pdf", "is_interesting": false}, "1123": {"title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras", "authors": ["Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau"], "abstract": "arXiv:2507.17664v2 Announce Type: replace \nEvent cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, we provide over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2507.17664", "pdf_url": "https://arxiv.org/pdf/2507.17664.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u591a\u6a21\u6001\u878d\u5408 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper introduces a large-scale benchmark and a new framework for language-driven object grounding using event cameras, which are highly relevant for real-time dynamic perception in autonomous driving systems. The focus on event-based perception, multi-modal fusion, and real-world driving data suggests strong applicability to the field of autonomous driving, especially in the context of sensor fusion and dynamic scene understanding."}}, "1124": {"title": "WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions", "authors": ["Quan Chen, Xiong Yang, Bolun Zheng, Rongfeng Lu, Xiaokai Yang, Qianyu Zhang, Yu Liu, Xiaofei Zhou"], "abstract": "arXiv:2508.12250v2 Announce Type: replace \nSalient object detection (SOD) in complex environments remains a challenging research topic. Most existing methods perform well in natural scenes with negligible noise, and tend to leverage multi-modal information (e.g., depth and infrared) to enhance accuracy. However, few studies are concerned with the damage of weather noise on SOD performance due to the lack of dataset with pixel-wise annotations. To bridge this gap, this paper introduces a novel Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of 14,945 RGB images with diverse weather noise, along with the corresponding ground truth annotations and weather labels. To verify algorithm generalization, WXSOD contains two test sets, i.e., a synthesized test set and a real test set. The former is generated by adding weather noise to clean images, while the latter contains real-world weather noise. Based on WXSOD, we propose an efficient baseline, termed Weather-aware Feature Aggregation Network (WFANet), which adopts a fully supervised two-branch architecture. Specifically, the weather prediction branch mines weather-related deep features, while the saliency detection branch fuses semantic features extracted from the backbone with weather features for SOD. Comprehensive comparisons against 17 SOD methods shows that our WFANet achieves superior performance on WXSOD. The code and benchmark results will be made publicly available at https://github.com/C-water/WXSOD", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2508.12250", "pdf_url": "https://arxiv.org/pdf/2508.12250.pdf", "is_interesting": false}, "1125": {"title": "CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution", "authors": ["Yulin Tong, Fengzong Zhang, Haiqin Cheng"], "abstract": "arXiv:2509.09163v2 Announce Type: replace \nHyperspectral remote sensing technology has significant application value in fields such as forestry ecology and precision agriculture, while also putting forward higher requirements for fine ground object classification. However, although hyperspectral images are rich in spectral information and can improve recognition accuracy, they tend to cause prominent feature redundancy due to their numerous bands, high dimensionality, and spectral mixing characteristics. To address this, this study used hyperspectral images from the ZY1F satellite as a data source and selected Yugan County, Shangrao City, Jiangxi Province as the research area to perform ground object classification research. A classification framework named CWSSNet was proposed, which integrates 3D spectral-spatial features and wavelet convolution. This framework integrates multimodal information us-ing a multiscale convolutional attention module and breaks through the classification performance bottleneck of traditional methods by introducing multi-band decomposition and convolution operations in the wavelet domain. The experiments showed that CWSSNet achieved 74.50\\%, 82.73\\%, and 84.94\\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and mean F1-score (mF1) respectively in Yugan County. It also obtained the highest Intersection over Union (IoU) in the classifica-tion of water bodies, vegetation, and bare land, demonstrating good robustness. Additionally, when the training set proportion was 70\\%, the increase in training time was limited, and the classification effect was close to the optimal level, indicating that the model maintains reliable performance under small-sample training conditions.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.09163", "pdf_url": "https://arxiv.org/pdf/2509.09163.pdf", "is_interesting": false}, "1126": {"title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "authors": ["Tong Yulin, Liang Xuechen"], "abstract": "arXiv:2509.13361v2 Announce Type: replace \nExpressway traffic congestion severely reduces travel efficiency and hinders regional connectivity. Existing \"detection-prediction\" systems have critical flaws: low vehicle perception accuracy under occlusion and loss of long-sequence dependencies in congestion forecasting. This study proposes an integrated technical framework to resolve these issues.For traffic flow perception, two baseline algorithms were optimized. Traditional YOLOv11 was upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort was improved by fusing Mahalanobis (motion) and cosine (appearance) distances. Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\% mAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss rate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT) with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km high-density scenarios), speed and density showed a strong negative correlation (r=-0.97), conforming to traffic flow theory. For congestion warning, a GRU-Attention model was built to capture congestion precursors. Trained 300 epochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9 percentage points higher than traditional GRU). In 10-minute advance warnings for 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an independent video showed 95\\% warning accuracy, over 90\\% spatial overlap of congestion points, and stable performance in high-flow ($>$5 vehicles/second) scenarios.This framework provides quantitative support for expressway congestion control, with promising intelligent transportation applications.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.13361", "pdf_url": "https://arxiv.org/pdf/2509.13361.pdf", "is_interesting": false}, "1127": {"title": "FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World", "authors": ["Zengli Luo, Canlong Zhang, Xiaochun Lu, Zhixin Li"], "abstract": "arXiv:2509.16674v2 Announce Type: replace \nText-based Pedestrian Retrieval (TPR) deals with retrieving specific target pedestrians in visual scenes according to natural language descriptions. Although existing methods have achieved progress under constrained settings, interactive retrieval in the open-world scenario still suffers from limited model generalization and insufficient semantic understanding. To address these challenges, we propose FitPro, an open-world interactive zero-shot TPR framework with enhanced semantic comprehension and cross-scene adaptability. FitPro has three innovative components: Feature Contrastive Decoding (FCD), Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval (QHR). The FCD integrates prompt-guided contrastive decoding to generate high-quality structured pedestrian descriptions from denoised images, effectively alleviating semantic drift in zero-shot scenarios. The ISM constructs holistic pedestrian representations from multi-view observations to achieve global semantic modeling in multi-turn interactions, thereby improving robustness against viewpoint shifts and fine-grained variations in descriptions. The QHR dynamically optimizes the retrieval pipeline according to query types, enabling efficient adaptation to multi-modal and multi-view inputs. Extensive experiments on five public datasets and two evaluation protocols demonstrate that FitPro significantly overcomes the generalization limitations and semantic modeling constraints of existing methods in interactive retrieval, paving the way for practical deployment.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.16674", "pdf_url": "https://arxiv.org/pdf/2509.16674.pdf", "is_interesting": false}, "1128": {"title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models", "authors": ["Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang"], "abstract": "arXiv:2510.11341v2 Announce Type: replace \nGeneral SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.11341", "pdf_url": "https://arxiv.org/pdf/2510.11341.pdf", "is_interesting": false}, "1129": {"title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding", "authors": ["Hui Xiang, Yifan Bian, Li Li, Jingran Wu, Xianguo Zhang, Dong Liu"], "abstract": "arXiv:2510.14431v4 Announce Type: replace \nNeural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 12.1% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.14431", "pdf_url": "https://arxiv.org/pdf/2510.14431.pdf", "is_interesting": false}, "1130": {"title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging", "authors": ["Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach"], "abstract": "arXiv:2510.14668v2 Announce Type: replace \nKnowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning and addresses the limitations of one-step KD. Each model in the chain is trained on only a fraction of the dataset and shows that effective learning can be achieved with minimal supervision. Extensive evaluation on six imaging datasets across otoscopic, microscopic, and magnetic resonance imaging modalities shows that it generalizes and outperforms existing methods. Furthermore, the proposed distillation chain resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.14668", "pdf_url": "https://arxiv.org/pdf/2510.14668.pdf", "is_interesting": false}, "1131": {"title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "authors": ["Huiming Yang, Wenzhuo Liu, Yicheng Qiao, Lei Yang, Xianzhu Zeng, Li Wang, Zhiwei Li, Zijian Zeng, Zhiying Jiang, Huaping Liu, Kunfeng Wang"], "abstract": "arXiv:2510.15991v3 Announce Type: replace \nThe sparse cross-modality detector offers more advantages than its counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of adaptability for downstream tasks and computational cost savings. However, existing sparse detectors overlook the quality of token representation, leaving it with a sub-optimal foreground quality and limited performance. In this paper, we identify that the geometric structure preserved and the class distribution are the key to improving the performance of the sparse detector, and propose a Sparse Selector (SS). The core module of SS is Ray-Aware Supervision (RAS), which preserves rich geometric information during the training stage, and Class-Balanced Supervision, which adaptively reweights the salience of class semantics, ensuring that tokens associated with small objects are retained during token sampling. Thereby, outperforming other sparse multi-modal detectors in the representation of tokens. Additionally, we design Ray Positional Encoding (Ray PE) to address the distribution differences between the LiDAR modality and the image. Finally, we integrate the aforementioned module into an end-to-end sparse multi-modality detector, dubbed CrossRay3D. Experiments show that, on the challenging nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS, while running 1.84 faster than other leading methods. Moreover, CrossRay3D demonstrates strong robustness even in scenarios where LiDAR or camera data are partially or entirely missing.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.15991", "pdf_url": "https://arxiv.org/pdf/2510.15991.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.9, "subfield": "3D\u611f\u77e5 / \u591a\u6a21\u6001\u878d\u5408", "reason": "The paper discusses a multi-modal 3D detection system that integrates LiDAR and camera data, which is highly relevant for autonomous driving. The focus on improving the detection performance, especially in scenarios with missing sensor data, is a key task in autonomous vehicle perception, directly related to 3D detection and multi-sensor fusion in autonomous driving systems."}}, "1132": {"title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback", "authors": ["Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Feize Wu, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, Shaodong Wang, Xinhua Cheng, Li Yuan"], "abstract": "arXiv:2510.16888v3 Announce Type: replace \nInstruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. \\texttt{UniWorld-V2}, trained with this framework, achieves \\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available to support further research.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.16888", "pdf_url": "https://arxiv.org/pdf/2510.16888.pdf", "is_interesting": false}, "1133": {"title": "Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation", "authors": ["Stefan M. Fischer, Johannes Kiechle, Laura Daza, Lina Felsner, Richard Osuala, Daniel M. Lang, Karim Lekadir, Jan C. Peeken, Julia A. Schnabel"], "abstract": "arXiv:2510.23241v2 Announce Type: replace \nIn this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.23241", "pdf_url": "https://arxiv.org/pdf/2510.23241.pdf", "is_interesting": false}, "1134": {"title": "DRIP: Dynamic patch Reduction via Interpretable Pooling", "authors": ["Yusen Peng, Sachin Kumar"], "abstract": "arXiv:2510.25067v2 Announce Type: replace \nRecently, the advances in vision-language models, including contrastive pretraining and instruction tuning, have greatly pushed the frontier of multimodal AI. However, owing to the large-scale and hence expensive pretraining, the efficiency concern has discouraged researchers from attempting to pretrain a vision language model from scratch. In this work, we propose Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the input images and dynamically merges tokens in the deeper layers of a visual encoder. Our results on both ImageNet training from scratch and CLIP contrastive pretraining demonstrate a significant GFLOP reduction while maintaining comparable classification/zero-shot performance. To further validate our proposed method, we conduct continual pretraining on a large biology dataset, extending its impact into scientific domains.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.25067", "pdf_url": "https://arxiv.org/pdf/2510.25067.pdf", "is_interesting": false}, "1135": {"title": "FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion", "authors": ["Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, Minghua Liu"], "abstract": "arXiv:2510.25765v2 Announce Type: replace \nArticulated 3D objects are central to many applications in robotics, AR/VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the object's geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility. Please check our website for more details: https://czzzzh.github.io/FreeArt3D", "categories": ["cs.CV", "cs.GR"], "abs_url": "https://arxiv.org/abs/2510.25765", "pdf_url": "https://arxiv.org/pdf/2510.25765.pdf", "is_interesting": false}, "1136": {"title": "Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions", "authors": ["Caixin Kang, Yifei Huang, Liangyang Ouyang, Mingfang Zhang, Yoichi Sato"], "abstract": "arXiv:2510.27195v2 Announce Type: replace \nAs AI systems become increasingly integrated into human lives, endowing them with robust social intelligence has emerged as a critical frontier. A key aspect of this intelligence is discerning truth from deception, a ubiquitous element of human interaction that is conveyed through a complex interplay of verbal language and non-verbal visual cues. However, automatic deception detection in dynamic, multi-party conversations remains a significant challenge. The recent rise of powerful Multimodal Large Language Models (MLLMs), with their impressive abilities in visual and textual understanding, makes them natural candidates for this task. Consequently, their capabilities in this crucial domain are mostly unquantified. To address this gap, we introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and present a novel multimodal dataset derived from the social deduction game Werewolf. This dataset provides synchronized video, text, with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating state-of-the-art MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to ground language in visual social cues effectively and may be overly conservative in their alignment, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems.", "categories": ["cs.CV", "cs.CL", "cs.SI"], "abs_url": "https://arxiv.org/abs/2510.27195", "pdf_url": "https://arxiv.org/pdf/2510.27195.pdf", "is_interesting": false}, "1137": {"title": "Parameterized Prompt for Incremental Object Detection", "authors": ["Zijia An, Boyu Diao, Ruiqi Liu, Libo Huang, Chuanguang Yang, Fei Wang, Zhulin An, Yongjun Xu"], "abstract": "arXiv:2510.27316v2 Announce Type: replace \nRecent studies have demonstrated that incorporating trainable prompts into pretrained models enables effective incremental learning. However, the application of prompts in incremental object detection (IOD) remains underexplored. Existing prompts pool based approaches assume disjoint class sets across incremental tasks, which are unsuitable for IOD as they overlook the inherent co-occurrence phenomenon in detection images. In co-occurring scenarios, unlabeled objects from previous tasks may appear in current task images, leading to confusion in prompts pool. In this paper, we hold that prompt structures should exhibit adaptive consolidation properties across tasks, with constrained updates to prevent catastrophic forgetting. Motivated by this, we introduce Parameterized Prompts for Incremental Object Detection (P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD employs networks as the parameterized prompts to adaptively consolidate knowledge across tasks. To constrain prompts structure updates, P$^2$IOD further engages a parameterized prompts fusion strategy. Extensive experiments on PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IOD's effectiveness in IOD and achieves the state-of-the-art performance among existing baselines.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27316", "pdf_url": "https://arxiv.org/pdf/2510.27316.pdf", "is_interesting": false}, "1138": {"title": "ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning", "authors": ["Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, Yu Cheng"], "abstract": "arXiv:2510.27492v2 Announce Type: replace \nMultimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary rather than isomorphic modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on approximately 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7 percent over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts. These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.27492", "pdf_url": "https://arxiv.org/pdf/2510.27492.pdf", "is_interesting": false}, "1139": {"title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model", "authors": ["John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin"], "abstract": "arXiv:2510.27607v2 Announce Type: replace \nRecently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark.", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2510.27607", "pdf_url": "https://arxiv.org/pdf/2510.27607.pdf", "is_interesting": false}, "1140": {"title": "Cross-modal Diffusion Modelling for Super-resolved Spatial Transcriptomics", "authors": ["Xiaofei Wang, Xingxu Huang, Stephen J. Price, Chao Li"], "abstract": "arXiv:2404.12973v3 Announce Type: replace-cross \nThe recent advancement of spatial transcriptomics (ST) allows to characterize spatial gene expression within tissue for discovery research. However, current ST platforms suffer from low resolution, hindering in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, current super-resolution methods are limited by restoration uncertainty and mode collapse. Although diffusion models have shown promise in capturing complex interactions between multi-modal conditions, it remains a challenge to integrate histology images and gene expression for super-resolved ST maps. This paper proposes a cross-modal conditional diffusion model for super-resolving ST maps with the guidance of histology images. Specifically, we design a multi-modal disentangling network with cross-modal adaptive modulation to utilize complementary information from histology images and spatial gene expression. Moreover, we propose a dynamic cross-attention modelling strategy to extract hierarchical cell-to-tissue information from histology images. Lastly, we propose a co-expression-based gene-correlation graph network to model the co-expression relationship of multiple genes. Experiments show that our method outperforms other state-of-the-art methods in ST super-resolution on three public datasets.", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2404.12973", "pdf_url": "https://arxiv.org/pdf/2404.12973.pdf", "is_interesting": false}, "1141": {"title": "Real World Federated Learning with a Knowledge Distilled Transformer for Cardiac CT Imaging", "authors": ["Malte T\\\"olle, Philipp Garthe, Clemens Scherer, Jan Moritz Seliger, Andreas Leha, Nina Kr\\\"uger, Stefan Simm, Simon Martin, Sebastian Eble, Halvar Kelm, Moritz Bednorz, Florian Andr\\'e, Peter Bannas, Gerhard Diller, Norbert Frey, Stefan Gro{\\ss}, Anja Hennemuth, Lars Kaderali, Alexander Meyer, Eike Nagel, Stefan Orwat, Moritz Seiffert, Tim Friede, Tim Seidler, Sandy Engelhardt"], "abstract": "arXiv:2407.07557v3 Announce Type: replace-cross \nFederated learning is a renowned technique for utilizing decentralized data while preserving privacy. However, real-world applications often face challenges like partially labeled datasets, where only a few locations have certain expert annotations, leaving large portions of unlabeled data unused. Leveraging these could enhance transformer architectures ability in regimes with small and diversely annotated sets. We conduct the largest federated cardiac CT analysis to date (n=8,104) in a real-world setting across eight hospitals. Our two-step semi-supervised strategy distills knowledge from task-specific CNNs into a transformer. First, CNNs predict on unlabeled data per label type and then the transformer learns from these predictions with label-specific heads. This improves predictive accuracy and enables simultaneous learning of all partial labels across the federation, and outperforms UNet-based models in generalizability on downstream tasks. Code and model weights are made openly available for leveraging future cardiac CT analysis.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2407.07557", "pdf_url": "https://arxiv.org/pdf/2407.07557.pdf", "is_interesting": false}, "1142": {"title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation", "authors": ["Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren"], "abstract": "arXiv:2508.05635v3 Announce Type: replace-cross \nWe introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2508.05635", "pdf_url": "https://arxiv.org/pdf/2508.05635.pdf", "is_interesting": false}, "1143": {"title": "Understanding Ice Crystal Habit Diversity with Self-Supervised Learning", "authors": ["Joseph Ko, Hariprasath Govindarajan, Fredrik Lindsten, Vanessa Przybylo, Kara Sulia, Marcus van Lier-Walqui, Kara Lamb"], "abstract": "arXiv:2509.07688v3 Announce Type: replace-cross \nIce-containing clouds strongly impact climate, but they are hard to model due to ice crystal habit (i.e., shape) diversity. We use self-supervised learning (SSL) to learn latent representations of crystals from ice crystal imagery. By pre-training a vision transformer with many cloud particle images, we learn robust representations of crystal morphology, which can be used for various science-driven tasks. Our key contributions include (1) validating that our SSL approach can be used to learn meaningful representations, and (2) presenting a relevant application where we quantify ice crystal diversity with these latent representations. Our results demonstrate the power of SSL-driven representations to improve the characterization of ice crystals and subsequently constrain their role in Earth's climate system.", "categories": ["physics.ao-ph", "cs.CV"], "abs_url": "https://arxiv.org/abs/2509.07688", "pdf_url": "https://arxiv.org/pdf/2509.07688.pdf", "is_interesting": false}, "1144": {"title": "Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification", "authors": ["Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin"], "abstract": "arXiv:2509.15591v2 Announce Type: replace-cross \nGenerative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abs_url": "https://arxiv.org/abs/2509.15591", "pdf_url": "https://arxiv.org/pdf/2509.15591.pdf", "is_interesting": false}, "1145": {"title": "MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via SlowFast Contrastive Audio-Visual Pretraining and Direct Preference Optimization", "authors": ["Jianxuan Yang, Xiaoran Yang, Lipan Zhang, Xinyue Guo, Zhao Wang, Gongping Huang"], "abstract": "arXiv:2509.19999v2 Announce Type: replace-cross \nCurrent video-to-audio (V2A) methods struggle in complex multi-event scenarios (video scenarios involving multiple sound sources, sound events, or transitions) due to two critical limitations. First, existing methods face challenges in precisely aligning intricate semantic information together with rapid dynamic features. Second, foundational training lacks quantitative preference optimization for semantic-temporal alignment and audio quality. As a result, it fails to enhance integrated generation quality in cluttered multi-event scenes. To address these core limitations, this study proposes a novel V2A framework: MultiSoundGen. It introduces direct preference optimization (DPO) into the V2A domain, leveraging audio-visual pretraining (AVP) to enhance performance in complex multi-event scenarios. Our contributions include two key innovations: the first is SlowFast Contrastive AVP (SF-CAVP), a pioneering AVP model with a unified dual-stream architecture. SF-CAVP explicitly aligns core semantic representations and rapid dynamic features of audio-visual data to handle multi-event complexity; second, we integrate the DPO method into V2A task and propose AVP-Ranked Preference Optimization (AVP-RPO). It uses SF-CAVP as a reward model to quantify and prioritize critical semantic-temporal matches while enhancing audio quality. Experiments demonstrate that MultiSoundGen achieves state-of-the-art (SOTA) performance in multi-event scenarios, delivering comprehensive gains across distribution matching, audio quality, semantic alignment, and temporal synchronization. Demos are available at https://v2aresearch.github.io/MultiSoundGen/.", "categories": ["cs.MM", "cs.CV", "cs.SD"], "abs_url": "https://arxiv.org/abs/2509.19999", "pdf_url": "https://arxiv.org/pdf/2509.19999.pdf", "is_interesting": false}, "1146": {"title": "SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion", "authors": ["Yufei Tong, Guanjie Cheng, Peihan Wu, Yicheng Zhu, Kexu Lu, Feiyi Chen, Meng Xi, Junqin Huang, Xueqiang Yan, Junfan Wang, Shuiguang Deng"], "abstract": "arXiv:2510.07905v2 Announce Type: replace-cross \nWith the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.", "categories": ["eess.IV", "cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2510.07905", "pdf_url": "https://arxiv.org/pdf/2510.07905.pdf", "is_interesting": false}, "1147": {"title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality", "authors": ["Anastasiya Pechko, Piotr Borycki, Joanna Waczy\\'nska, Daniel Barczyk, Agata Szyma\\'nska, S{\\l}awomir Tadeja, Przemys{\\l}aw Spurek"], "abstract": "arXiv:2510.11878v2 Announce Type: replace-cross \nAs the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce GS-Verse (Gaussian Splatting for Virtual Environment Rendering and Scene Editing), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, GS-Verse facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.", "categories": ["cs.GR", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.11878", "pdf_url": "https://arxiv.org/pdf/2510.11878.pdf", "is_interesting": false}, "1148": {"title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "authors": ["Yu Gao, Anqing Jiang, Yiru Wang, Wang Jijun, Hao Jiang, Zhigang Sun, Heng Yuwen, Wang Shuo, Hao Zhao, Sun Hao"], "abstract": "arXiv:2510.17148v4 Announce Type: replace-cross \nConventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2510.17148", "pdf_url": "https://arxiv.org/pdf/2510.17148.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "\u7aef\u5230\u7aef\u5b66\u4e60 / \u89c4\u5212\u63a7\u5236 / \u8ba4\u77e5\u63a8\u7406", "reason": "The paper proposes DiffVLA++, an enhanced end-to-end autonomous driving framework that integrates Vision-Language-Action reasoning with trajectory planning via metric-guided alignment. It explicitly addresses trajectory generation, physical feasibility, and autonomous driving benchmarks, making it fully within the autonomous driving domain."}}, "1149": {"title": "Evaluating Control Protocols for Untrusted AI Agents", "authors": ["Jon Kutasov, Chloe Loughridge, Yuqi Sun, Henry Sleight, Buck Shlegeris, Tyler Tracy, Joe Benton"], "abstract": "arXiv:2511.02997v1 Announce Type: new \nAs AI systems become more capable and widely deployed as agents, ensuring their safe operation becomes critical. AI control offers one approach to mitigating the risk from untrusted AI agents by monitoring their actions and intervening or auditing when necessary. Evaluating the safety of these protocols requires understanding both their effectiveness against current attacks and their robustness to adaptive adversaries. In this work, we systematically evaluate a range of control protocols in SHADE-Arena, a dataset of diverse agentic environments. First, we evaluate blue team protocols, including deferral to trusted models, resampling, and deferring on critical actions, against a default attack policy. We find that resampling for incrimination and deferring on critical actions perform best, increasing safety from 50% to 96%. We then iterate on red team strategies against these protocols and find that attack policies with additional affordances, such as knowledge of when resampling occurs or the ability to simulate monitors, can substantially improve attack success rates against our resampling strategy, decreasing safety to 17%. However, deferring on critical actions is highly robust to even our strongest red team strategies, demonstrating the importance of denying attack policies access to protocol internals.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02997", "pdf_url": "https://arxiv.org/pdf/2511.02997.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1150": {"title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework", "authors": ["Sina Montazeri, Yunhe Feng, Kewei Sha"], "abstract": "arXiv:2511.03023v1 Announce Type: new \nOpen data repositories hold potential for evidence-based decision-making, yet are inaccessible to non-experts lacking expertise in dataset discovery, schema mapping, and statistical analysis. Large language models show promise for individual tasks, but end-to-end analytical workflows expose fundamental limitations: attention dilutes across growing contexts, specialized reasoning patterns interfere, and errors propagate undetected. We present PublicAgent, a multi-agent framework that addresses these limitations through decomposition into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This architecture maintains focused attention within agent contexts and enables validation at each stage. Evaluation across five models and 50 queries derives five design principles for multi-agent LLM systems. First, specialization provides value independent of model strength--even the strongest model shows 97.5% agent win rates, with benefits orthogonal to model scale. Second, agents divide into universal (discovery, analysis) and conditional (report, intent) categories. Universal agents show consistent effectiveness (std dev 12.4%) while conditional agents vary by model (std dev 20.5%). Third, agents mitigate distinct failure modes--removing discovery or analysis causes catastrophic failures (243-280 instances), while removing report or intent causes quality degradation. Fourth, architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery), indicating workflow management value rather than reasoning enhancement. Fifth, wide variance in agent effectiveness across models (42-96% for analysis) requires model-aware architecture design. These principles guide when and why specialization is necessary for complex analytical workflows while enabling broader access to public data through natural language interfaces.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03023", "pdf_url": "https://arxiv.org/pdf/2511.03023.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1151": {"title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation", "authors": ["Tao Zhang, Kehui Yao, Luyi Ma, Jiao Chen, Reza Yousefi Maragheh, Kai Zhao, Jianpeng Xu, Evren Korpeoglu, Sushant Kumar, Kannan Achan"], "abstract": "arXiv:2511.03051v1 Announce Type: new \nEvaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.", "categories": ["cs.AI", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.03051", "pdf_url": "https://arxiv.org/pdf/2511.03051.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1152": {"title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge", "authors": ["Drago Plecko, Patrik Okanovic, Torsten Hoefler, Elias Bareinboim"], "abstract": "arXiv:2511.03070v1 Announce Type: new \nArtificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., \"what is the capital of England?\"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., \"what is the sex of a computer science graduate in the US?\"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.", "categories": ["cs.AI", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.03070", "pdf_url": "https://arxiv.org/pdf/2511.03070.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1153": {"title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators", "authors": ["Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar"], "abstract": "arXiv:2511.03092v1 Announce Type: new \nThe proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.", "categories": ["cs.AI", "cs.AR", "cs.DC"], "abs_url": "https://arxiv.org/abs/2511.03092", "pdf_url": "https://arxiv.org/pdf/2511.03092.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1154": {"title": "Large language models require a new form of oversight: capability-based monitoring", "authors": ["Katherine C. Kellogg, Bingyang Ye, Yifan Hu, Guergana K. Savova, Byron Wallace, Danielle S. Bitterman"], "abstract": "arXiv:2511.03106v1 Announce Type: new \nThe rapid adoption of large language models (LLMs) in healthcare has been accompanied by scrutiny of their oversight. Existing monitoring approaches, inherited from traditional machine learning (ML), are task-based and founded on assumed performance degradation arising from dataset drift. In contrast, with LLMs, inevitable model degradation due to changes in populations compared to the training dataset cannot be assumed, because LLMs were not trained for any specific task in any given population. We therefore propose a new organizing principle guiding generalist LLM monitoring that is scalable and grounded in how these models are developed and used in practice: capability-based monitoring. Capability-based monitoring is motivated by the fact that LLMs are generalist systems whose overlapping internal capabilities are reused across numerous downstream tasks. Instead of evaluating each downstream task independently, this approach organizes monitoring around shared model capabilities, such as summarization, reasoning, translation, or safety guardrails, in order to enable cross-task detection of systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss. We describe considerations for developers, organizational leaders, and professional societies for implementing a capability-based monitoring approach. Ultimately, capability-based monitoring will provide a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist artificial intelligence models in healthcare.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03106", "pdf_url": "https://arxiv.org/pdf/2511.03106.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1155": {"title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward", "authors": ["Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh"], "abstract": "arXiv:2511.03108v1 Announce Type: new \nWe perform a thorough analysis of the formal and informal statements in the miniF2F benchmark from the perspective of an AI system that is tasked to participate in a math Olympiad consisting of the problems in miniF2F. In such setting, the model has to read and comprehend the problems in natural language, formalize them in Lean language, then proceed with proving the problems, and it will get credit for each problem if the formal proof corresponds to the original informal statement presented to the model. Our evaluation results reveal that the best accuracy of such pipeline can be about 36% using the SoTA models in the literature, considerably lower than the individual SoTA accuracies, 97% and 69% reported in the autoformalization and theorem proving literature. Analyzing the failure modes, we trace back a considerable portion of this drop to discrepancies between the formal and informal statements for more than half of the problems in miniF2F. We proceed with correcting all the errors, discrepancies and simplifications in formal and informal statements, and present the miniF2F-v2 with fully verified formal and informal statements and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to the best accuracy of 70%, a significant improvement from the 40% on the original miniF2F, yet indicating considerable misalignment between the autoformalization models and theorem provers. Our deep analysis suggests that a higher quality benchmark can help the community better evaluate progress in the field of formal reasoning and also better diagnose the failure and success modes of autoformalization and theorem proving models. Our dataset is available at https://github.com/roozbeh-yz/miniF2F_v2.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03108", "pdf_url": "https://arxiv.org/pdf/2511.03108.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1156": {"title": "Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks", "authors": ["Shipeng Cen, Ying Tan"], "abstract": "arXiv:2511.03137v1 Announce Type: new \nAs optimization problems grow increasingly complex and diverse, advancements in optimization techniques and paradigm innovations hold significant importance. The challenges posed by optimization problems are primarily manifested in their non-convexity, high-dimensionality, black-box nature, and other unfavorable characteristics. Traditional zero-order or first-order methods, which are often characterized by low efficiency, inaccurate gradient information, and insufficient utilization of optimization information, are ill-equipped to address these challenges effectively. In recent years, the rapid development of large language models (LLM) has led to substantial improvements in their language understanding and code generation capabilities. Consequently, the design of optimization algorithms leveraging large language models has garnered increasing attention from researchers. In this study, we choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel approach to assist the design of the FWA by incorporating multi-modal large language model(MLLM). To put it simply, we propose the concept of Critical Part(CP), which extends FWA to complex high-dimensional tasks, and further utilizes the information in the optimization process with the help of the multi-modal characteristics of large language models. We focus on two specific tasks: the \\textit{traveling salesman problem }(TSP) and \\textit{electronic design automation problem} (EDA). The experimental results show that FWAs generated under our new framework have achieved or surpassed SOTA results on many problem instances.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03137", "pdf_url": "https://arxiv.org/pdf/2511.03137.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1157": {"title": "A Proprietary Model-Based Safety Response Framework for AI Agents", "authors": ["Qi Li, Jianjun Xu, Pingtao Wei, Jiu Li, Peiqiang Zhao, Jiwei Shi, Xuan Zhang, Yanhui Yang, Xiaodong Hui, Peng Xu, Wenqin Shao"], "abstract": "arXiv:2511.03138v1 Announce Type: new \nWith the widespread application of Large Language Models (LLMs), their associated security issues have become increasingly prominent, severely constraining their trustworthy deployment in critical domains. This paper proposes a novel safety response framework designed to systematically safeguard LLMs at both the input and output levels. At the input level, the framework employs a supervised fine-tuning-based safety classification model. Through a fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention), it performs precise risk identification and differentiated handling of user queries, significantly enhancing risk coverage and business scenario adaptability, and achieving a risk recall rate of 99.3%. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a specifically fine-tuned interpretation model, ensuring all responses are grounded in a real-time, trustworthy knowledge base. This approach eliminates information fabrication and enables result traceability. Experimental results demonstrate that our proposed safety control model achieves a significantly higher safety score on public safety evaluation benchmarks compared to the baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk test set, the framework's components attained a perfect 100% safety score, validating their exceptional protective capabilities in complex risk scenarios. This research provides an effective engineering pathway for building high-security, high-trust LLM applications.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03138", "pdf_url": "https://arxiv.org/pdf/2511.03138.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1158": {"title": "Uncovering Bugs in Formal Explainers: A Case Study with PyXAI", "authors": ["Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, Joao Marques-Silva"], "abstract": "arXiv:2511.03169v1 Announce Type: new \nFormal explainable artificial intelligence (XAI) offers unique theoretical guarantees of rigor when compared to other non-formal methods of explainability. However, little attention has been given to the validation of practical implementations of formal explainers. This paper develops a novel methodology for validating formal explainers and reports on the assessment of the publicly available formal explainer PyXAI. The paper documents the existence of incorrect explanations computed by PyXAI on most of the datasets analyzed in the experiments, thereby confirming the importance of the proposed novel methodology for the validation of formal explainers.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03169", "pdf_url": "https://arxiv.org/pdf/2511.03169.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1159": {"title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework", "authors": ["Varun Kumar, George Em Karniadakis"], "abstract": "arXiv:2511.03179v1 Announce Type: new \nThe engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.", "categories": ["cs.AI", "cs.LG", "cs.MA"], "abs_url": "https://arxiv.org/abs/2511.03179", "pdf_url": "https://arxiv.org/pdf/2511.03179.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1160": {"title": "Adobe Summit Concierge Evaluation with Human in the Loop", "authors": ["Yiru Chen, Sally Fang, Sai Sree Harsha, Dan Luo, Vaishnavi Muppala, Fei Wu, Shun Jiang, Kun Qian, Yunyao Li"], "abstract": "arXiv:2511.03186v1 Announce Type: new \nGenerative AI assistants offer significant potential to enhance productivity, streamline information access, and improve user experience in enterprise contexts. In this work, we present Summit Concierge, a domain-specific AI assistant developed for Adobe Summit. The assistant handles a wide range of event-related queries and operates under real-world constraints such as data sparsity, quality assurance, and rapid deployment. To address these challenges, we adopt a human-in-the-loop development workflow that combines prompt engineering, retrieval grounding, and lightweight human validation. We describe the system architecture, development process, and real-world deployment outcomes. Our experience shows that agile, feedback-driven development enables scalable and reliable AI assistants, even in cold-start scenarios.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03186", "pdf_url": "https://arxiv.org/pdf/2511.03186.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1161": {"title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers", "authors": ["Yi-Fei Liu, Yi-Long Lu, Di He, Hang Zhang"], "abstract": "arXiv:2511.03235v1 Announce Type: new \nPsychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 > 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03235", "pdf_url": "https://arxiv.org/pdf/2511.03235.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1162": {"title": "Towards Scalable Web Accessibility Audit with MLLMs as Copilots", "authors": ["Ming Gu, Ziwei Wang, Sicen Lai, Zirui Gao, Sheng Zhou, Jiajun Bu"], "abstract": "arXiv:2511.03471v1 Announce Type: new \nEnsuring web accessibility is crucial for advancing social welfare, justice, and equality in digital spaces, yet the vast majority of website user interfaces remain non-compliant, due in part to the resource-intensive and unscalable nature of current auditing practices. While WCAG-EM offers a structured methodology for site-wise conformance evaluation, it involves great human efforts and lacks practical support for execution at scale. In this work, we present an auditing framework, AAA, which operationalizes WCAG-EM through a human-AI partnership model. AAA is anchored by two key innovations: GRASP, a graph-based multimodal sampling method that ensures representative page coverage via learned embeddings of visual, textual, and relational cues; and MaC, a multimodal large language model-based copilot that supports auditors through cross-modal reasoning and intelligent assistance in high-effort tasks. Together, these components enable scalable, end-to-end web accessibility auditing, empowering human auditors with AI-enhanced assistance for real-world impact. We further contribute four novel datasets designed for benchmarking core stages of the audit pipeline. Extensive experiments demonstrate the effectiveness of our methods, providing insights that small-scale language models can serve as capable experts when fine-tuned.", "categories": ["cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.03471", "pdf_url": "https://arxiv.org/pdf/2511.03471.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1163": {"title": "Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)", "authors": ["Sebastian Ordyniak, Giacomo Paesani, Mateusz Rychlicki, Stefan Szeider"], "abstract": "arXiv:2511.03545v1 Announce Type: new \nThis paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03545", "pdf_url": "https://arxiv.org/pdf/2511.03545.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1164": {"title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning", "authors": ["Richard Dewey, Janos Botyanszki, Ciamac C. Moallemi, Andrew T. Zheng"], "abstract": "arXiv:2511.03724v1 Announce Type: new \nAI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.", "categories": ["cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2511.03724", "pdf_url": "https://arxiv.org/pdf/2511.03724.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1165": {"title": "An extended reality-based framework for user risk training in urban built environment", "authors": ["Sotirios Konstantakos, Sotirios Asparagkathos, Moatasim Mahmoud, Stamatia Rizou, Enrico Quagliarini, Gabriele Bernardini"], "abstract": "arXiv:2511.02837v1 Announce Type: cross \nIn the context of increasing urban risks, particularly from climate change-induced flooding, this paper presents an extended Reality (XR)-based framework to improve user risk training within urban built environments. The framework is designed to improve risk awareness and preparedness among various stakeholders, including citizens, local authorities, and emergency responders. Using immersive XR technologies, the training experience simulates real-world emergency scenarios, contributing to active participation and a deeper understanding of potential hazards and especially for floods. The framework highlights the importance of stakeholder participation in its development, ensuring that training modules are customized to address the specific needs of different user groups. The iterative approach of the framework supports ongoing refinement through user feedback and performance data, thus improving the overall effectiveness of risk training initiatives. This work outlines the methodological phases involved in the framework's implementation, including i) user flow mapping, ii) scenario selection, and iii) performance evaluation, with a focus on the pilot application in Senigallia, Italy. The findings underscore the potential of XR technologies to transform urban risk training, promoting a culture of preparedness and resilience against urban hazards.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02837", "pdf_url": "https://arxiv.org/pdf/2511.02837.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1166": {"title": "Evaluating Generative AI as an Educational Tool for Radiology Resident Report Drafting", "authors": ["Antonio Verdone, Aidan Cardall, Fardeen Siddiqui, Motaz Nashawaty, Danielle Rigau, Youngjoon Kwon, Mira Yousef, Shalin Patel, Alex Kieturakis, Eric Kim, Laura Heacock, Beatriu Reig, Yiqiu Shen"], "abstract": "arXiv:2511.02839v1 Announce Type: cross \nObjective: Radiology residents require timely, personalized feedback to develop accurate image analysis and reporting skills. Increasing clinical workload often limits attendings' ability to provide guidance. This study evaluates a HIPAA-compliant GPT-4o system that delivers automated feedback on breast imaging reports drafted by residents in real clinical settings.\n  Methods: We analyzed 5,000 resident-attending report pairs from routine practice at a multi-site U.S. health system. GPT-4o was prompted with clinical instructions to identify common errors and provide feedback. A reader study using 100 report pairs was conducted. Four attending radiologists and four residents independently reviewed each pair, determined whether predefined error types were present, and rated GPT-4o's feedback as helpful or not. Agreement between GPT and readers was assessed using percent match. Inter-reader reliability was measured with Krippendorff's alpha. Educational value was measured as the proportion of cases rated helpful.\n  Results: Three common error types were identified: (1) omission or addition of key findings, (2) incorrect use or omission of technical descriptors, and (3) final assessment inconsistent with findings. GPT-4o showed strong agreement with attending consensus: 90.5%, 78.3%, and 90.4% across error types. Inter-reader reliability showed moderate variability ({\\alpha} = 0.767, 0.595, 0.567), and replacing a human reader with GPT-4o did not significantly affect agreement ({\\Delta} = -0.004 to 0.002). GPT's feedback was rated helpful in most cases: 89.8%, 83.0%, and 92.0%.\n  Discussion: ChatGPT-4o can reliably identify key educational errors. It may serve as a scalable tool to support radiology education.", "categories": ["cs.HC", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.02839", "pdf_url": "https://arxiv.org/pdf/2511.02839.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1167": {"title": "Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs", "authors": ["Jiawei Zheng, Gokcen Yilmaz, Ji Han, Saeema Ahmed-Kristensen"], "abstract": "arXiv:2511.02842v1 Announce Type: cross \nMany organisations pursue digital transformation to enhance operational efficiency, reduce manual efforts, and optimise processes by automation and digital tools. To achieve this, a comprehensive understanding of their unique needs is required. However, traditional methods, such as expert interviews, while effective, face several challenges, including scheduling conflicts, resource constraints, inconsistency, etc. To tackle these issues, we investigate the use of a Large Language Model (LLM)-powered chatbot to acquire organisations' digital transformation needs. Specifically, the chatbot integrates workflow-based instruction with LLM's planning and reasoning capabilities, enabling it to function as a virtual expert and conduct interviews. We detail the chatbot's features and its implementation. Our preliminary evaluation indicates that the chatbot performs as designed, effectively following predefined workflows and supporting user interactions with areas for improvement. We conclude by discussing the implications of employing chatbots to elicit user information, emphasizing their potential and limitations.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02842", "pdf_url": "https://arxiv.org/pdf/2511.02842.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1168": {"title": "AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair", "authors": ["Yuxuan Liu, Chiya Zhang, Yifeng Yuan, Chunlong He, Weizheng Zhang, Gaojie Chen"], "abstract": "arXiv:2511.02845v1 Announce Type: cross \nThe advancement of next-generation Wi-Fi technology heavily relies on sensing capabilities, which play a pivotal role in enabling sophisticated applications. In response to the growing demand for large-scale deployments, contemporary Wi-Fi sensing systems strive to achieve high-precision perception while maintaining minimal bandwidth consumption and antenna count requirements. Remarkably, various AI-driven perception technologies have demonstrated the ability to surpass the traditional resolution limitations imposed by radar theory. However, the theoretical underpinnings of this phenomenon have not been thoroughly investigated in existing research. In this study, we found that under hardware-constrained conditions, the performance gains brought by AI to Wi-Fi sensing systems primarily originate from two aspects: prior information and temporal correlation. Prior information enables the AI to generate plausible details based on vague input, while temporal correlation helps reduce the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system using a single transceiver pair and designed experiments focusing on human pose estimation and indoor localization to validate the theoretical claims. The results confirm the performance gains contributed by temporal correlation and prior information.", "categories": ["eess.SP", "cs.AI", "physics.ins-det"], "abs_url": "https://arxiv.org/abs/2511.02845", "pdf_url": "https://arxiv.org/pdf/2511.02845.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1169": {"title": "Spatio-Temporal Attention Network for Epileptic Seizure Prediction", "authors": ["Zan Li, Kyongmin Yeo, Wesley Gifford, Lara Marcuse, Madeline Fields, B\\\"ulent Yener"], "abstract": "arXiv:2511.02846v1 Announce Type: cross \nIn this study, we present a deep learning framework that learns complex spatio-temporal correlation structures of EEG signals through a Spatio-Temporal Attention Network (STAN) for accurate predictions of onset of seizures for Epilepsy patients. Unlike existing methods, which rely on feature engineering and/or assume fixed preictal durations, our approach simultaneously models spatio-temporal correlations through STAN and employs an adversarial discriminator to distinguish preictal from interictal attention patterns, enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets demonstrates 96.6\\% sensitivity with 0.011/h false detection rate on CHB-MIT, and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming state-of-the-art methods. The framework reliably detects preictal states at least 15 minutes before an onset, with patient-specific windows extending to 45 minutes, providing sufficient intervention time for clinical applications.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02846", "pdf_url": "https://arxiv.org/pdf/2511.02846.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1170": {"title": "EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding", "authors": ["Shantanu Sarkar, Piotr Nabrzyski, Saurabh Prasad, Jose Luis Contreras-Vidal"], "abstract": "arXiv:2511.02848v1 Announce Type: cross \nElectroencephalography (EEG) is a widely used non-invasive technique for monitoring brain activity, but low signal-to-noise ratios (SNR) due to various artifacts often compromise its utility. Conventional artifact removal methods require manual intervention or risk suppressing critical neural features during filtering/reconstruction. Recent advances in generative models, including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown promise for EEG reconstruction; however, these approaches often lack integrated temporal-spectral-spatial sensitivity and are computationally intensive, limiting their suitability for real-time applications like brain-computer interfaces (BCIs). To overcome these challenges, we introduce EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction via cross-subject transfer learning - developed using Keras TensorFlow (v2.15.1). EEGReXferNet employs a modular architecture that leverages volume conduction across neighboring channels, band-specific convolution encoding, and dynamic latent feature extraction through sliding windows. By integrating reference-based scaling, the framework ensures continuity across successive windows and generalizes effectively across subjects. This design improves spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate overfitting, and maintains computational efficiency for robust, real-time EEG preprocessing in neurophysiological and BCI applications.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02848", "pdf_url": "https://arxiv.org/pdf/2511.02848.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1171": {"title": "Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation", "authors": ["Rushuang Zhou, Yuan-Ting Zhang, M. Jamal Deen, Yining Dong"], "abstract": "arXiv:2511.02851v1 Announce Type: cross \nDeploying advanced cardiac artificial intelligence for daily cardiac monitoring is hindered by its reliance on extensive medical data and high computational resources. Low-cost cardiac intelligence (LCCI) offers a promising alternative by using wearable device data, such as 1-lead electrocardiogram (ECG), but it suffers from a significant diagnostic performance gap compared to high-cost cardiac intelligence (HCCI). To bridge this gap, we propose LiteHeart, a semi-supervised knowledge distillation framework. LiteHeart introduces a region-aware distillation module to mimic how cardiologists focus on diagnostically relevant ECG regions and a cross-layer mutual information module to align the decision processes of LCCI and HCCI systems. Using a semi-supervised training strategy, LiteHeart further improves model robustness under limited supervision. Evaluated on five datasets covering over 38 cardiovascular diseases, LiteHeart substantially reduces the performance gap between LCCI and HCCI, outperforming existing methods by 4.27% to 7.10% in macro F1 score. These results demonstrate that LiteHeart significantly enhances the diagnostic capabilities of low-cost cardiac intelligence systems, paving the way for scalable, affordable, and accurate daily cardiac healthcare using wearable technologies.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02851", "pdf_url": "https://arxiv.org/pdf/2511.02851.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1172": {"title": "Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring", "authors": ["Young-Seok Kweon, Gi-Hwan Shin, Ji-Yong Kim, Bokyeong Ryu, Seong-Whan Lee"], "abstract": "arXiv:2511.02853v1 Announce Type: cross \nConscious state estimation is important in various medical settings, including sleep staging and anesthesia management, to ensure patient safety and optimize health outcomes. Traditional methods predominantly utilize electroencephalography (EEG), which faces challenges such as high sensitivity to noise and the requirement for controlled environments. In this study, we propose the consciousness-ECG transformer that leverages electrocardiography (ECG) signals for non-invasive and reliable conscious state estimation. Our approach employs a transformer with decoupled query attention to effectively capture heart rate variability features that distinguish between conscious and unconscious states. We implemented the conscious state estimation system with real-time monitoring and validated our system on datasets involving sleep staging and anesthesia level monitoring during surgeries. Experimental results demonstrate that our model outperforms baseline models, achieving accuracies of 0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our model achieves the highest area under curve values of 0.786 and 0.895 on sleep staging and anesthesia level monitoring, respectively. The proposed system offers a practical and robust alternative to EEG-based methods, particularly suited for dynamic clinical environments. Our results highlight the potential of ECG-based consciousness monitoring to enhance patient safety and advance our understanding of conscious states.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02853", "pdf_url": "https://arxiv.org/pdf/2511.02853.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1173": {"title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation", "authors": ["Yixiang Chen, Tianshi Zheng, Shijue Huang, Zhitao He, Yi R. Fung"], "abstract": "arXiv:2511.02854v1 Announce Type: cross \nTest-time scaling without interpreter feedback is essential for real-world code generation scenarios where test cases are not readily available. While existing paradigms often rely on either greedy exploitation (i.e., iterative refinement) or stochastic exploration (i.e., relying on sample-based voting or reranking mechanisms), the balance between these two dimensions remains underexplored. To investigate the LLM's intrinsic ability to balance exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon Self-Refine that encourages the model to propose new drafts for solutions that are fundamentally flawed. Our results show that SELF-REDRAFT consistently achieves better performance than Self-Refine when converged under the same maximum number of iterations. Still, we observe that significant room for improvement remains, largely due to two core aspects of current self-redraft capabilities: constrained capacity for generating instructive feedback and fragile discriminative judgment. We also find that balancing strategies vary notably across different LLMs, reflecting distinct, model-specific behaviors. Overall, our study establishes a baseline for intrinsic exploration-exploitation balancing in test-time scaling and identifies feedback and discrimination as key areas with potential for future advances.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02854", "pdf_url": "https://arxiv.org/pdf/2511.02854.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1174": {"title": "Digitizing Spermatogenesis Lineage at Nanoscale Resolution In Tissue-Level Electron Microscopy", "authors": ["Li Xiao, Liqing Liu, Hongjun Wu, Jiayi Zhong, Yan Zhang, Junjie Hu, Sun Fei, Ge Yang, Tao Xu"], "abstract": "arXiv:2511.02860v1 Announce Type: cross \nRecent advances in 2D large-scale and 3D volume electron microscopy have stimulated the rapid development of nanoscale functional analysis at the tissue and organ levels. Digitizing the cell by mapping the intricate organellar networks into its physiological and pathological textures will revolutionarize the contents of cell atlases. To meet the requirements of characterizing intracellular organelles and their interactions within defined cellular cohorts at tissue level, we have developed DeepOrganelle. It adopts a lightweighted Mask2Former frameworks as a universal segmentor and is capable of segmenting and extracting organelles within different cell types, performing statistical quantitative analysis, as well as visualizing and quantifying the spatial distribution of organelle morphologies and interactions across different cell types at tissue scales. Using DeepOrganelle, we systemically perform cross-scale quantification of membrane contact sites(MCSs) dynamics across the progression of the seminiferous epithelial cycle, covering 12 distinct developmental stages and 24 statuses of germ cells. DeepOrganelle uncovers the spatiotemporal gradient of the germ cell differentiation atlas according to different types of organelles and their interactions. Noticeably, it discovers a waved pattern of mitochondria(Mito)-endoplasmic reticulum(ER) contact with a significant increase specifically at Stage X pachytene preceding the transition to diplotene, which aligns well with a newly reported experiment that mitochondrial metabolic proteins like PDHA2 are essential for this transition by maintaining ATP supply for double-strand break(DSB) repair. DeepOrganelle also observes a dynamic restructuring of the blood-testis barrier and stage-specific reorganization of organelle topography in Sertoli cells from preleptotene to leptotene phases of prophase I.", "categories": ["physics.bio-ph", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02860", "pdf_url": "https://arxiv.org/pdf/2511.02860.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1175": {"title": "Mathematical exploration and discovery at scale", "authors": ["Bogdan Georgiev, Javier G\\'omez-Serrano, Terence Tao, Adam Zsolt Wagner"], "abstract": "arXiv:2511.02864v1 Announce Type: cross \nAlphaEvolve is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.\n  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.\n  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.", "categories": ["cs.NE", "cs.AI", "math.CA", "math.CO", "math.MG"], "abs_url": "https://arxiv.org/abs/2511.02864", "pdf_url": "https://arxiv.org/pdf/2511.02864.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1176": {"title": "LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models", "authors": ["Ahmad Tahmasivand, Noureldin Zahran, Saba Al-Sayouri, Mohammed Fouda, Khaled N. Khasawneh"], "abstract": "arXiv:2511.02866v1 Announce Type: cross \nThis paper presents LM-Fix, a lightweight detection and rapid recovery framework for faults in large language models (LLMs). Existing integrity approaches are often heavy or slow for modern LLMs. LM-Fix runs a short test-vector pass and uses hash-guided checks to detect bit-flip faults, then repairs them locally without a full reload. Across multiple models, it detects over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster than reloading. These results show a practical, low-overhead solution to keep LLMs reliable in production", "categories": ["cs.SE", "cs.AI", "cs.AR", "cs.CR"], "abs_url": "https://arxiv.org/abs/2511.02866", "pdf_url": "https://arxiv.org/pdf/2511.02866.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1177": {"title": "Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains", "authors": ["M. Z. Haider, M. U Ghouri, Tayyaba Noreen, M. Salman"], "abstract": "arXiv:2511.02868v1 Announce Type: cross \nBlockchain systems face persistent challenges of scalability, latency, and energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) either consume excessive resources or risk centralization. This paper proposes \\textit{Proof-of-Spiking-Neurons (PoSN)}, a neuromorphic consensus protocol inspired by spiking neural networks. PoSN encodes transactions as spike trains, elects leaders through competitive firing dynamics, and finalizes blocks via neural synchronization, enabling parallel and event-driven consensus with minimal energy overhead. A hybrid system architecture is implemented on neuromorphic platforms, supported by simulation frameworks such as Nengo and PyNN. Experimental results show significant gains in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN establishes a foundation for sustainable, adaptive blockchains suitable for IoT, edge, and large-scale distributed systems.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02868", "pdf_url": "https://arxiv.org/pdf/2511.02868.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1178": {"title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models", "authors": ["Amirreza Esmaeili, Fahd Seddik, Yongyi Ji, Fatemeh Fard, Fuxiang Chen"], "abstract": "arXiv:2511.02869v1 Announce Type: cross \nProgramming languages can benefit from one another by utilizing a language model for software engineering tasks. Full fine-tuning and Parameter Efficient Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims to enhance task performance by leveraging information from multiple programming languages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that effectively learns from other programming languages before adapting to the target task. Though previous experiments showed that AdvFusion outperformed AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited to only two tasks, code summarization and method name prediction. In this study, we expanded our work and investigated AdvFusion on Code Large Language Models (Code-LLMs), considering three new tasks: code generation, code translation, and commit message generation. We observed that different Code-LLMs/tasks exhibit different characteristics. In code generation, AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA, Compacter, and TaskAdapter). In commit message generation, AdapterFusion performed better than AdvFusion, and contrary to code generation, we found that the other PEFT methods do not have better performance. In code translation, AdvFusion performed worse than AdapterFusion overall, with the performance gap marginally widening as the model size increases. However, consistent with code generation, other PEFT methods showed better performance.", "categories": ["cs.SE", "cs.AI", "cs.PL"], "abs_url": "https://arxiv.org/abs/2511.02869", "pdf_url": "https://arxiv.org/pdf/2511.02869.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1179": {"title": "FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels", "authors": ["Jiedong Jiang, Wanyi He, Yuefeng Wang, Guoxiong Gao, Yongle Hu, Jingting Wang, Nailing Guan, Peihao Wu, Chunbo Dai, Liang Xiao, Bin Dong"], "abstract": "arXiv:2511.02872v1 Announce Type: cross \nRecent advances in large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, particularly on contest-based mathematical benchmarks like the IMO. However, these contests do not reflect the depth, breadth, and abstraction of modern mathematical research. To bridge this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra designed to chart a course toward advanced mathematical reasoning. We present two new components, FATE-H and FATE-X, each with 100 problems in abstract and commutative algebra. The FATE series spans a difficulty spectrum from undergraduate exercises to problems exceeding PhD qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both PhD-level exam difficulty and the coverage of the Mathlib library. Our evaluations of state-of-the-art LLM provers on this new benchmark reveal a stark performance gap compared to contest math: the best model achieves only 3% (pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals that models' natural-language reasoning is notably more accurate than their ability to formalize this reasoning. We systematically classify the common errors that arise during this formalization process. Furthermore, a comparative study shows that a specialized prover can exhibit less effective reflection than general-purpose models, reducing its accuracy at the natural-language stage. We believe FATE provides a robust and challenging benchmark that establishes essential checkpoints on the path toward research-level formal mathematical reasoning.", "categories": ["cs.LG", "cs.AI", "cs.FL", "cs.LO"], "abs_url": "https://arxiv.org/abs/2511.02872", "pdf_url": "https://arxiv.org/pdf/2511.02872.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1180": {"title": "Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids", "authors": ["R. Yamamoto Ravenor"], "abstract": "arXiv:2511.02875v1 Announce Type: cross \nAs generative AI diffuses through academia, policy-practice divergence becomes consequential, creating demand for auditable indicators of alignment. This study prototypes a ten-item, indirect-elicitation instrument embedded in a structured interpretive framework to surface voids between institutional rules and practitioner AI use. The framework extracts empirical and epistemic signals from academics, yielding three filtered indicators of such voids: (1) AI-integrated assessment capacity (proxy) - within a three-signal screen (AI skill, perceived teaching benefit, detection confidence), the share who would fully allow AI in exams; (2) sector-level necessity (proxy) - among high output control users who still credit AI with high contribution, the proportion who judge AI capable of challenging established disciplines; and (3) ontological stance - among respondents who judge AI different in kind from prior tools, report practice change, and pass a metacognition gate, the split between material and immaterial views as an ontological map aligning procurement claims with evidence classes.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02875", "pdf_url": "https://arxiv.org/pdf/2511.02875.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1181": {"title": "A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features", "authors": ["S. K. Laha"], "abstract": "arXiv:2511.02877v1 Announce Type: cross \nForecasting chaotic time series requires models that can capture the intrinsic geometry of the underlying attractor while remaining computationally efficient. We introduce a novel reservoir computing (RC) framework that integrates time-delay embedding with Random Fourier Feature (RFF) mappings to construct a dynamical reservoir without the need for traditional recurrent architectures. Unlike standard RC, which relies on high-dimensional recurrent connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel transformations that uncover latent dynamical relations in the reconstructed phase space. This hybrid formulation offers two key advantages: (i) it provides a principled way to approximate complex nonlinear interactions among delayed coordinates, thereby enriching the effective dynamical representation of the reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters such as spectral radius and leaking rate. We evaluate the framework on canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC not only achieves superior prediction accuracy but also yields robust attractor reconstructions and long-horizon forecasts. These results show that the combination of delay embedding and RFF-based reservoirs reveals new dynamical structure by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to modeling chaotic dynamics.", "categories": ["cs.NE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02877", "pdf_url": "https://arxiv.org/pdf/2511.02877.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1182": {"title": "Stochastic Deep Graph Clustering for Practical Group Formation", "authors": ["Junhyung Park, Hyungjin Kim, Seokho Ahn, Young-Duk Seo"], "abstract": "arXiv:2511.02879v1 Announce Type: cross \nWhile prior work on group recommender systems (GRSs) has primarily focused on improving recommendation accuracy, most approaches assume static or predefined groups, making them unsuitable for dynamic, real-world scenarios. We reframe group formation as a core challenge in GRSs and propose DeepForm (Stochastic Deep Graph Clustering for Practical Group Formation), a framework designed to meet three key operational requirements: (1) the incorporation of high-order user information, (2) real-time group formation, and (3) dynamic adjustment of the number of groups. DeepForm employs a lightweight GCN architecture that effectively captures high-order structural signals. Stochastic cluster learning enables adaptive group reconfiguration without retraining, while contrastive learning refines groups under dynamic conditions. Experiments on multiple datasets demonstrate that DeepForm achieves superior group formation quality, efficiency, and recommendation accuracy compared with various baselines.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02879", "pdf_url": "https://arxiv.org/pdf/2511.02879.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1183": {"title": "NEF-NET+: Adapting Electrocardio panorama in the wild", "authors": ["Zehui Zhan, Yaojun Hu, Jiajing Zhan, Wanchen Lian, Wanqing Wu, Jintai Chen"], "abstract": "arXiv:2511.02880v1 Announce Type: cross \nConventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and com- pensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.", "categories": ["eess.SP", "cs.AI", "cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.02880", "pdf_url": "https://arxiv.org/pdf/2511.02880.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1184": {"title": "AgentSLA : Towards a Service Level Agreement for AI Agents", "authors": ["Gwendal Jouneaux, Jordi Cabot"], "abstract": "arXiv:2511.02885v1 Announce Type: cross \nAI components are increasingly becoming a key element of all types of software systems to enhance their functionality. These AI components are often implemented as AI Agents, offering more autonomy than a plain integration of Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an Agent-as-a-Service one, bringing new challenges to the development of smart software systems. Indeed, while support for the design, implementation, and deployment of those agents exist, the specification of Quality of Service (QoS) and definition of Service Level Agreements (SLAs) aspects for those agents, important to ensure the quality of the resulting systems, remains an open challenge. Part of this is due to the difficulty to clearly define quality in the context of AI components, resulting in a lack of consensus on how to best approach Quality Assurance (QA) for these types of systems. To address this challenge, this paper proposes both a quality model for AI agents based on the ISO/IEC 25010 standard, and a domain specific language to support the definition of SLAs for the services provided by these AI agents.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02885", "pdf_url": "https://arxiv.org/pdf/2511.02885.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1185": {"title": "Test-time Adaptation of Tiny Recursive Models", "authors": ["Ronan Killian McGovern"], "abstract": "arXiv:2511.02886v1 Announce Type: cross \nPrior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02886", "pdf_url": "https://arxiv.org/pdf/2511.02886.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1186": {"title": "Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets", "authors": ["Chaitanya Rele, Aditya Rathod, Kaustubh Natu, Saurabh Kulkarni, Ajay Koli, Swapnali Makdey"], "abstract": "arXiv:2511.02887v1 Announce Type: cross \nThe North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02887", "pdf_url": "https://arxiv.org/pdf/2511.02887.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1187": {"title": "NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction", "authors": ["Zhongmin Li, Runze Ma, Jiahao Tan, Chengzi Tan, Shuangjia Zheng"], "abstract": "arXiv:2511.02888v1 Announce Type: cross \nNucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. Here we introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative foundation models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to advance nucleic acid modeling, supporting downstream applications in RNA/DNA design, synthetic biology, and biochemistry. Our code is available at https://github.com/mrzzmrzz/NABench.", "categories": ["q-bio.GN", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02888", "pdf_url": "https://arxiv.org/pdf/2511.02888.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1188": {"title": "A Criminology of Machines", "authors": ["Gian Maria Campedelli"], "abstract": "arXiv:2511.02895v1 Announce Type: cross \nWhile the possibility of reaching human-like Artificial Intelligence (AI) remains controversial, the likelihood that the future will be characterized by a society with a growing presence of autonomous machines is high. Autonomous AI agents are already deployed and active across several industries and digital environments and alongside human-human and human-machine interactions, machine-machine interactions are poised to become increasingly prevalent. Given these developments, I argue that criminology must begin to address the implications of this transition for crime and social control. Drawing on Actor-Network Theory and Woolgar's decades-old call for a sociology of machines -- frameworks that acquire renewed relevance with the rise of generative AI agents -- I contend that criminologists should move beyond conceiving AI solely as a tool. Instead, AI agents should be recognized as entities with agency encompassing computational, social, and legal dimensions. Building on the literature on AI safety, I thus examine the risks associated with the rise of multi-agent AI systems, proposing a dual taxonomy to characterize the channels through which interactions among AI agents may generate deviant, unlawful, or criminal outcomes. I then advance and discuss four key questions that warrant theoretical and empirical attention: (1) Can we assume that machines will simply mimic humans? (2) Will crime theories developed for humans suffice to explain deviant or criminal behaviors emerging from interactions between autonomous AI agents? (3) What types of criminal behaviors will be affected first? (4) How might this unprecedented societal shift impact policing? These questions underscore the urgent need for criminologists to theoretically and empirically engage with the implications of multi-agent AI systems for the study of crime and play a more active role in debates on AI safety and governance.", "categories": ["cs.CY", "cs.AI", "cs.HC", "physics.soc-ph"], "abs_url": "https://arxiv.org/abs/2511.02895", "pdf_url": "https://arxiv.org/pdf/2511.02895.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1189": {"title": "Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework", "authors": ["Clyde Meli, Vitezslav Nezval, Zuzana Kominkova Oplatkova, Victor Buttigieg, Anthony Spiteri Staines"], "abstract": "arXiv:2511.02897v1 Announce Type: cross \nDifferent bitstring representations can yield varying computational performance. This work compares three bitstring implementations in C++: std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their performance is benchmarked in the context of concatenation within a Linear Genetic Programming system. Benchmarks were conducted on three platforms (macOS, Linux, and Windows MSYS2) to assess platform specific performance variations. The results show that the custom direct implementation delivers the fastest performance on Linux and Windows, while std::bitset performs best on macOS. Although consistently slower, boost::dynamic_bitset remains a viable and flexible option. These findings highlight the influence of compiler optimisations and system architecture on performance, providing practical guidance for selecting the optimal method based on platform and application requirements.", "categories": ["cs.NE", "cs.AI", "cs.PF"], "abs_url": "https://arxiv.org/abs/2511.02897", "pdf_url": "https://arxiv.org/pdf/2511.02897.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1190": {"title": "Generative Hints", "authors": ["Andy Dimnaku, Abdullah Yusuf Kavrano\\u{g}lu, Yaser Abu-Mostafa"], "abstract": "arXiv:2511.02933v1 Announce Type: cross \nData augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02933", "pdf_url": "https://arxiv.org/pdf/2511.02933.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1191": {"title": "Zero-shot data citation function classification using transformer-based large language models (LLMs)", "authors": ["Neil Byers, Ali Zaidi, Valerie Skye, Chris Beecroft, Kjiersten Fagnan"], "abstract": "arXiv:2511.02936v1 Announce Type: cross \nEfforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.02936", "pdf_url": "https://arxiv.org/pdf/2511.02936.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1192": {"title": "From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery", "authors": ["Sepideh KhakzadGharamaleki, Hassan Rivaz, Brandon Helfield"], "abstract": "arXiv:2511.02938v1 Announce Type: cross \nConventional pulse-echo ultrasound suffers when low-cost probes deliver only narrow fractional bandwidths, elongating pulses and erasing high-frequency detail. We address this limitation by learning a data-driven mapping from band-limited to broadband spectrogram of radio-frequency (RF) lines. To this end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by 6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also sharpens point-target rows in a completely unseen resolution phantom, demonstrating strong out-of-distribution generalisation without sacrificing frame rate or phase information. These results indicate that a purely software upgrade can endow installed narrow-band probes with broadband-like performance, potentially widening access to high-resolution ultrasound in resource-constrained settings.", "categories": ["eess.SP", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02938", "pdf_url": "https://arxiv.org/pdf/2511.02938.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1193": {"title": "Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics", "authors": ["Fengxu Li, Stephanie M. Carpenter, Matthew P. Buman, Yonatan Mintz"], "abstract": "arXiv:2511.02944v1 Announce Type: cross \nA common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abs_url": "https://arxiv.org/abs/2511.02944", "pdf_url": "https://arxiv.org/pdf/2511.02944.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1194": {"title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation", "authors": ["Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield"], "abstract": "arXiv:2511.02953v1 Announce Type: cross \nEvent cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.02953", "pdf_url": "https://arxiv.org/pdf/2511.02953.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "3D\u611f\u77e5 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper focuses on event-based depth estimation, which is relevant to autonomous driving, especially in scenarios where robust perception in challenging environments is required. The proposed dataset, EvtSlowTV, could potentially be useful for depth perception tasks in autonomous vehicles, although the paper primarily discusses event cameras in a broader context rather than specifically autonomous driving."}, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1195": {"title": "Value of Information-Enhanced Exploration in Bootstrapped DQN", "authors": ["Stergios Plataniotis, Charilaos Akasiadis, Georgios Chalkiadakis"], "abstract": "arXiv:2511.02969v1 Announce Type: cross \nEfficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $\\epsilon$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02969", "pdf_url": "https://arxiv.org/pdf/2511.02969.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1196": {"title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications", "authors": ["Esther Sun, Zichu Wu"], "abstract": "arXiv:2511.02979v1 Announce Type: cross \nThe design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional compan- ions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applica- tions in work, gaming, and mental health, highlighting the shift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02979", "pdf_url": "https://arxiv.org/pdf/2511.02979.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1197": {"title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment", "authors": ["Wenbo Lu"], "abstract": "arXiv:2511.03019v1 Announce Type: cross \nVision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03019", "pdf_url": "https://arxiv.org/pdf/2511.03019.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1198": {"title": "Adaptive-Sensorless Monitoring of Shipping Containers", "authors": ["Lingqing Shen, Chi Heem Wong, Misaki Mito, Arnab Chakrabarti"], "abstract": "arXiv:2511.03022v1 Announce Type: cross \nMonitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\\sim$ 2.31$^\\circ$C (vs 2.43$^\\circ$C by sensorless) for temperature and 5.72 $\\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\\sim$ 3.26$^\\circ$C for temperature (vs 3.38$^\\circ$C by sensorless) and 7.70 $\\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.", "categories": ["cs.LG", "cs.AI", "cs.CE"], "abs_url": "https://arxiv.org/abs/2511.03022", "pdf_url": "https://arxiv.org/pdf/2511.03022.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1199": {"title": "Reading Between the Lines: The One-Sided Conversation Problem", "authors": ["Victoria Ebert, Rishabh Singh, Tuochao Chen, Noah A. Smith, Shyamnath Gollakota"], "abstract": "arXiv:2511.03056v1 Announce Type: cross \nConversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03056", "pdf_url": "https://arxiv.org/pdf/2511.03056.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1200": {"title": "Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies", "authors": ["Gaia Grosso, Sai Sumedh R. Hindupur, Thomas Fel, Samuel Bright-Thonney, Philip Harris, Demba Ba"], "abstract": "arXiv:2511.03095v1 Announce Type: cross \nModern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or rare signals can remain hidden within the apparent regularity of normal data, creating a gap in our ability to detect and interpret anomalies. We examine this gap and identify a set of structural desiderata for detection methods operating under minimal prior information: sparsity, to enforce parsimony; locality, to preserve geometric sensitivity; and competition, to promote efficient allocation of model capacity. These principles define a class of self-organizing local kernels that adaptively partition the representation space around regions of statistical imbalance. As an instantiation of these principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained within a semi-supervised Neyman--Pearson framework to locally model the likelihood ratio between a sample that may contain anomalies and a nominal, anomaly-free reference. We provide theoretical insights into the mechanisms that drive detection and self-organization in the proposed model, and demonstrate the effectiveness of this approach on realistic high-dimensional problems of scientific discovery, open-world novelty detection, intrusion detection, and generative-model validation. Our applications span both the natural- and computer-science domains. We demonstrate that ensembles containing only a handful of kernels can identify statistically significant anomalous locations within representation spaces of thousands of dimensions, underscoring both the interpretability, efficiency and scalability of the proposed approach.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03095", "pdf_url": "https://arxiv.org/pdf/2511.03095.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1201": {"title": "Scaling Multi-Agent Environment Co-Design with Diffusion Models", "authors": ["Hao Xiang Li, Michael Amir, Amanda Prorok"], "abstract": "arXiv:2511.03100v1 Announce Type: cross \nThe agent-environment co-design paradigm jointly optimises agent policies and environment configurations in search of improved system performance. With application domains ranging from warehouse logistics to windfarm management, co-design promises to fundamentally change how we deploy multi-agent systems. However, current co-design methods struggle to scale. They collapse under high-dimensional environment design spaces and suffer from sample inefficiency when addressing moving targets inherent to joint optimisation. We address these challenges by developing Diffusion Co-Design (DiCoDe), a scalable and sample-efficient co-design framework pushing co-design towards practically relevant settings. DiCoDe incorporates two core innovations. First, we introduce Projected Universal Guidance (PUG), a sampling technique that enables DiCoDe to explore a distribution of reward-maximising environments while satisfying hard constraints such as spatial separation between obstacles. Second, we devise a critic distillation mechanism to share knowledge from the reinforcement learning critic, ensuring that the guided diffusion model adapts to evolving agent policies using a dense and up-to-date learning signal. Together, these improvements lead to superior environment-policy pairs when validated on challenging multi-agent environment co-design benchmarks including warehouse automation, multi-agent pathfinding and wind farm optimisation. Our method consistently exceeds the state-of-the-art, achieving, for example, 39% higher rewards in the warehouse setting with 66% fewer simulation samples. This sets a new standard in agent-environment co-design, and is a stepping stone towards reaping the rewards of co-design in real world domains.", "categories": ["cs.LG", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2511.03100", "pdf_url": "https://arxiv.org/pdf/2511.03100.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1202": {"title": "CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic", "authors": ["Saad Mankarious, Ayah Zirikly"], "abstract": "arXiv:2511.03102v1 Announce Type: cross \nMental health disorders affect millions worldwide, yet early detection remains a major challenge, particularly for Arabic-speaking populations where resources are limited and mental health discourse is often discouraged due to cultural stigma. While substantial research has focused on English-language mental health detection, Arabic remains significantly underexplored, partly due to the scarcity of annotated datasets. We present CARMA, the first automatically annotated large-scale dataset of Arabic Reddit posts. The dataset encompasses six mental health conditions, such as Anxiety, Autism, and Depression, and a control group. CARMA surpasses existing resources in both scale and diversity. We conduct qualitative and quantitative analyses of lexical and semantic differences between users, providing insights into the linguistic markers of specific mental health conditions. To demonstrate the dataset's potential for further mental health analysis, we perform classification experiments using a range of models, from shallow classifiers to large language models. Our results highlight the promise of advancing mental health detection in underrepresented languages such as Arabic.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03102", "pdf_url": "https://arxiv.org/pdf/2511.03102.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1203": {"title": "Adaptive Detection of Software Aging under Workload Shift", "authors": ["Rafael Jos\\'e Moura, Maria Gizele Nascimento, Fumio Machida, Ermeson Andrade"], "abstract": "arXiv:2511.03103v1 Announce Type: cross \nSoftware aging is a phenomenon that affects long-running systems, leading to progressive performance degradation and increasing the risk of failures. To mitigate this problem, this work proposes an adaptive approach based on machine learning for software aging detection in environments subject to dynamic workload conditions. We evaluate and compare a static model with adaptive models that incorporate adaptive detectors, specifically the Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept drift scenarios and applied in this work to handle workload shifts. Experiments with simulated sudden, gradual, and recurring workload transitions show that static models suffer a notable performance drop when applied to unseen workload profiles, whereas the adaptive model with ADWIN maintains high accuracy, achieving an F1-Score above 0.93 in all analyzed scenarios.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03103", "pdf_url": "https://arxiv.org/pdf/2511.03103.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1204": {"title": "FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation", "authors": ["Jiameng Chen, Yida Xiong, Kun Li, Hongzhi Zhang, Xiantao Cai, Wenbin Hu, Jia Wu"], "abstract": "arXiv:2511.03113v1 Announce Type: cross \nComputational antibody design holds immense promise for therapeutic discovery, yet existing generative models are fundamentally limited by two core challenges: (i) a lack of dynamical consistency, which yields physically implausible structures, and (ii) poor generalization due to data scarcity and structural bias. We introduce FP-AbDiff, the first antibody generator to enforce Fokker-Planck Equation (FPE) physics along the entire generative trajectory. Our method minimizes a novel FPE residual loss over the mixed manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising scores to assemble into a globally coherent probability flow. This physics-informed regularizer is synergistically integrated with deep biological priors within a state-of-the-art SE(3)-equivariant diffusion framework. Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean Square Deviation of 0.99 {\\AA} when superposing on the variable region, a 25% improvement over the previous state-of-the-art model, AbX, and the highest reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored in the more challenging six-CDR co-design task, where our model delivers consistently superior geometric precision, cutting the average full-chain Root Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness and generalizability, establishing a principled approach for physically faithful and functionally viable antibody design.", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2511.03113", "pdf_url": "https://arxiv.org/pdf/2511.03113.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1205": {"title": "An Augmentation Overlap Theory of Contrastive Learning", "authors": ["Qi Zhang, Yifei Wang, Yisen Wang"], "abstract": "arXiv:2511.03114v1 Announce Type: cross \nRecently, self-supervised contrastive learning has achieved great success on various tasks. However, its underlying working mechanism is yet unclear. In this paper, we first provide the tightest bounds based on the widely adopted assumption of conditional independence. Further, we relax the conditional independence assumption to a more practical assumption of augmentation overlap and derive the asymptotically closed bounds for the downstream performance. Our proposed augmentation overlap theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Moreover, from the newly derived augmentation overlap perspective, we develop an unsupervised metric for the representation evaluation of contrastive learning, which aligns well with the downstream performance almost without relying on additional modules. Code is available at https://github.com/PKU-ML/GARC.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03114", "pdf_url": "https://arxiv.org/pdf/2511.03114.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1206": {"title": "Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning", "authors": ["Botong. Zhao, Xubin. Wang, Shujing. Lyu, Yue. Lu"], "abstract": "arXiv:2511.03120v1 Announce Type: cross \nIntegrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03120", "pdf_url": "https://arxiv.org/pdf/2511.03120.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1207": {"title": "Control Barrier Function for Aligning Large Language Models", "authors": ["Yuya Miyaoka, Masaki Inoue"], "abstract": "arXiv:2511.03121v1 Announce Type: cross \nThis paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the CBF safety filter to the predicted token generated from the baseline LLM, to intervene in the generated text. The safety filter includes two significant advantages: this safety filter is an add-on type, allowing it to be used for alignment purposes without fine-tuning the baseline LLM, and if there is an evaluation model regarding the desired alignment, it can be directly applied to the filter design. The overall text-generation system is implemented with open-source language models, aiming to generate positive text.", "categories": ["cs.CL", "cs.AI", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.03121", "pdf_url": "https://arxiv.org/pdf/2511.03121.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1208": {"title": "EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture", "authors": ["Seunghee Han, Yeonghun Kang, Taeun Bae, Varinia Bernales, Alan Aspuru-Guzik, Jihan Kim"], "abstract": "arXiv:2511.03122v1 Announce Type: cross \nDesigning materials with targeted properties remains challenging due to the vastness of chemical space and the scarcity of property-labeled data. While recent advances in generative models offer a promising way for inverse design, most approaches require large datasets and must be retrained for every new target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a hybrid diffusion-transformer framework that overcomes these limitations through a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired properties to chemically meaningful descriptors followed by (2) a transformer model (Desc2MOF) that generates structures from these descriptors. This modular hybrid design enables minimal retraining and maintains high accuracy even under small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95% validity and 84% hit rate, representing significant improvements of up to 57% in validity and 14% in hit rate compared to existing methods, while remaining effective with only 1,000 training samples. Moreover, our model successfully performed conditional generation across 29 diverse property datasets, including CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models have not. This work presents a data-efficient, generalizable approach to the inverse design of diverse MOFs and highlights the potential of modular inverse design workflows for broader materials discovery.", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03122", "pdf_url": "https://arxiv.org/pdf/2511.03122.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1209": {"title": "Optimal Boundary Control of Diffusion on Graphs via Linear Programming", "authors": ["Harbir Antil, Rainald L\\\"ohner, Felipe P\\'erez"], "abstract": "arXiv:2511.03129v1 Announce Type: cross \nWe propose a linear programming (LP) framework for steady-state diffusion and flux optimization on geometric networks. The state variable satisfies a discrete diffusion law on a weighted, oriented graph, where conductances are scaled by edge lengths to preserve geometric fidelity. Boundary potentials act as controls that drive interior fluxes according to a linear network Laplacian. The optimization problem enforces physically meaningful sign and flux-cap constraints at all boundary edges, derived directly from a gradient bound. This yields a finite-dimensional LP whose feasible set is polyhedral, and whose boundedness and solvability follow from simple geometric or algebraic conditions on the network data.\n  We prove that under the absence of negative recession directions--automatically satisfied in the presence of finite box bounds, flux caps, or sign restrictions--the LP admits a global minimizer. Several sufficient conditions guaranteeing boundedness of the feasible region are identified, covering both full-rank and rank-deficient flux maps. The analysis connects classical results such as the Minkowski--Weyl decomposition, Hoffman's bound, and the fundamental theorem of linear programming with modern network-based diffusion modeling.\n  Two large-scale examples illustrate the framework: (i) A typical large stadium in a major modern city, which forms a single connected component with relatively uniform corridor widths, and a (ii) A complex street network emanating from a large, historical city center, which forms a multi-component system.", "categories": ["math.OC", "cs.AI", "physics.comp-ph"], "abs_url": "https://arxiv.org/abs/2511.03129", "pdf_url": "https://arxiv.org/pdf/2511.03129.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1210": {"title": "Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response", "authors": ["Thomas Manzini, Priyankari Perali, Robin R. Murphy"], "abstract": "arXiv:2511.03132v1 Announce Type: cross \nThis paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.", "categories": ["cs.CV", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.03132", "pdf_url": "https://arxiv.org/pdf/2511.03132.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1211": {"title": "From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents", "authors": ["Erfan Shayegani, Jina Suh, Andy Wilson, Nagu Rangan, Javier Hernandez"], "abstract": "arXiv:2511.03143v1 Announce Type: cross \nEmpathy is a critical factor in fostering positive user experiences in conversational AI. While models can display empathy, it is often generic rather than tailored to specific tasks and contexts. In this work, we introduce a novel framework for developing and evaluating context-specific empathetic large language models (LLMs). We first analyze a real-world conversational dataset consisting of 672 multi-turn conversations across 8 tasks, revealing significant differences in terms of expected and experienced empathy before and after the conversations, respectively. To help minimize this gap, we develop a synthetic multi-turn conversational generation pipeline and steer responses toward our defined empathy patterns based on the context that more closely matches users' expectations. We then train empathetic expert adapters for context-specific empathy that specialize in varying empathy levels based on the recognized task. Our empirical results demonstrate a significant gap reduction of 72.66% between perceived and desired empathy with scores increasing by an average factor of 2.43 as measured by our metrics and reward models. Additionally, our trained empathetic expert adapters demonstrate superior effectiveness in preserving empathy patterns throughout conversation turns, outperforming system prompts, which tend to dramatically diminish in impact as conversations lengthen.", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03143", "pdf_url": "https://arxiv.org/pdf/2511.03143.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1212": {"title": "Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction", "authors": ["Atif Hassan, Tarun Kumar, Ashish Mishra, Sergey Serebryakov, Satish Kumar Mopur, Phanidhar Koganti, Murthy Chelankuri, Ramanagopal Vogety, Suparna Bhattacharya, Martin Foltin"], "abstract": "arXiv:2511.03149v1 Announce Type: cross \nForecasting anomalies (anomaly prediction) in multivariate time series from different real-world, dynamic, and complex systems is vital for preempting critical failures, leading to a substantial minimization in operational costs and human labor. Yet, existing methods are limited to specific systems while failing to generalize to evolving anomaly patterns over time. In contrast, pretrained Time Series Foundation Models (TSFMs) have recently demonstrated strong generalization and zero-shot forecasting capabilities. However, their potential remains untapped for anomaly prediction, a task fundamentally different from forecasting normal behavior. Thus, we present Forecast2Anomaly (F2A), a novel framework that empowers TSFMs with anomaly prediction abilities through two key innovations. First, we propose a joint forecast-anomaly loss that fine-tunes TSFMs to accurately forecast future signals even at anomalous time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module that retrieves historically relevant horizons and conditions predictions on them. This component dynamically adapts to distributional shifts at inference time, enabling F2A to track evolving anomalies without requiring model updates. By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap between robust TSFM zero-shot forecasting and zero-shot anomaly prediction. Extensive experiments across 16 diverse datasets and multiple TSFM backbones show that F2A consistently outperforms state-of-the-art methods, offering a scalable, zero-shot anomaly prediction solution for real-world applications.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03149", "pdf_url": "https://arxiv.org/pdf/2511.03149.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1213": {"title": "Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment", "authors": ["Srishti Yadav, Jasmina Gajcin, Erik Miehling, Elizabeth Daly"], "abstract": "arXiv:2511.03152v1 Announce Type: cross \nUnderstanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03152", "pdf_url": "https://arxiv.org/pdf/2511.03152.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u98ce\u9669\u8bc4\u4f30 / \u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u98ce\u9669\u7ba1\u7406", "reason": "The paper discusses the application of a framework for stakeholder-based risk assessment, specifically including autonomous vehicles as one of the use cases. Although the focus is not on technical aspects like perception or control, the inclusion of autonomous vehicles in risk evaluation suggests relevance to the autonomous driving domain, particularly in terms of risk assessment and human-centered governance in autonomous systems."}, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1214": {"title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring", "authors": ["Khouloud Oueslati, Maxime Lamothe, Foutse Khomh"], "abstract": "arXiv:2511.03153v1 Announce Type: cross \nLarge Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03153", "pdf_url": "https://arxiv.org/pdf/2511.03153.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1215": {"title": "GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes", "authors": ["Hajung Kim, Jueon Park, Junseok Choe, Sheunheun Baek, Hyeon Hwang, Jaewoo Kang"], "abstract": "arXiv:2511.03170v1 Announce Type: cross \nQuantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.", "categories": ["cs.CE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03170", "pdf_url": "https://arxiv.org/pdf/2511.03170.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1216": {"title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies", "authors": ["Arsalan Muhammad, Wasiu Akande Ahmed, Omada Friday Ojonugwa, Paul Puspendu Biswas"], "abstract": "arXiv:2511.03173v1 Announce Type: cross \nThe rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.", "categories": ["astro-ph.EP", "cs.AI", "cs.LG", "cs.RO"], "abs_url": "https://arxiv.org/abs/2511.03173", "pdf_url": "https://arxiv.org/pdf/2511.03173.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1217": {"title": "Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality", "authors": ["Mingtao Zhang, Guoli Yang, Zhanxing Zhu, Mengzhu Wang, Xiaoying Bai"], "abstract": "arXiv:2511.03190v1 Announce Type: cross \nAttention mechanisms have been extensively employed in various applications, including time series modeling, owing to their capacity to capture intricate dependencies; however, their utility is often constrained by quadratic computational complexity, which impedes scalability for long sequences. In this work, we propose a novel linear attention mechanism designed to overcome these limitations. Our approach is grounded in a theoretical demonstration that entropy, as a strictly concave function on the probability simplex, implies that distributions with aligned probability rankings and similar entropy values exhibit structural resemblance. Building on this insight, we develop an efficient approximation algorithm that computes the entropy of dot-product-derived distributions with only linear complexity, enabling the implementation of a linear attention mechanism based on entropy equality. Through rigorous analysis, we reveal that the effectiveness of attention in spatio-temporal time series modeling may not primarily stem from the non-linearity of softmax but rather from the attainment of a moderate and well-balanced weight distribution. Extensive experiments on four spatio-temporal datasets validate our method, demonstrating competitive or superior forecasting performance while achieving substantial reductions in both memory usage and computational time.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03190", "pdf_url": "https://arxiv.org/pdf/2511.03190.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1218": {"title": "A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies", "authors": ["Hassan Wasswa, Hussein Abbass, Timothy Lynar"], "abstract": "arXiv:2511.03201v1 Announce Type: cross \nIn an effort to counter the increasing IoT botnet-based attacks, state-of-the-art deep learning methods have been proposed and have achieved impressive detection accuracy. However, their computational intensity restricts deployment on resource-constrained IoT devices, creating a critical need for lightweight detection models. A common solution to this challenge is model compression via quantization. This study proposes a VAE-MLP model framework where an MLP-based classifier is trained on 8-dimensional latent vectors derived from the high-dimensional train data using the encoder component of a pretrained variational autoencoder (VAE). Two widely used quantization strategies--Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ)--are then systematically evaluated in terms of their impact on detection performance, storage efficiency, and inference latency using two benchmark IoT botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with respect to detection accuracy, the QAT strategy experienced a more noticeable decline,whereas PTQ incurred only a marginal reduction compared to the original unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in size, while QAT achieved a 3x speedup and 24x compression, demonstrating the practicality of quantization for device-level IoT botnet detection.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03201", "pdf_url": "https://arxiv.org/pdf/2511.03201.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1219": {"title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models", "authors": ["Kuei-Chun Kao, Hsu Tzu-Yin, Yunqi Hong, Ruochen Wang, Cho-Jui Hsieh"], "abstract": "arXiv:2511.03206v1 Announce Type: cross \nRecently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03206", "pdf_url": "https://arxiv.org/pdf/2511.03206.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1220": {"title": "Retrofitters, pragmatists and activists: Public interest litigation for accountable automated decision-making", "authors": ["Henry Fraser, Zahra Stardust"], "abstract": "arXiv:2511.03211v1 Announce Type: cross \nThis paper examines the role of public interest litigation in promoting accountability for AI and automated decision-making (ADM) in Australia. Since ADM regulatio faces geopolitical headwinds, effective governance will have to rely at least in part on the enforcement of existing laws. Drawing on interviews with Australian public interest litigators, technology policy activists, and technology law scholars, the paper positions public interest litigation as part of a larger ecosystem for transparency, accountability and justice with respect to ADM. It builds on one participants's characterisation of litigation about ADM as an exercise in legal retrofitting: adapting old laws to new circumstances. The paper's primary contribution is to aggregate, organise and present original insights on pragmatic strategies and tactics for effective public interest litigation about ADM. Naturally, it also contends with the limits of these strategies, and of the legal system. Where limits are, however, capable of being overcome, the paper presents findings on urgent needs: the enabling institutional arrangements without which effective litigation and accountability will falter. The paper is relevant to law and technology scholars; individuals and groups harmed by ADM; public interest litigators and technology lawyers; civil society and advocacy organisations; and policymakers.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03211", "pdf_url": "https://arxiv.org/pdf/2511.03211.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1221": {"title": "LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval", "authors": ["Wenchang Lei, Ping Zou, Yue Wang, Feng Sun, Lei Zhao"], "abstract": "arXiv:2511.03214v1 Announce Type: cross \nLarge language models (LLMs) exhibit strong semantic understanding, yet struggle when user instructions involve ambiguous or conceptually misaligned terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity by extracting meta-relations-inheritance, alias, and composition-from natural language. The model further employs a reflection mechanism to validate these meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these relations and related descriptions are dynamically supplied to the LLM, improving its ability to interpret concepts and generate accurate responses. Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely on extended context windows, our method enables large language models to process texts of any length without the need for truncation. Experiments on standard benchmarks demonstrate that the LGM consistently outperforms existing RAG baselines.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03214", "pdf_url": "https://arxiv.org/pdf/2511.03214.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1222": {"title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification", "authors": ["Shaghayegh Kolli, Richard Rosenbaum, Timo Cavelius, Lasse Strothe, Andrii Lata, Jana Diesner"], "abstract": "arXiv:2511.03217v1 Announce Type: cross \nLarge language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one - hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task- specific fine - tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "abs_url": "https://arxiv.org/abs/2511.03217", "pdf_url": "https://arxiv.org/pdf/2511.03217.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1223": {"title": "Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide", "authors": ["Alexander Htet Kyaw, Lenin Ravindranath Sivalingam"], "abstract": "arXiv:2511.03227v1 Announce Type: cross \nWe present a node-based storytelling system for multimodal content generation. The system represents stories as graphs of nodes that can be expanded, edited, and iteratively refined through direct user edits and natural-language prompts. Each node can integrate text, images, audio, and video, allowing creators to compose multimodal narratives. A task selection agent routes between specialized generative tasks that handle story generation, node structure reasoning, node diagram formatting, and context generation. The interface supports targeted editing of individual nodes, automatic branching for parallel storylines, and node-based iterative refinement. Our results demonstrate that node-based editing supports control over narrative structure and iterative generation of text, images, audio, and video. We report quantitative outcomes on automatic story outline generation and qualitative observations of editing workflows. Finally, we discuss current limitations such as scalability to longer narratives and consistency across multiple nodes, and outline future work toward human-in-the-loop and user-centered creative AI tools.", "categories": ["cs.HC", "cs.AI", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.03227", "pdf_url": "https://arxiv.org/pdf/2511.03227.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1224": {"title": "GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models", "authors": ["Zhibin Wang, Zhixing Zhang, Shuqi Wang, Xuanting Xie, Zhao Kang"], "abstract": "arXiv:2511.03251v1 Announce Type: cross \nGraph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.", "categories": ["cs.LG", "cs.AI", "cs.SI"], "abs_url": "https://arxiv.org/abs/2511.03251", "pdf_url": "https://arxiv.org/pdf/2511.03251.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1225": {"title": "Generative deep learning for foundational video translation in ultrasound", "authors": ["Nikolina Tomic Roshni Bhatnagar, Sarthak Jain, Connor Lau, Tien-Yu Liu, Laura Gambini, Rima Arnaout"], "abstract": "arXiv:2511.03255v1 Announce Type: cross \nDeep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03255", "pdf_url": "https://arxiv.org/pdf/2511.03255.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1226": {"title": "Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature", "authors": ["Ranul Dayarathne, Uvini Ranaweera, Upeksha Ganegoda"], "abstract": "arXiv:2511.03261v1 Announce Type: cross \nRetrieval Augmented Generation (RAG) is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination. Thus, the increasing prominence of RAG alongside Large Language Models (LLMs) has sparked interest in comparing the performance of different LLMs in question-answering (QA) in diverse domains. This study compares the performance of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA tasks within the computer science literature leveraging RAG support. Evaluation metrics employed in the study include accuracy and precision for binary questions and ranking by a human expert, ranking by Google's AI model Gemini, alongside cosine similarity for long-answer questions. GPT-3.5, when paired with RAG, effectively answers binary and long-answer questions, reaffirming its status as an advanced LLM. Regarding open-source LLMs, Mistral AI's Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b reports the shortest average latency in generating responses, whereas LLaMa2-7b-chat by Meta reports the highest average latency. This research underscores the fact that open-source LLMs, too, can go hand in hand with proprietary models like GPT-3.5 with better infrastructure.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03261", "pdf_url": "https://arxiv.org/pdf/2511.03261.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1227": {"title": "When Generative Artificial Intelligence meets Extended Reality: A Systematic Review", "authors": ["Xinyu Ning, Yan Zhuo, Xian Wang, Chan-In Devin Sio, Lik-Hang Lee"], "abstract": "arXiv:2511.03282v1 Announce Type: cross \nWith the continuous advancement of technology, the application of generative artificial intelligence (AI) in various fields is gradually demonstrating great potential, particularly when combined with Extended Reality (XR), creating unprecedented possibilities. This survey article systematically reviews the applications of generative AI in XR, covering as much relevant literature as possible from 2023 to 2025. The application areas of generative AI in XR and its key technology implementations are summarised through PRISMA screening and analysis of the final 26 articles. The survey highlights existing articles from the last three years related to how XR utilises generative AI, providing insights into current trends and research gaps. We also explore potential opportunities for future research to further empower XR through generative AI, providing guidance and information for future generative XR research.", "categories": ["cs.HC", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03282", "pdf_url": "https://arxiv.org/pdf/2511.03282.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1228": {"title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics", "authors": ["Mauro Cettolo, Marco Gaido, Matteo Negri, Sara Papi, Luisa Bentivogli"], "abstract": "arXiv:2511.03295v1 Announce Type: cross \nAutomatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03295", "pdf_url": "https://arxiv.org/pdf/2511.03295.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1229": {"title": "Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods", "authors": ["Felix St\\\"orck, Fabian Hinder, Barbara Hammer"], "abstract": "arXiv:2511.03304v1 Announce Type: cross \nWith the on-going integration of machine learning systems into the everyday social life of millions the notion of fairness becomes an ever increasing priority in their development. Fairness notions commonly rely on protected attributes to assess potential biases. Here, the majority of literature focuses on discrete setups regarding both target and protected attributes. The literature on continuous attributes especially in conjunction with regression -- we refer to this as \\emph{continuous fairness} -- is scarce. A common strategy is iterative null-space projection which as of now has only been explored for linear models or embeddings such as obtained by a non-linear encoder. We improve on this by generalizing to kernel methods, significantly extending the scope. This yields a model and fairness-score agnostic method for kernel embeddings applicable to continuous protected attributes. We demonstrate that our novel approach in conjunction with Support Vector Regression (SVR) provides competitive or improved performance across multiple datasets in comparisons to other contemporary methods.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03304", "pdf_url": "https://arxiv.org/pdf/2511.03304.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1230": {"title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks", "authors": ["Jindong Hong, Tianjie Chen, Lingjie Luo, Chuanyang Zheng, Ting Xu, Haibao Yu, Jianing Qiu, Qianzhong Chen, Suning Huang, Yan Xu, Yong Gui, Yijun He, Jiankai Sun"], "abstract": "arXiv:2511.03328v1 Announce Type: cross \nA recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes (normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these \"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active \"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03328", "pdf_url": "https://arxiv.org/pdf/2511.03328.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1231": {"title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning", "authors": ["Shenghua Wang, Zhen Yin"], "abstract": "arXiv:2511.03330v1 Announce Type: cross \nThe rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.", "categories": ["cs.IR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03330", "pdf_url": "https://arxiv.org/pdf/2511.03330.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1232": {"title": "Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances", "authors": ["Riasad Alvi, Sayeem Been Zaman, Wasimul Karim, Arefin Ittesafun Abian, Mohaimenul Azam Khan Raiaan, Saddam Mukta, Md Rafi Ur Rashid, Md Rafiqul Islam, Yakub Sebastian, Sami Azam"], "abstract": "arXiv:2511.03354v1 Announce Type: cross \nGenerative artificial intelligence (GenAI) has become a transformative approach in bioinformatics that often enables advancements in genomics, proteomics, transcriptomics, structural biology, and drug discovery. To systematically identify and evaluate these growing developments, this review proposed six research questions (RQs), according to the preferred reporting items for systematic reviews and meta-analysis methods. The objective is to evaluate impactful GenAI strategies in methodological advancement, predictive performance, and specialization, and to identify promising approaches for advanced modeling, data-intensive discovery, and integrative biological analysis. RQ1 highlights diverse applications across multiple bioinformatics subfields (sequence analysis, molecular design, and integrative data modeling), which demonstrate superior performance over traditional methods through pattern recognition and output generation. RQ2 reveals that adapted specialized model architectures outperformed general-purpose models, an advantage attributed to targeted pretraining and context-aware strategies. RQ3 identifies significant benefits in the bioinformatics domains, focusing on molecular analysis and data integration, which improves accuracy and reduces errors in complex analysis. RQ4 indicates improvements in structural modeling, functional prediction, and synthetic data generation, validated by established benchmarks. RQ5 suggests the main constraints, such as the lack of scalability and biases in data that impact generalizability, and proposes future directions focused on robust evaluation and biologically grounded modeling. RQ6 examines that molecular datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly support the training and generalization of GenAI models.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03354", "pdf_url": "https://arxiv.org/pdf/2511.03354.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1233": {"title": "Open Source State-Of-the-Art Solution for Romanian Speech Recognition", "authors": ["Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu"], "abstract": "arXiv:2511.03361v1 Announce Type: cross \nIn this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.", "categories": ["eess.AS", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03361", "pdf_url": "https://arxiv.org/pdf/2511.03361.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1234": {"title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models", "authors": ["Gahyeon Kim, Sohee Kim, Seokju Lee"], "abstract": "arXiv:2511.03367v1 Announce Type: cross \nRecent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03367", "pdf_url": "https://arxiv.org/pdf/2511.03367.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1235": {"title": "Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain Gliomas", "authors": ["Syed Muqeem Mahmood, Hassan Mohy-ud-Din"], "abstract": "arXiv:2511.03376v1 Announce Type: cross \nWe present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N = 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at https://github.com/ATPLab-LUMS/CIM-LLM.", "categories": ["eess.IV", "cs.AI", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2511.03376", "pdf_url": "https://arxiv.org/pdf/2511.03376.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1236": {"title": "Adaptable Hindsight Experience Replay for Search-Based Learning", "authors": ["Alexandros Vazaios, Jannis Brugger, Cedric Derstroff, Kristian Kersting, Mira Mezini"], "abstract": "arXiv:2511.03405v1 Announce Type: cross \nAlphaZero-like Monte Carlo Tree Search systems, originally introduced for two-player games, dynamically balance exploration and exploitation using neural network guidance. This combination makes them also suitable for classical search problems. However, the original method of training the network with simulation results is limited in sparse reward settings, especially in the early stages, where the network cannot yet give guidance. Hindsight Experience Replay (HER) addresses this issue by relabeling unsuccessful trajectories from the search tree as supervised learning signals. We introduce Adaptable HER (\\ours{}), a flexible framework that integrates HER with AlphaZero, allowing easy adjustments to HER properties such as relabeled goals, policy targets, and trajectory selection. Our experiments, including equation discovery, show that the possibility of modifying HER is beneficial and surpasses the performance of pure supervised or reinforcement learning.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03405", "pdf_url": "https://arxiv.org/pdf/2511.03405.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1237": {"title": "Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement", "authors": ["Shihai Wang, Tao Chen"], "abstract": "arXiv:2511.03421v1 Announce Type: cross \nElicited performance requirements need to be quantified for compliance in different engineering tasks, e.g., configuration tuning and performance testing. Much existing work has relied on manual quantification, which is expensive and error-prone due to the imprecision. In this paper, we present LQPR, a highly efficient automatic approach for performance requirements quantification.LQPR relies on a new theoretical framework that converts quantification as a classification problem. Despite the prevalent applications of Large Language Models (LLMs) for requirement analytics, LQPR takes a different perspective to address the classification: we observed that performance requirements can exhibit strong patterns and are often short/concise, therefore we design a lightweight linguistically induced matching mechanism. We compare LQPR against nine state-of-the-art learning-based approaches over diverse datasets, demonstrating that it is ranked as the sole best for 75% or more cases with two orders less cost. Our work proves that, at least for performance requirement quantification, specialized methods can be more suitable than the general LLM-driven approaches.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03421", "pdf_url": "https://arxiv.org/pdf/2511.03421.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1238": {"title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond", "authors": ["Botao 'Amber' Hu, Helena Rong"], "abstract": "arXiv:2511.03434v1 Announce Type: cross \nAs the \"agentic web\" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.", "categories": ["cs.HC", "cs.AI", "cs.MA", "cs.NI", "cs.SI"], "abs_url": "https://arxiv.org/abs/2511.03434", "pdf_url": "https://arxiv.org/pdf/2511.03434.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1239": {"title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field", "authors": ["Doria Bonzi, Alexandre Guiggi, Fr\\'ed\\'eric B\\'echet, Carlos Ramisch, Benoit Favre"], "abstract": "arXiv:2511.03441v1 Announce Type: cross \nCritical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03441", "pdf_url": "https://arxiv.org/pdf/2511.03441.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1240": {"title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control", "authors": ["Jianbo Yuan, Haohua Zhu, Jing Dai, Sheng Yi"], "abstract": "arXiv:2511.03481v1 Announce Type: cross \nThe human hand plays a vital role in daily life and industrial applications, yet replicating its multifunctional capabilities-including motion, sensing, and coordinated manipulation-with robotic systems remains a formidable challenge. Developing a dexterous robotic hand requires balancing human-like agility with engineering constraints such as complexity, size-to-weight ratio, durability, and force-sensing performance. This letter presents Dex-Hand 021, a high-performance, cable-driven five-finger robotic hand with 12 active and 7 passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight 1 kg design. We propose a proprioceptive force-sensing-based admittance control method to enhance manipulation. Experimental results demonstrate its superior performance: a single-finger load capacity exceeding 10 N, fingertip repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared to PID control, joint torques in multi-object grasping are reduced by 31.19%, significantly improves force-sensing capability while preventing overload during collisions. The hand excels in both power and precision grasps, successfully executing 33 GRASP taxonomy motions and complex manipulation tasks. This work advances the design of lightweight, industrial-grade dexterous hands and enhances proprioceptive control, contributing to robotic manipulation and intelligent manufacturing.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03481", "pdf_url": "https://arxiv.org/pdf/2511.03481.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1241": {"title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications", "authors": ["Lei Fu, Sahar Salimpour, Leonardo Militano, Harry Edelman, Jorge Pe\\~na Queralta, Giovanni Toffetti"], "abstract": "arXiv:2511.03497v1 Announce Type: cross \nAgentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.", "categories": ["cs.RO", "cs.AI", "cs.SE"], "abs_url": "https://arxiv.org/abs/2511.03497", "pdf_url": "https://arxiv.org/pdf/2511.03497.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u79fb\u52a8\u673a\u5668\u4eba\u6570\u636e\u5206\u6790 / \u673a\u5668\u4eba\u611f\u77e5\u4e0e\u8f68\u8ff9\u5206\u6790\u5de5\u5177", "reason": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eROS/ROS2\u7684MCP\u670d\u52a1\u5668\uff0c\u7528\u4e8e\u901a\u8fc7LLMs\u5206\u6790\u548c\u53ef\u89c6\u5316\u673a\u5668\u4eba\u6570\u636e\uff0c\u5305\u62ec\u8f68\u8ff9\u3001\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u3001\u53d8\u6362\u7b49\u3002\u8fd9\u4e9b\u7c7b\u578b\u7684\u6570\u636e\u548c\u5206\u6790\u65b9\u6cd5\u4e0e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4f7f\u7528\u7684\u611f\u77e5\u548c\u8f68\u8ff9\u6570\u636e\u5904\u7406\u5bc6\u5207\u76f8\u5173\uff0c\u867d\u7136\u8bba\u6587\u4e3b\u8981\u9762\u5411\u901a\u7528\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u4f46\u5de5\u5177\u548c\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u6570\u636e\u5206\u6790\u548c\u4eff\u771f\u8bc4\u4f30\u3002"}, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1242": {"title": "A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways", "authors": ["Gabriel Spadon, Vaishnav Vaidheeswaran, Claudio DiBacco"], "abstract": "arXiv:2511.03499v1 Announce Type: cross \nMarine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.", "categories": ["cs.CE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03499", "pdf_url": "https://arxiv.org/pdf/2511.03499.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1243": {"title": "Efficient Neural Networks with Discrete Cosine Transform Activations", "authors": ["Marc Martinez-Gost, Sara Pepe, Ana P\\'erez-Neira, Miguel \\'Angel Lagunas"], "abstract": "arXiv:2511.03531v1 Announce Type: cross \nIn this paper, we extend our previous work on the Expressive Neural Network (ENN), a multilayer perceptron with adaptive activation functions parametrized using the Discrete Cosine Transform (DCT). Building upon previous work that demonstrated the strong expressiveness of ENNs with compact architectures, we now emphasize their efficiency, interpretability and pruning capabilities. The DCT-based parameterization provides a structured and decorrelated representation that reveals the functional role of each neuron and allows direct identification of redundant components. Leveraging this property, we propose an efficient pruning strategy that removes unnecessary DCT coefficients with negligible or no loss in performance. Experimental results across classification and implicit neural representation tasks confirm that ENNs achieve state-of-the-art accuracy while maintaining a low number of parameters. Furthermore, up to 40% of the activation coefficients can be safely pruned, thanks to the orthogonality and bounded nature of the DCT basis. Overall, these findings demonstrate that the ENN framework offers a principled integration of signal processing concepts into neural network design, achieving a balanced trade-off between expressiveness, compactness, and interpretability.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03531", "pdf_url": "https://arxiv.org/pdf/2511.03531.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1244": {"title": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties", "authors": ["Roberta Di Marino, Giovanni Dioguardi, Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Flora Amato, Vincenzo Moscato"], "abstract": "arXiv:2511.03542v1 Announce Type: cross \nMedical question answering systems face deployment challenges including hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains. Here, we present SOLVE-Med, a multi-agent architecture combining domain-specialized small language models for complex medical queries. The system employs a Router Agent for dynamic specialist selection, ten specialized models (1B parameters each) fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses. Evaluated on Italian medical forum data across ten specialties, SOLVE-Med achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters while enabling local deployment. Our code is publicly available on GitHub: https://github.com/PRAISELab-PicusLab/SOLVE-Med.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03542", "pdf_url": "https://arxiv.org/pdf/2511.03542.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1245": {"title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding", "authors": ["Ziv Nevo, Orna Raz, Karen Yorav"], "abstract": "arXiv:2511.03549v1 Announce Type: cross \nUnderstanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub -- such as pull request descriptions, issue descriptions and discussions, and commit messages -- to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the code's purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanations' quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03549", "pdf_url": "https://arxiv.org/pdf/2511.03549.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1246": {"title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark", "authors": ["Sofie Helene Bruun, Dan Saattrup Smart"], "abstract": "arXiv:2511.03553v1 Announce Type: cross \nMeasuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03553", "pdf_url": "https://arxiv.org/pdf/2511.03553.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1247": {"title": "AILA--First Experiments with Localist Language Models", "authors": ["Joachim Diederich"], "abstract": "arXiv:2511.03559v1 Announce Type: cross \nThis paper presents the first empirical demonstration of controllable locality in transformer language models, a novel architectural framework that enables continuous control over the degree of representation localization through a tunable locality dial parameter. Unlike traditional language models that rely exclusively on distributed representations, our approach allows dynamic interpolation between highly interpretable localist encodings and efficient distributed representations without requiring model retraining. We conducted experiments on the WikiText corpus using a two-layer transformer architecture, systematically varying the locality parameter {\\lambda} across the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our results demonstrate that localist configurations achieve dramatically lower attention entropy, with {\\lambda} = 1.0 yielding 5.36 bits compared to 7.18 bits at {\\lambda} = 0.0, while maintaining substantially higher pointer fidelity scores reflecting stronger alignment with rule-specified targets. Prediction experiments reveal that intermediate locality values optimize the tradeoff between interpretability and performance, with {\\lambda} = 0.6 achieving test perplexity of 4.65 and accuracy of 84.7%. These findings establish that localist language models provide a practical framework for applications in regulated domains requiring both transparency and capability, offering precise mathematical control over the interpretability-performance spectrum through explicit penalty thresholds and information-theoretic design principles.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03559", "pdf_url": "https://arxiv.org/pdf/2511.03559.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1248": {"title": "Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances", "authors": ["Iason Chrysomallis, Georgios Chalkiadakis"], "abstract": "arXiv:2511.03565v1 Announce Type: cross \nImitation learning (IL) enables agents to acquire skills by observing and replicating the behavior of one or multiple experts. In recent years, advances in deep learning have significantly expanded the capabilities and scalability of imitation learning across a range of domains, where expert data can range from full state-action trajectories to partial observations or unlabeled sequences. Alongside this growth, novel approaches have emerged, with new methodologies being developed to address longstanding challenges such as generalization, covariate shift, and demonstration quality. In this survey, we review the latest advances in imitation learning research, highlighting recent trends, methodological innovations, and practical applications. We propose a novel taxonomy that is distinct from existing categorizations to better reflect the current state of the IL research stratum and its trends. Throughout the survey, we critically examine the strengths, limitations, and evaluation practices of representative works, and we outline key challenges and open directions for future research.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03565", "pdf_url": "https://arxiv.org/pdf/2511.03565.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1249": {"title": "Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution", "authors": ["Aniol Civit, Antonio Andriella, Carles Sierra, Guillem Aleny\\`a"], "abstract": "arXiv:2511.03576v1 Announce Type: cross \nWhile personalisation in Human-Robot Interaction (HRI) has advanced significantly, most existing approaches focus on single-user adaptation, overlooking scenarios involving multiple stakeholders with potentially conflicting preferences. To address this, we propose the Multi-User Preferences Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user personalisation framework based on Quantitative Bipolar Argumentation Frameworks (QBAFs) that explicitly models and resolves multi-user preference conflicts. Unlike prior work in Argumentation Frameworks, which typically assumes static inputs, our approach is tailored to robotics: it incorporates both users' arguments and the robot's dynamic observations of the environment, allowing the system to adapt over time and respond to changing contexts. Preferences, both positive and negative, are represented as arguments whose strength is recalculated iteratively based on new information. The framework's properties and capabilities are presented and validated through a realistic case study, where an assistive robot mediates between the conflicting preferences of a caregiver and a care recipient during a frailty assessment task. This evaluation further includes a sensitivity analysis of argument base scores, demonstrating how preference outcomes can be shaped by user input and contextual observations. By offering a transparent, structured, and context-sensitive approach to resolving competing user preferences, this work advances the field of multi-user HRI. It provides a principled alternative to data-driven methods, enabling robots to navigate conflicts in real-world environments.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03576", "pdf_url": "https://arxiv.org/pdf/2511.03576.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1250": {"title": "Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations", "authors": ["Mainak Singha"], "abstract": "arXiv:2511.03578v1 Announce Type: cross \nNeural networks can approximate solutions to partial differential equations, but they often break the very laws they are meant to model-creating mass from nowhere, drifting shocks, or violating conservation and entropy. We address this by training within the laws of physics rather than beside them. Our framework, called Constraint-Projected Learning (CPL), keeps every update physically admissible by projecting network outputs onto the intersection of constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and positivity. The projection is differentiable and adds only about 10% computational overhead, making it fully compatible with back-propagation. We further stabilize training with total-variation damping (TVD) to suppress small oscillations and a rollout curriculum that enforces consistency over long prediction horizons. Together, these mechanisms eliminate both hard and soft violations: conservation holds at machine precision, total-variation growth vanishes, and entropy and error remain bounded. On Burgers and Euler systems, CPL produces stable, physically lawful solutions without loss of accuracy. Instead of hoping neural solvers will respect physics, CPL makes that behavior an intrinsic property of the learning process.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03578", "pdf_url": "https://arxiv.org/pdf/2511.03578.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1251": {"title": "PerfDojo: Automated ML Library Generation for Heterogeneous Architectures", "authors": ["Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler"], "abstract": "arXiv:2511.03586v1 Announce Type: cross \nThe increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM's ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.", "categories": ["cs.PF", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03586", "pdf_url": "https://arxiv.org/pdf/2511.03586.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1252": {"title": "Step-Audio-EditX Technical Report", "authors": ["Chao Yan (Tony), Boyong Wu (Tony), Peng Yang (Tony), Pengfei Tan (Tony), Guoqiang Hu (Tony), Yuxin Zhang (Tony),  Xiangyu (Tony),  Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu"], "abstract": "arXiv:2511.03601v1 Announce Type: cross \nWe present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "abs_url": "https://arxiv.org/abs/2511.03601", "pdf_url": "https://arxiv.org/pdf/2511.03601.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1253": {"title": "Visualization Biases MLLM's Decision Making in Network Data Tasks", "authors": ["Timo Brand, Henry F\\\"orster, Stephen G. Kobourov, Jacob Miller"], "abstract": "arXiv:2511.03617v1 Announce Type: cross \nWe evaluate how visualizations can influence the judgment of MLLMs about the presence or absence of bridges in a network. We show that the inclusion of visualization improves confidence over a structured text-based input that could theoretically be helpful for answering the question. On the other hand, we observe that standard visualization techniques create a strong bias towards accepting or refuting the presence of a bridge -- independently of whether or not a bridge actually exists in the network. While our results indicate that the inclusion of visualization techniques can effectively influence the MLLM's judgment without compromising its self-reported confidence, they also imply that practitioners must be careful of allowing users to include visualizations in generative AI applications so as to avoid undesired hallucinations.", "categories": ["cs.GR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03617", "pdf_url": "https://arxiv.org/pdf/2511.03617.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1254": {"title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models", "authors": ["Haofei Yu, Fenghai Li, Jiaxuan You"], "abstract": "arXiv:2511.03628v1 Announce Type: cross \nLarge language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.CL"], "abs_url": "https://arxiv.org/abs/2511.03628", "pdf_url": "https://arxiv.org/pdf/2511.03628.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1255": {"title": "Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology", "authors": ["Thomas Souverain"], "abstract": "arXiv:2511.03641v1 Announce Type: cross \nTo foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "abs_url": "https://arxiv.org/abs/2511.03641", "pdf_url": "https://arxiv.org/pdf/2511.03641.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1256": {"title": "Explaining Human Choice Probabilities with Simple Vector Representations", "authors": ["Peter DiBerardino, Britt Anderson"], "abstract": "arXiv:2511.03643v1 Announce Type: cross \nWhen people pursue rewards in stochastic environments, they often match their choice frequencies to the observed target frequencies, even when this policy is demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this behavior under conditions where pursuit (seeking) could be toggled to avoidance (hiding), while leaving the probability distribution fixed, or varying complexity by changing the number of possible choices. We developed a model for participant choice built from choice frequency histograms treated as vectors. We posited the existence of a probability antimatching strategy for avoidance (hiding) rounds, and formalized this as a vector reflection of probability matching. We found that only two basis policies: matching/antimatching and maximizing/minimizing were sufficient to account for participant choices across a range of room numbers and opponent probability distributions. This schema requires only that people have the ability to remember the relative frequency of the different outcomes. With this knowledge simple operations can construct the maximizing and minimizing policies as well as matching and antimatching strategies. A mixture of these two policies captures human choice patterns in a stochastic environment.", "categories": ["q-bio.NC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03643", "pdf_url": "https://arxiv.org/pdf/2511.03643.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1257": {"title": "ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation", "authors": ["Jing Gao, Shutiao Luo, Yumeng Liu, Yuanming Li, Hongji Zeng"], "abstract": "arXiv:2511.03656v1 Announce Type: cross \nWith the rapid advancement of natural language processing (NLP) technologies, the demand for high-quality Chinese document question-answering datasets is steadily growing. To address this issue, we present the Chinese Multi-Document Question Answering Dataset(ChiMDQA), specifically designed for downstream business scenarios across prevalent domains including academic, education, finance, law, medical treatment, and news. ChiMDQA encompasses long-form documents from six distinct fields, consisting of 6,068 rigorously curated, high-quality question-answer (QA) pairs further classified into ten fine-grained categories. Through meticulous document screening and a systematic question-design methodology, the dataset guarantees both diversity and high quality, rendering it applicable to various NLP tasks such as document comprehension, knowledge extraction, and intelligent QA systems. Additionally, this paper offers a comprehensive overview of the dataset's design objectives, construction methodologies, and fine-grained evaluation system, supplying a substantial foundation for future research and practical applications in Chinese QA. The code and data are available at: https://anonymous.4open.science/r/Foxit-CHiMDQA/.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03656", "pdf_url": "https://arxiv.org/pdf/2511.03656.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1258": {"title": "DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay", "authors": ["Daniel Perkins, Oscar J. Escobar, Luke Green"], "abstract": "arXiv:2511.03670v1 Announce Type: cross \nWe present a detailed study of Deep Q-Networks in finite environments, emphasizing the impact of epsilon-greedy exploration schedules and prioritized experience replay. Through systematic experimentation, we evaluate how variations in epsilon decay schedules affect learning efficiency, convergence behavior, and reward optimization. We investigate how prioritized experience replay leads to faster convergence and higher returns and show empirical results comparing uniform, no replay, and prioritized strategies across multiple simulations. Our findings illuminate the trade-offs and interactions between exploration strategies and memory management in DQN training, offering practical recommendations for robust reinforcement learning in resource-constrained settings.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03670", "pdf_url": "https://arxiv.org/pdf/2511.03670.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1259": {"title": "Whisper Leak: a side-channel attack on Large Language Models", "authors": ["Geoff McDonald, Jonathan Bar Or"], "abstract": "arXiv:2511.03675v1 Announce Type: cross \nLarge Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount. This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption protecting content, these metadata patterns leak sufficient information to enable topic classification. We demonstrate the attack across 28 popular LLMs from major providers, achieving near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio). For many models, we achieve 100% precision in identifying sensitive topics like \"money laundering\" while recovering 5-20% of target conversations. This industry-wide vulnerability poses significant risks for users under network surveillance by ISPs, governments, or local adversaries. We evaluate three mitigation strategies - random padding, token batching, and packet injection - finding that while each reduces attack effectiveness, none provides complete protection. Through responsible disclosure, we have collaborated with providers to implement initial countermeasures. Our findings underscore the need for LLM providers to address metadata leakage as AI systems handle increasingly sensitive information.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03675", "pdf_url": "https://arxiv.org/pdf/2511.03675.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1260": {"title": "Structured Matrix Scaling for Multi-Class Calibration", "authors": ["Eug\\`ene Berta, David Holzm\\\"uller, Michael I. Jordan, Francis Bach"], "abstract": "arXiv:2511.03685v1 Announce Type: cross \nPost-hoc recalibration methods are widely used to ensure that classifiers provide faithful probability estimates. We argue that parametric recalibration functions based on logistic regression can be motivated from a simple theoretical setting for both binary and multiclass classification. This insight motivates the use of more expressive calibration methods beyond standard temperature scaling. For multi-class calibration however, a key challenge lies in the increasing number of parameters introduced by more complex models, often coupled with limited calibration data, which can lead to overfitting. Through extensive experiments, we demonstrate that the resulting bias-variance tradeoff can be effectively managed by structured regularization, robust preprocessing and efficient optimization. The resulting methods lead to substantial gains over existing logistic-based calibration techniques. We provide efficient and easy-to-use open-source implementations of our methods, making them an attractive alternative to common temperature, vector, and matrix scaling implementations.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03685", "pdf_url": "https://arxiv.org/pdf/2511.03685.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1261": {"title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents", "authors": ["Xingyao Wang, Simon Rosenberg, Juan Michelini, Calvin Smith, Hoang Tran, Engel Nyst, Rohit Malhotra, Xuhui Zhou, Valerie Chen, Robert Brennan, Graham Neubig"], "abstract": "arXiv:2511.03690v1 Announce Type: cross \nAgents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.", "categories": ["cs.SE", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03690", "pdf_url": "https://arxiv.org/pdf/2511.03690.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1262": {"title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing", "authors": ["Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen"], "abstract": "arXiv:2511.03697v1 Announce Type: cross \nAnalog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.", "categories": ["cs.LG", "cs.AI", "cs.AR"], "abs_url": "https://arxiv.org/abs/2511.03697", "pdf_url": "https://arxiv.org/pdf/2511.03697.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1263": {"title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask", "authors": ["Nan Li, Albert Gatt, Massimo Poesio"], "abstract": "arXiv:2511.03718v1 Announce Type: cross \nCollaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.03718", "pdf_url": "https://arxiv.org/pdf/2511.03718.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1264": {"title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative Knowledge Retrieval", "authors": ["Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim"], "abstract": "arXiv:2503.14234v4 Announce Type: replace \nGraph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications.", "categories": ["cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2503.14234", "pdf_url": "https://arxiv.org/pdf/2503.14234.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1265": {"title": "TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems", "authors": ["Xiao Zhang, Qi Wang, Mingyi Li, Yuan Yuan, Mengbai Xiao, Fuzhen Zhuang, Dongxiao Yu"], "abstract": "arXiv:2504.20462v5 Announce Type: replace \nImplementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2504.20462", "pdf_url": "https://arxiv.org/pdf/2504.20462.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1266": {"title": "Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes", "authors": ["Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor"], "abstract": "arXiv:2505.02184v2 Announce Type: replace \nWhile large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel codes through a multi-stage, iterative approach integrating runtime power profiling, energy-aware prompting, self-correcting feedback loops, and an LLM-as-a-Judge agent for automated screening of code solutions. We introduce energy-reduction@k, a novel metric that quantifies expected energy reduction when generating k code candidates and selecting the most energy-efficient, enabling systematic evaluation of multi-attempt generation strategies. Evaluating 20 HeCBench applications and two miniApps on NVIDIA A100 and AMD MI100 GPUs, a single run (k=1) with LASSI-EE delivers refactored parallel codes with an average 29% expected energy reduction at an 81% pass rate, representing a 2.8x improvement over vanilla LLM prompting. Multiple runs (k=3) achieve an average 48% expected energy reduction at a 97% pass rate. These results are consistent across devices, demonstrating LASSI-EE's effectiveness across diverse hardware architectures.", "categories": ["cs.AI", "cs.DC", "cs.PL", "cs.SE"], "abs_url": "https://arxiv.org/abs/2505.02184", "pdf_url": "https://arxiv.org/pdf/2505.02184.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1267": {"title": "Meta-Semantics Augmented Few-Shot Relational Learning", "authors": ["Han Wu, Jie Yin"], "abstract": "arXiv:2505.05684v4 Announce Type: replace \nFew-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While current methods have focused primarily on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To bridge this gap, we propose PromptMeta, a novel prompted meta-learning framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta introduces two core innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics shared across tasks, enabling effective knowledge transfer and adaptation to newly emerging relations; and (2) a learnable fusion mechanism that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG benchmarks validate the effectiveness of PromptMeta in adapting to new relations with limited supervision.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.05684", "pdf_url": "https://arxiv.org/pdf/2505.05684.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1268": {"title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning", "authors": ["Ruiyi Yang, Hao Xue, Imran Razzak, Shirui Pan, Hakim Hacid, Flora D. Salim"], "abstract": "arXiv:2505.13994v2 Announce Type: replace \nRetrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.", "categories": ["cs.AI", "cs.IR", "cs.MA"], "abs_url": "https://arxiv.org/abs/2505.13994", "pdf_url": "https://arxiv.org/pdf/2505.13994.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1269": {"title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "authors": ["Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, Jiawei Han"], "abstract": "arXiv:2505.14146v2 Announce Type: replace \nRetrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.", "categories": ["cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2505.14146", "pdf_url": "https://arxiv.org/pdf/2505.14146.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1270": {"title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study", "authors": ["Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk"], "abstract": "arXiv:2506.15207v2 Announce Type: replace \nThe exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.", "categories": ["cs.AI", "cs.MA", "cs.RO"], "abs_url": "https://arxiv.org/abs/2506.15207", "pdf_url": "https://arxiv.org/pdf/2506.15207.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1271": {"title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning", "authors": ["Bo Hou, Xin Tan, Kai Zheng, Fang Liu, Yinghao Zhu, Li Zhang"], "abstract": "arXiv:2507.16395v2 Announce Type: replace \nAtomic commits, which address a single development concern, are a best practice in software development. In practice, however, developers often produce tangled commits that mix unrelated changes, complicating code review and maintenance. Prior untangling approaches (rule-based, feature-based, or graph-based) have made progress but typically rely on shallow signals and struggle to distinguish explicit dependencies (e.g., control/data flow) from implicit ones (e.g., semantic or conceptual relationships). In this paper, we propose ColaUntangle, a new collaborative consultation framework for commit untangling that models both explicit and implicit dependencies among code changes. ColaUntangle integrates Large Language Model (LLM)-driven agents in a multi-agent architecture: one agent specializes in explicit dependencies, another in implicit ones, and a reviewer agent synthesizes their perspectives through iterative consultation. To capture structural and contextual information, we construct Explicit and Implicit Contexts, enabling agents to reason over code relationships with both symbolic and semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C# and 14k Java tangled commits). Experimental results show that ColaUntangle outperforms the best-performing baseline, achieving an improvement of 44% on the C# dataset and 82% on the Java dataset. These findings highlight the potential of LLM-based collaborative frameworks for advancing automated commit untangling tasks.", "categories": ["cs.AI", "cs.SE"], "abs_url": "https://arxiv.org/abs/2507.16395", "pdf_url": "https://arxiv.org/pdf/2507.16395.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1272": {"title": "Data Dependency-Aware Code Generation from Enhanced UML Sequence Diagrams", "authors": ["Wenxin Mao, Zhitao Wang, Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin"], "abstract": "arXiv:2508.03379v3 Announce Type: replace \nLarge language models (LLMs) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with LLMs' excellent mathematical strengths. Additional static parsing and dependency pruning further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.", "categories": ["cs.AI", "cs.SE"], "abs_url": "https://arxiv.org/abs/2508.03379", "pdf_url": "https://arxiv.org/pdf/2508.03379.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1273": {"title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "authors": ["Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, Yong Liu"], "abstract": "arXiv:2509.06733v2 Announce Type: replace \nDeep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.", "categories": ["cs.AI", "cs.CL", "cs.IR"], "abs_url": "https://arxiv.org/abs/2509.06733", "pdf_url": "https://arxiv.org/pdf/2509.06733.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1274": {"title": "ForTIFAI: Fending Off Recursive Training Induced Failure for AI Model Collapse", "authors": ["Soheil Zibakhsh Shabgahi, Pedram Aghazadeh, Azalia Mirhoseini, Farinaz Koushanfar"], "abstract": "arXiv:2509.08972v4 Announce Type: replace \nThe increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. While the causes of model collapse are increasingly understood, effective mitigation strategies remain scarce. We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce the Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse. In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings. Our results demonstrate that confidence-aware training objectives can substantially delay collapse onset, offering a practical and generalizable tool for model robustness under synthetic-data exposure.", "categories": ["cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2509.08972", "pdf_url": "https://arxiv.org/pdf/2509.08972.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1275": {"title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?", "authors": ["Dhruv Jain, Harshit Shukla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, Shubham Agarwal"], "abstract": "arXiv:2510.07978v2 Announce Type: replace \nLarge-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.07978", "pdf_url": "https://arxiv.org/pdf/2510.07978.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1276": {"title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty", "authors": ["Ziwei Deng, Mian Deng, Chenjing Liang, Zeming Gao, Chennan Ma, Chenxing Lin, Haipeng Zhang, Songzhu Mei, Siqi Shen, Cheng Wang"], "abstract": "arXiv:2510.18442v2 Announce Type: replace \nLarge Language Models (LLMs) are increasingly being explored across a range of reasoning tasks. However, LLMs sometimes struggle with reasoning tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for reasoning is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step reasoning tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based reasoning tasks under uncertainty.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.18442", "pdf_url": "https://arxiv.org/pdf/2510.18442.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1277": {"title": "Misalignment Bounty: Crowdsourcing AI Agent Misbehavior", "authors": ["Rustem Turtayev, Natalia Fedorova, Oleg Serikov, Sergey Koldyba, Lev Avagyan, Dmitrii Volkov"], "abstract": "arXiv:2510.19738v2 Announce Type: replace \nAdvanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.\n  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.19738", "pdf_url": "https://arxiv.org/pdf/2510.19738.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1278": {"title": "Agentic Meta-Orchestrator for Multi-task Copilots", "authors": ["Xiaofeng Zhu, Yunshen Zhou"], "abstract": "arXiv:2510.22781v2 Announce Type: replace \nMicrosoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \\& external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22781", "pdf_url": "https://arxiv.org/pdf/2510.22781.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1279": {"title": "Mirror-Neuron Patterns in AI Alignment", "authors": ["Robyn Wyrick"], "abstract": "arXiv:2511.01885v2 Announce Type: replace \nAs artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.\n  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?\n  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.\n  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "abs_url": "https://arxiv.org/abs/2511.01885", "pdf_url": "https://arxiv.org/pdf/2511.01885.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1280": {"title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data", "authors": ["Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng"], "abstract": "arXiv:2511.02219v2 Announce Type: replace \nComplex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose TabDSR, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that TabDSR consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and TabDSR, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02219", "pdf_url": "https://arxiv.org/pdf/2511.02219.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1281": {"title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models", "authors": ["Claudia Herambourg, Dawid Siuda, Julia Kopczy\\'nska, Joao R. L. Santos, Wojciech Sas, Joanna \\'Smieta\\'nska-Nowak"], "abstract": "arXiv:2511.02589v2 Announce Type: replace \nWe present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$) and calculation mistakes ($33\\,\\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02589", "pdf_url": "https://arxiv.org/pdf/2511.02589.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1282": {"title": "Kosmos: An AI Scientist for Autonomous Discovery", "authors": ["Ludovico Mitchener, Angela Yiu, Benjamin Chang, Mathieu Bourdenx, Tyler Nadolski, Arvis Sulovari, Eric C. Landsness, Daniel L. Barabasi, Siddharth Narayanan, Nicky Evans, Shriya Reddy, Martha Foiani, Aizad Kamal, Leah P. Shriver, Fang Cao, Asmamaw T. Wassie, Jon M. Laurent, Edwin Melville-Green, Mayk Caldas, Albert Bou, Kaleigh F. Roberts, Sladjana Zagorac, Timothy C. Orr, Miranda E. Orr, Kevin J. Zwezdaryk, Ali E. Ghareeb, Laurie McCoy, Bruna Gomes, Euan A. Ashley, Karen E. Duff, Tonio Buonassisi, Tom Rainforth, Randall J. Bateman, Michael Skarlinski, Samuel G. Rodriques, Michaela M. Hinks, Andrew D. White"], "abstract": "arXiv:2511.02824v2 Announce Type: replace \nData-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.", "categories": ["cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02824", "pdf_url": "https://arxiv.org/pdf/2511.02824.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1283": {"title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything", "authors": ["Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh"], "abstract": "arXiv:2511.02834v2 Announce Type: replace \nMultimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02834", "pdf_url": "https://arxiv.org/pdf/2511.02834.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1284": {"title": "Emotion Detection From Social Media Posts", "authors": ["Md Mahbubur Rahman, Shaila Sharmin"], "abstract": "arXiv:2302.05610v2 Announce Type: replace-cross \nOver the last few years, social media has evolved into a medium for expressing personal views, emotions, and even business and political proposals, recommendations, and advertisements. We address the topic of identifying emotions from text data obtained from social media posts like Twitter in this research. We have deployed different traditional machine learning techniques such as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random Forest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM, BiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy, and Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble model. The evaluation result shows that the deep neural network models(BiGRU, to be specific) produce the most promising results compared to traditional machine learning models, with an 87.53 % accuracy rate. The ensemble model performs even better (87.66 %), albeit the difference is not significant. This result will aid in the development of a decision-making tool that visualizes emotional fluctuations.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2302.05610", "pdf_url": "https://arxiv.org/pdf/2302.05610.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1285": {"title": "Transfer Learning-based Real-time Handgun Detection", "authors": ["Youssef Elmir"], "abstract": "arXiv:2311.13559v3 Announce Type: replace-cross \nTraditional surveillance systems rely on human attention, limiting their effectiveness. This study employs convolutional neural networks and transfer learning to develop a real-time computer vision system for automatic handgun detection. Comprehensive analysis of online handgun detection methods is conducted, emphasizing reducing false positives and learning time. Transfer learning is demonstrated as an effective approach. Despite technical challenges, the proposed system achieves a precision rate of 84.74%, demonstrating promising performance comparable to related works, enabling faster learning and accurate automatic handgun detection for enhanced security. This research advances security measures by reducing human monitoring dependence, showcasing the potential of transfer learning-based approaches for efficient and reliable handgun detection.", "categories": ["cs.CV", "cs.AI", "cs.HC"], "abs_url": "https://arxiv.org/abs/2311.13559", "pdf_url": "https://arxiv.org/pdf/2311.13559.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1286": {"title": "Survey on AI Ethics: A Socio-technical Perspective", "authors": ["Dave Mbiazi, Meghana Bhange, Maryam Babaei, Ivaxi Sheth, Patrik Kenfack, Samira Ebrahimi Kahou"], "abstract": "arXiv:2311.17228v2 Announce Type: replace-cross \nThe past decade has observed a significant advancement in AI with deep learning-based models being deployed in diverse scenarios, including safety-critical applications. As these AI systems become deeply embedded in our societal infrastructure, the repercussions of their decisions and actions have significant consequences, making the ethical implications of AI deployment highly relevant and essential. The ethical concerns associated with AI are multifaceted, including challenging issues of fairness, privacy and data protection, responsibility and accountability, safety and robustness, transparency and explainability, and environmental impact. These principles together form the foundations of ethical AI considerations that concern every stakeholder in the AI system lifecycle. In light of the present ethical and future x-risk concerns, governments have shown increasing interest in establishing guidelines for the ethical deployment of AI. This work unifies the current and future ethical concerns of deploying AI into society. While we acknowledge and appreciate the technical surveys for each of the ethical principles concerned, in this paper, we aim to provide a comprehensive overview that not only addresses each principle from a technical point of view but also discusses them from a social perspective.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2311.17228", "pdf_url": "https://arxiv.org/pdf/2311.17228.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1287": {"title": "Neural Physics: Using AI Libraries to Develop Physics-Based Solvers for Incompressible Computational Fluid Dynamics", "authors": ["Boyang Chen, Claire E. Heaney, Christopher C. Pain"], "abstract": "arXiv:2402.17913v2 Announce Type: replace-cross \nNumerical discretisations of partial differential equations (PDEs) can be written as discrete convolutions, which, themselves, are a key tool in AI libraries and used in convolutional neural networks (CNNs). We therefore propose to implement numerical discretisations as convolutional layers of a neural network, where the weights or filters are determined analytically rather than by training. Furthermore, we demonstrate that these systems can be solved entirely by functions in AI libraries, either by using Jacobi iteration or multigrid methods, the latter realised through a U-Net architecture. Some advantages of the Neural Physics approach are that (1) the methods are platform agnostic; (2) the resulting solvers are fully differentiable, ideal for optimisation tasks; and (3) writing CFD solvers as (untrained) neural networks means that they can be seamlessly integrated with trained neural networks to form hybrid models. We demonstrate the proposed approach on a number of test cases of increasing complexity from advection-diffusion problems, the non-linear Burgers equation to the Navier-Stokes equations. We validate the approach by comparing our results with solutions obtained from traditionally written code and common benchmarks from the literature. We show that the proposed methodology can solve all these problems using repurposed AI libraries in an efficient way, without training, and presents a new avenue to explore in the development of methods to solve PDEs with implicit methods.", "categories": ["physics.flu-dyn", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2402.17913", "pdf_url": "https://arxiv.org/pdf/2402.17913.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1288": {"title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges", "authors": ["Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Philip S. Yu, Ming Zhang"], "abstract": "arXiv:2403.04468v2 Announce Type: replace-cross \nGraph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "abs_url": "https://arxiv.org/abs/2403.04468", "pdf_url": "https://arxiv.org/pdf/2403.04468.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1289": {"title": "A Reliable Cryptographic Framework for Empirical Machine Unlearning Evaluation", "authors": ["Yiwen Tu, Pingbang Hu, Jiaqi Ma"], "abstract": "arXiv:2404.11577v4 Announce Type: replace-cross \nMachine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \\emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2404.11577", "pdf_url": "https://arxiv.org/pdf/2404.11577.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1290": {"title": "Autonomous Robotic Drilling System for Mice Cranial Window Creation", "authors": ["Enduo Zhao, Murilo M. Marinho, Kanako Harada"], "abstract": "arXiv:2406.14135v2 Announce Type: replace-cross \nRobotic assistance for experimental manipulation in the life sciences is expected to enable favorable outcomes, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and hence require intricate algorithms for successful autonomous robotic control. As a use case, we are studying the cranial window creation in mice. This operation requires the removal of an 8-mm circular patch of the skull, which is approximately 300 um thick, but the shape and thickness of the mouse skull significantly varies depending on the strain of the mouse, sex, and age. In this work, we develop an autonomous robotic drilling system with no offline planning, consisting of a trajectory planner with execution-time feedback with drilling completion level recognition based on image and force information. In the experiments, we first evaluate the image-and-force-based drilling completion level recognition by comparing it with other state-of-the-art deep learning image processing methods and conduct an ablation study in eggshell drilling to evaluate the impact of each module on system performance. Finally, the system performance is further evaluated in postmortem mice, achieving a success rate of 70% (14/20 trials) with an average drilling time of 9.3 min.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2406.14135", "pdf_url": "https://arxiv.org/pdf/2406.14135.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1291": {"title": "MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping", "authors": ["Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh"], "abstract": "arXiv:2409.11316v5 Announce Type: replace-cross \nFew-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2409.11316", "pdf_url": "https://arxiv.org/pdf/2409.11316.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1292": {"title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization", "authors": ["Mikhail Persiianov, Arip Asadulaev, Nikita Andreev, Nikita Starodubcev, Dmitry Baranchuk, Anastasis Kratsios, Evgeny Burnaev, Alexander Korotin"], "abstract": "arXiv:2410.02628v4 Announce Type: replace-cross \nLearning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \\sim \\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\\textbf{end-to-end}$ learning algorithm to get $\\pi^*(\\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.02628", "pdf_url": "https://arxiv.org/pdf/2410.02628.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1293": {"title": "Mastering Contact-rich Tasks by Combining Soft and Rigid Robotics with Imitation Learning", "authors": ["Mariano Ram\\'irez Montero, Ebrahim Shahabi, Giovanni Franzese, Jens Kober, Barbara Mazzolai, Cosimo Della Santina"], "abstract": "arXiv:2410.07787v3 Announce Type: replace-cross \nSoft robots have the potential to revolutionize the use of robotic systems with their capability of establishing safe, robust, and adaptable interactions with their environment, but their precise control remains challenging. In contrast, traditional rigid robots offer high accuracy and repeatability but lack the flexibility of soft robots. We argue that combining these characteristics in a hybrid robotic platform can significantly enhance overall capabilities. This work presents a novel hybrid robotic platform that integrates a rigid manipulator with a fully developed soft arm. This system is equipped with the intelligence necessary to perform flexible and generalizable tasks through imitation learning autonomously. The physical softness and machine learning enable our platform to achieve highly generalizable skills, while the rigid components ensure precision and repeatability.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.07787", "pdf_url": "https://arxiv.org/pdf/2410.07787.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1294": {"title": "Intelligent Computing Social Modeling and Methodological Innovations in Political Science in the Era of Large Language Models", "authors": ["Zhenyu Wang, Dequan Wang, Yi Xu, Lingfeng Zhou, Yiqi Zhou"], "abstract": "arXiv:2410.16301v2 Announce Type: replace-cross \nThe recent wave of artificial intelligence, epitomized by large language models (LLMs),has presented opportunities and challenges for methodological innovation in political science,sparking discussions on a potential paradigm shift in the social sciences. However, how can weunderstand the impact of LLMs on knowledge production and paradigm transformation in thesocial sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political scienceresearch? These questions, particularly from a practical methodological standpoint, remainunderexplored. This paper proposes the \"Intelligent Computing Social Modeling\" (ICSM) methodto address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration inpolitical science through \"simulated social construction\" and \"simulation validation.\" Bysimulating the U.S. presidential election, this study empirically demonstrates the operationalpathways and methodological advantages of ICSM. By integrating traditional social scienceparadigms, ICSM not only enhances the quantitative paradigm's capability to apply big data toassess the impact of factors but also provides qualitative paradigms with evidence for socialmechanism discovery at the individual level, offering a powerful tool that balances interpretabilityand predictability in social science research. The findings suggest that LLMs will drivemethodological innovation in political science through integration and improvement rather thandirect substitution.", "categories": ["cs.CY", "cs.AI"], "abs_url": "https://arxiv.org/abs/2410.16301", "pdf_url": "https://arxiv.org/pdf/2410.16301.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1295": {"title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs", "authors": ["Haolin Li, Haoyu Wang, Luana Ruiz"], "abstract": "arXiv:2410.16593v5 Announce Type: replace-cross \nGraph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian trace -- a proxy for the graph connectivity -- than random sampling, while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving Laplacian trace and GNN transferability compared to random sampling.", "categories": ["eess.SP", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2410.16593", "pdf_url": "https://arxiv.org/pdf/2410.16593.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1296": {"title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs", "authors": ["Changhao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, Bo Dai"], "abstract": "arXiv:2410.20749v3 Announce Type: replace-cross \nDespite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2410.20749", "pdf_url": "https://arxiv.org/pdf/2410.20749.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1297": {"title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation", "authors": ["Sanjana Ramprasad, Byron C. Wallace"], "abstract": "arXiv:2411.16638v4 Announce Type: replace-cross \nModern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2411.16638", "pdf_url": "https://arxiv.org/pdf/2411.16638.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1298": {"title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis", "authors": ["Van-Duc Le, Hai-Thien To"], "abstract": "arXiv:2412.08179v2 Announce Type: replace-cross \nFinancial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLM's adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.", "categories": ["q-fin.ST", "cs.AI"], "abs_url": "https://arxiv.org/abs/2412.08179", "pdf_url": "https://arxiv.org/pdf/2412.08179.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1299": {"title": "REFA: Reference Free Alignment for multi-preference optimization", "authors": ["Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan"], "abstract": "arXiv:2412.16378v4 Announce Type: replace-cross \nTo mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the URSLA shortcut. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce REFA, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17% length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2412.16378", "pdf_url": "https://arxiv.org/pdf/2412.16378.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1300": {"title": "From Haystack to Needle: Label Space Reduction for Zero-shot Classification", "authors": ["Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke"], "abstract": "arXiv:2502.08436v2 Announce Type: replace-cross \nWe present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2502.08436", "pdf_url": "https://arxiv.org/pdf/2502.08436.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1301": {"title": "Beyond Covariance Matrix: The Statistical Complexity of Private Linear Regression", "authors": ["Fan Chen, Jiachun Li, Alexander Rakhlin, David Simchi-Levi"], "abstract": "arXiv:2502.13115v2 Announce Type: replace-cross \nWe study the statistical complexity of private linear regression under an unknown, potentially ill-conditioned covariate distribution. Somewhat surprisingly, under privacy constraints the intrinsic complexity is \\emph{not} captured by the usual covariance matrix but rather its $L_1$ analogues. Building on this insight, we establish minimax convergence rates for both the central and local privacy models and introduce an Information-Weighted Regression method that attains the optimal rates.\n  As application, in private linear contextual bandits, we propose an efficient algorithm that achieves rate-optimal regret bounds of order $\\sqrt{T}+\\frac{1}{\\alpha}$ and $\\sqrt{T}/\\alpha$ under joint and local $\\alpha$-privacy models, respectively. Notably, our results demonstrate that joint privacy comes at almost no additional cost, addressing the open problems posed by Azize and Basu (2024).", "categories": ["cs.LG", "cs.AI", "cs.CR", "math.ST", "stat.ML", "stat.TH"], "abs_url": "https://arxiv.org/abs/2502.13115", "pdf_url": "https://arxiv.org/pdf/2502.13115.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1302": {"title": "A Survey on Text-Driven 360-Degree Panorama Generation", "authors": ["Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue"], "abstract": "arXiv:2502.14799v3 Announce Type: replace-cross \nThe advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms. We extend our analysis to two closely related domains: text-driven 360-degree 3D scene generation and text-driven 360-degree panoramic video generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2502.14799", "pdf_url": "https://arxiv.org/pdf/2502.14799.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1303": {"title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models", "authors": ["Nghia Bui, Guergana Savova, Lijing Wang"], "abstract": "arXiv:2503.07329v2 Announce Type: replace-cross \nThe impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.07329", "pdf_url": "https://arxiv.org/pdf/2503.07329.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1304": {"title": "Revisiting semi-supervised learning in the era of foundation models", "authors": ["Ping Zhang, Zheda Mai, Quang-Huy Nguyen, Wei-Lun Chao"], "abstract": "arXiv:2503.09707v4 Announce Type: replace-cross \nSemi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.", "categories": ["cs.LG", "cs.AI", "cs.CV"], "abs_url": "https://arxiv.org/abs/2503.09707", "pdf_url": "https://arxiv.org/pdf/2503.09707.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1305": {"title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories", "authors": ["Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen"], "abstract": "arXiv:2504.21205v2 Announce Type: replace-cross \nThis paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.", "categories": ["cs.CR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2504.21205", "pdf_url": "https://arxiv.org/pdf/2504.21205.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1306": {"title": "A data-driven framework for team selection in Fantasy Premier League", "authors": ["Danial Ramezani, Tai Dinh"], "abstract": "arXiv:2505.02170v2 Announce Type: replace-cross \nFantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.", "categories": ["cs.CE", "cs.AI", "cs.LG", "math.OC"], "abs_url": "https://arxiv.org/abs/2505.02170", "pdf_url": "https://arxiv.org/pdf/2505.02170.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1307": {"title": "Deep Learning Warm Starts for Trajectory Optimization on the International Space Station", "authors": ["Somrita Banerjee, Abhishek Cauligi, Marco Pavone"], "abstract": "arXiv:2505.05588v3 Announce Type: replace-cross \nTrajectory optimization is a cornerstone of modern robot autonomy, enabling systems to compute trajectories and controls in real-time while respecting safety and physical constraints. However, it has seen limited usage in spaceflight applications due to its heavy computational demands that exceed the capability of most flight computers. In this work, we provide results on the first in-space demonstration of using machine learning-based warm starts for accelerating trajectory optimization for the Astrobee free-flying robot onboard the International Space Station (ISS). We formulate a data-driven optimal control approach that trains a neural network to learn the structure of the trajectory generation problem being solved using sequential convex programming (SCP). Onboard, this trained neural network predicts solutions for the trajectory generation problem and relies on using the SCP solver to enforce safety constraints for the system. Our trained network reduces the number of solver iterations required for convergence in cases including rotational dynamics by 60% and in cases with obstacles drawn from the training distribution of the warm start model by 50%. This work represents a significant milestone in the use of learning-based control for spaceflight applications and a stepping stone for future advances in the use of machine learning for autonomous guidance, navigation, & control.", "categories": ["cs.RO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.05588", "pdf_url": "https://arxiv.org/pdf/2505.05588.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1308": {"title": "Traversal Verification for Speculative Tree Decoding", "authors": ["Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi"], "abstract": "arXiv:2505.12398v2 Announce Type: replace-cross \nSpeculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.12398", "pdf_url": "https://arxiv.org/pdf/2505.12398.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1309": {"title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation", "authors": ["Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez"], "abstract": "arXiv:2505.14526v2 Announce Type: replace-cross \nAutonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.14526", "pdf_url": "https://arxiv.org/pdf/2505.14526.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.6, "subfield": "\u89c4\u5212\u63a7\u5236 / \u4eff\u771f\u8bc4\u4f30 / \u7aef\u5230\u7aef\u5b66\u4e60", "reason": "The paper focuses on reinforcement learning-based autonomous navigation across multiple robotic domains, including wheeled ground vehicles. Although it is not exclusively centered on autonomous driving, its RL framework for navigation, sim-to-real transfer, and evaluation in ground vehicle contexts make it applicable to autonomous driving research, particularly in planning and control as well as simulation-based evaluation."}, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1310": {"title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "authors": ["Ben Cohen, Emaad Khwaja, Youssef Doubli, Salahidine Lemaachi, Chris Lettieri, Charles Masson, Hugo Miccinilli, Elise Ram\\'e, Qiqi Ren, Afshin Rostamizadeh, Jean Ogier du Terrail, Anna-Monica Toon, Kan Wang, Stephan Xie, Zongzhe Xu, Viktoriya Zhukova, David Asker, Ameet Talwalkar, Othmane Abou-Amal"], "abstract": "arXiv:2505.14766v2 Announce Type: replace-cross \nWe introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.14766", "pdf_url": "https://arxiv.org/pdf/2505.14766.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1311": {"title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools", "authors": ["Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang"], "abstract": "arXiv:2505.17612v2 Announce Type: replace-cross \nLarge language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2505.17612", "pdf_url": "https://arxiv.org/pdf/2505.17612.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1312": {"title": "Large Language Models Miss the Multi-Agent Mark", "authors": ["Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M. Zhang, Elizabeth Black, Michael Luck, Philip Torr, Michael Wooldridge"], "abstract": "arXiv:2505.21298v3 Announce Type: replace-cross \nRecent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.", "categories": ["cs.MA", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2505.21298", "pdf_url": "https://arxiv.org/pdf/2505.21298.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1313": {"title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "authors": ["Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang"], "abstract": "arXiv:2505.21600v2 Announce Type: replace-cross \nLarge Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "abs_url": "https://arxiv.org/abs/2505.21600", "pdf_url": "https://arxiv.org/pdf/2505.21600.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1314": {"title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos"], "abstract": "arXiv:2506.09397v5 Announce Type: replace-cross \nThe growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \\acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "abs_url": "https://arxiv.org/abs/2506.09397", "pdf_url": "https://arxiv.org/pdf/2506.09397.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1315": {"title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics", "authors": ["Imanol Solano, Julian Fierrez, Aythami Morales, Alejandro Pe\\~na, Ruben Tolosana, Francisco Zamora-Martinez, Javier San Agustin"], "abstract": "arXiv:2506.10564v2 Announce Type: replace-cross \nDemographic bias in high-performance face recognition (FR) systems often eludes detection by existing metrics, especially with respect to subtle disparities in the tails of the score distribution. We introduce the Comprehensive Equity Index (CEI), a novel metric designed to address this limitation. CEI uniquely analyzes genuine and impostor score distributions separately, enabling a configurable focus on tail probabilities while also considering overall distribution shapes. Our extensive experiments (evaluating state-of-the-art FR systems, intentionally biased models, and diverse datasets) confirm CEI's superior ability to detect nuanced biases where previous methods fall short. Furthermore, we present CEI^A, an automated version of the metric that enhances objectivity and simplifies practical application. CEI provides a robust and sensitive tool for operational FR fairness assessment. The proposed methods have been developed particularly for bias evaluation in face biometrics but, in general, they are applicable for comparing statistical distributions in any problem where one is interested in analyzing the distribution tails.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.10564", "pdf_url": "https://arxiv.org/pdf/2506.10564.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1316": {"title": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "authors": ["Di He, Songjun Tu, Ajay Jaiswal, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin"], "abstract": "arXiv:2506.14562v3 Announce Type: replace-cross \nWeight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify \"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.14562", "pdf_url": "https://arxiv.org/pdf/2506.14562.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1317": {"title": "Dense SAE Latents Are Features, Not Bugs", "authors": ["Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark"], "abstract": "arXiv:2506.15679v2 Announce Type: replace-cross \nSparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2506.15679", "pdf_url": "https://arxiv.org/pdf/2506.15679.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1318": {"title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging", "authors": ["Filippo Ruffini, Elena Mulero Ayllon, Linlin Shen, Paolo Soda, Valerio Guarrasi"], "abstract": "arXiv:2506.18434v2 Announce Type: replace-cross \nDespite the significant potential of Foundation Models (FMs) in medical imaging, their application to prognosis prediction remains challenging due to data scarcity, class imbalance, and task complexity, which limit their clinical adoption. This study introduces the first structured benchmark to assess the robustness and efficiency of transfer learning strategies for FMs compared with convolutional neural networks (CNNs) in predicting COVID-19 patient outcomes from chest X-rays. The goal is to systematically compare finetuning strategies, both classical and parameter efficient, under realistic clinical constraints related to data scarcity and class imbalance, offering empirical guidance for AI deployment in clinical workflows. Four publicly available COVID-19 chest X-ray datasets were used, covering mortality, severity, and ICU admission, with varying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs pretrained on general or biomedical datasets were adapted using full finetuning, linear probing, and parameter-efficient methods. Models were evaluated under full data and few shot regimes using the Matthews Correlation Coefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and class weighted losses. CNNs with full fine-tuning performed robustly on small, imbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT), particularly LoRA and BitFit, achieved competitive results on larger datasets. Severe class imbalance degraded PEFT performance, whereas balanced data mitigated this effect. In few-shot settings, FMs showed limited generalization, with linear probing yielding the most stable results. No single fine-tuning strategy proved universally optimal: CNNs remain dependable for low-resource scenarios, whereas FMs benefit from parameter-efficient methods when data are sufficient.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.18434", "pdf_url": "https://arxiv.org/pdf/2506.18434.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1319": {"title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "authors": ["Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Jalal Naghiyev, Ravid Shwartz-Ziv, Keith Ross"], "abstract": "arXiv:2506.22638v2 Announce Type: replace-cross \nLarge language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post- training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2506.22638", "pdf_url": "https://arxiv.org/pdf/2506.22638.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1320": {"title": "FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model", "authors": ["Taehwan Yoon, Bongjun Choi, Wesley De Neve"], "abstract": "arXiv:2506.23210v3 Announce Type: replace-cross \nFederated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.", "categories": ["cs.LG", "cs.AI", "cs.DC"], "abs_url": "https://arxiv.org/abs/2506.23210", "pdf_url": "https://arxiv.org/pdf/2506.23210.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1321": {"title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition", "authors": ["Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly"], "abstract": "arXiv:2507.05724v3 Announce Type: replace-cross \nMixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "abs_url": "https://arxiv.org/abs/2507.05724", "pdf_url": "https://arxiv.org/pdf/2507.05724.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1322": {"title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "authors": ["Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang, Yang Zhang"], "abstract": "arXiv:2507.06853v2 Announce Type: replace-cross \nMolecular structure elucidation from spectra is a fundamental challenge in molecular science. Conventional approaches rely heavily on expert interpretation and lack scalability, while retrieval-based machine learning approaches remain constrained by limited reference libraries. Generative models offer a promising alternative, yet most adopt autoregressive architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that formulates molecular structure elucidation as a conditional generation process, directly inferring 2D and 3D molecular structures from multi-modal spectra using diffusion models. Its denoising network is parameterized by the Diffusion Molecule Transformer, an SE(3)-equivariant architecture for geometric modeling, conditioned by SpecFormer, a Transformer-based spectral encoder capturing multi-modal spectral dependencies. Extensive experiments demonstrate that DiffSpectra accurately elucidates molecular structures, achieving 40.76% top-1 and 99.49% top-10 accuracy. Its performance benefits substantially from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. To our knowledge, DiffSpectra is the first framework that unifies multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "abs_url": "https://arxiv.org/abs/2507.06853", "pdf_url": "https://arxiv.org/pdf/2507.06853.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1323": {"title": "Flow matching for reaction pathway generation", "authors": ["Ping Tuo, Jiale Chen, Ju Li"], "abstract": "arXiv:2507.10530v4 Announce Type: replace-cross \nElucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemistry searches. But diffusion models remain constrained by their stochastic differential equation (SDE) dynamics, which suffer from inefficiency and limited controllability. We show that flow matching, a deterministic ordinary differential (ODE) formulation, can replace SDE-based diffusion for molecular and reaction generation. We introduce MolGEN, a conditional flow-matching framework that learns an optimal transport path to transport Gaussian priors to target chemical distributions. On benchmarks used by TSDiff and OA-ReactDiff, MolGEN surpasses TS geometry accuracy and barrier-height prediction while reducing sampling to sub-second inference. MolGEN also supports open-ended product generation with competitive top-k accuracy and avoids mass/electron-balance violations common to sequence models. In a realistic test on the $\\gamma$-ketohydroperoxide decomposition network, MolGEN yields higher fractions of valid and intended TSs with markedly fewer quantum-chemistry evaluations than string-based baselines. These results demonstrate that deterministic flow matching provides a unified, accurate, and computationally efficient foundation for molecular generative modeling, signaling that flow matching is the future for molecular generation across chemistry.", "categories": ["physics.chem-ph", "cs.AI"], "abs_url": "https://arxiv.org/abs/2507.10530", "pdf_url": "https://arxiv.org/pdf/2507.10530.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1324": {"title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "authors": ["Chang Peng, Bao Yang, Meiqi Li, Ge Zhang, Hui Sun, Zhenyu Jiang"], "abstract": "arXiv:2507.11081v2 Announce Type: replace-cross \nGround penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. Deep learning-based automatic RSD recognition, though ameliorating the burden of data processing, suffers from data scarcity and insufficient capability to recognize defects. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. A novel cross-verification strategy was proposed to fully exploit the complementary abilities of region proposal networks in object recognition from different views of GPR images. The method achieves outstanding accuracy with a recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the human labor of inspection by around 90%.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2507.11081", "pdf_url": "https://arxiv.org/pdf/2507.11081.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1325": {"title": "Enhancing Fatigue Detection through Heterogeneous Multi-Source Data Integration and Cross-Domain Modality Imputation", "authors": ["Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li"], "abstract": "arXiv:2507.16859v3 Announce Type: replace-cross \nFatigue detection for human operators plays a key role in safety critical applications such as aviation, mining, and long haul transport. While numerous studies have demonstrated the effectiveness of high fidelity sensors in controlled laboratory environments, their performance often degrades when ported to real world settings due to noise, lighting conditions, and field of view constraints, thereby limiting their practicality. This paper formalizes a deployment oriented setting for real world fatigue detection, where high quality sensors are often unavailable in practical applications. To address this challenge, we propose leveraging knowledge from heterogeneous source domains, including high fidelity sensors that are difficult to deploy in the field but commonly used in controlled environments, to assist fatigue detection in the real world target domain. Building on this idea, we design a heterogeneous and multiple source fatigue detection framework that adaptively utilizes the available modalities in the target domain while exploiting diverse configurations in the source domains through alignment across domains and modality imputation. Our experiments, conducted using a field deployed sensor setup and two publicly available human fatigue datasets, demonstrate the practicality, robustness, and improved generalization of our approach across subjects and domains. The proposed method achieves consistent gains over strong baselines in sensor constrained scenarios.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2507.16859", "pdf_url": "https://arxiv.org/pdf/2507.16859.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1326": {"title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "authors": ["Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan"], "abstract": "arXiv:2508.00079v2 Announce Type: replace-cross \nThe discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.00079", "pdf_url": "https://arxiv.org/pdf/2508.00079.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1327": {"title": "TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing", "authors": ["Jun Qi, Chao-Han Yang, Pin-Yu Chen, Min-Hsiu Hsieh"], "abstract": "arXiv:2508.01116v2 Announce Type: replace-cross \nVariational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2508.01116", "pdf_url": "https://arxiv.org/pdf/2508.01116.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1328": {"title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning", "authors": ["Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun"], "abstract": "arXiv:2508.01522v3 Announce Type: replace-cross \nThis paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl", "categories": ["cs.RO", "cs.AI", "cs.MA"], "abs_url": "https://arxiv.org/abs/2508.01522", "pdf_url": "https://arxiv.org/pdf/2508.01522.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1329": {"title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction", "authors": ["Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang"], "abstract": "arXiv:2508.03159v2 Announce Type: replace-cross \nDrug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.03159", "pdf_url": "https://arxiv.org/pdf/2508.03159.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1330": {"title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs", "authors": ["Ben Zhang, LuLu Yu, Lei Gao, QuanJiang Guo, Jing Liu, Hui Gao"], "abstract": "arXiv:2508.04201v2 Announce Type: replace-cross \nDuring reasoning in vision-language models (VLMs), false positive (FP) reasoning occurs when a model produces the correct answer but follows an incorrect reasoning path, resulting in undermined reasoning reliability. Existing approaches mainly rely on prompt engineering, knowledge distillation or reinforcement learning to improve reasoning reliability, both of which require large amounts of high-quality data and thus limit practical applicability. Few approaches have focused on directly detecting and correcting FPs. To address these issues, we propose ViFP, a framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs. ViFP builds effective reasoning paths through multi-turn QA and dynamically analyzes the consistency of the reasoning path to identify potential FPs. It also introduces a targeted reasoning chain correction mechanism to modify FP reasoning, thereby improving logical consistency and accuracy. Finally, we introduce a reliability evaluation metric, VoC, which integrates answer accuracy and the FP rate, providing a quantitative tool to assess whether a VLM not only answers correctly but also reasons reliably. Our experiments on closed-source VLMs show that ViFP consistently improves performance across three datasets: A-OKVQA, OK-VQA, and FVQA. On A-OKVQA, ViFP improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by 4.3%, and significantly reduces the number of FPs, validating its benefits in enhancing reasoning reliability.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.04201", "pdf_url": "https://arxiv.org/pdf/2508.04201.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1331": {"title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off", "authors": ["Seungyong Lee, Jeong-gi Kwak"], "abstract": "arXiv:2508.04825v2 Announce Type: replace-cross \nVirtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2508.04825", "pdf_url": "https://arxiv.org/pdf/2508.04825.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1332": {"title": "Fast weight programming and linear transformers: from machine learning to neurobiology", "authors": ["Kazuki Irie, Samuel J. Gershman"], "abstract": "arXiv:2508.08435v2 Announce Type: replace-cross \nRecent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "abs_url": "https://arxiv.org/abs/2508.08435", "pdf_url": "https://arxiv.org/pdf/2508.08435.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1333": {"title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination", "authors": ["Meng Gai, Guoping Wang, Sheng Li"], "abstract": "arXiv:2508.08826v3 Announce Type: replace-cross \nReal-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.", "categories": ["cs.GR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.08826", "pdf_url": "https://arxiv.org/pdf/2508.08826.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1334": {"title": "Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology", "authors": ["Jonathan Williams Ramirez, Dina Zemlyanker, Lucas Deden-Binder, Rogeny Herisse, Erendira Garcia Pallares, Karthik Gopinath, Harshvardhan Gazula, Christopher Mount, Liana N. Kozanno, Michael S. Marshall, Theresa R. Connors, Matthew P. Frosch, Mark Montine, Derek H. Oakley, Christine L. Mac Donald, C. Dirk Keene, Bradley T. Hyman, Juan Eugenio Iglesias"], "abstract": "arXiv:2508.09805v2 Announce Type: replace-cross \nAdvances in image registration and machine learning have recently enabled volumetric analysis of postmortem brain tissue from conventional photographs of coronal slabs, which are routinely collected in brain banks and neuropathology laboratories worldwide. One caveat of this methodology is the requirement of segmentation of the tissue from photographs, which currently requires costly manual intervention. In this article, we present a deep learning model to automate this process. The automatic segmentation tool relies on a U-Net architecture that was trained with a combination of 1,414 manually segmented images of both fixed and fresh tissue, from specimens with varying diagnoses, photographed at two different sites. Automated model predictions on a subset of photographs not seen in training were analyzed to estimate performance compared to manual labels, including both inter- and intra-rater variability. Our model achieved a median Dice score over 0.98, mean surface distance under 0.4mm, and 95\\% Hausdorff distance under 1.60mm, which approaches inter-/intra-rater levels. Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.09805", "pdf_url": "https://arxiv.org/pdf/2508.09805.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1335": {"title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design", "authors": ["Zihao Chen, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Xinyi Yang, Mingjie Zhou, Zhuoyao Du, Xu Yan, Zhouyang Wu, Zhenyu Xu, Jiangli Huang, Li Shang, Xuan Zeng, Fan Yang"], "abstract": "arXiv:2508.10409v2 Announce Type: replace-cross \nIn this paper, we propose AnalogSeeker, an effort toward an open-source foundation language model for analog circuit design, with the aim of integrating domain knowledge and giving design assistance. To overcome the scarcity of data in this field, we employ a corpus collection strategy based on the domain knowledge framework of analog circuits. High-quality, accessible textbooks across relevant subfields are systematically curated and cleaned into a textual domain corpus. To address the complexity of knowledge of analog circuits, we introduce a granular domain knowledge distillation method. Raw, unlabeled domain corpus is decomposed into typical, granular learning nodes, where a multi-agent framework distills implicit knowledge embedded in unstructured text into question-answer data pairs with detailed reasoning processes, yielding a fine-grained, learnable dataset for fine-tuning. To address the unexplored challenges in training analog circuit foundation models, we explore and share our training methods through both theoretical analysis and experimental validation. We finally establish a fine-tuning-centric training paradigm, customizing and implementing a neighborhood self-constrained supervised fine-tuning algorithm. This approach enhances training outcomes by constraining the perturbation magnitude between the model's output distributions before and after training. In practice, we train the Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04% accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark, with a 15.67% point improvement over the original model and is competitive with mainstream commercial models. Furthermore, AnalogSeeker also shows effectiveness in the downstream operational amplifier design task. AnalogSeeker is open-sourced at https://huggingface.co/analogllm/analogseeker for research use.", "categories": ["cs.AR", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.10409", "pdf_url": "https://arxiv.org/pdf/2508.10409.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1336": {"title": "\"Accessibility people, you go work on that thing of yours over there\": Addressing Disability Inclusion in AI Product Organizations", "authors": ["Sanika Moharana, Cynthia L. Bennett, Erin Buehler, Michael Madaio, Vinita Tibdewal, Shaun K. Kane"], "abstract": "arXiv:2508.16607v2 Announce Type: replace-cross \nThe rapid emergence of generative AI has changed the way that technology is designed, constructed, maintained, and evaluated. Decisions made when creating AI-powered systems may impact some users disproportionately, such as people with disabilities. In this paper, we report on an interview study with 25 AI practitioners across multiple roles (engineering, research, UX, and responsible AI) about how their work processes and artifacts may impact end users with disabilities. We found that practitioners experienced friction when triaging problems at the intersection of responsible AI and accessibility practices, navigated contradictions between accessibility and responsible AI guidelines, identified gaps in data about users with disabilities, and gathered support for addressing the needs of disabled stakeholders by leveraging informal volunteer and community groups within their company. Based on these findings, we offer suggestions for new resources and process changes to better support people with disabilities as end users of AI.", "categories": ["cs.HC", "cs.AI"], "abs_url": "https://arxiv.org/abs/2508.16607", "pdf_url": "https://arxiv.org/pdf/2508.16607.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1337": {"title": "Activation Transport Operators", "authors": ["Andrzej Szablewski, Marek Masiak"], "abstract": "arXiv:2508.17540v2 Announce Type: replace-cross \nThe residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2508.17540", "pdf_url": "https://arxiv.org/pdf/2508.17540.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1338": {"title": "GDS Agent for Graph Algorithmic Reasoning", "authors": ["Borun Shi, Ioannis Panagiotas"], "abstract": "arXiv:2508.20637v2 Announce Type: replace-cross \nLarge language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2508.20637", "pdf_url": "https://arxiv.org/pdf/2508.20637.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1339": {"title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs", "authors": ["Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, Leandros Maglaras"], "abstract": "arXiv:2509.10594v2 Announce Type: replace-cross \nArtificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, \"SME-TEAM\" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.", "categories": ["cs.LG", "cs.AI", "cs.CR"], "abs_url": "https://arxiv.org/abs/2509.10594", "pdf_url": "https://arxiv.org/pdf/2509.10594.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1340": {"title": "Evaluating Large Language Models for Detecting Antisemitism", "authors": ["Jay Patel, Hrudayangam Mehta, Jeremy Blackburn"], "abstract": "arXiv:2509.18293v2 Announce Type: replace-cross \nDetecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition. We also study how LLMs understand and explain their decisions given a moderation policy as a guideline. First, we explore various prompting techniques and design a new CoT-like prompt, Guided-CoT, and find that injecting domain-specific thoughts increases performance and utility. Guided-CoT handles the in-context policy well, improving performance and utility by reducing refusals across all evaluated models, regardless of decoding configuration, model size, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability. Code and resources available at: https://github.com/idramalab/quantify-llm-explanations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "abs_url": "https://arxiv.org/abs/2509.18293", "pdf_url": "https://arxiv.org/pdf/2509.18293.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1341": {"title": "A Unified Formal Theory on the Logical Limits of Symbol Grounding", "authors": ["Zhangchi Liu"], "abstract": "arXiv:2509.20409v3 Announce Type: replace-cross \nThis paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We demonstrate through a four-stage argument that meaning within a formal system must arise from a process that is external, dynamic, and non-algorithmic. First, we prove that any purely symbolic system, devoid of external connections, cannot internally establish a consistent foundation for meaning due to self-referential paradoxes. Second, we extend this limitation to systems with any finite, static set of pre-established meanings, proving they are inherently incomplete. Third, we demonstrate that the grounding process is logically incomplete; specifically, the 'act' of connecting internal symbols to novel, emergent external meanings cannot be a product of logical inference within the system but must be an axiomatic, meta-level update. Finally, we prove that any attempt to automate this update process using a fixed, external \"judgment\" algorithm will inevitably construct a larger, yet equally incomplete, symbolic system. Together, these conclusions formally establish that the grounding of meaning is a necessarily open-ended, non-algorithmic process, revealing a fundamental, G\\\"odel-style limitation for any self-contained intelligent system.", "categories": ["cs.LO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.20409", "pdf_url": "https://arxiv.org/pdf/2509.20409.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1342": {"title": "Automatic Discovery of One-Parameter Subgroups of Lie Groups: Compact and Non-Compact Cases of $\\mathbf{SO(n)}$ and $\\mathbf{SL(n)}$", "authors": ["Pavan Karjol, Vivek V Kashyap, Rohan Kashyap, Prathosh A P"], "abstract": "arXiv:2509.22219v3 Announce Type: replace-cross \nWe introduce a novel framework for the automatic discovery of one-parameter subgroups ($H_{\\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter subgroups of $SO(n)$ are crucial in a wide range of applications, including robotics, quantum mechanics, and molecular structure analysis. Our method utilizes the standard Jordan form of skew-symmetric matrices, which define the Lie algebra of $SO(n)$, to establish a canonical form for orbits under the action of $H_{\\gamma}$. This canonical form is then employed to derive a standardized representation for $H_{\\gamma}$-invariant functions. By learning the appropriate parameters, the framework uncovers the underlying one-parameter subgroup $H_{\\gamma}$. The effectiveness of the proposed approach is demonstrated through tasks such as double pendulum modeling, moment of inertia prediction, top quark tagging and invariant polynomial regression, where it successfully recovers meaningful subgroup structure and produces interpretable, symmetry-aware representations.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2509.22219", "pdf_url": "https://arxiv.org/pdf/2509.22219.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1343": {"title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG", "authors": ["Vaskar Chakma, Ju Xiaolin, Heling Cao, Xue Feng, Ji Xiaodong, Pan Haiyan, Gao Zhan"], "abstract": "arXiv:2509.25804v2 Announce Type: replace-cross \nThis study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.", "categories": ["cs.LG", "cs.AI", "cs.NI"], "abs_url": "https://arxiv.org/abs/2509.25804", "pdf_url": "https://arxiv.org/pdf/2509.25804.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1344": {"title": "Training Optimal Large Diffusion Language Models", "authors": ["Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, Michael Qizhe Shieh"], "abstract": "arXiv:2510.03280v2 Announce Type: replace-cross \nWe introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.03280", "pdf_url": "https://arxiv.org/pdf/2510.03280.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1345": {"title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing", "authors": ["Joseph Ramsey, Bryan Andrews, Peter Spirtes"], "abstract": "arXiv:2510.04263v3 Announce Type: replace-cross \nLearning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but performs exhaustive conditional independence tests across many subsets, often leading to spurious independences, missing or extra edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that extend this framework. First, we introduce BOSS-FCI and GRaSP-FCI, variants of GFCI (Greedy Fast Causal Inference) that substitute BOSS (Best Order Score Search) or GRaSP (Greedy Relaxations of Sparsest Permutation) for FGES (Fast Greedy Equivalence Search), preserving correctness while trading off scalability and conservativeness. Second, we develop FCI Targeted-Testing (FCIT), a novel hybrid method that replaces exhaustive testing with targeted, score-informed tests guided by BOSS. FCIT guarantees well-formed PAGs and achieves higher precision and efficiency across sample sizes. Finally, we propose a lightweight heuristic, LV-Dumb (Latent Variable \"Dumb\"), which returns the PAG of the BOSS DAG (Directed Acyclic Graph). Though not strictly sound for latent confounding, LV-Dumb often matches FCIT's accuracy while running substantially faster. Simulations and real-data analyses show that BOSS-FCI and GRaSP-FCI provide robust baselines, FCIT yields the best balance of precision and reliability, and LV-Dumb offers a fast, near-equivalent alternative. Together, these methods demonstrate that targeted and score-guided strategies can dramatically improve the efficiency and correctness of latent-variable causal discovery.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.04263", "pdf_url": "https://arxiv.org/pdf/2510.04263.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1346": {"title": "DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification", "authors": ["Tao Xie, Zexi Tan, Haoyi Xiao, Binbin Sun, Yiqun Zhang"], "abstract": "arXiv:2510.12214v2 Announce Type: replace-cross \nEarly Time Series Classification (ETSC) is critical in time-sensitive medical applications such as sepsis, yet it presents an inherent trade-off between accuracy and earliness. This trade-off arises from two core challenges: 1) models should effectively model inherently weak and noisy early-stage snippets, and 2) they should resolve the complex, dual requirement of simultaneously capturing local, subject-specific variations and overarching global temporal patterns. Existing methods struggle to overcome these underlying challenges, often forcing a severe compromise: sacrificing accuracy to achieve earliness, or vice-versa. We propose \\textbf{DE3S}, a \\textbf{D}ual-\\textbf{E}nhanced \\textbf{S}oft-\\textbf{S}parse \\textbf{S}equence Learning framework, which systematically solves these challenges. A dual enhancement mechanism is proposed to enhance the modeling of weak, early signals. Then, an attention-based patch module is introduced to preserve discriminative information while reducing noise and complexity. A dual-path fusion architecture is designed, using a sparse mixture of experts to model local, subject-specific variations. A multi-scale inception module is also employed to capture global dependencies. Experiments on six real-world medical datasets show the competitive performance of DE3S, particularly in early prediction windows. Ablation studies confirm the effectiveness of each component in addressing its targeted challenge. The source code is available \\href{https://github.com/kuxit/DE3S}{\\textbf{here}}.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.12214", "pdf_url": "https://arxiv.org/pdf/2510.12214.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1347": {"title": "FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs", "authors": ["Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo"], "abstract": "arXiv:2510.12839v2 Announce Type: replace-cross \nEvaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \\textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \\textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY"], "abs_url": "https://arxiv.org/abs/2510.12839", "pdf_url": "https://arxiv.org/pdf/2510.12839.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1348": {"title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness", "authors": ["Fali Wang, Jihai Chen, Shuhua Yang, Ali Al-Lawati, Linli Tang, Hui Liu, Suhang Wang"], "abstract": "arXiv:2510.13890v2 Announce Type: replace-cross \nLarge language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.13890", "pdf_url": "https://arxiv.org/pdf/2510.13890.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1349": {"title": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction", "authors": ["Qianheng Xu"], "abstract": "arXiv:2510.18938v2 Announce Type: replace-cross \nOver 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions. This work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription. StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic-linguistic representations. Both architectures are trained on paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset. Across all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore. The results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human-computer interaction, speech therapy, and accessibility-oriented AI systems.", "categories": ["eess.AS", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.18938", "pdf_url": "https://arxiv.org/pdf/2510.18938.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1350": {"title": "AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation", "authors": ["Xianyang Liu, Yilin Liu, Shuai Wang, Hao Cheng, Andrew Estornell, Yuzhi Zhao, Jiaheng Wei"], "abstract": "arXiv:2510.19361v2 Announce Type: replace-cross \nThe creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.", "categories": ["cs.CL", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.19361", "pdf_url": "https://arxiv.org/pdf/2510.19361.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1351": {"title": "A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems", "authors": ["Nivar Anwer (Georgia Institute of Technology, USA), Ezequiel L\\'opez-Rubio (University of M\\'alaga, Spain,IBIMA Plataforma BIONAND, Spain), David Elizondo (De Montfort University, United Kingdom), Rafael M. Luque-Baena (University of M\\'alaga, Spain,IBIMA Plataforma BIONAND, Spain)"], "abstract": "arXiv:2510.19444v2 Announce Type: replace-cross \nThe analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\\varepsilon$, yielding an adjunction $Q_\\varepsilon \\dashv R_\\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $\\mu$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.", "categories": ["cs.LO", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.19444", "pdf_url": "https://arxiv.org/pdf/2510.19444.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1352": {"title": "The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems", "authors": ["Bentley DeVilling (Course Correct Labs, Independent Research Group)"], "abstract": "arXiv:2510.21861v2 Announce Type: replace-cross \nLarge language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.21861", "pdf_url": "https://arxiv.org/pdf/2510.21861.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1353": {"title": "Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity", "authors": ["Siddharth Chaudhary"], "abstract": "arXiv:2510.21908v2 Announce Type: replace-cross \nLarge language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.", "categories": ["cs.NE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.21908", "pdf_url": "https://arxiv.org/pdf/2510.21908.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1354": {"title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery", "authors": ["Bo Yue, Sheng Xu, Kui Jia, Guiliang Liu"], "abstract": "arXiv:2510.22336v2 Announce Type: replace-cross \nHumanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that RoboCraft achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.", "categories": ["cs.RO", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.22336", "pdf_url": "https://arxiv.org/pdf/2510.22336.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1355": {"title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression (Technical Report)", "authors": ["Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Panos Kalnis"], "abstract": "arXiv:2510.23632v2 Announce Type: replace-cross \nThe rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.23632", "pdf_url": "https://arxiv.org/pdf/2510.23632.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1356": {"title": "Using latent representations to link disjoint longitudinal data for mixed-effects regression", "authors": ["Clemens Sch\\\"achter, Maren Hackenberg, Michelle Pfaffenlehner, F\\'elix B. Tambe-Ndonfack, Thorsten Schmidt, Astrid Pechmann, Janbernd Kirschner, Jan Hasenauer, Harald Binder"], "abstract": "arXiv:2510.25531v2 Announce Type: replace-cross \nMany rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.", "categories": ["stat.ML", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.25531", "pdf_url": "https://arxiv.org/pdf/2510.25531.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1357": {"title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios", "authors": ["Runsheng Xu, Hubert Lin, Wonseok Jeon, Hao Feng, Yuliang Zou, Liting Sun, John Gorman, Kate Tolstaya, Sarah Tang, Brandyn White, Ben Sapp, Mingxing Tan, Jyh-Jing Hwang, Dragomir Anguelov"], "abstract": "arXiv:2510.26125v2 Announce Type: replace-cross \nVision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.", "categories": ["cs.CV", "cs.AI"], "abs_url": "https://arxiv.org/abs/2510.26125", "pdf_url": "https://arxiv.org/pdf/2510.26125.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 1.0, "subfield": "\u7aef\u5230\u7aef\u5b66\u4e60 / \u4eff\u771f\u8bc4\u4f30 / \u8f68\u8ff9\u9884\u6d4b", "reason": "The paper introduces the Waymo Open Dataset for End-to-End Driving (WOD-E2E), specifically targeting challenging long-tail scenarios for vision-based autonomous driving. It proposes a new evaluation metric and benchmark for assessing E2E driving performance, focusing on trajectory prediction and robustness in complex driving environments. The content directly addresses autonomous driving system development and evaluation."}, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1358": {"title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation", "authors": ["Musfiqur Rahman, SayedHassan Khatoonabadi, Emad Shihab"], "abstract": "arXiv:2510.26130v2 Announce Type: replace-cross \nLarge language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.\n  We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.\n  Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.\n  These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.", "categories": ["cs.SE", "cs.AI", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.26130", "pdf_url": "https://arxiv.org/pdf/2510.26130.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1359": {"title": "CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization", "authors": ["Zijian Zhang, Rong Wang, Shiyang Li, Yuebo Luo, Mingyi Hong, Caiwen Ding"], "abstract": "arXiv:2511.01884v2 Announce Type: replace-cross \nDeveloping efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\\% correctness of generated kernels and an average 1.68$\\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \\$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \\$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "abs_url": "https://arxiv.org/abs/2511.01884", "pdf_url": "https://arxiv.org/pdf/2511.01884.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1360": {"title": "LA-MARRVEL: A Knowledge-Grounded and Language-Aware LLM Reranker for AI-MARRVEL in Rare Disease Diagnosis", "authors": ["Jaeyeon Lee, Hyun-Hwan Jeong, Zhandong Liu"], "abstract": "arXiv:2511.02263v2 Announce Type: replace-cross \nDiagnosing rare diseases often requires connecting variant-bearing genes to evidence that is written as unstructured clinical prose, which the current established pipelines still leave for clinicians to reconcile manually. To this end, we introduce LA-MARRVEL, a knowledge-grounded and language-aware reranking layer that operates on top of AI-MARRVEL: it supplies expert-engineered context, queries a large language model multiple times, and aggregates the resulting partial rankings with a ranked voting method to produce a stable, explainable gene ranking. Evaluated on three real-world cohorts (BG, DDD, UDN), LA-MARRVEL consistently improves Recall@K over AI-MARRVEL and established phenotype-driven tools such as Exomiser and LIRICAL, with especially large gains on cases where the first-stage ranker placed the causal gene lower. Each ranked gene is accompanied by LLM-generated reasoning that integrates phenotypic, inheritance, and variant-level evidence, thereby making the output more interpretable and facilitating clinical review.", "categories": ["q-bio.GN", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02263", "pdf_url": "https://arxiv.org/pdf/2511.02263.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1361": {"title": "In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization", "authors": ["Cooper Simpson, Stephen Becker, Alireza Doostan"], "abstract": "arXiv:2511.02659v2 Announce Type: replace-cross \nFocusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "abs_url": "https://arxiv.org/abs/2511.02659", "pdf_url": "https://arxiv.org/pdf/2511.02659.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1362": {"title": "Scalable Evaluation and Neural Models for Compositional Generalization", "authors": ["Giacomo Camposampiero, Pietro Barbiero, Michael Hersche, Roger Wattenhofer, Abbas Rahimi"], "abstract": "arXiv:2511.02667v2 Announce Type: replace-cross \nCompositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. Our code is available at https://github.com/IBM/scalable-compositional-generalization.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02667", "pdf_url": "https://arxiv.org/pdf/2511.02667.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1363": {"title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models", "authors": ["Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu"], "abstract": "arXiv:2511.02802v2 Announce Type: replace-cross \nTabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.", "categories": ["cs.LG", "cs.AI"], "abs_url": "https://arxiv.org/abs/2511.02802", "pdf_url": "https://arxiv.org/pdf/2511.02802.pdf", "is_interesting": false, "publish_date": "Fri, 07 Nov 2025 00:00:00 -0500"}, "1364": {"title": "Cropland Mapping using Geospatial Embeddings", "authors": ["Ivan Zvonkov, Gabriel Tseng, Inbal Becker-Reshef, Hannah Kerner"], "abstract": "arXiv:2511.02923v1 Announce Type: new \nAccurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02923", "pdf_url": "https://arxiv.org/pdf/2511.02923.pdf", "is_interesting": false}, "1365": {"title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology", "authors": ["Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher, Phoenix Jarosz, Nathan Jacobs"], "abstract": "arXiv:2511.02946v1 Announce Type: new \nWe introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at https://vishu26.github.io/prom3e.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02946", "pdf_url": "https://arxiv.org/pdf/2511.02946.pdf", "is_interesting": false}, "1366": {"title": "Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification", "authors": ["Mikhael Djajapermana, Moritz Reiber, Daniel Mueller-Gritschneder, Ulf Schlichtmann"], "abstract": "arXiv:2511.02992v1 Announce Type: new \nHybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.02992", "pdf_url": "https://arxiv.org/pdf/2511.02992.pdf", "is_interesting": false}, "1367": {"title": "SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics", "authors": ["Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos, Leonid Sigal"], "abstract": "arXiv:2511.02996v1 Announce Type: new \nVision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02996", "pdf_url": "https://arxiv.org/pdf/2511.02996.pdf", "is_interesting": false}, "1368": {"title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning", "authors": ["Dakota Hester, Vitor S. Martins, Lucas B. Ferreira, Thainara M. A. Lima"], "abstract": "arXiv:2511.03004v1 Announce Type: new \nDeep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the \"Bootstrap Your Own Latent\" pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03004", "pdf_url": "https://arxiv.org/pdf/2511.03004.pdf", "is_interesting": false}, "1369": {"title": "A Foundation Model for Brain MRI with Dynamic Modality Integration", "authors": ["Minh Sao Khue Luu, Bair N. Tuchinov"], "abstract": "arXiv:2511.03014v1 Announce Type: new \nWe present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at https://github.com/BrainFM/brainfm", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03014", "pdf_url": "https://arxiv.org/pdf/2511.03014.pdf", "is_interesting": false}, "1370": {"title": "From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth", "authors": ["Ziyang Xu, Olaf Wysocki, Christoph Holst"], "abstract": "arXiv:2511.03053v1 Announce Type: new \nEvaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds' uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03053", "pdf_url": "https://arxiv.org/pdf/2511.03053.pdf", "is_interesting": false}, "1371": {"title": "A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction", "authors": ["Yi Gong, Xinyuan Zhang, Jichen Chai, Yichen Ding, Yifei Lou"], "abstract": "arXiv:2511.03093v1 Announce Type: new \nCardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.", "categories": ["cs.CV", "cs.NA", "math.NA"], "abs_url": "https://arxiv.org/abs/2511.03093", "pdf_url": "https://arxiv.org/pdf/2511.03093.pdf", "is_interesting": false}, "1372": {"title": "ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly", "authors": ["Miftahur Rahman, Samuel Adebayo, Dorian A. Acevedo-Mejia, David Hester, Daniel McPolin, Karen Rafferty, Debra F. Laefer"], "abstract": "arXiv:2511.03098v1 Announce Type: new \nThe Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.", "categories": ["cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.03098", "pdf_url": "https://arxiv.org/pdf/2511.03098.pdf", "is_interesting": false}, "1373": {"title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs", "authors": ["Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang, Youpeng Yang, Angelos Stefanidis, Limin Yu, Jionglong Su"], "abstract": "arXiv:2511.03099v1 Announce Type: new \nIn orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03099", "pdf_url": "https://arxiv.org/pdf/2511.03099.pdf", "is_interesting": false}, "1374": {"title": "Accelerating Physical Property Reasoning for Augmented Visual Cognition", "authors": ["Hongbo Lan, Zhenlin An, Haoyu Li, Vaibhav Singh, Longfei Shangguan"], "abstract": "arXiv:2511.03126v1 Announce Type: new \nThis paper introduces \\sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \\sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \\sysname reduces the end-to-end latency of this reasoning pipeline from 10--20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \\sysname achieves this 62.9$\\times$--287.2$\\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \\sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \\sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.", "categories": ["cs.CV", "cs.HC"], "abs_url": "https://arxiv.org/abs/2511.03126", "pdf_url": "https://arxiv.org/pdf/2511.03126.pdf", "is_interesting": false}, "1375": {"title": "Finetuning-Free Personalization of Text to Image Generation via Hypernetworks", "authors": ["Sagar Shrestha, Gopal Sharma, Luowei Zhou, Suren Kumar"], "abstract": "arXiv:2511.03156v1 Announce Type: new \nPersonalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03156", "pdf_url": "https://arxiv.org/pdf/2511.03156.pdf", "is_interesting": false}, "1376": {"title": "Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation", "authors": ["Yun-Chen Lin, Jiayuan Huang, Hanyuan Zhang, Sergi Kavtaradze, Matthew J. Clarkson, Mobarak I. Hoque"], "abstract": "arXiv:2511.03163v1 Announce Type: new \nAccurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03163", "pdf_url": "https://arxiv.org/pdf/2511.03163.pdf", "is_interesting": false}, "1377": {"title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention", "authors": ["Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque"], "abstract": "arXiv:2511.03178v1 Announce Type: new \nAnticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03178", "pdf_url": "https://arxiv.org/pdf/2511.03178.pdf", "is_interesting": false}, "1378": {"title": "PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research", "authors": ["Le Xue, Gang Feng, Wenbo Zhang, Yichi Zhang, Lanlan Li, Shuqi Wang, Liling Peng, Sisi Peng, Xin Gao"], "abstract": "arXiv:2511.03194v1 Announce Type: new \nPublicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed Tomography (PET/CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03194", "pdf_url": "https://arxiv.org/pdf/2511.03194.pdf", "is_interesting": false}, "1379": {"title": "MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction", "authors": ["Ruting Cheng, Boyuan Feng, Yijiang Zheng, Chuhui Qiu, Aizierjiang Aiersilan, Joaquin A. Calderon, Wentao Zhao, Qing Pan, James K. Hahn"], "abstract": "arXiv:2511.03212v1 Announce Type: new \nAccurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model's predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model's decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03212", "pdf_url": "https://arxiv.org/pdf/2511.03212.pdf", "is_interesting": false}, "1380": {"title": "Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation", "authors": ["Pengyu Jie, Wanquan Liu, Rui He, Yihui Wen, Deyu Meng, Chenqiang Gao"], "abstract": "arXiv:2511.03219v1 Announce Type: new \nAugmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03219", "pdf_url": "https://arxiv.org/pdf/2511.03219.pdf", "is_interesting": false}, "1381": {"title": "Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution", "authors": ["Sichen Guo, Wenjie Li, Yuanyang Liu, Guangwei Gao, Jian Yang, Chia-Wen Lin"], "abstract": "arXiv:2511.03232v1 Announce Type: new \nRecently, Mamba-based super-resolution (SR) methods have demonstrated the ability to capture global receptive fields with linear complexity, addressing the quadratic computational cost of Transformer-based SR approaches. However, existing Mamba-based methods lack fine-grained transitions across different modeling scales, which limits the efficiency of feature representation. In this paper, we propose T-PMambaSR, a lightweight SR framework that integrates window-based self-attention with Progressive Mamba. By enabling interactions among receptive fields of different scales, our method establishes a fine-grained modeling paradigm that progressively enhances feature representation with linear complexity. Furthermore, we introduce an Adaptive High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost during Transformer and Mamba processing. Extensive experiments demonstrate that T-PMambaSR progressively enhances the model's receptive field and expressiveness, yielding better performance than recent Transformer- or Mamba-based methods while incurring lower computational cost. Our codes will be released after acceptance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03232", "pdf_url": "https://arxiv.org/pdf/2511.03232.pdf", "is_interesting": false}, "1382": {"title": "Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning", "authors": ["Liwei Luo, Shuaitengyuan Li, Dongwei Ren, Qilong Wang, Pengfei Zhu, Qinghua Hu"], "abstract": "arXiv:2511.03245v1 Announce Type: new \nRecently, remarkable progress has been made in large-scale pre-trained model tuning, and inference efficiency is becoming more crucial for practical deployment. Early exiting in conjunction with multi-stage predictors, when cooperated with a parameter-efficient fine-tuning strategy, offers a straightforward way to achieve an inference-efficient model. However, a key challenge remains unresolved: How can early stages provide low-level fundamental features to deep stages while simultaneously supplying high-level discriminative features to early-stage predictors? To address this problem, we propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively decouple the low-level representative ability and high-level discriminative ability in early stages. First, in terms of architecture, we introduce a lightweight bypass module into multi-stage predictors for functional decomposition of shallow features from early stages, while a high-order statistics-based predictor is developed for early stages to effectively enhance their discriminative ability. To reasonably train our multi-predictor architecture, a decoupled optimization is proposed to allocate two-phase loss weights for multi-stage predictors during model tuning, where the initial training phase enables the model to prioritize the acquisition of discriminative ability of deep stages via emphasizing representative ability of early stages, and the latter training phase drives discriminative ability towards earlier stages as much as possible. As such, our DMPO can effectively decouple representative and discriminative abilities in early stages in terms of architecture design and model optimization. Experiments across various datasets and pre-trained backbones demonstrate that DMPO clearly outperforms its counterparts when reducing computational cost.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03245", "pdf_url": "https://arxiv.org/pdf/2511.03245.pdf", "is_interesting": false}, "1383": {"title": "Enhancing Medical Image Segmentation via Heat Conduction Equation", "authors": ["Rong Wu, Yim-Sang Yu"], "abstract": "arXiv:2511.03260v1 Announce Type: new \nMedical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03260", "pdf_url": "https://arxiv.org/pdf/2511.03260.pdf", "is_interesting": false}, "1384": {"title": "IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection", "authors": ["Bingyang Guo, Hongjie Li, Ruiyun Yu, Hanzhe Liang, Jinbao Wang"], "abstract": "arXiv:2511.03267v1 Announce Type: new \n3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03267", "pdf_url": "https://arxiv.org/pdf/2511.03267.pdf", "is_interesting": false}, "1385": {"title": "Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising", "authors": ["Shuangquan Lyu, Steven Mao, Yue Ma"], "abstract": "arXiv:2511.03272v1 Announce Type: new \nGenerating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting/outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03272", "pdf_url": "https://arxiv.org/pdf/2511.03272.pdf", "is_interesting": false}, "1386": {"title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models", "authors": ["Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang"], "abstract": "arXiv:2511.03317v1 Announce Type: new \nText-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03317", "pdf_url": "https://arxiv.org/pdf/2511.03317.pdf", "is_interesting": false}, "1387": {"title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding", "authors": ["Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque"], "abstract": "arXiv:2511.03325v1 Announce Type: new \nVideo Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\\% on REAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03325", "pdf_url": "https://arxiv.org/pdf/2511.03325.pdf", "is_interesting": false}, "1388": {"title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge", "authors": ["Yi Yang, Yiming Xu, Timo Kaiser, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang"], "abstract": "arXiv:2511.03332v1 Announce Type: new \nIn this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03332", "pdf_url": "https://arxiv.org/pdf/2511.03332.pdf", "is_interesting": false}, "1389": {"title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions", "authors": ["Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang"], "abstract": "arXiv:2511.03334v1 Announce Type: new \nDue to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03334", "pdf_url": "https://arxiv.org/pdf/2511.03334.pdf", "is_interesting": false}, "1390": {"title": "Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort", "authors": ["Nikolai Herrmann, Marcella C. Zijta, Stefan Klein, R\\'egine P. M. Steegers-Theunissen, Rene M. H. Wijnen, Bernadette S. de Bakker, Melek Rousian, Wietske A. P. Bastiaansen"], "abstract": "arXiv:2511.03416v1 Announce Type: new \nStandardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo's principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson's correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03416", "pdf_url": "https://arxiv.org/pdf/2511.03416.pdf", "is_interesting": false}, "1391": {"title": "Generalizing Shape-from-Template to Topological Changes", "authors": ["Kevin Manogue, Tomasz M Schang, Dilara Ku\\c{s}, Jonas M\\\"uller, Stefan Zachow, Agniva Sengupta"], "abstract": "arXiv:2511.03459v1 Announce Type: new \nReconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03459", "pdf_url": "https://arxiv.org/pdf/2511.03459.pdf", "is_interesting": false}, "1392": {"title": "Human Mesh Modeling for Anny Body", "authors": ["Romain Br\\'egier, Gu\\'enol\\'e Fiche, Laura Bravo-S\\'anchez, Thomas Lucas, Matthieu Armando, Philippe Weinzaepfel, Gr\\'egory Rogez, Fabien Baradel"], "abstract": "arXiv:2511.03589v1 Announce Type: new \nParametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms -- across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling -- supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03589", "pdf_url": "https://arxiv.org/pdf/2511.03589.pdf", "is_interesting": false}, "1393": {"title": "Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals", "authors": ["Vittal L. Rao"], "abstract": "arXiv:2511.03645v1 Announce Type: new \nLocalisation tasks in biomedical data often require models to learn meaningful spatial or temporal relationships from signals with complex intensity distributions. A common strategy, exemplified by CoordConv layers, is to append coordinate channels to convolutional inputs, enabling networks to learn absolute positions. In this work, we propose a signal intensity-weighted coordinate representation that replaces the pure coordinate channels with channels scaled by local signal intensity. This modification embeds an intensity-position coupling directly in the input representation, introducing a simple and modality-agnostic inductive bias. We evaluate the approach on two distinct localisation problems: (i) predicting the time of morphological transition in 20-second, two-lead ECG signals, and (ii) regressing the coordinates of nuclear centres in cytological images from the SiPaKMeD dataset. In both cases, the proposed representation yields faster convergence and higher generalisation performance relative to conventional coordinate-channel approaches, demonstrating its effectiveness across both one-dimensional and two-dimensional biomedical signals.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03645", "pdf_url": "https://arxiv.org/pdf/2511.03645.pdf", "is_interesting": false}, "1394": {"title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential", "authors": ["Mehdi Sefidgar Dilmaghani, Francis Fowley, Peter Corcoran"], "abstract": "arXiv:2511.03665v1 Announce Type: new \nThis paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03665", "pdf_url": "https://arxiv.org/pdf/2511.03665.pdf", "is_interesting": false}, "1395": {"title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection", "authors": ["Dongkeun Kim, Minsu Cho, Suha Kwak"], "abstract": "arXiv:2511.03666v1 Announce Type: new \nSocial interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03666", "pdf_url": "https://arxiv.org/pdf/2511.03666.pdf", "is_interesting": false}, "1396": {"title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition", "authors": ["Jongseo Lee, Wooil Lee, Gyeong-Moon Park, Seong Tae Kim, Jinwoo Choi"], "abstract": "arXiv:2511.03725v1 Announce Type: new \nEffective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03725", "pdf_url": "https://arxiv.org/pdf/2511.03725.pdf", "is_interesting": false}, "1397": {"title": "Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData", "authors": ["Beyza Cinar, Maria Maleshkova"], "abstract": "arXiv:2511.02849v1 Announce Type: cross \nIndividualized therapy is driven forward by medical data analysis, which provides insight into the patient's context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\\le$ 25 min) are imputed with linear interpolation and larger gaps ($\\ge$ 30 and $<$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\\le$ 70 mg/dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.", "categories": ["eess.SP", "cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.02849", "pdf_url": "https://arxiv.org/pdf/2511.02849.pdf", "is_interesting": false}, "1398": {"title": "Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset", "authors": ["Chukwuemeka Arua Kalu, Adaobi Chiazor Emegoakor, Fortune Okafor, Augustine Okoh Uchenna, Chijioke Kelvin Ukpai, Godsent Erere Onyeugbo"], "abstract": "arXiv:2511.02893v1 Announce Type: cross \nMedical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks.\n  This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net's robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02893", "pdf_url": "https://arxiv.org/pdf/2511.02893.pdf", "is_interesting": false}, "1399": {"title": "Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI", "authors": ["Ilerioluwakiiye Abolade, Aniekan Udo, Augustine Ojo, Abdulbasit Oyetunji, Hammed Ajigbotosho, Aondana Iorumbur, Confidence Raymond, Maruf Adewole"], "abstract": "arXiv:2511.02928v1 Announce Type: cross \nGlioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02928", "pdf_url": "https://arxiv.org/pdf/2511.02928.pdf", "is_interesting": false}, "1400": {"title": "Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data", "authors": ["Syed Mostaquim Ali, Taufiq Rahman, Ghazal Farhani, Mohamed H. Zaki, Benoit Anctil, Dominique Charlebois"], "abstract": "arXiv:2511.02994v1 Announce Type: cross \nFor developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21\\% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.", "categories": ["cs.RO", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.02994", "pdf_url": "https://arxiv.org/pdf/2511.02994.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u4eff\u771f\u8bc4\u4f30 / \u8f66\u8f7d\u4f20\u611f\u5668\u7b97\u6cd5", "reason": "The paper discusses the comparison of real-world and simulated LiDAR scans, a key topic in autonomous driving system testing and sensor evaluation. The use of Virtual Testing Environments (VTE) for assessing sensor outputs, such as LiDAR scans, is highly relevant to autonomous driving, as it directly impacts sensor performance and system perception in ADS development."}}, "1401": {"title": "Data-Efficient Realized Volatility Forecasting with Vision Transformers", "authors": ["Emi Soroka, Artem Arzyn"], "abstract": "arXiv:2511.03046v1 Announce Type: cross \nRecent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03046", "pdf_url": "https://arxiv.org/pdf/2511.03046.pdf", "is_interesting": false}, "1402": {"title": "Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models", "authors": ["Haotian Yin, Przemyslaw Musialski"], "abstract": "arXiv:2511.03147v1 Announce Type: cross \nNeural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.", "categories": ["cs.GR", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2511.03147", "pdf_url": "https://arxiv.org/pdf/2511.03147.pdf", "is_interesting": false}, "1403": {"title": "Test Time Adaptation Using Adaptive Quantile Recalibration", "authors": ["Paria Mehrbod, Pedro Vianna, Geraldin Nanfack, Guy Wolf, Eugene Belilovsky"], "abstract": "arXiv:2511.03148v1 Announce Type: cross \nDomain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR's potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03148", "pdf_url": "https://arxiv.org/pdf/2511.03148.pdf", "is_interesting": false}, "1404": {"title": "A Probabilistic U-Net Approach to Downscaling Climate Simulations", "authors": ["Maryam Alipourhajiagha, Pierre-Louis Lemaire, Youssef Diouane, Julie Carreau"], "abstract": "arXiv:2511.03197v1 Announce Type: cross \nClimate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.", "categories": ["cs.LG", "cs.CV", "physics.ao-ph"], "abs_url": "https://arxiv.org/abs/2511.03197", "pdf_url": "https://arxiv.org/pdf/2511.03197.pdf", "is_interesting": false}, "1405": {"title": "A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams", "authors": ["Philipp Reis, Philipp Rigoll, Christian Steinhauser, Jacob Langner, Eric Sax"], "abstract": "arXiv:2511.03239v1 Announce Type: cross \nModern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \\ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \\ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \\ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \\ac{FCDC} produces more balanced datasets by $\\SI{25.9}{\\percent}$ while reducing data storage by $\\SI{39.8}{\\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2511.03239", "pdf_url": "https://arxiv.org/pdf/2511.03239.pdf", "is_interesting": false}, "1406": {"title": "Decoupled Entropy Minimization", "authors": ["Jing Ma, Hanlin Li, Xiang Xiang"], "abstract": "arXiv:2511.03256v1 Announce Type: cross \nEntropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.", "categories": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "abs_url": "https://arxiv.org/abs/2511.03256", "pdf_url": "https://arxiv.org/pdf/2511.03256.pdf", "is_interesting": false}, "1407": {"title": "Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology", "authors": ["Gabriela Fernandes"], "abstract": "arXiv:2511.03365v1 Announce Type: cross \nOvarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2\\%$ (Macro AUC of $0.87 \\pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} = 0.82 \\pm 0.02$, $AUC_{BRCA1} = 0.76 \\pm 0.04$, and $AUC_{ARID1A} = 0.73 \\pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "abs_url": "https://arxiv.org/abs/2511.03365", "pdf_url": "https://arxiv.org/pdf/2511.03365.pdf", "is_interesting": false}, "1408": {"title": "Seeing What You Say: Expressive Image Generation from Speech", "authors": ["Jiyoung Lee, Song Park, Sanghyuk Chun, Soo-Whan Chung"], "abstract": "arXiv:2511.03423v1 Announce Type: cross \nThis paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.", "categories": ["eess.AS", "cs.CV", "cs.MM"], "abs_url": "https://arxiv.org/abs/2511.03423", "pdf_url": "https://arxiv.org/pdf/2511.03423.pdf", "is_interesting": false}, "1409": {"title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera", "authors": ["Hao Shi, Ze Wang, Shangwei Guo, Mengfei Duan, Song Wang, Teng Chen, Kailun Yang, Lin Wang, Kaiwei Wang"], "abstract": "arXiv:2511.03571v1 Announce Type: cross \nRobust 3D semantic occupancy is crucial for legged/humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range/occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic occupancy; standardized within-/cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged/humanoid robots. Datasets and code will be publicly available at https://github.com/MasterHow/OneOcc.", "categories": ["cs.RO", "cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2511.03571", "pdf_url": "https://arxiv.org/pdf/2511.03571.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper primarily focuses on semantic occupancy prediction for legged robots using panoramic cameras, which is not directly related to autonomous driving. However, the techniques for vision-based perception could have indirect applications in autonomous driving, especially in scenarios involving vision-based sensing, but it is not explicitly centered on driving tasks."}}, "1410": {"title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural", "authors": ["Andrei A. Korigodskii, Oleg D. Kalachev, Artem E. Vasiunik, Matvei V. Urvantsev, Georgii E. Bondar"], "abstract": "arXiv:2511.03651v1 Announce Type: cross \nThis paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world's largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone's propellers, which also protects the drone's critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system's robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "abs_url": "https://arxiv.org/abs/2511.03651", "pdf_url": "https://arxiv.org/pdf/2511.03651.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0\u5173", "reason": "The paper discusses the development of an autonomous unmanned aerial system for creating large-scale murals, focusing on navigation, trajectory planning, and artistic applications. It does not involve autonomous driving, vehicle perception, or any related tasks in the field of autonomous driving."}}, "1411": {"title": "A New Comprehensive Framework for Multi-Exposure Stereo Coding Utilizing Low Rank Tucker-ALS and 3D-HEVC Techniques", "authors": ["Mansi Sharma, Jyotsana Grover"], "abstract": "arXiv:2104.04726v2 Announce Type: replace \nDisplay technology must offer high dynamic range (HDR) contrast-based depth induction and 3D personalization simultaneously. Efficient algorithms to compress HDR stereo data is critical. Direct capturing of HDR content is complicated due to the high expense and scarcity of HDR cameras. The HDR 3D images could be generated in low-cost by fusing low-dynamic-range (LDR) images acquired using a stereo camera with various exposure settings. In this paper, an efficient scheme for coding multi-exposure stereo images is proposed based on a tensor low-rank approximation scheme. The multi-exposure fusion can be realized to generate HDR stereo output at the decoder for increased realism and exaggerated binocular 3D depth cues.\n  For exploiting spatial redundancy in LDR stereo images, the stack of multi-exposure stereo images is decomposed into a set of projection matrices and a core tensor following an alternating least squares Tucker decomposition model. The compact, low-rank representation of the scene, thus, generated is further processed by 3D extension of High Efficiency Video Coding standard. The encoding with 3D-HEVC enhance the proposed scheme efficiency by exploiting intra-frame, inter-view and the inter-component redundancies in low-rank approximated representation. We consider constant luminance property of IPT and Y'CbCr color space to precisely approximate intensity prediction and perceptually minimize the encoding distortion. Besides, the proposed scheme gives flexibility to adjust the bitrate of tensor latent components by changing the rank of core tensor and its quantization. Extensive experiments on natural scenes demonstrate that the proposed scheme outperforms state-of-the-art JPEG-XT and 3D-HEVC range coding standards.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2104.04726", "pdf_url": "https://arxiv.org/pdf/2104.04726.pdf", "is_interesting": false}, "1412": {"title": "Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised Document Seal Data Generation and Realisation", "authors": ["Mingfu Yan, Jiancheng Huang, Shifeng Chen"], "abstract": "arXiv:2310.00546v2 Announce Type: replace \nSeal-related tasks in document processing-such as seal segmentation, authenticity verification, seal removal, and text recognition under seals-hold substantial commercial importance. However, progress in these areas has been hindered by the scarcity of labeled document seal datasets, which are essential for supervised learning. To address this limitation, we propose Seal2Real, a novel generative framework designed to synthesize large-scale labeled document seal data. As part of this work, we also present Seal-DB, a comprehensive dataset containing 20,000 labeled images to support seal-related research. Seal2Real introduces a prompt prior learning architecture built upon a pre-trained Stable Diffusion model, effectively transferring its generative capability to the unsupervised domain of seal image synthesis. By producing highly realistic synthetic seal images, Seal2Real significantly enhances the performance of downstream seal-related tasks on real-world data. Experimental evaluations on the Seal-DB dataset demonstrate the effectiveness and practical value of the proposed framework.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2310.00546", "pdf_url": "https://arxiv.org/pdf/2310.00546.pdf", "is_interesting": false}, "1413": {"title": "BoxCell: Leveraging SAM for Cell Segmentation with Box Supervision", "authors": ["Aayush Kumar Tyagi, Vaibhav Mishra, Prathosh A. P.,  Mausam"], "abstract": "arXiv:2311.17960v2 Announce Type: replace \nCell segmentation in histopathological images is vital for diagnosis, and treatment of several diseases. Annotating data is tedious, and requires medical expertise, making it difficult to employ supervised learning. Instead, we study a weakly supervised setting, where only bounding box supervision is available, and present the use of Segment Anything (SAM) for this without any finetuning, i.e., directly utilizing the pre-trained model. We propose BoxCell, a cell segmentation framework that utilizes SAM's capability to interpret bounding boxes as prompts, \\emph{both} at train and test times. At train time, gold bounding boxes given to SAM produce (pseudo-)masks, which are used to train a standalone segmenter. At test time, BoxCell generates two segmentation masks: (1) generated by this standalone segmenter, and (2) a trained object detector outputs bounding boxes, which are given as prompts to SAM to produce another mask. Recognizing complementary strengths, we reconcile the two segmentation masks using a novel integer programming formulation with intensity and spatial constraints. We experiment on three publicly available cell segmentation datasets namely, CoNSep, MoNuSeg, and TNBC, and find that BoxCell significantly outperforms existing box supervised image segmentation models, obtaining 6-10 point Dice gains.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2311.17960", "pdf_url": "https://arxiv.org/pdf/2311.17960.pdf", "is_interesting": false}, "1414": {"title": "A Label Propagation Strategy for CutMix in Multi-Label Remote Sensing Image Classification", "authors": ["Tom Burgert, Kai Norman Clasen, Jonas Klotz, Tim Siebert, Beg\\\"um Demir"], "abstract": "arXiv:2405.13451v3 Announce Type: replace \nThe development of supervised deep learning-based methods for multi-label scene classification (MLC) is one of the prominent research directions in remote sensing (RS). However, collecting annotations for large RS image archives is time-consuming and costly. To address this issue, several data augmentation methods have been introduced in RS. Among others, the CutMix data augmentation technique, which combines parts of two existing training images to generate an augmented image, stands out as a particularly effective approach. However, the direct application of CutMix in RS MLC can lead to the erasure or addition of class labels (i.e., label noise) in the augmented (i.e., combined) training image. To address this problem, we introduce a label propagation (LP) strategy that allows the effective application of CutMix in the context of MLC problems in RS without being affected by label noise. To this end, our proposed LP strategy exploits pixel-level class positional information to update the multi-label of the augmented training image. We propose to access such class positional information from reference maps (e.g., thematic products) associated with each training image or from class explanation masks provided by an explanation method if no reference maps are available. Similarly to pairing two training images, our LP strategy carries out a pairing operation on the associated pixel-level class positional information to derive the updated multi-label for the augmented image. Experimental results show the effectiveness of our LP strategy in general (e.g., an improvement of 2% to 4% mAP macro compared to standard CutMix) and its robustness in the case of various simulated and real scenarios with noisy class positional information in particular. Code is available at https://git.tu-berlin.de/rsim/cutmix_lp.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2405.13451", "pdf_url": "https://arxiv.org/pdf/2405.13451.pdf", "is_interesting": false}, "1415": {"title": "ROADWork: A Dataset and Benchmark for Learning to Recognize, Observe, Analyze and Drive Through Work Zones", "authors": ["Anurag Ghosh, Shen Zheng, Robert Tamburo, Khiem Vuong, Juan Alvarez-Padilla, Hailiang Zhu, Michael Cardei, Nicholas Dunn, Christoph Mertz, Srinivasa G. Narasimhan"], "abstract": "arXiv:2406.07661v3 Announce Type: replace \nPerceiving and autonomously navigating through work zones is a challenging and underexplored problem. Open datasets for this long-tailed scenario are scarce. We propose the ROADWork dataset to learn to recognize, observe, analyze, and drive through work zones. State-of-the-art foundation models fail when applied to work zones. Fine-tuning models on our dataset significantly improves perception and navigation in work zones. With ROADWork dataset, we discover new work zone images with higher precision (+32.5%) at a much higher rate (12.8$\\times$) around the world. Open-vocabulary methods fail too, whereas fine-tuned detectors improve performance (+32.2 AP). Vision-Language Models (VLMs) struggle to describe work zones, but fine-tuning substantially improves performance (+36.7 SPICE).\n  Beyond fine-tuning, we show the value of simple techniques. Video label propagation provides additional gains (+2.6 AP) for instance segmentation. While reading work zone signs, composing a detector and text spotter via crop-scaling improves performance +14.2% 1-NED). Composing work zone detections to provide context further reduces hallucinations (+3.9 SPICE) in VLMs. We predict navigational goals and compute drivable paths from work zone videos. Incorporating road work semantics ensures 53.6% goals have angular error (AE) < 0.5 (+9.9 %) and 75.3% pathways have AE < 0.5 (+8.1 %).", "categories": ["cs.CV", "cs.RO"], "abs_url": "https://arxiv.org/abs/2406.07661", "pdf_url": "https://arxiv.org/pdf/2406.07661.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "3D\u611f\u77e5 / \u8f68\u8ff9\u9884\u6d4b / \u8f66\u8def\u534f\u540c", "reason": "The paper focuses on perception and navigation through work zones, which is a crucial task for autonomous driving, especially for the detection and navigation in complex environments. The ROADWork dataset aims to improve work zone recognition, which can directly enhance autonomous vehicle navigation and decision-making systems, making it highly relevant to autonomous driving tasks like 3D perception and trajectory prediction."}}, "1416": {"title": "Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models", "authors": ["Hao Cheng, Erjia Xiao, Yichi Wang, Chengyuan Yu, Mengshu Sun, Qiang Zhang, Jiahang Cao, Yijie Guo, Ning Liu, Kaidi Xu, Jize Zhang, Chao Shen, Philip Torr, Jindong Gu, Renjing Xu"], "abstract": "arXiv:2409.13174v4 Announce Type: replace \nRecently, driven by advancements in Multimodal Large Language Models (MLLMs), Vision Language Action Models (VLAMs) are being proposed to achieve better performance in open-vocabulary scenarios for robotic manipulation tasks. Since manipulation tasks involve direct interaction with the physical world, ensuring robustness and safety during the execution of this task is always a very critical issue. In this paper, by synthesizing current safety research on MLLMs and the specific application scenarios of the manipulation task in the physical world, we comprehensively evaluate VLAMs in the face of potential physical threats. Specifically, we propose the Physical Vulnerability Evaluating Pipeline (PVEP) that can incorporate as many visual modal physical threats as possible for evaluating the physical robustness of VLAMs. The physical threats in PVEP specifically include Out-of-Distribution, Typography-based Visual Prompt, and Adversarial Patch Attacks. By comparing the performance fluctuations of VLAMs before and after being attacked, we provide generalizable \\textbf{\\textit{Analyses}} of how VLAMs respond to different physical threats.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2409.13174", "pdf_url": "https://arxiv.org/pdf/2409.13174.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.2, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "\u8be5\u8bba\u6587\u7814\u7a76\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7269\u7406\u5b89\u5168\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLAMs\uff09\u5728\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\u8bc4\u4f30\u3002\u867d\u7136\u6d89\u53ca\u7269\u7406\u4e16\u754c\u4e2d\u7684\u611f\u77e5\u4e0e\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u5e76\u672a\u8ba8\u8bba\u81ea\u52a8\u9a7e\u9a76\u3001\u8f66\u8f86\u611f\u77e5\u6216\u63a7\u5236\u7b49\u573a\u666f\uff0c\u4ec5\u53ef\u5728\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u6216\u9c81\u68d2\u6027\u7814\u7a76\u4e0a\u95f4\u63a5\u501f\u9274\uff0c\u56e0\u6b64\u4e0e\u81ea\u52a8\u9a7e\u9a76\u5173\u7cfb\u8f83\u5f31\u3002"}}, "1417": {"title": "Disentanglement with Factor Quantized Variational Autoencoders", "authors": ["Gulcin Baykal, Melih Kandemir, Gozde Unal"], "abstract": "arXiv:2409.14851v3 Announce Type: replace \nDisentangled representation learning aims to represent the underlying generative factors of a dataset in a latent representation independently of one another. In our work, we propose a discrete variational autoencoder (VAE) based model where the ground truth information about the generative factors are not provided to the model. We demonstrate the advantages of learning discrete representations over learning continuous representations in facilitating disentanglement. Furthermore, we propose incorporating an inductive bias into the model to further enhance disentanglement. Precisely, we propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook, and we add a total correlation term to the optimization as an inductive bias. Our method called FactorQVAE combines optimization based disentanglement approaches with discrete representation learning, and it outperforms the former disentanglement methods in terms of two disentanglement metrics (DCI and InfoMEC) while improving the reconstruction performance. Our code can be found at https://github.com/ituvisionlab/FactorQVAE.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2409.14851", "pdf_url": "https://arxiv.org/pdf/2409.14851.pdf", "is_interesting": false}, "1418": {"title": "FusionRF: High-Fidelity Satellite Neural Radiance Fields from Multispectral and Panchromatic Acquisitions", "authors": ["Michael Sprintson, Rama Chellappa, Cheng Peng"], "abstract": "arXiv:2409.15132v3 Announce Type: replace \nWe introduce FusionRF, a novel framework for digital surface reconstruction from satellite multispectral and panchromatic images. Current work has demonstrated the increased accuracy of neural photogrammetry for surface reconstruction from optical satellite images compared to algorithmic methods. Common satellites produce both a panchromatic and multispectral image, which contain high spatial and spectral information respectively. Current neural reconstruction methods require multispectral images to be upsampled with a pansharpening method using the spatial data in the panchromatic image. However, these methods may introduce biases and hallucinations due to domain gaps. FusionRF introduces joint image fusion during optimization through a novel cross-resolution kernel that learns to resolve spatial resolution loss present in multispectral images. As input, FusionRF accepts the original multispectral and panchromatic data, eliminating the need for image preprocessing. FusionRF also leverages multimodal appearance embeddings that encode the image characteristics of each modality and view within a uniform representation. By optimizing on both modalities, FusionRF learns to fuse image modalities while performing reconstruction tasks and eliminates the need for a pansharpening preprocessing step. We evaluate our method on multispectral and panchromatic satellite images from the WorldView-3 satellite in various locations, and show that FusionRF provides an average of 17% reduction in depth reconstruction error, and renders sharp training and novel views.", "categories": ["cs.CV", "eess.IV"], "abs_url": "https://arxiv.org/abs/2409.15132", "pdf_url": "https://arxiv.org/pdf/2409.15132.pdf", "is_interesting": false}, "1419": {"title": "SAM-EM: Real-Time Segmentation for Automated Liquid Phase Transmission Electron Microscopy", "authors": ["Alexander Wang, Max Xu, Risha Goel, Zain Shabeeb, Isabel Panicker, Vida Jamali"], "abstract": "arXiv:2501.03153v2 Announce Type: replace \nThe absence of robust segmentation frameworks for noisy liquid phase transmission electron microscopy (LPTEM) videos prevents reliable extraction of particle trajectories, creating a major barrier to quantitative analysis and to connecting observed dynamics with materials characterization and design. To address this challenge, we present Segment Anything Model for Electron Microscopy (SAM-EM), a domain-adapted foundation model that unifies segmentation, tracking, and statistical analysis for LPTEM data. Built on Segment Anything Model 2 (SAM~2), SAM-EM is derived through full-model fine-tuning on 46,600 curated LPTEM synthetic video frames, substantially improving mask quality and temporal identity stability compared to zero-shot SAM~2 and existing baselines. Beyond segmentation, SAM-EM integrates particle tracking with statistical tools, including mean-squared displacement and particle displacement distribution analysis, providing an end-to-end framework for extracting and interpreting nanoscale dynamics. Crucially, full fine-tuning allows SAM-EM to remain robust under low signal-to-noise conditions, such as those caused by increased liquid sample thickness in LPTEM experiments. By establishing a reliable analysis pipeline, SAM-EM transforms LPTEM into a quantitative single-particle tracking platform and accelerates its integration into data-driven materials discovery and design. Project page: \\href{https://github.com/JamaliLab/SAM-EM}{github.com/JamaliLab/SAM-EM}.", "categories": ["cs.CV", "physics.data-an"], "abs_url": "https://arxiv.org/abs/2501.03153", "pdf_url": "https://arxiv.org/pdf/2501.03153.pdf", "is_interesting": false}, "1420": {"title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models", "authors": ["Hao Cheng, Erjia Xiao, Yichi Wang, Lingfeng Zhang, Qiang Zhang, Jiahang Cao, Kaidi Xu, Mengshu Sun, Xiaoshuai Hao, Jindong Gu, Renjing Xu"], "abstract": "arXiv:2503.11519v4 Announce Type: replace \nCurrent Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2503.11519", "pdf_url": "https://arxiv.org/pdf/2503.11519.pdf", "is_interesting": false}, "1421": {"title": "ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation", "authors": ["Lingfeng Wang, Hualing Lin, Senda Chen, Tao Wang, Changxu Cheng, Yangyang Zhong, Dong Zheng, Wuyue Zhao"], "abstract": "arXiv:2505.16495v2 Announce Type: replace \nWhile humans effortlessly draw visual objects and shapes by adaptively allocating attention based on their complexity, existing multimodal large language models (MLLMs) remain constrained by rigid token representations. Bridging this gap, we propose ALTo, an adaptive length tokenizer for autoregressive mask generation. To achieve this, a novel token length predictor is designed, along with a length regularization term and a differentiable token chunking strategy. We further build ALToLLM that seamlessly integrates ALTo into MLLM. Preferences on the trade-offs between mask quality and efficiency is implemented by group relative policy optimization (GRPO). Experiments demonstrate that ALToLLM achieves state-of-the-art performance with adaptive token cost on popular segmentation benchmarks. Code and models are released at https://github.com/yayafengzi/ALToLLM.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.16495", "pdf_url": "https://arxiv.org/pdf/2505.16495.pdf", "is_interesting": false}, "1422": {"title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS", "authors": ["Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang"], "abstract": "arXiv:2505.23734v3 Announce Type: replace \nFeed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2505.23734", "pdf_url": "https://arxiv.org/pdf/2505.23734.pdf", "is_interesting": false}, "1423": {"title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs", "authors": ["Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang"], "abstract": "arXiv:2506.04220v3 Announce Type: replace \nUnlocking spatial reasoning in Multimodal Large Language Models (MLLMs) is crucial for enabling intelligent interaction with 3D environments. While prior efforts often rely on explicit 3D inputs or specialized model architectures, we ask: can MLLMs reason about 3D space using only structured 2D representations derived from perception? We introduce Struct2D, a perception-guided prompting framework that combines bird's-eye-view (BEV) images with object marks and object-centric metadata, optionally incorporating egocentric keyframes when needed. Using Struct2D, we conduct an in-depth zero-shot analysis of closed-source MLLMs (e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning abilities when provided with structured 2D inputs, effectively handling tasks such as relative direction estimation and route planning. Building on these insights, we construct Struct2D-Set, a large-scale instruction tuning dataset with 200K fine-grained QA pairs across eight spatial reasoning categories, generated automatically from 3D indoor scenes. We fine-tune an open-source MLLM (Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple benchmarks, including 3D question answering, dense captioning, and object grounding. Our approach demonstrates that structured 2D inputs can effectively bridge perception and language reasoning in MLLMs-without requiring explicit 3D representations as input. We will release both our code and dataset to support future research.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.04220", "pdf_url": "https://arxiv.org/pdf/2506.04220.pdf", "is_interesting": false}, "1424": {"title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations", "authors": ["Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath"], "abstract": "arXiv:2506.04789v3 Announce Type: replace \nLearning effective multi-modal 3D representations of objects is essential for numerous applications, such as augmented reality and robotics. Existing methods often rely on task-specific embeddings that are tailored either for semantic understanding or geometric reconstruction. As a result, these embeddings typically cannot be decoded into explicit geometry and simultaneously reused across tasks. In this paper, we propose Object-X, a versatile multi-modal object representation framework capable of encoding rich object embeddings (e.g. images, point cloud, text) and decoding them back into detailed geometric and visual reconstructions. Object-X operates by geometrically grounding the captured modalities in a 3D voxel grid and learning an unstructured embedding fusing the information from the voxels with the object attributes. The learned embedding enables 3D Gaussian Splatting-based object reconstruction, while also supporting a range of downstream tasks, including scene alignment, single-image 3D object reconstruction, and localization. Evaluations on two challenging real-world datasets demonstrate that Object-X produces high-fidelity novel-view synthesis comparable to standard 3D Gaussian Splatting, while significantly improving geometric accuracy. Moreover, Object-X achieves competitive performance with specialized methods in scene alignment and localization. Critically, our object-centric descriptors require 3-4 orders of magnitude less storage compared to traditional image- or point cloud-based approaches, establishing Object-X as a scalable and highly practical solution for multi-modal 3D scene representation.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.04789", "pdf_url": "https://arxiv.org/pdf/2506.04789.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.1, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses multi-modal 3D object representations and reconstruction, which can be applied in a variety of domains including augmented reality and robotics. While the techniques could be relevant for autonomous driving tasks like scene understanding or localization, the paper does not explicitly focus on autonomous driving systems or related tasks such as perception or motion planning, making its connection to autonomous driving indirect."}}, "1425": {"title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "authors": ["Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, Zihan Zhou"], "abstract": "arXiv:2506.07491v2 Announce Type: replace \nSpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.07491", "pdf_url": "https://arxiv.org/pdf/2506.07491.pdf", "is_interesting": false}, "1426": {"title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "authors": ["Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian"], "abstract": "arXiv:2506.09045v2 Announce Type: replace \nExisting acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2506.09045", "pdf_url": "https://arxiv.org/pdf/2506.09045.pdf", "is_interesting": false}, "1427": {"title": "CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection", "authors": ["Byeongchan Lee, John Won, Seunghyun Lee, Jinwoo Shin"], "abstract": "arXiv:2506.11772v3 Announce Type: replace \nAnomaly detection is a complex problem due to the ambiguity in defining anomalies, the diversity of anomaly types (e.g., local and global defect), and the scarcity of training data. As such, it necessitates a comprehensive model capable of capturing both low-level and high-level features, even with limited data. To address this, we propose CLIPFUSION, a method that leverages both discriminative and generative foundation models. Specifically, the CLIP-based discriminative model excels at capturing global features, while the diffusion-based generative model effectively captures local details, creating a synergistic and complementary approach. Notably, we introduce a methodology for utilizing cross-attention maps and feature maps extracted from diffusion models specifically for anomaly detection. Experimental results on benchmark datasets (MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline methods, achieving outstanding performance in both anomaly segmentation and classification. We believe that our method underscores the effectiveness of multi-modal and multi-model fusion in tackling the multifaceted challenges of anomaly detection, providing a scalable solution for real-world applications.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2506.11772", "pdf_url": "https://arxiv.org/pdf/2506.11772.pdf", "is_interesting": false}, "1428": {"title": "P3P Made Easy", "authors": ["Seong Hun Lee, Patrick Vandewalle, Javier Civera"], "abstract": "arXiv:2508.01312v2 Announce Type: replace \nWe revisit the classical Perspective-Three-Point (P3P) problem, which aims to recover the absolute pose of a calibrated camera from three 2D-3D correspondences. It has long been known that P3P can be reduced to a quartic polynomial with analytically simple and computationally efficient coefficients. However, this elegant formulation has been largely overlooked in modern literature. Building on the theoretical foundation that traces back to Grunert's work in 1841, we propose a compact algebraic solver that achieves accuracy and runtime comparable to state-of-the-art methods. Our results show that this classical formulation remains highly competitive when implemented with modern insights, offering an excellent balance between simplicity, efficiency, and accuracy.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2508.01312", "pdf_url": "https://arxiv.org/pdf/2508.01312.pdf", "is_interesting": false}, "1429": {"title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation", "authors": ["Hao Zhang, Chun-Han Yao, Simon Donn\\'e, Narendra Ahuja, Varun Jampani"], "abstract": "arXiv:2509.10687v2 Announce Type: replace \nWe present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.10687", "pdf_url": "https://arxiv.org/pdf/2509.10687.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "N/A", "reason": "The paper focuses on generating paired RGB and kinematic part videos for object articulation and animation tasks, which is unrelated to autonomous driving. It discusses a model for video generation and part segmentation, without reference to autonomous vehicle perception, prediction, or control tasks."}}, "1430": {"title": "SmartWilds: Multimodal Wildlife Monitoring Dataset", "authors": ["Jenna Kline, Anirudh Potlapally, Bharath Pillai, Tanishka Wani, Rugved Katole, Vedant Patil, Penelope Covey, Hari Subramoni, Tanya Berger-Wolf, Christopher Stewart"], "abstract": "arXiv:2509.18894v2 Announce Type: replace \nWe present the first release of SmartWilds, a multimodal wildlife monitoring dataset. SmartWilds is a synchronized collection of drone imagery, camera trap photographs and videos, and bioacoustic recordings collected during summer 2025 at The Wilds safari park in Ohio. This dataset supports multimodal AI research for comprehensive environmental monitoring, addressing critical needs in endangered species research, conservation ecology, and habitat management. Our pilot deployment captured four days of synchronized monitoring across three modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin, Przewalski's horses, as well as species native to Ohio. We provide a comparative analysis of sensor modality performance, demonstrating complementary strengths for landuse patterns, species detection, behavioral analysis, and habitat monitoring. This work establishes reproducible protocols for multimodal wildlife monitoring while contributing open datasets to advance conservation computer vision research. Future releases will include synchronized GPS tracking data from tagged individuals, citizen science data, and expanded temporal coverage across multiple seasons.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.18894", "pdf_url": "https://arxiv.org/pdf/2509.18894.pdf", "is_interesting": false}, "1431": {"title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding", "authors": ["I\\~nigo Alonso, Imanol Miranda, Eneko Agirre, Mirella Lapata"], "abstract": "arXiv:2509.21205v2 Announce Type: replace \nWhile table understanding increasingly relies on pixel-only settings where tables are processed as visual representations, current benchmarks predominantly use synthetic renderings that lack the complexity and visual diversity of real-world tables. Additionally, existing visual table understanding (VTU) datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation. We introduce TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. Each example includes paired image-HTML representations, comprehensive metadata, and provenance information linking back to the source datasets. Fine-tuning vision-language models like Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while increasing robustness on real-world table visualizations. By preserving original visualizations and maintaining example traceability in a unified large-scale collection, TABLET establishes a foundation for robust training and extensible evaluation of future VTU models.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2509.21205", "pdf_url": "https://arxiv.org/pdf/2509.21205.pdf", "is_interesting": false}, "1432": {"title": "Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A Two-Stage Rank-Learning Metric", "authors": ["Bingyang Cui, Yujie Zhang, Qi Yang, Zhu Li, Yiling Xu"], "abstract": "arXiv:2509.23841v2 Announce Type: replace \nRecent advances in Text-to-3D (T23D) generative models have enabled the synthesis of diverse, high-fidelity 3D assets from textual prompts. However, existing challenges restrict the development of reliable T23D quality assessment (T23DQA). First, existing benchmarks are outdated, fragmented, and coarse-grained, making fine-grained metric training infeasible. Moreover, current objective metrics exhibit inherent design limitations, resulting in non-representative feature extraction and diminished metric robustness. To address these limitations, we introduce T23D-CompBench, a comprehensive benchmark for compositional T23D generation. We define five components with twelve sub-components for compositional prompts, which are used to generate 3,600 textured meshes from ten state-of-the-art generative models. A large-scale subjective experiment is conducted to collect 129,600 reliable human ratings across different perspectives. Based on T23D-CompBench, we further propose Rank2Score, an effective evaluator with two-stage training for T23DQA. Rank2Score enhances pairwise training via supervised contrastive regression and curriculum learning in the first stage, and subsequently refines predictions using mean opinion scores to achieve closer alignment with human judgments in the second stage. Extensive experiments and downstream applications demonstrate that Rank2Score consistently outperforms existing metrics across multiple dimensions and can additionally serve as a reward function to optimize generative models. The project is available at https://cbysjtu.github.io/Rank2Score/.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.23841", "pdf_url": "https://arxiv.org/pdf/2509.23841.pdf", "is_interesting": false}, "1433": {"title": "FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing", "authors": ["Yi Yang, Xiaokun Zhang, Qingchen Fang, Jing Liu, Ziqi Ye, Rui Li, Li Liu, Haipeng Wang"], "abstract": "arXiv:2509.23927v2 Announce Type: replace \nCross-modal artificial intelligence has garnered widespread attention in recent years, achieving significant progress in the study of natural images. However, existing methods are mostly designed for RGB imagery, leaving a significant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with its all-day, all-weather imaging capabilities, plays an irreplaceable role in remote sensing scene understanding. To address this gap, this paper proposes FUSAR-KLIP, the first universal SAR multimodal foundational model, along with reusable data and evaluation baselines. Specifically: (1) This work introduces the critical yet long-overlooked attribute of geographic information into remote sensing research, constructing FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection properties), covering multiple satellite platforms, 120,000 images, and 135 cities. (2) Aligned structured text is generated through a hierarchical cognitive chain-of-thought (HCoT), providing more than one million multi-dimensional semantic annotations of landforms, regional functions, target attributes, and spatial relationships. (3) We design a Self-Consistent Iterative Optimization mechanism that continuously enhances cross-modal alignment through a self-supervised closed loop of contrastive, matching, and reconstruction learning on a transferable multimodal encoder. (4) A unified evaluation benchmark is established across 11 representative downstream vision and vision-language tasks, with comparisons against 14 leading foundation models, where FUSAR-KLIP demonstrates leading performance, particularly in object counting and land-cover classification. We expect that FUSAR-KLIP's large-scale multimodal data, transferable model architecture, and comprehensive experimental benchmark will significantly advance the development of SAR multimodal baseline models.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.23927", "pdf_url": "https://arxiv.org/pdf/2509.23927.pdf", "is_interesting": false}, "1434": {"title": "DA$^2$: Depth Anything in Any Direction", "authors": ["Haodong Li, Wangguangdong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, Ying-Cong Chen, Chunchao Guo"], "abstract": "arXiv:2509.26618v4 Announce Type: replace \nPanorama has a full FoV (360$^\\circ\\times$180$^\\circ$), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose $\\textbf{DA}$$^{\\textbf{2}}$: $\\textbf{D}$epth $\\textbf{A}$nything in $\\textbf{A}$ny $\\textbf{D}$irection, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create $\\sim$543K panoramic RGB-depth pairs, bringing the total to $\\sim$607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA$^{2}$'s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA$^{2}$ even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data has be released. Project page: https://depth-any-in-any-dir.github.io/.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2509.26618", "pdf_url": "https://arxiv.org/pdf/2509.26618.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.2, "subfield": "\u901a\u7528\u89c6\u89c9\u4f46\u53ef\u5e94\u7528\u4e8eAD", "reason": "The paper discusses panoramic depth estimation, a general 3D vision task, and introduces a new method to address challenges in this domain. While this work is not directly focused on autonomous driving, the technique could potentially be applied to tasks such as vehicle perception in AD systems. However, it does not specifically address autonomous driving tasks or sensor integration, making it only indirectly related to the field."}}, "1435": {"title": "Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding", "authors": ["Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao Tang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao, Bin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun, Jian Wu, Zuozhu Liu"], "abstract": "arXiv:2510.08668v2 Announce Type: replace \nReal-world clinical decision-making requires integrating heterogeneous data, including medical text, 2D images, 3D volumes, and videos, while existing AI systems fail to unify all these signals, limiting their utility. In this paper, we introduce Hulu-Med, a transparent, generalist medical Vision-Language Model (VLM) designed to unify language-only, 2D/3D vision-language, and video understanding within a single architecture. Hulu-Med is trained on a curated corpus of 16.7 million samples, comprising exclusively public or synthetic data, spanning 12 major anatomical systems and 14 medical imaging modalities. Hulu-Med employs a medical-aware token-reduction strategy that prunes redundant visual tokens, achieving up to a 55% reduction for 3D and video inputs, improving cross-modal efficiency, and enabling training at 7B-32B parameter scales in approximately 4,000-40,000 GPU hours. Across 30 public in-domain and out-of-domain medical benchmarks-covering text reasoning, visual question answering, report generation, multilingual dialogue, video understanding, and rare disease diagnosis-Hulu-Med surpasses existing open-source models on 27 of 30 benchmarks and outperforms proprietary systems such as GPT-4o on 16 benchmarks. Despite being a VLM, Hulu-Med outperforms GPT-4o and matches GPT-o1 on the text-only HealthBench. For the first time in the community, we provide a fully transparent, reproducible and cost-effective pipeline for holistic medical vision-language understanding by releasing our end-to-end data curation, training procedures, and model parameters. Code and models are available at https://github.com/ZJUI-AI4H/Hulu-Med.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.08668", "pdf_url": "https://arxiv.org/pdf/2510.08668.pdf", "is_interesting": false}, "1436": {"title": "MobileGeo: Exploring Hierarchical Knowledge Distillation for Resource-Efficient Cross-view Drone Geo-Localization", "authors": ["Jian Sun, Kangdao Liu, Chi Zhang, Chuangquan Chen, Junge Shen, Chi-Man Vong"], "abstract": "arXiv:2510.22582v2 Announce Type: replace \nCross-view geo-localization (CVGL) enables drone localization by matching aerial images to geo-tagged satellite databases, which is critical for autonomous navigation in GNSS-denied environments. However, existing methods rely on resource-intensive feature alignment and multi-branch architectures, incurring high inference costs that limit their deployment on mobile edge devices. We propose MobileGeo, a mobile-friendly framework designed for efficient on-device CVGL. MobileGeo achieves its efficiency through two key components: 1) During training, a Hierarchical Distillation (HD-CVGL) paradigm, coupled with Uncertainty-Aware Prediction Alignment (UAPA), distills essential information into a compact model without incurring inference overhead. 2) During inference, an efficient Multi-view Selection Refinement Module (MSRM) leverages mutual information to filter redundant views and reduce computational load. Extensive experiments demonstrate that MobileGeo outperforms previous state-of-the-art methods, achieving a 4.19\\% improvement in AP on University-1652 dataset while being over 5$\\times$ more efficient in FLOPs and 3$\\times$ faster. Crucially, MobileGeo runs at 251.5 FPS on an NVIDIA AGX Orin edge device, demonstrating its practical viability for real-time on-device drone geo-localization.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.22582", "pdf_url": "https://arxiv.org/pdf/2510.22582.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0\u76f4\u63a5\u5173\u7cfb", "reason": "The paper focuses on drone geo-localization using aerial images and satellite databases, which is not related to autonomous driving systems or vehicle perception, prediction, decision-making, or control tasks."}}, "1437": {"title": "Revisiting Multimodal Positional Encoding in Vision-Language Models", "authors": ["Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai"], "abstract": "arXiv:2510.23095v2 Announce Type: replace \nMultimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.23095", "pdf_url": "https://arxiv.org/pdf/2510.23095.pdf", "is_interesting": false}, "1438": {"title": "Interpretable Tile-Based Classification of Paclitaxel Exposure", "authors": ["Sean Fletcher, Gabby Scott, Douglas Currie, Xin Zhang, Yuqi Song, Bruce MacLeod"], "abstract": "arXiv:2510.23363v2 Announce Type: replace \nMedical image analysis is central to drug discovery and preclinical evaluation, where scalable, objective readouts can accelerate decision-making. We address classification of paclitaxel (Taxol) exposure from phase-contrast microscopy of C6 glioma cells -- a task with subtle dose differences that challenges full-image models. We propose a simple tiling-and-aggregation pipeline that operates on local patches and combines tile outputs into an image label, achieving state-of-the-art accuracy on the benchmark dataset and improving over the published baseline by around 20 percentage points, with trends confirmed by cross-validation. To understand why tiling is effective, we further apply Grad-CAM and Score-CAM and attention analyses, which enhance model interpretability and point toward robustness-oriented directions for future medical image research. Code is released to facilitate reproduction and extension.", "categories": ["cs.CV"], "abs_url": "https://arxiv.org/abs/2510.23363", "pdf_url": "https://arxiv.org/pdf/2510.23363.pdf", "is_interesting": false}, "1439": {"title": "Generative View Stitching", "authors": ["Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann"], "abstract": "arXiv:2510.24718v2 Announce Type: replace \nAutoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.", "categories": ["cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2510.24718", "pdf_url": "https://arxiv.org/pdf/2510.24718.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": false, "relevance_score": 0.0, "subfield": "\u65e0", "reason": "The paper discusses a video generation technique focused on camera-guided video stitching, which is unrelated to autonomous driving or tasks like vehicle perception, trajectory prediction, or motion planning."}}, "1440": {"title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models", "authors": ["Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa"], "abstract": "arXiv:2510.26241v2 Announce Type: replace \nModern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.", "categories": ["cs.CV", "cs.CL"], "abs_url": "https://arxiv.org/abs/2510.26241", "pdf_url": "https://arxiv.org/pdf/2510.26241.pdf", "is_interesting": false}, "1441": {"title": "Human Perception-Inspired Grain Segmentation Refinement Using Conditional Random Fields", "authors": ["Doruk Aksoy, Huolin L. Xin, Timothy J. Rupert, William J. Bowman"], "abstract": "arXiv:2312.09968v3 Announce Type: replace-cross \nAutomated detection of grain boundaries (GBs) in electron microscope images of polycrystalline materials could help accelerate the nanoscale characterization of myriad engineering materials and novel materials under scientific research. Accurate segmentation of interconnected line networks, such as GBs in polycrystalline material microstructures, poses a significant challenge due to the fragmented masks produced by conventional computer vision (CV) algorithms, including convolutional neural networks. These algorithms struggle with thin masks, often necessitating post-processing for effective contour closure and continuity. Previous approaches in this domain have typically relied on custom post-processing techniques that are problem-specific and heavily dependent on the quality of the mask obtained from a CV algorithm. Addressing this issue, this paper introduces a fast, high-fidelity post-processing technique that is universally applicable to segmentation masks of interconnected line networks. Leveraging domain knowledge about grain boundary connectivity, this method employs conditional random fields and perceptual grouping rules to refine segmentation masks of any image with a discernible grain structure. This approach significantly enhances segmentation mask accuracy by correctly reconstructing fragmented GBs in electron microscopy images of a polycrystalline oxide. The refinement improves the statistical representation of the microstructure, reflected by a 51 % improvement in a grain alignment metric that provides a more physically meaningful assessment of complex microstructures than conventional metrics. This method enables rapid and accurate characterization, facilitating an unprecedented level of data analysis and improving the understanding of GB networks, making it suitable for a range of disciplines where precise segmentation of interconnected line networks is essential.", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "abs_url": "https://arxiv.org/abs/2312.09968", "pdf_url": "https://arxiv.org/pdf/2312.09968.pdf", "is_interesting": false}, "1442": {"title": "MAROON: A Framework for the Joint Characterization of Near-Field High-Resolution Radar and Optical Depth Imaging Techniques", "authors": ["Vanessa Wirth, Johanna Br\\\"aunig, Nikolai Hofmann, Martin Vossiek, Tim Weyrich, Marc Stamminger"], "abstract": "arXiv:2411.00527v3 Announce Type: replace-cross \nUtilizing the complementary strengths of wavelength-specific range or depth sensors is crucial for robust computer-assisted tasks such as autonomous driving. Despite this, there is still little research done at the intersection of optical depth sensors and radars operating close range, where the target is decimeters away from the sensors. Together with a growing interest in high-resolution imaging radars operating in the near field, the question arises how these sensors behave in comparison to their traditional optical counterparts.\n  In this work, we take on the unique challenge of jointly characterizing depth imagers from both, the optical and radio-frequency domain using a multimodal spatial calibration. We collect data from four depth imagers, with three optical sensors of varying operation principle and an imaging radar. We provide a comprehensive evaluation of their depth measurements with respect to distinct object materials, geometries, and object-to-sensor distances. Specifically, we reveal scattering effects of partially transmissive materials and investigate the response of radio-frequency signals. All object measurements will be made public in form of a multimodal dataset, called MAROON.", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2411.00527", "pdf_url": "https://arxiv.org/pdf/2411.00527.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.7, "subfield": "\u591a\u4f20\u611f\u5668\u878d\u5408", "reason": "The paper discusses the joint characterization of optical and radar depth sensors, which are crucial for enhancing sensor fusion in autonomous driving systems. The research evaluates how these sensors operate in close-range environments, which is relevant for improving the robustness and accuracy of perception systems in autonomous vehicles. Although not explicitly focused on autonomous driving, the findings contribute to sensor technologies widely used in the field."}}, "1443": {"title": "Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization", "authors": ["Wenhao Chi, Haiping Liu, Hongqiao Dong, Wenhua Liang, Bo Liu"], "abstract": "arXiv:2411.06184v2 Announce Type: replace-cross \nIn the field of non-invasive medical imaging, radiomic features are utilized to measure tumor characteristics. However, these features can be affected by the techniques used to discretize the images, ultimately impacting the accuracy of diagnosis. To investigate the influence of various image discretization methods on diagnosis, it is common practice to evaluate multiple discretization strategies individually. This approach often leads to redundant and time-consuming tasks such as training predictive models and fine-tuning hyperparameters separately. This study examines the feasibility of employing multi-task Bayesian optimization to accelerate the hyperparameters search for classifying benign and malignant pulmonary nodules using RBF SVM. Our findings suggest that multi-task Bayesian optimization significantly accelerates the search for hyperparameters in comparison to a single-task approach. To the best of our knowledge, this is the first investigation to utilize multi-task Bayesian optimization in a critical medical context.", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "abs_url": "https://arxiv.org/abs/2411.06184", "pdf_url": "https://arxiv.org/pdf/2411.06184.pdf", "is_interesting": false}, "1444": {"title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents", "authors": ["Mihaela-Larisa Clement, M\\'onika Farsang, Felix Resch, Mihai-Teodor Stanusoiu, Radu Grosu"], "abstract": "arXiv:2503.16711v2 Announce Type: replace-cross \nAutonomous agents that rely purely on perception to make real-time control decisions require efficient and robust architectures. In this work, we demonstrate that augmenting RGB input with depth information significantly enhances our agents' ability to predict steering commands compared to using RGB alone. We benchmark lightweight recurrent controllers that leverage the fused RGB-D features for sequential decision-making. To train our models, we collect high-quality data using a small-scale autonomous car controlled by an expert driver via a physical steering wheel, capturing varying levels of steering difficulty. Our models were successfully deployed on real hardware and inherently avoided dynamic and static obstacles, under out-of-distribution conditions. Specifically, our findings reveal that the early fusion of depth data results in a highly robust controller, which remains effective even with frame drops and increased noise levels, without compromising the network's focus on the task.", "categories": ["cs.RO", "cs.CV", "cs.LG"], "abs_url": "https://arxiv.org/abs/2503.16711", "pdf_url": "https://arxiv.org/pdf/2503.16711.pdf", "is_interesting": true, "is_interesting_2nd_pass": {"is_autonomous_driving_related": true, "relevance_score": 0.8, "subfield": "\u7aef\u5230\u7aef\u63a7\u5236 / \u591a\u6a21\u6001\u878d\u5408", "reason": "The paper discusses the use of RGB-D perception for enhancing autonomous agents' ability to predict steering commands and make real-time control decisions. The focus on using depth data to improve control robustness and deployment on real hardware is directly applicable to autonomous driving systems, particularly in end-to-end control and multimodal fusion tasks."}}, "1445": {"title": "BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification", "authors": ["Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh, Vahid Abolghasemi"], "abstract": "arXiv:2506.14318v4 Announce Type: replace-cross \nAccurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, primarily due to the lack of high-quality, balanced, and diverse datasets with expert annotations. In this work, we address this gap by introducing BRISC, a dataset designed for brain tumor segmentation and classification tasks, featuring high-resolution segmentation masks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans, which were collated from multiple public datasets that lacked segmentation labels. Our primary contribution is the subsequent expert annotation of these images, performed by certified radiologists and physicians. It includes three major tumor types, namely glioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we provide benchmark results for both tasks using standard deep learning models. The BRISC dataset is made publicly available. datasetlink: Kaggle (https://www.kaggle.com/datasets/briscdataset/brisc2025/), Figshare (https://doi.org/10.6084/m9.figshare.30533120), Zenodo (https://doi.org/10.5281/zenodo.17524350)", "categories": ["eess.IV", "cs.CV"], "abs_url": "https://arxiv.org/abs/2506.14318", "pdf_url": "https://arxiv.org/pdf/2506.14318.pdf", "is_interesting": false}, "1446": {"title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing", "authors": ["Huadai Liu, Kaicheng Luo, Jialei Wang, Wen Wang, Qian Chen, Zhou Zhao, Wei Xue"], "abstract": "arXiv:2506.21448v3 Announce Type: replace-cross \nWhile end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, this generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics, and excels in the out-of-distribution Movie Gen Audio benchmark. The project page is available at https://ThinkSound-Project.github.io.", "categories": ["eess.AS", "cs.CV", "cs.SD"], "abs_url": "https://arxiv.org/abs/2506.21448", "pdf_url": "https://arxiv.org/pdf/2506.21448.pdf", "is_interesting": false}, "1447": {"title": "Harmonious Color Pairings: Insights from Human Preference and Natural Hue Statistics", "authors": ["Ortensia Forni, Alexandre Darmon, Michael Benzaquen"], "abstract": "arXiv:2508.15777v2 Announce Type: replace-cross \nWhile color harmony has long been studied in art and design, a clear consensus remains elusive, as most models are grounded in qualitative insights or limited datasets. In this work, we present a quantitative, data-driven study of color pairing preferences using controlled hue-based palettes in the HSL color space. Participants evaluated combinations of thirteen distinct hues, enabling us to construct a preference matrix and define a combinability index for each color. Our results reveal that preferences are highly hue dependent, challenging the assumption of universal harmony rules proposed in the literature. Yet, when averaged over hues, statistically meaningful patterns of aesthetic preference emerge, with certain hue separations perceived as more harmonious. Strikingly, these patterns align with hue distributions found in natural landscapes, pointing to a statistical correspondence between human color preferences and the structure of color in nature. Finally, we analyze our color-pairing score matrix through principal component analysis, which uncovers two complementary hue groups whose interplay underlies the global structure of color-pairing preferences. Together, these findings offer a quantitative framework for studying color harmony and its potential perceptual and ecological underpinnings.", "categories": ["cs.HC", "cs.CV", "physics.soc-ph"], "abs_url": "https://arxiv.org/abs/2508.15777", "pdf_url": "https://arxiv.org/pdf/2508.15777.pdf", "is_interesting": false}, "1448": {"title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few", "authors": ["Qishuai Wen, Zhiyuan Huang, Chun-Guang Li"], "abstract": "arXiv:2509.16875v3 Announce Type: replace-cross \nAttention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \\emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms.", "categories": ["cs.LG", "cs.CV"], "abs_url": "https://arxiv.org/abs/2509.16875", "pdf_url": "https://arxiv.org/pdf/2509.16875.pdf", "is_interesting": false}}}