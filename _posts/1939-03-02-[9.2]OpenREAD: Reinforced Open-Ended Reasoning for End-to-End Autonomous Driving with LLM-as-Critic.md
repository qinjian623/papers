---
layout: default
title: "[9.2]OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic"
---

# [9.2] OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic

- Authors: Songyan Zhang, Wenhui Huang, Zhan Chen, Chua Jiahao Collister, Qihang Huang, Chen Lv
- [arXiv Link](https://arxiv.org/abs/2512.01830v2)
- [PDF Link](https://arxiv.org/pdf/2512.01830v2.pdf)

## Subfields
 端到端自动驾驶 / 具身智能 (VLM-based End-to-End AD)
## Reason for Interest

1. 创新性极强（9.5/10）：紧跟大模型前沿，率先将DeepSeek-R1的GRPO（Group Relative Policy Optimization）强化学习范式应用于自动驾驶，并提出'LLM-as-Critic'机制解决开放式推理任务奖励难以量化的问题，属于'System 2'自动驾驶的先驱工作。
2. 实验验证充分（9.0/10）：通过对比SFT与RFT，有力证明了'逻辑推理能力的提升能直接反哺驾驶规划的安全性'这一关键假设。实验覆盖了Reasoning（LingoQA）和Planning（nuScenes）双维度的SOTA对比。
3. 行业价值（9.0/10）：为解决端到端模型的可解释性和长尾场景泛化问题提供了切实可行的技术路径，数据构建（CoT标注）和训练策略（Cold Start -> RFT）具有很高的参考价值。
4. 瑕疵：在纯轨迹规划的平均L2指标上未完全超越所有Baseline（如OpenDrive-VLA），但在更关键的安全指标（碰撞率）上表现优异。
## Abstract: 

