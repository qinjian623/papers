---
layout: default
title: "[4.8]Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision"
---

# [4.8] Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision

- Authors: Wentao Zhou, Xuweiyi Chen, Vignesh Rajagopal, Jeffrey Chen, Rohan Chandra, Zezhou Cheng
- [arXiv Link](https://arxiv.org/abs/2512.10956v1)
- [PDF Link](https://arxiv.org/pdf/2512.10956v1.pdf)

## Subfields
 机器人视觉导航 / 具身智能
## Reason for Interest

该论文提出了一种结合立体视觉和中层视觉特征（深度、点跟踪）的端到端导航方法，在机器人人行道导航任务中表现出色，实验设计严谨（含真机部署），具有较高的学术价值。然而，根据评分规则，该研究明确针对低速物流机器人（Last-mile delivery）和人行道场景，属于具身智能/机器人领域，而非直接的‘车端自动驾驶’（如乘用车在道路行驶），因此触发‘非车端相关最多5分’的严格限制。分数定为4.8分以认可其在机器人领域的质量，但严格区分于自动驾驶研究。
## Abstract: 
  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.
