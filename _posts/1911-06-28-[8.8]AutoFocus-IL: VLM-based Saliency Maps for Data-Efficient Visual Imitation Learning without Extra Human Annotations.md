---
layout: default
title: "[8.8]AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations"
---

# [8.8] AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations

- Authors: Litian Gong (University of California, Riverside, USA), Fatemeh Bahrani (University of So...
- [arXiv Link](https://arxiv.org/abs/2511.18617)
- [PDF Link](https://arxiv.org/pdf/2511.18617.pdf)

## Subfields
 端到端自动驾驶 / 模仿学习 (End-to-End AD / Imitation Learning)
## Reason for Interest

该论文提出了一种极具创新性和实用价值的方法，利用视觉语言模型（VLM）自动生成时序显著性图（Saliency Maps）来监督模仿学习策略，解决了端到端自动驾驶中常见的'因果混淆'（Causal Confusion）问题。核心贡献在于无需昂贵的人类视线数据即可获得高质量的注意力监督，显著提升了模型在未见场景下的泛化能力。实验设计严谨，涵盖了CARLA自动驾驶仿真和真机机械臂操作，且在抗干扰测试（Confounded setting）中表现优异，证明了该方法能有效引导策略关注道路、交通参与者等关键要素，具有很高的行业应用潜力。
## Abstract: 
AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.
