---
layout: default
title: "[9.2]Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles"
---

# [9.2] Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles

- Authors: Younggun Kim, Mohamed Abdel-Aty, Beomsik Cho, Seonghoon Ryoo, Soomok Lee
- [arXiv Link](https://arxiv.org/abs/2501.16289)
- [PDF Link](https://arxiv.org/pdf/2501.16289.pdf)

## Subfields
 3D感知 / 激光雷达点云分类 / 域泛化与鲁棒性
## Reason for Interest

该论文针对自动驾驶领域激光雷达点云感知的域不变性（domain-invariance）问题，提出了一种名为Multi-view Structural Convolution Network (MSCN) 的新型深度学习模型，并结合渐进式域扩展框架（Progressive Domain Expansion Framework）进行训练。

**创新性（9.5/10）：**
1. **MSCN架构：** 提出了Structural Convolution Layers (SCL) 和 Structural Aggregation Layers (SAL)。SCL通过创新的“方向感知核”和“距离感知核”从不规则点云中提取局部几何结构特征，SAL则进一步融合局部和全局上下文特征，增强了特征的鲁棒性。这种基于结构感知的卷积设计对于处理点云的几何变化具有独到之处。
2. **域扩展框架：** 创造性地将2D图像的渐进式域扩展网络（PDEN）概念扩展到3D点云领域，通过生成未见过的虚拟域点云来训练模型，使其获得域不变性表示。该框架结合了对比学习、对抗性学习、重构损失和多样性损失，以确保生成的域多样且有助于模型泛化，这是3D域泛化领域的显著贡献。

**实验完整性（9.0/10）：**
1. **多维度数据集验证：** 实验涵盖了合成数据集（ShapeNetPart, ModelNet40）、真实室内数据集（ScanObjectNN）以及关键的真实自动驾驶数据集（KITTI, PanKyo, nuScenes）和模拟器数据集。这确保了模型在各种复杂场景下的性能评估。
2. **全面性能评估：** 不仅进行了域内分类、几何形变鲁棒性测试，更着重评估了关键的跨域分类性能（包括Sim-to-Real和Real-to-Real）。还测试了不同LiDAR通道配置、地理位置变化以及同时存在几何扰动和传感器差异的综合挑战。
3. **详尽的消融研究：** 对MSCN的网络层配置（SCL vs. SCL+SAL）、域生成方法（MSCN vs. MSCN†）以及各项损失函数的作用进行了系统性的消融分析，充分证明了每个组件的有效性。
4. **实时性考量：** 提供了模型的推理时间（3.8ms），验证了其在自动驾驶场景下实现实时感知的潜力。

**可信度（9.0/10）：**
1. **问题定义清晰：** 论文明确指出了自动驾驶中，不同LiDAR传感器配置、地理位置差异和Sim-to-Real鸿沟导致的域偏移问题，并提出方法直接应对这些挑战。
2. **结果支持：** 跨域性能上的显著提升有数据支撑，与多个主流基线方法（如PointTransformer）进行了公平且详尽的比较，并承认了域不变性模型在域内性能上可能存在的轻微权衡（bias-variance trade-off）。
3. **方法论严谨：** 网络架构和域生成框架的各个组成部分都经过详细阐述和验证。

**行业潜力（9.5/10）：**
该研究直接解决了自动驾驶领域的一个核心挑战：如何在多变的环境和异构传感器配置下保持鲁棒的感知性能。域泛化能力的提升对于自动驾驶系统的大规模部署和安全性至关重要，能有效降低因新环境或新传感器而进行昂贵模型再训练的需求。实时推理能力也使其具备实际应用价值。

**总结：** 论文在创新性、实验验证和解决实际行业痛点方面均表现出色，尤其在跨域鲁棒性方面取得了显著突破。尽管域内性能略低于某些专为域内优化设计的SOTA模型，但这正是域泛化研究的合理权衡。其对自动驾驶领域的贡献和潜力巨大。
## Abstract: 
Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Furthermore, MSCN enhances feature robustness by training with unseen domain point clouds generated from the source domain, enabling the model to acquire domain-invariant representations. Extensive cross-domain experiments demonstrate that MSCN achieves an average accuracy of 82.0%, surpassing the strong baseline PointTransformer by 15.8%, confirming its effectiveness under real-world domain shifts. Our code is available at https://github.com/MLMLab/MSCN.
