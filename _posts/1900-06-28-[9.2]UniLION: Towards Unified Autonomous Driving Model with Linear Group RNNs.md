---
layout: default
title: "[9.2]UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs"
---

# [9.2] UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs

- Authors: Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai
- [arXiv Link](https://arxiv.org/abs/2511.01768)
- [PDF Link](https://arxiv.org/pdf/2511.01768.pdf)

## Subfields
 统一自动驾驶模型 / 3D基础模型
## Reason for Interest

这篇论文提出了一种创新的统一自动驾驶模型UniLION，其核心在于使用线性群RNN作为3D骨干网络，以解决传统Transformer在处理自动驾驶场景中大规模、异构、时序数据时面临的二次计算复杂度问题。通过将LiDAR点云、多视角图像甚至时序信息直接拼接为token，并通过线性RNN进行处理，实现了多模态、多时序的隐式融合，而非依赖显式的、手设计的融合模块，这在架构上具有很强的创新性，符合自动驾驶领域构建基础模型的趋势。

实验部分非常完整和全面，在nuScenes数据集上对3D感知（目标检测、跟踪、占用预测、BEV地图分割）、轨迹预测和规划七项核心任务进行了评估，并测试了LiDAR-only、LiDAR-temporal、multi-modal和multi-modal temporal四种不同的输入配置，充分验证了模型的通用性和性能。大量的消融实验清晰地证明了各个组件（如3D空间特征描述符、自动回归体素生成、动态损失平衡）的有效性，以及模型在不同图像骨干网络、不同线性RNN算子、窗口大小和组大小方面的鲁棒性，尤其值得称赞的是其在传感器错位下的鲁棒性分析，这对于实际部署具有重要意义。最重要的是，论文成功展示了“一个模型支持所有”（One Model for All）的能力，即在多模态时序数据上训练一次后，无需重新训练即可在LiDAR-only、temporal LiDAR或multi-modal等不同配置下进行推理，这大大简化了系统设计和部署复杂度，提升了行业潜力。

在结果可信度方面，论文提供了详细的定量比较，UniLION在多项任务和配置下均达到了SOTA或领先水平，尽管少数特定指标（如部分规划任务的碰撞率）未能超越所有现有最佳方法，但整体性能表现依然卓越且一致。其线性复杂度和统一架构的优势，使其在效率和系统整合方面展现出巨大潜力。

综上所述，UniLION代表了自动驾驶系统设计的一个重要进步方向，有望为未来的端到端自动驾驶系统提供一个高效、鲁棒且灵活的统一基础模型，因此给予高分。
## Abstract: 
Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION
