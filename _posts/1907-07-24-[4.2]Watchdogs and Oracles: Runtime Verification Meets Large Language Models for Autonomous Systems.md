---
layout: default
title: "[4.2]Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems"
---

# [4.2] Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems

- Authors: Angelo Ferrando (University of Modena,Reggio Emilia)
- [arXiv Link](https://arxiv.org/abs/2511.14435)
- [PDF Link](https://arxiv.org/pdf/2511.14435.pdf)

## Subfields
 自动驾驶安全 / 形式化方法 (Safety Verification / Formal Methods)
## Reason for Interest

1. 论文性质：这是一篇发表于FMAS 2025 Workshop的愿景论文（Vision Paper），主要讨论大语言模型（LLM）与运行时验证（Runtime Verification, RV）在自主系统中的结合架构。 2. 创新性：提出了一种双向架构，即RV作为LLM的安全护栏（guardrail），同时LLM辅助RV进行规范生成和预测推理。这一概念对于解决LLM在自动驾驶中的安全信任问题具有一定启发意义。 3. 实验缺失：论文完全缺乏实验验证，没有具体的数据集测试、算法实现细节或对比分析。它仅停留在概念设计和文献综述阶段。 4. 落地价值：虽然提及了ISO 26262等汽车安全标准，但缺乏针对车端环境的具体工程实现方案。对于寻找具体算法或实证结果的自动驾驶研究员来说，参考价值有限，属于偏理论的探讨。
## Abstract: 
Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.
