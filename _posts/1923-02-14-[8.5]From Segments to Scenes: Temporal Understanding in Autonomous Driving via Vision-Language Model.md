---
layout: default
title: "[8.5]From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model"
---

# [8.5] From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model

- Authors: Kevin Cannons, Saeed Ranjbar Alvar, Mohammad Asiful Hossain, Ahmad Rezaei, Mohsen Gholami...
- [arXiv Link](https://arxiv.org/abs/2512.05277v1)
- [PDF Link](https://arxiv.org/pdf/2512.05277v1.pdf)

## Subfields
 自动驾驶视频理解 / 视觉语言模型 (VLM) / 时序推理
## Reason for Interest

该论文针对自动驾驶场景中VLM（视觉语言模型）时序理解能力不足的问题，做出了极其扎实的贡献：
1. 创新性：提出了首个专注于自动驾驶时序理解的VQA基准（TAD），包含细粒度的动作标注，填补了行业空白。同时提出了TCogMap（时序认知地图）和Scene-CoT两种免训练的推理增强方法，利用自车轨迹先验有效提升了模型性能。
2. 实验完整性：评估了涵盖开源/闭源通用大模型及AD专用模型在内的9种SOTA模型（共30种配置），实验设计严谨，消融实验充分。
3. 行业价值：虽然目前主要体现为VQA形式的离线评测，但对提升端到端大模型的时序推理和场景认知能力具有重要指导意义，数据与代码开源也利于社区发展。
## Abstract: 

