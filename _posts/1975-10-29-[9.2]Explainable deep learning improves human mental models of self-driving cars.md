---
layout: default
title: "[9.2]Explainable deep learning improves human mental models of self-driving cars"
---

# [9.2] Explainable deep learning improves human mental models of self-driving cars

- Authors: Eoin M. Kenny, Akshay Dharmavaram, Sang Uk Lee, Tung Phan-Minh, Shreyas Rajesh, Yunqing H...
- [arXiv Link](https://arxiv.org/abs/2411.18714)
- [PDF Link](https://arxiv.org/pdf/2411.18714.pdf)

## Subfields
 Explainable AI (XAI) / Motion Planning / Human-Machine Interaction
## Reason for Interest

1. 创新性与价值：该论文不仅提出了一种基于概念瓶颈（Concept-Bottleneck）的可解释规划网络（CW-Net），更关键的是在真实的自动驾驶车辆（Motional）上进行了闭环部署和验证。在自动驾驶领域，能将学术界的XAI方法应用到实车并证明其对安全员（Safety Driver）的实际效用是非常罕见且极具价值的。
2. 实验完整性：实验设计非常严谨，涵盖了大规模仿真（nuPlan）、封闭场地测试、公共道路测试（拉斯维加斯）以及针对专家（安全员）和非专家的人类受试者研究。这种全链路的验证方式极大地增强了结论的说服力。
3. 行业影响力：论文直击端到端/深度学习规划器“黑盒”不可解释的痛点，通过提升系统的透明度来改善人机共驾（Human-AI Teaming）的安全性，这对L3/L4级自动驾驶的落地及监管具有重要的参考意义。
4. 局限性：虽然驾驶性能与黑盒基线持平，但在某些特定场景（如起步）略有下降，且依赖有监督的概念标签，但这不影响其作为一篇顶级应用研究工作的地位。
## Abstract: 
Self-driving cars increasingly rely on deep neural networks to achieve human-like driving. The opacity of such black-box planners makes it challenging for the human behind the wheel to accurately anticipate when they will fail, with potentially catastrophic consequences. While research into interpreting these systems has surged, most of it is confined to simulations or toy setups due to the difficulty of real-world deployment, leaving the practical utility of such techniques unknown. Here, we introduce the Concept-Wrapper Network (CW-Net), a method for explaining the behavior of machine-learning-based planners by grounding their reasoning in human-interpretable concepts. We deploy CW-Net on a real self-driving car and show that the resulting explanations improve the human driver's mental model of the car, allowing them to better predict its behavior. To our knowledge, this is the first demonstration that explainable deep learning integrated into self-driving cars can be both understandable and useful in a realistic deployment setting. CW-Net accomplishes this level of intelligibility while providing explanations which are causally faithful and do not sacrifice driving performance. Overall, our study establishes a general pathway to interpretability for autonomous agents by way of concept-based explanations, which could help make them more transparent and safe.
