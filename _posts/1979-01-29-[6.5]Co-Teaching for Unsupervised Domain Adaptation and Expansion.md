---
layout: default
title: "[6.5]Co-Teaching for Unsupervised Domain Adaptation and Expansion"
---

# [6.5] Co-Teaching for Unsupervised Domain Adaptation and Expansion

- Authors: Hailan Lin, Qijie Wei, Kaibin Tian, Ruixiang Zhao, Xirong Li
- [arXiv Link](https://arxiv.org/abs/2204.01210)
- [PDF Link](https://arxiv.org/pdf/2204.01210.pdf)

## Subfields
 感知 / 语义分割 (域扩展/Domain Expansion)
## Reason for Interest

该论文研究了自动驾驶中关键的‘域扩展’（UDE）问题，旨在使模型适应恶劣天气（ACDC）的同时不遗忘正常场景（Cityscapes）。提出的Co-Teaching方法通过双教师蒸馏和Mixup有效缓解了跨域视觉歧义，在UDE设定下优于基准方法KDDE。然而，论文在自动驾驶核心任务（分割）上仍使用较为过时的DeepLabv2架构和FDA/AdaSegNet基准，导致绝对性能远低于当前主流的域适应SOTA水平，限制了其在工业界的直接应用价值。
## Abstract: 
Unsupervised Domain Adaptation (UDA) essentially trades a model's performance on a source domain for improving its performance on a target domain. To overcome this, Unsupervised Domain Expansion (UDE) has been introduced, which adapts the model to the target domain while preserving its performance in the source domain. In both UDA and UDE, a model tailored to a given domain is assumed to well handle samples from the given domain. We question the assumption by reporting the existence of cross-domain visual ambiguity: Due to the unclear boundary between the two domains, samples from one domain can be visually close to the other domain. Such sorts of samples are typically in the minority in their host domain, so they tend to be overlooked by the domain-specific model, but can be better handled by a model from the other domain. We exploit this finding by proposing Co-Teaching (CT), which is instantiated with knowledge distillation based CT (kdCT) plus mixup based CT (miCT). Specifically, kdCT leverages a dual-teacher architecture to enhance the student network's ability to handle cross-domain ambiguity. Meanwhile, miCT further enhances the generalization ability of the student. Extensive experiments on image classification and driving-scene segmentation show the viability of CT for UDE.
