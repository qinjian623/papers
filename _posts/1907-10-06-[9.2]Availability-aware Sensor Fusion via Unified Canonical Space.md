---
layout: default
title: "[9.2]Availability-aware Sensor Fusion via Unified Canonical Space"
---

# [9.2] Availability-aware Sensor Fusion via Unified Canonical Space

- Authors: Dong-Hee Paek, Seung-Hyun Kong
- [arXiv Link](https://arxiv.org/abs/2503.07029)
- [PDF Link](https://arxiv.org/pdf/2503.07029.pdf)

## Subfields
 多传感器融合 / 3D目标检测 (Multi-sensor Fusion / 3D Object Detection)
## Reason for Interest

1. 创新性与实用性高：论文提出的ASF框架通过统一规范空间投影（UCP）和基于Patch的传感器间交叉注意力（CASAP），有效解决了多模态特征对齐和传感器失效问题。特别是CASAP模块，在保持低计算成本（O(NqNs)）的同时实现了动态的传感器可靠性加权，这对于实现自动驾驶中的'真冗余'（True Redundancy）至关重要。
2. 实验充分且结果惊人：在K-Radar数据集的恶劣天气场景（雨雪雾）中展示了卓越的鲁棒性，证明了在Camera/LiDAR失效时模型能自动依赖4D Radar保持检测能力。提出的传感器组合损失（SCL）使得单一模型能适应任意传感器组合，无需针对每种故障情况重新训练。
3. 工程价值：推理速度快（RTX3090上全传感器13.5 FPS，L+R组合20.5 FPS），显存占用低，非常贴合车端部署需求。
## Abstract: 
Sensor fusion of camera, LiDAR, and 4-dimensional (4D) Radar has brought a significant performance improvement in autonomous driving. However, there still exist fundamental challenges: deeply coupled fusion methods assume continuous sensor availability, making them vulnerable to sensor degradation and failure, whereas sensor-wise cross-attention fusion methods struggle with computational cost and unified feature representation. This paper presents availability-aware sensor fusion (ASF), a novel method that employs unified canonical projection (UCP) to enable consistency in all sensor features for fusion and cross-attention across sensors along patches (CASAP) to enhance robustness of sensor fusion against sensor degradation and failure. As a result, the proposed ASF shows a superior object detection performance to the existing state-of-the-art fusion methods under various weather and sensor degradation (or failure) conditions. Extensive experiments on the K-Radar dataset demonstrate that ASF achieves improvements of 9.7% in AP BEV (87.2%) and 20.1% in AP 3D (73.6%) in object detection at IoU=0.5, while requiring a low computational cost. All codes are available at https://github.com/kaist-avelab/k-radar.
