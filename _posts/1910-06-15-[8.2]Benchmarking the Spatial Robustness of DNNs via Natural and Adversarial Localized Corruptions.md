---
layout: default
title: "[8.2]Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions"
---

# [8.2] Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions

- Authors: Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo
- [arXiv Link](https://arxiv.org/abs/2504.01632)
- [PDF Link](https://arxiv.org/pdf/2504.01632.pdf)

## Subfields
 感知 / 语义分割 (鲁棒性与安全)
## Reason for Interest

1. 选题价值高：针对自动驾驶场景中常见的局部遮挡、污损（如泥点、强光）及潜在的对抗攻击，填补了针对“局部”腐蚀鲁棒性评估的空白。
2. 方法论严谨：提出了区分受损区域（M）与未受损区域（Non-M）的细粒度评估指标，能够量化局部干扰对全局预测的扩散影响。
3. 实验发现深刻：通过对比14种模型，揭示了Transformer架构（如SegFormer）在抗自然腐蚀方面优于CNN，但在抗局部对抗攻击方面显著弱于CNN的“互补”特性，这对车端感知模型的架构选型和安全冗余设计（如模型集成）具有重要的指导意义。
4. 完备性：实验设计详实，代码已开源，具有较高的行业参考价值。
## Abstract: 
The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remains underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.
