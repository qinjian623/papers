---
layout: default
title: "[9.2]FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies"
---

# [9.2] FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies

- Authors: Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, Wei Tsang Ooi
- [arXiv Link](https://arxiv.org/abs/2412.06708)
- [PDF Link](https://arxiv.org/pdf/2412.06708.pdf)

## Subfields
 事件相机感知 / 多传感器融合 (事件-帧) / 目标检测
## Reason for Interest

该论文提出了一种名为FlexEvent的事件-帧融合目标检测框架，旨在解决现有事件相机检测器在动态环境中操作频率固定和性能下降的限制。其创新性主要体现在以下两点：
1. FlexFuse模块：提出了一种自适应事件-帧融合模块，通过动态软权重和引入学习噪声的门控函数，有效地将高频事件数据与RGB帧的语义信息融合，克服了事件数据在高频下语义信息不足的问题，显著提升了检测精度。
2. FlexTune机制：设计了一种频率自适应微调机制，通过生成高频调整标签和迭代自训练（包括高频自举、时间一致性校准和循环自训练），使模型能够在不依赖手动高频标注的情况下学习，并泛化到不同操作频率，从而实现跨频率的鲁棒检测。

实验部分非常充分和严谨：
- 在DSEC-Det、DSEC-Detection和DSEC-MOD三个大规模自动驾驶事件相机数据集上进行了广泛评估，这些数据集均与车端自动驾驶场景高度相关。
- 与大量最先进的纯事件和事件-帧融合方法进行了全面比较，包括了多个CVPR/ICCV/Nature等顶会/期刊的最新工作，并明确指出了基线的重训练和结果引用方式，确保了对比的公平性。
- 在所有数据集和标准COCO指标（mAP, AP50, AP75, APS, APM, APL）上均取得了显著的SOTA性能提升。
- 详细的消融研究清晰地证明了FlexFuse和FlexTune模块各自的贡献，并分析了不同融合策略和超参数的影响。
- 效率分析表明，FlexEvent虽然参数略多，但推理速度与SAST相当，远超DAGr等方法，具有实时应用潜力。
- 大量的定性结果展示了在快速移动、遮挡、不同光照和高频等挑战性场景下，FlexEvent相比现有方法的优势。

该研究直接针对自动驾驶领域事件相机感知的关键挑战，即在多变动态环境下保持高频、高精度、鲁棒的物体检测能力。FlexFuse和FlexTune的创新性结合，不仅在技术上取得了突破，也在实际应用中具有巨大潜力，尤其是在处理高速运动、复杂光照等传统相机难以胜任的场景，以及缓解高频数据标注成本方面。论文结构清晰，论证扎实，数据支持充分，对自动驾驶领域的事件感知发展具有重要推动作用。
## Abstract: 
Event cameras offer unparalleled advantages for real-time perception in dynamic environments, thanks to the microsecond-level temporal resolution and asynchronous operation. Existing event detectors, however, are limited by fixed-frequency paradigms and fail to fully exploit the high-temporal resolution and adaptability of event data. To address these limitations, we propose FlexEvent, a novel framework that enables detection at varying frequencies. Our approach consists of two key components: FlexFuse, an adaptive event-frame fusion module that integrates high-frequency event data with rich semantic information from RGB frames, and FlexTune, a frequency-adaptive fine-tuning mechanism that generates frequency-adjusted labels to enhance model generalization across varying operational frequencies. This combination allows our method to detect objects with high accuracy in both fast-moving and static scenarios, while adapting to dynamic environments. Extensive experiments on large-scale event camera datasets demonstrate that our approach surpasses state-of-the-art methods, achieving significant improvements in both standard and high-frequency settings. Notably, our method maintains robust performance when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving its effectiveness in extreme conditions. Our framework sets a new benchmark for event-based object detection and paves the way for more adaptable, real-time vision systems.
