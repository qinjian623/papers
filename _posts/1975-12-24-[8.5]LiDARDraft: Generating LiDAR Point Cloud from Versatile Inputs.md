---
layout: default
title: "[8.5]LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs"
---

# [8.5] LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs

- Authors: Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Ro...
- [arXiv Link](https://arxiv.org/abs/2512.20105)
- [PDF Link](https://arxiv.org/pdf/2512.20105.pdf)

## Subfields
 自动驾驶仿真 / LiDAR 点云生成 (Generative Simulation)
## Reason for Interest

该论文提出了一种基于 '3D Layout' 的统一中间表征框架，有效地连接了文本、图像、点云等多种输入模态与 LiDAR 点云生成任务。1. 创新性：利用 LLM 或视觉模型提取场景 Layout（几何图元），结合 Raycasting 和 ControlNet 进行条件控制，技术路径清晰且具有很强的工程实用价值，解决了多模态控制生成的一致性难题。2. 实验完整性：在 KITTI-360 等主流数据集上进行了详尽的对比实验和消融研究，指标提升显著，且展示了跨数据集（train on KITTI-360, test on nuScenes）的泛化能力。3. 行业价值：支持'从零生成'（Simulation from scratch）和场景编辑，对于自动驾驶长尾场景数据合成、Corner Case 构建具有极高的应用潜力。尽管依赖几何图元可能限制极其细微的结构还原，但作为仿真工具其灵活性和生成质量极具竞争力。
## Abstract: 
Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.
