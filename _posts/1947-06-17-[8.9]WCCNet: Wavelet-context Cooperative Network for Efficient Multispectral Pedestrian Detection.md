---
layout: default
title: "[8.9]WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection"
---

# [8.9] WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection

- Authors: Xingjian Wang, Li Chai, Jiming Chen, Zhiguo Shi
- [arXiv Link](https://arxiv.org/abs/2308.01042)
- [PDF Link](https://arxiv.org/pdf/2308.01042.pdf)

## Subfields
 多模态感知 / RGB-红外(Thermal) 目标检测
## Reason for Interest

1. 创新性强且符合直觉：论文提出的非对称骨干网络（Asymmetric Backbone）极具价值。针对红外图像缺乏纹理但结构信息丰富的特点，设计了基于小波变换的混合专家模型（MoWE）替代传统CNN，在大幅降低计算量的同时有效提取了频域特征，打破了传统双流网络对称设计的思维定势。
2. 实用价值高：在自动驾驶车载端计算资源受限的背景下，该方法在保持SOTA精度的同时实现了极高的推理速度（WCCNet-S达到152 FPS），具有极高的工程落地潜力。
3. 实验严谨：在两大主流红外感知数据集上进行了详尽的对比和消融实验，不仅验证了精度，还深入分析了不同光照条件（白天/夜间）下专家模型的激活模式，解释性强。
## Abstract: 
Multispectral pedestrian detection is essential to various tasks especially autonomous driving, for which both the accuracy and computational cost are of paramount importance. Most existing approaches treat RGB and infrared modalities equally. They typically adopt two symmetrical backbones for multimodal feature extraction, which ignore the substantial differences between modalities and bring great difficulty for the reduction of the computational cost as well as effective crossmodal fusion. In this work, we propose a novel and efficient framework named Wavelet-context Cooperative Network (WCCNet), which differentially extracts complementary features across spectra with low computational cost and further fuses these diverse features based on their spatially relevant cross-modal semantics. WCCNet explores an asymmetric but cooperative dual-stream backbone, in which WCCNet utilizes generic neural layers for texture-rich feature extraction from RGB modality, while proposing Mixture of Wavelet Experts (MoWE) to capture complementary frequency patterns of infrared modality. By assessing multispectral environmental context, MoWE generates routing scores to selectively activate specific learnable Adaptive DWT (ADWT) layers, alongside shared static DWT, which are both considerible lightwight and efficient to significantly reduce computational overhead and facilitate subsequent fusion. To further fuse these multispectral features with significant semantic differences, we elaborately design the crossmodal rearranging fusion module (CMRF), which aims to mitigate misalignment and merge semantically complementary features in spatially-related local regions to amplify the crossmodal reciprocal information. Results from comprehensive evaluations on KAIST and FLIR benchmarks indicate that WCCNet outperforms state-of-the-art methods with considerable computational efficiency and competitive accuracy.
