---
layout: default
title: "[9.2]MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion"
---

# [9.2] MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion

- Authors: Minghui Hou, Wei-Hsing Huang, Shaofeng Liang, Daizong Liu, Tai-Hao Wen, Gang Wang, Runwei...
- [arXiv Link](https://arxiv.org/abs/2512.13177)
- [PDF Link](https://arxiv.org/pdf/2512.13177.pdf)

## Subfields
 自动驾驶视觉语言模型 (Autonomous Driving VLM) / 多模态场景理解
## Reason for Interest

论文提出了MMDrive框架，针对现有自动驾驶VLM主要依赖2D图像、缺乏3D空间理解的痛点，创新性地融合了Occupancy（占用栅格）、LiDAR点云以及通过两阶段策略生成的文本场景描述。技术上设计了TMM（文本导向多模态调节器）实现基于问题的自适应模态权重调整，以及CMA（跨模态抽象器）利用抽象Token提取关键信息。实验表明该方法在DriveLM和NuScenes-QA两个主流基准上均取得了SOTA性能，且消融实验充分验证了引入3D模态和新模块的有效性，具有较高的行业应用潜力和研究价值。
## Abstract: 
Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.
