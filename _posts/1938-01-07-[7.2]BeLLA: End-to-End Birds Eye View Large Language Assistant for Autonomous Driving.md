---
layout: default
title: "[7.2]BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving"
---

# [7.2] BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving

- Authors: Karthik Mohan, Sonam Singh, Amit Arvind Kale
- [arXiv Link](https://arxiv.org/abs/2512.06096v1)
- [PDF Link](https://arxiv.org/pdf/2512.06096v1.pdf)

## Subfields
 Autonomous Driving / Vision-Language Models (VQA) / BEV Perception
## Reason for Interest

The paper introduces BeLLA, a novel architecture aligning BEV features with LLMs to enhance spatial reasoning in autonomous driving VQA tasks. 

Strengths:
1. **Innovation**: The two-stage training (alignment + finetuning) to project BEV features into LLM space is a logical and effective method to address the lack of spatial context in 2D-view VLMs.
2. **Specific Performance**: It demonstrates strong capabilities in spatial and status-related queries, outperforming baselines by 9.3% in the 'Status' category of NuScenes-QA.

Weaknesses:
1. **Overall Performance**: It fails to beat existing SOTA baselines (like MSMDFusion or DriveLM-Agent) on aggregate metrics (Overall Accuracy, BLEU-4). 
2. **Limitations**: The reliance on BEV results in a loss of fine-grained visual details (e.g., traffic light colors), leading to poor performance on 'Perception' tasks. Additionally, the model currently processes single frames, lacking the temporal context crucial for accurate trajectory prediction compared to video-based methods.

Conclusion: A solid architectural contribution with clear industry relevance, but the experimental results show it is a specialized solution (spatial reasoning) rather than a comprehensive SOTA replacement.
## Abstract: 

