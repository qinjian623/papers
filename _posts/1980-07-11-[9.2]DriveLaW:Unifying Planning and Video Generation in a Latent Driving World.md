---
layout: default
title: "[9.2]DriveLaW:Unifying Planning and Video Generation in a Latent Driving World"
---

# [9.2] DriveLaW:Unifying Planning and Video Generation in a Latent Driving World

- Authors: Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, Haiyang Sun, Bing Wang, ...
- [arXiv Link](https://arxiv.org/abs/2512.23421)
- [PDF Link](https://arxiv.org/pdf/2512.23421.pdf)

## Subfields
 世界模型 / 端到端规划 / 视频生成
## Reason for Interest

论文提出了一种创新的'串联'式世界模型架构，成功将大规模视频生成预训练的先验知识转移到运动规划任务中，解决了生成与规划脱节的问题。提出的噪声再注入机制有效解决了高速场景下的生成失真问题。实验数据扎实，在生成质量（FID/FVD）和闭环规划指标（PDMS）上双双达成SOTA，且对比了大量前沿基线（如UniAD, Epona, VADv2），具有极高的学术价值和应用前景。
## Abstract: 
World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.
