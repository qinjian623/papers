---
layout: default
title: "[8.8]Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation"
---

# [8.8] Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation

- Authors: Wenyao Zhang, Hongsi Liu, Bohan Li, Jiawei He, Zekun Qi, Yunnan Wang, Shengyang Zhao, Xin...
- [arXiv Link](https://arxiv.org/abs/2510.09320v1)
- [PDF Link](https://arxiv.org/pdf/2510.09320v1.pdf)

## Subfields
 Visual Perception / Monocular Depth Estimation
## Reason for Interest

The paper presents a high-value contribution to the field of low-cost autonomous driving perception by successfully integrating Vision-Language Foundation Models (CLIP and DINO) into self-supervised depth estimation.

1. **Innovation**: The proposed 'Hybrid-depth' framework introduces a novel coarse-to-fine language guidance mechanism. This effectively solves the granularity mismatch between global semantic features (CLIP) and local spatial features (DINO), a non-trivial challenge when adapting foundational models to geometric tasks.
2. **Performance**: It achieves State-of-the-Art (SOTA) results on the highly competitive KITTI benchmark. While the numerical improvement on standard depth metrics is incremental over the very latest 2024 baselines (e.g., 0.093 vs 0.094), the consistency across metrics is strong.
3. **Industry Relevance**: Crucially, the paper goes beyond basic depth metrics to demonstrate significant gains in downstream 3D BEV perception tasks (Table 8), directly proving its utility in an autonomous driving stack.
4. **Completeness**: The experiments are comprehensive, covering ablation studies on prompt design and feature fusion, and the inclusion of code availability adds to its credibility.
## Abstract: 

