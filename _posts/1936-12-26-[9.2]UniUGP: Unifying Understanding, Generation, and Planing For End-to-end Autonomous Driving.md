---
layout: default
title: "[9.2]UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving"
---

# [9.2] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving

- Authors: Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Ch...
- [arXiv Link](https://arxiv.org/abs/2512.09864v1)
- [PDF Link](https://arxiv.org/pdf/2512.09864v1.pdf)

## Subfields
 端到端自动驾驶 / 世界模型 / 视觉语言动作模型 (VLA)
## Reason for Interest

该论文提出了极具创新性的混合专家（Hybrid Expert）架构，成功将大语言模型（VLM）的语义理解推理能力、生成式世界模型的视频预测能力与流匹配（Flow Matching）规划策略统一在一个端到端框架中。论文不仅解决了传统VLA模型难以利用非结构化视频数据的问题，还通过四阶段训练策略（基础理解->视觉动力学->文本推理->混合融合）有效提升了模型在长尾场景下的泛化性。实验验证非常扎实，构建了包含异常检测、事故预测等多维度的长尾测试集。特别是在仅依赖前视相机的受限条件下，其规划安全性指标（碰撞率）能比肩甚至超越依赖全景相机的现有SOTA模型（如GenAD），证明了引入世界知识和推理能力对提升驾驶安全性的显著价值，具有极高的行业指导意义。
## Abstract: 

