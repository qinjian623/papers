---
layout: default
title: "[5.0]Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models"
---

# [5.0] Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models

- Authors: Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay
- [arXiv Link](https://arxiv.org/abs/2503.16724)
- [PDF Link](https://arxiv.org/pdf/2503.16724.pdf)

## Subfields
 可解释强化学习 / VLM驱动的特征提取
## Reason for Interest

本论文提出了一种名为iTRACE的创新框架，旨在通过预训练的视觉-语言模型（VLM）实现强化学习（RL）的自动化语义可解释性。其核心创新在于：1) 利用VLM进行零样本语义特征提取和特征提取代码生成，显著减少了对人工标注的依赖；2) 将VLM生成的特征提取过程蒸馏到一个轻量级CNN模型中，解决了VLM推理速度慢的问题，使其在RL训练中具有实用性；3) 结合可解释的树形控制策略（Interpretable Control Tree, ICT）进行策略学习，确保决策过程的透明性和可验证性。

实验在Atari游戏、网格世界导航和2D驾驶（OpenAI Gym Highway）等多个环境中进行了验证，基线设置全面，包括黑盒模型（CNN、VLM-MLP）、其他可解释模型（VLM-EQL）以及直接由VLM生成决策树的策略。结果表明，iTRACE在可解释策略中表现出色，并在多数情况下能够达到与黑盒策略相当的性能。

然而，根据评审要求，本论文在“车端自动驾驶研究”领域的直接价值受到严格限制。尽管其中一个实验领域是“驾驶”，但OpenAI Gym Highway是一个高度简化的2D模拟环境，并非现代自动驾驶中涉及的复杂3D感知、多传感器融合、轨迹预测或高精度规划控制等真实场景。其主要贡献在于通用可解释强化学习领域，为从像素输入到动作输出的决策过程提供透明度。由于其与真实的“车端自动驾驶”缺乏直接关联，本研究成果在自动驾驶行业的直接应用潜力目前较低。严格按照评审标准，如果不直接和车端自动驾驶相关，最高分限制为5分。
## Abstract: 
Semantic interpretability in Reinforcement Learning (RL) enables transparency and verifiability of decision-making. Achieving semantic interpretability in reinforcement learning requires (1) a feature space composed of human-understandable concepts and (2) a policy that is interpretable and verifiable. However, constructing such a feature space has traditionally relied on manual human specification, which often fails to generalize to unseen environments. Moreover, even when interpretable features are available, most reinforcement learning algorithms employ black-box models as policies, thereby hindering transparency. We introduce interpretable Tree-based Reinforcement learning via Automated Concept Extraction (iTRACE), an automated framework that leverages pre-trained vision-language models (VLM) for semantic feature extraction and train a interpretable tree-based model via RL. To address the impracticality of running VLMs in RL loops, we distill their outputs into a lightweight model. By leveraging Vision-Language Models (VLMs) to automate tree-based reinforcement learning, iTRACE loosens the reliance the need for human annotation that is traditionally required by interpretable models. In addition, it addresses key limitations of VLMs alone, such as their lack of grounding in action spaces and their inability to directly optimize policies. We evaluate iTRACE across three domains: Atari games, grid-world navigation, and driving. The results show that iTRACE outperforms other interpretable policy baselines and matches the performance of black-box policies on the same interpretable feature space.
