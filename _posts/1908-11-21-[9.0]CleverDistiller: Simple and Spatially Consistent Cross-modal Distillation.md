---
layout: default
title: "[9.0]CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation"
---

# [9.0] CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation

- Authors: Hariprasath Govindarajan, Maciej K. Wozniak, Marvin Klingner, Camille Maurice, B Ravi Kir...
- [arXiv Link](https://arxiv.org/abs/2503.09878)
- [PDF Link](https://arxiv.org/pdf/2503.09878.pdf)

## Subfields
 3D感知 / 跨模态知识蒸馏 (3D Perception / Cross-modal Knowledge Distillation)
## Reason for Interest

该论文提出了一种名为 CleverDistiller 的高效自监督跨模态蒸馏框架，具有极高的行业应用价值。主要贡献点明确且有效：1. 发现并解决了以往方法中线性投影头（Linear Head）导致 3D 骨干网络特征表达能力受限的问题，通过 MLP 投影头显著提升了特征含金量（RankMe 指标验证）；2. 引入自监督辅助任务（Occupancy Prediction）来弥补从 2D 视觉大模型（如 DINOv2）蒸馏语义特征时丢失的空间几何信息。实验非常扎实，涵盖了 9 个数据集，证明了该方法在少样本学习（Data-efficient）、全量数据微调以及域泛化（Domain Generalization）和鲁棒性（Robustness）方面均达到 SOTA 水平。该方法摒弃了复杂的语义先验（如 SAM masks）和对比学习负样本挖掘，不仅性能更优且训练流程更简洁，非常适合自动驾驶长尾场景下的数据挖掘与模型预训练。
## Abstract: 
Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy
