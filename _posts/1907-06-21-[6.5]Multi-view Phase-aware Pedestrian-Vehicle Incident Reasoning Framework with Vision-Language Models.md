---
layout: default
title: "[6.5]Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models"
---

# [6.5] Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models

- Authors: Hao Zhen, Yunxiang Yang, Jidong J. Yang
- [arXiv Link](https://arxiv.org/abs/2511.14120)
- [PDF Link](https://arxiv.org/pdf/2511.14120.pdf)

## Subfields
 场景理解与安全分析 / VLM应用 / V2X协同感知
## Reason for Interest

论文提出了一种利用视觉-语言大模型（VLM）进行行人-车辆事故深度推理的框架。其核心创新在于将传统的交通安全行为阶段理论（感知-判断-行动等）与现代VLM结合，并通过多视角（车端+路侧）协同分析生成结构化事故报告。虽然该系统主要用于离线分析、事故归因或V2X监控，而非实时的车端规划控制，但其对于自动驾驶的长尾场景挖掘、数据标注、仿真场景构建及安全性评估具有重要价值。实验设计完整，在WTS数据集上证明了针对性微调VLM处理多视角时序数据的有效性。
## Abstract: 
Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.
