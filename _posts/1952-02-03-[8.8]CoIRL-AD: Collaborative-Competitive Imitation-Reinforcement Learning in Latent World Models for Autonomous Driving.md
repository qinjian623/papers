---
layout: default
title: "[8.8]CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving"
---

# [8.8] CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving

- Authors: Xiaoji Zheng, Ziyuan Yang, Yanhao Chen, Yuhang Peng, Yuanrong Tang, Gengyuan Liu, Bokui C...
- [arXiv Link](https://arxiv.org/abs/2510.12560v1)
- [PDF Link](https://arxiv.org/pdf/2510.12560v1.pdf)

## Subfields
 End-to-End Autonomous Driving / World Models / Reinforcement Learning
## Reason for Interest

The paper addresses a critical bottleneck in End-to-End Autonomous Driving: the poor generalization and long-tail performance of pure Imitation Learning (IL). The proposed CoIRL-AD framework innovatively combines IL and Reinforcement Learning (RL) using a 'Competitive-Collaborative' dual-policy mechanism and a Latent World Model for imagination-based training. 

Key strengths include:
1. **Innovation**: The competitive mechanism to transfer knowledge between IL and RL actors effectively solves the training instability typically seen when combining these modalities. The 'Backward Planning' (inverse causality) strategy is a counter-intuitive but effective technical insight.
2. **Performance**: It achieves State-of-the-Art (SOTA) on the emerging Navsim benchmark, which is more focused on closed-loop safety than pure open-loop metrics. On nuScenes, it demonstrates a significant reduction in collision rates compared to strong baselines like LAW and SSR.
3. **Completeness**: Extensive ablations (causality, integration strategies) and analysis of long-tail scenarios validate the method's robustness.

The paper is highly relevant to the industry's shift towards 'System 2' thinking (reasoning/RL) in autonomous driving.
## Abstract: 

