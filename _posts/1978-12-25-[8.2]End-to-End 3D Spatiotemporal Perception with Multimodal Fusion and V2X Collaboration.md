---
layout: default
title: "[8.2]End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration"
---

# [8.2] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration

- Authors: Zhenwei Yang, Yibo Ai, Weidong Zhang
- [arXiv Link](https://arxiv.org/abs/2512.21831)
- [PDF Link](https://arxiv.org/pdf/2512.21831.pdf)

## Subfields
 V2X Cooperative Perception / Multimodal 3D Object Detection & Tracking
## Reason for Interest

论文提出了一种统一的端到端 V2X 协同感知框架 (XET-V2X)，创新性地将多模态融合（LiDAR + Camera）、多视角协同和时序跟踪整合在基于 Transformer 的架构中。通过双层空间交叉注意力模块（dual-layer spatial cross-attention）有效解决了异构传感器和不同视角间的特征对齐问题。实验评估非常全面，涵盖了真实世界数据 (V2X-Seq-SPD) 和仿真数据 (V2X-Sim)，验证了模型在通信延迟和遮挡场景下的鲁棒性。相比于传统的模块化或单模态方法，该方案在检测精度和跟踪稳定性上均有显著提升，具有较高的学术价值和应用潜力。
## Abstract: 
Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.
