---
layout: default
title: "[9.2]TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References"
---

# [9.2] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References

- Authors: Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
- [arXiv Link](https://arxiv.org/abs/2512.21641v1)
- [PDF Link](https://arxiv.org/pdf/2512.21641v1.pdf)

## Subfields
 3D Vision-Language Grounding / Multimodal Object Tracking
## Reason for Interest

The paper addresses a critical and under-explored gap in autonomous driving: grounding object references based on temporal behavior (e.g., 'the car that just crossed') rather than just static attributes. The proposed TrackTeller framework innovatively integrates a unified multimodal representation (UniScene) with a temporal reasoning module that explicitly models history and future motion. The experimental results on the NuPrompt benchmark are highly impressive, showing significant improvements in tracking accuracy and robustness against distractors compared to existing state-of-the-art methods like PromptTrack-3D. The methodology is rigorous, and the ablation studies clearly justify the contributions.
## Abstract: 

