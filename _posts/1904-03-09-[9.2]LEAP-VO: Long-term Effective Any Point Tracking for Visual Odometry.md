---
layout: default
title: "[9.2]LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry"
---

# [9.2] LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry

- Authors: Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys
- [arXiv Link](https://arxiv.org/abs/2401.01887)
- [PDF Link](https://arxiv.org/pdf/2401.01887.pdf)

## Subfields
 视觉里程计 (VO) / 长期点跟踪 / 动态场景感知
## Reason for Interest

该论文提出了LEAP-VO，一个基于新型长期有效任意点跟踪（LEAP）模块的鲁棒视觉里程计系统。其核心创新点包括：1) 引入了基于锚点的动态轨迹估计模块，通过融合视觉、轨迹间（inter-track）和时间线索，有效区分静态和动态场景元素，这对于在复杂动态环境中运行的自动驾驶车辆至关重要。2) 提出了一种时间概率建模方法，通过多变量柯西分布和迭代优化，对点级不确定性进行量化，并在跟踪和滤波过程中加以利用，显著增强了系统在遮挡、低纹理区域等挑战性场景下的鲁棒性。

LEAP-VO将这一先进的长期点跟踪作为前端，结合基于可见性、动态标签和不确定性的智能轨迹滤波，以及滑动窗口束调整（Bundle Adjustment）进行姿态优化。这种系统性的集成极大地提升了VO在现实动态场景下的性能和可靠性。

实验评估非常全面，涵盖了静态（Replica）和多种动态（MPI Sintel，TartanAir-Shibuya）数据集。LEAP-VO在动态环境中持续展现出最先进的性能，在ATE、RPE trans和RPE rot等关键指标上取得了显著提升，并与广泛的SLAM/VO基线（如DROID-SLAM、DPVO、DytanVO）进行了充分比较。消融实验也设计得很详尽，清晰地验证了所提出各模块（轨迹滤波、关键点提取策略、不确定性估计）的有效性。定性结果也进一步支持了动态轨迹估计和不确定性建模的有效性。

该研究与自动驾驶领域高度相关，直接解决了动态、杂乱环境中鲁棒自车运动估计以及提供关键不确定性感知等核心挑战。其提高自动驾驶系统可靠性和安全性的潜力巨大，因此给予高分。
## Abstract: 
Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.
