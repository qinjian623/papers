---
layout: default
title: "[8.9]DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection"
---

# [8.9] DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection

- Authors: Feiyang Jia, Caiyan Jia, Ailin Liu, Shaoqing Xu, Qiming Xia, Lin Liu, Lei Yang, Yan Gong,...
- [arXiv Link](https://arxiv.org/abs/2511.10035)
- [PDF Link](https://arxiv.org/pdf/2511.10035.pdf)

## Subfields
 多传感器融合3D目标检测 / 鲁棒性 / 困难样本检测
## Reason for Interest

论文提出了一个新颖的“双向引导”多模态融合范式DGFusion，旨在解决自动驾驶中3D目标检测的困难样本问题，尤其关注不同模态间的信息密度差距。其核心创新在于**Difficulty-aware Instance Pair Matcher (DIPM)**，它能根据实例难度进行特征匹配，从而形成易实例对（EIP）和两种硬实例对（HIP），并利用**Dual-guided Modules (PGIE/IGPE)**对LiDAR和Camera BEV特征进行有针对性的增强，充分利用图像和激光雷达的互补优势。这一框架设计具有较高的创新性，概念明确且解决了自动驾驶中一个重要的实际挑战。

实验部分非常完整和严格，在主流的nuScenes数据集上进行了广泛评估，不仅与多种SOTA单模态和多模态方法进行了对比，还针对困难样本（不同距离、可见度、大小）和不同规模（mini、10%、25%）的训练数据进行了详细的鲁棒性分析。论文提供了全面的消融实验，验证了每个核心模块（Instance Match Modules, Dual-guided Modules）的有效性以及不同策略的影响，实验设计严谨，结果可信。

结果显示，DGFusion在nuScenes上显著超越了基线BEVFusion，尤其在长距离、低可见度、小目标等传统困难场景下展现出优异的鲁棒性，这对于提升自动驾驶系统的安全性至关重要。虽然在mAP和NDS指标上略低于少数方法（如FocalFormer3D），但其推理延迟更低，在实际应用中具有良好潜力。该研究直接针对自动驾驶安全中的关键感知挑战，具有很高的行业应用价值和广阔的发展前景。
## Abstract: 
As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.
