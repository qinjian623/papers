---
layout: default
title: "[7.2]BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving"
---

# [7.2] BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving

- Authors: Karthik Mohan, Sonam Singh, Amit Arvind Kale
- [arXiv Link](https://arxiv.org/abs/2512.06096)
- [PDF Link](https://arxiv.org/pdf/2512.06096.pdf)

## Subfields
 自动驾驶多模态大模型 (VLM for AD) / BEV感知
## Reason for Interest

论文提出了一种将纯视觉BEV特征直接投影并对齐到LLM输入空间的端到端架构。创新点在于设计了BEV-Text对齐的预训练阶段，实验证明这对提升模型在空间关系和物体状态理解（NuScenes-QA 'Status'类）上的能力非常有效。然而，论文存在明显短板：1. 仅依赖单帧BEV输入，缺乏时序信息，限制了对动态场景的推理能力；2. BEV表征丢失了纹理和颜色细节（如红绿灯），导致在DriveLM数据集上表现不如利用2D特征的基线。虽然在纯视觉BEV赛道有竞争力，但距离通用的SOTA和实际上车应用（需解决时序和细粒度视觉问题）仍有距离。
## Abstract: 
The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360{\deg} BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.
