---
layout: default
title: "[9.0]LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments"
---

# [9.0] LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments

- Authors: Jin Huang, Yuchao Jin, Le An, Josh Park
- [arXiv Link](https://arxiv.org/abs/2506.07416)
- [PDF Link](https://arxiv.org/pdf/2506.07416.pdf)

## Subfields
 自动驾驶VLM推理效率 / 边缘设备部署优化
## Reason for Interest

该论文提出了一种为资源受限环境（特别是自动驾驶领域的嵌入式设备）优化的Vision-Language Model (VLM) 推理管线LiteVLM。其核心创新在于联合优化了三个关键技术：补丁选择（Patch Selection）以过滤不相关的相机视角，Token选择（Token Selection）以减少LLM输入序列长度，以及推测解码（Speculative Decoding）以加速Token生成。这些技术共同解决了VLM在边缘设备上部署的延迟瓶颈问题。

**创新性：** 单个技术并非全新，但论文的关键在于将这些技术（尤其是新提出的Patch Selection Module和Token Selection Module）进行联合优化，并将其集成到一个针对自动驾驶多视角输入的VLM架构中。这种系统级的协同优化是其创新所在，而非仅仅堆叠现有方法。

**实验完整性：** 论文在NVIDIA DRIVE Thor自动驾驶平台上进行了严格的基准测试，并提供了详细的端到端以及各阶段的延迟分解数据（ViT、LLM Prefill、LLM Decode、Selection Module）。对比了FP16和FP8量化前后的性能，并与FastV、Eagle等现有效率方法进行了比较。所使用的DriveLM数据集和评估指标（GPT scores, BLEU, ROUGE-L, CIDEr, match score的加权平均）也具有专业性。实验结果清晰地展示了各项优化措施的有效性及其对整体性能的贡献。

**可信度：** 论文作者来自NVIDIA，并在其自研的DRIVE Thor平台上进行验证，这大大增加了结果的可靠性和工业应用价值。延迟数据和加速比具有很强的说服力。虽然FP8量化导致了轻微的准确率下降，但论文清晰地展示了性能权衡，并指出未来工作将探索量化感知训练以缓解此问题。

**行业潜力：** 自动驾驶对实时性和资源效率有极高要求。将VLM部署到车端嵌入式设备是实现更高级别感知、理解、推理和规划的关键。该工作直接解决了VLM在自动驾驶领域落地时的主要技术障碍——推理延迟，具有巨大的行业应用潜力。能够将VLM的推理速度提升数倍，使其在实时决策中变得可行，对于自动驾驶的发展具有里程碑式的意义。

综合来看，这是一项高质量的工程实践和系统优化工作，对自动驾驶领域VLM的实际部署具有直接且重要的推动作用。
## Abstract: 
This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.
