---
layout: default
title: "[8.5]UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations"
---

# [8.5] UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations

- Authors: Yuan Ren, Guile Wu, Runhao Li, Zheyuan Yang, Yibo Liu, Xingxin Chen, Tongtong Cao, Bingbi...
- [arXiv Link](https://arxiv.org/abs/2411.15355)
- [PDF Link](https://arxiv.org/pdf/2411.15355.pdf)

## Subfields
 自动驾驶仿真 / 3D场景重建
## Reason for Interest

1. 创新性：针对自动驾驶中关键的鱼眼相机仿真难题，提出了一种基于仿射变换的3D Gaussian Splatting微分渲染方法，在保持实时性的同时有效解决了大视场畸变问题，并通过统一的高斯表示实现了多传感器（针孔、鱼眼、LiDAR）和多模态（语义、深度、法向量）的融合，具有较高的技术价值。
2. 实用性：能够进行高质量、实时的自动驾驶场景重建与仿真，支持闭环评测，直接服务于端到端自动驾驶的开发工具链。
3. 实验完整性：在KITTI-360等真实数据集上进行了充分对比，涵盖了与NeRF类（Zip-NeRF）、3DGS类（HUGS, Fisheye-GS）方法的对比，且消融实验清晰证明了所提几何变换的有效性。
## Abstract: 
Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.
