---
layout: default
title: "[8.6]OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation"
---

# [8.6] OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation

- Authors: Bo-Wen Yin, Jiao-Long Cao, Xuying Zhang, Yuming Chen, Ming-Ming Cheng, Qibin Hou
- [arXiv Link](https://arxiv.org/abs/2509.15096v1)
- [PDF Link](https://arxiv.org/pdf/2509.15096v1.pdf)

## Subfields
 多模态感知 / 语义分割 (Multi-modal Semantic Segmentation)
## Reason for Interest

1. 创新性与方法：针对多模态感知任务中预训练数据匮乏的问题，构建了包含5种模态（RGB, Depth, LiDAR, Thermal, Event）的大规模合成数据集 ImageNeXt，并提出了 OmniSegmentor 框架。采用“单RGB+随机辅助模态”的交替预训练策略，有效解决了多模态联合预训练的优化难题，且实现了模型参数的高效利用。
2. 实验效果：在 KITTI-360、MFNet、DeLiVER 等多个自动驾驶核心数据集上取得了显著的 SOTA 性能提升（如 KITTI-360 提升近 3%），且模型参数量（39M）和计算量（65.7G FLOPs）优于大部分竞品（如 CMNeXt, MultiMAE），具有很高的落地潜力。
3. 行业价值：该工作证明了通过合成数据进行大规模多模态预训练的可行性，对于解决自动驾驶中异构传感器（如热成像、事件相机）数据不足的问题具有重要参考价值。
4. 缺陷：预训练数据中的热成像和LiDAR数据主要由RGB或深度图合成/估计而来，物理真实性有限，虽然在特征学习上有效，但理论上限可能受制于合成质量。
## Abstract: 

