---
layout: default
title: "[8.8]VG3T: Visual Geometry Grounded Gaussian Transformer"
---

# [8.8] VG3T: Visual Geometry Grounded Gaussian Transformer

- Authors: Junho Kim, Seongwon Lee
- [arXiv Link](https://arxiv.org/abs/2512.05988)
- [PDF Link](https://arxiv.org/pdf/2512.05988.pdf)

## Subfields
 3D感知 / 语义占据栅格预测 (Semantic Occupancy Prediction)
## Reason for Interest

1. 创新性：论文针对现有基于3D高斯（3D Gaussian）的方法中存在的视角独立处理导致的几何不一致问题，以及像素反投影导致的近密远疏密度偏差问题，提出了VGGT特征主干进行早期多视角融合，并设计了基于网格的采样（Grid-Based Sampling）和位置精炼（Positional Refinement）模块。这套组合拳逻辑严密，有效提升了表达效率。
2. 实验完整性：在nuScenes数据集上进行了详尽的对比实验和消融实验。不仅比较了精度（mIoU），还详细分析了计算效率（延迟、显存、基元数量），实验设计扎实。
3. 结果可信度：mIoU提升明显（+1.7%），且显著降低了基元数量（从2.5万降至1.3万），证明了稀疏表示的优越性。
4. 行业潜力：3D高斯作为一种稀疏紧凑的3D表征，相比稠密体素方案更适合车端部署。该文提出的采样策略大幅优化了资源消耗，具有较高的落地参考价值，但443ms的推理延迟距离实时运行仍有优化空间。
## Abstract: 
Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
