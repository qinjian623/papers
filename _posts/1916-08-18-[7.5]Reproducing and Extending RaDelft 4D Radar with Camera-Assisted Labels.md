---
layout: default
title: "[7.5]Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels"
---

# [7.5] Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels

- Authors: Kejia Hu, Mohammed Alsakabi, John M. Dolan, Ozan K. Tonguz
- [arXiv Link](https://arxiv.org/abs/2512.02394)
- [PDF Link](https://arxiv.org/pdf/2512.02394.pdf)

## Subfields
 4D Radar Perception / Semantic Segmentation / Auto-labeling
## Reason for Interest

该论文主要贡献在于解决了RaDelft数据集缺乏开源代码和Radar语义标签的问题。1. 复现性贡献：复现了封闭的RaDelft baseline，承诺开源代码，对社区价值较大。2. 方法创新：提出了一种基于相机辅助的自动标注流程（投影+聚类），证明了相比原始方法能生成质量更高的Radar标签，从而训练出性能更好的分割模型。3. 实验分析：详细探究了合成雾天对跨模态（Camera-to-Radar）监督信号的影响，结论具有启发性。4. 不足之处：核心算法（跨模态投影和DBSCAN聚类）较为常规，且雾天数据基于合成生成而非真实采集，限制了评分上限。
## Abstract: 
Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.
