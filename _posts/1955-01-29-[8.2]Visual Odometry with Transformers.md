---
layout: default
title: "[8.2]Visual Odometry with Transformers"
---

# [8.2] Visual Odometry with Transformers

- Authors: Vlardimir Yugay, Duy-Kien Nguyen, Theo Gevers, Cees G. M. Snoek, Martin R. Oswald
- [arXiv Link](https://arxiv.org/abs/2510.03348v2)
- [PDF Link](https://arxiv.org/pdf/2510.03348v2.pdf)

## Subfields
 Visual Odometry (视觉里程计) / End-to-end Localization (端到端定位)
## Reason for Interest

该论文提出了一种完全端到端（End-to-End）的视觉里程计架构 VoT，创新性地摒弃了传统 VO 系统中复杂的后端优化（Bundle Adjustment）和手工特征匹配模块。核心价值在于：
1. **架构极简与速度优势**：通过预训练 Transformer 编码器（CroCo/Dust3R）与时空注意力解码器直接回归位姿，实现了实时推理（~55 FPS），解决了现有深度学习 VO 方法（如 DPVO）依赖优化层导致速度慢的问题。
2. **Scaling Law 验证**：展示了模型性能随数据量和模型大小增加而提升的 Scaling 行为，符合当前大模型发展趋势。
3. **实验表现**：在自动驾驶主流数据集 KITTI 上，其平移误差（ATE）达到 SOTA 水平（9.66m vs DPVO 9.74m），证明了纯学习方法的可行性。

**扣分点与局限性**：
1. **旋转误差较高**：在 KITTI 上，VoT 的旋转误差（ARE 31.12°）显著高于基于优化的 DPVO（13.63°），表明在长距离驾驶场景下的姿态航向保持能力仍不如几何方法，这对自动驾驶是致命的短板。
2. **训练数据依赖**：主要依赖大规模室内数据（ARKitScenes）进行预训练，虽然在 KITTI 上进行了微调，但主要贡献仍偏向通用 3D 视觉而非专用于自动驾驶场景的优化。

综合来看，该工作为“去优化模块”的端到端定位提供了强有力的实证，具有较高的学术价值和行业潜力。
## Abstract: 

