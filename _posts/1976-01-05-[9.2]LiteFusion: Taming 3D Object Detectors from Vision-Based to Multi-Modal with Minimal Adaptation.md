---
layout: default
title: "[9.2]LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation"
---

# [9.2] LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation

- Authors: Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma
- [arXiv Link](https://arxiv.org/abs/2512.20217)
- [PDF Link](https://arxiv.org/pdf/2512.20217.pdf)

## Subfields
 3D感知 / 多传感器融合 (Camera-LiDAR Fusion)
## Reason for Interest

该论文提出了一种极具工程价值和创新性的融合范式。1. **创新性强**：打破了传统融合方法依赖3D稀疏卷积（Sparse Conv）提取点云特征的惯例，提出'LiDAR辅助'的视觉主导架构。通过四元数特征适配（Quaternion Feature Adaptation）在特征空间高效融合几何信息，不仅大幅降低了计算复杂度，还解决了稀疏卷积在非NVIDIA硬件（如NPU、FPGA）上部署难的行业痛点。2. **鲁棒性SOTA**：解决了多模态模型在传感器失效（LiDAR丢失）时性能崩溃的问题，实现了平滑降级，这对自动驾驶安全至关重要。3. **实验充分**：在nuScenes上取得了SOTA级别的NDS分数，且参数量增加极少（仅约1.1%）。虽然仅在nuScenes上进行了验证（缺少Waymo等），但其对部署友好性和鲁棒性的贡献使其具有极高的行业落地潜力。
## Abstract: 
3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
