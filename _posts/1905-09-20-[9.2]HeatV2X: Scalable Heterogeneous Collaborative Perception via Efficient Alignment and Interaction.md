---
layout: default
title: "[9.2]HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction"
---

# [9.2] HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction

- Authors: Yueran Zhao, Zhang Zhang, Chao Sun, Tianze Wang, Chao Yue, Nuoran Li
- [arXiv Link](https://arxiv.org/abs/2511.10211)
- [PDF Link](https://arxiv.org/pdf/2511.10211.pdf)

## Subfields
 V2X协同感知 / 多模态异构感知
## Reason for Interest

该论文提出了HeatV2X，一个处理V2X协同感知中异构性和可扩展性问题的创新框架。它通过分阶段的参数高效微调来解决现有方法存在的全参数训练成本高、难以适应新Agent以及忽视新Agent反馈等痛点。创新性体现在：
1. **双阶段适配器设计：** 引入局部异构微调（LHFT）结合异构感知适配器（HA Adapter）处理模态特定差异，以及全局协同微调（GCFT）结合多认知适配器（MC Adapter）增强跨Agent交互，实现了参数高效的异构对齐和协同融合。
2. **可扩展性与效率：** 与现有SOTA方法相比，HeatV2X在实现更高感知性能（尤其在AP0.5和DAIR-V2X上）的同时，显著降低了可训练参数量和训练GPU时长，这对于自动驾驶V2X系统在实际部署和Agent动态加入时的可扩展性至关重要。
3. **全面的实验验证：** 论文在OPV2V-H和DAIR-V2X两个具有代表性的V2X数据集上进行了广泛实验，与多个强基线（HM-ViT, HEAL, MACP, COPEFT, STAMP）进行了详细对比。此外，还进行了详细的消融研究（验证LHFT和GCFT的贡献），以及鲁棒性评估（姿态噪声和通信延迟）和可扩展性评估，实验设计非常完整和有说服力。可视化结果也清晰地展示了方法的优势。

综合来看，该论文在自动驾驶V2X协同感知领域做出了重要的贡献，其提出的方法具有很强的理论和实际应用价值，能够有效提升异构多Agent协同感知的性能、效率和可扩展性。严格的实验验证和详细的分析使其可信度很高。
## Abstract: 
Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.
