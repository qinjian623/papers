---
layout: default
title: "[9.2]DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images"
---

# [9.2] DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images

- Authors: Xiaoxue Chen, Ziyi Xiong, Yuantao Chen, Gen Li, Nan Wang, Hongcheng Luo, Long Chen, Haiya...
- [arXiv Link](https://arxiv.org/abs/2512.03004)
- [PDF Link](https://arxiv.org/pdf/2512.03004.pdf)

## Subfields
 自动驾驶仿真 / 4D场景重建 (AD Simulation / 4D Scene Reconstruction)
## Reason for Interest

该论文在自动驾驶场景重建与仿真领域具有极高的研究价值和工程潜力。
1. **核心突破**：提出了首个针对动态驾驶场景的“无位姿(Pose-free)、前馈(Feedforward)”4D重建框架。相比传统NeRF/3DGS需要耗时的逐场景优化和预先计算相机位姿(SfM)，该方法能以0.4秒的速度直接从视频流生成可编辑的4D场景，极大提升了从行车日志构建仿真场景的效率。
2. **架构创新**：设计了统一的Transformer架构，同时输出相机参数、动/静态3D高斯分布、以及物体3D运动轨迹。引入Lifespan参数处理动态物体消隐，并结合扩散模型进行渲染增强，有效解决了稀疏视角下的伪影问题。
3. **实验充分**：在Waymo、nuScenes、Argoverse2三大主流数据集上验证了方法的有效性，展现了优异的跨数据集Zero-shot泛化能力。实验不仅涵盖视觉指标，还评估了3D跟踪精度(EPE3D)和场景编辑能力（如移除/增加车辆）。
4. **行业应用**：直接服务于自动驾驶的数据闭环与仿真测试（World Simulators），代码已开源，对行业技术进步有直接推动作用。
## Abstract: 
Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
