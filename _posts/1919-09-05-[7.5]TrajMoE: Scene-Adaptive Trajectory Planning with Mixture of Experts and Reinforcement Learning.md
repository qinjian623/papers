---
layout: default
title: "[7.5]TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning"
---

# [7.5] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning

- Authors: Zebin Xing, Pengxuan Yang, Linbo Wang, Yichen Zhang, Yiming Hu, Yupeng Zheng, Junli Wang,...
- [arXiv Link](https://arxiv.org/abs/2512.07135)
- [PDF Link](https://arxiv.org/pdf/2512.07135.pdf)

## Subfields
 端到端自动驾驶 / 轨迹规划
## Reason for Interest

该论文提出了一种结合稀疏混合专家（MoE）和强化学习（GRPO）的端到端轨迹规划方法。创新点在于：1. 利用MoE根据不同驾驶场景（如直行、转弯）动态选择轨迹先验专家，解决了单一先验的局限性；2. 创新性地将大模型领域的GRPO算法应用于轨迹评分机制的微调，以提升评分准确性和鲁棒性。实验方面，在NAVSIM权威基准上取得了前三的成绩，证明了方案的有效性。扣分项主要在于：文章篇幅较短（4页技术报告风格），对方法背后的理论深度挖掘有限，且并未取得绝对的SOTA（排名第三）。
## Abstract: 
Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
