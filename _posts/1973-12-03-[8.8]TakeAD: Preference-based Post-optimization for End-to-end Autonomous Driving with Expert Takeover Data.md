---
layout: default
title: "[8.8]TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data"
---

# [8.8] TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data

- Authors: Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng,...
- [arXiv Link](https://arxiv.org/abs/2512.17370)
- [PDF Link](https://arxiv.org/pdf/2512.17370.pdf)

## Subfields
 端到端自动驾驶 (E2E AD) / 模仿学习与强化学习微调
## Reason for Interest

1. 创新性：论文切中端到端自动驾驶中‘开环训练与闭环部署分布差异’的核心痛点，创造性地将专家接管数据（Takeover Data）转化为偏好对（Preference Pairs），并结合 DAgger（数据聚合）与 DPO（直接偏好优化）进行后优化。这种将‘接管即偏好’引入驾驶策略微调的思路具有较强的启发性。
2. 实验完整性：在权威且具有挑战性的 Bench2Drive 闭环榜单上进行了验证，不仅对比了多个主流 Baseline（UniAD, VAD, DriveAdapter 等），还进行了多轮迭代优化分析和详细的消融实验（如控制分支、混合输出策略的有效性）。
3. 结果可信度：提升幅度显著（DS +12.5%），且对‘惯性问题（inertia issue）’和安全蠕行策略有诚实的讨论和工程处理。
4. 行业潜力：虽然实验基于仿真（利用 PDM-Lite 专家），但该框架逻辑完全兼容实车开发流程（利用人类接管数据），为利用海量非完美驾驶数据和接管数据提供了高效的技术路径。
## Abstract: 
Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.
