---
layout: default
title: "[8.2]From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model"
---

# [8.2] From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model

- Authors: Kevin Cannons, Saeed Ranjbar Alvar, Mohammad Asiful Hossain, Ahmad Rezaei, Mohsen Gholami...
- [arXiv Link](https://arxiv.org/abs/2512.05277)
- [PDF Link](https://arxiv.org/pdf/2512.05277.pdf)

## Subfields
 自动驾驶视频理解 / VLM评测基准
## Reason for Interest

1. 创新性：针对现有自动驾驶VQA基准缺乏细粒度时序推理（如动作顺序、持续时间）的问题，提出了TAD基准，具有较高的行业研究价值。提出的TCogMap方法创新地将自车轨迹（Ego-Pose）转化为文本化的“认知地图”输入VLM，有效解决了通用VLM难以从视频像素精确推断自车运动（如速度、转向角度）的痛点，这是一种高效的免训练多模态融合思路。
2. 实验完整性：实验涵盖了9种通用及专用VLM模型（包括未来的GPT-5-mini, Qwen2.5-VL等），对比了Closed-source和Open-source模型，消融实验详实（如Blind Test, Image Only等），证明了视觉与轨迹文本结合的必要性。
3. 可信度与局限：论文坦诚指出了Scene-CoT方法推理耗时过长（47s/题），不适合车端实时应用，主要适用于离线数据挖掘或高层推理；而TCogMap推理延迟低（2.2s），具备一定的车端部署潜力。基于nuScenes构建数据集保证了数据的真实性。
4. 综合评价：该工作为大模型在自动驾驶长时序理解方面的应用提供了可靠的评测工具和有效的优化思路，得分为8.2。
## Abstract: 
Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
