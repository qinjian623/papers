---
layout: default
title: "[9.2]Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving"
---

# [9.2] Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving

- Authors: Haohong Lin, Yunzhi Zhang, Wenhao Ding, Jiajun Wu, Ding Zhao
- [arXiv Link](https://arxiv.org/abs/2511.21584)
- [PDF Link](https://arxiv.org/pdf/2511.21584.pdf)

## Subfields
 端到端自动驾驶 / 闭环策略优化 (End-to-End AD / Closed-Loop Policy Adaptation)
## Reason for Interest

论文切中端到端自动驾驶当前最核心的痛点——开环训练与闭环部署的性能鸿沟。创新性地提出了MPA框架，利用基于3D Gaussian Splatting (3DGS) 的高保真仿真器生成反事实数据，训练扩散残差策略适配器（Policy Adapter）和多步Q值模型，实现了推理时的价值引导。该方法无需重新训练庞大的基座模型即可大幅提升闭环安全性，在HUGSIM基准上的实验结果极具说服力，为利用生成式世界模型进行策略优化提供了范式。
## Abstract: 
End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.
