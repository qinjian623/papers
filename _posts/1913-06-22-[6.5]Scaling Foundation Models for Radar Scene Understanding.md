---
layout: default
title: "[6.5]Scaling Foundation Models for Radar Scene Understanding"
---

# [6.5] Scaling Foundation Models for Radar Scene Understanding

- Authors: Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia
- [arXiv Link](https://arxiv.org/abs/2511.21105)
- [PDF Link](https://arxiv.org/pdf/2511.21105.pdf)

## Subfields
 雷达感知 / 多模态基础模型
## Reason for Interest

1. 创新性较强：论文提出了一种新颖的范式，即利用结构化的自然语言描述来监督雷达场景表征的学习（RadarFM），并设计了Hash-aware对比损失函数来处理连续的空间相似性，打破了传统雷达任务（检测/分割）的碎片化限制。
2. 实验局限性明显：研究完全依赖CARLA模拟器生成的数据。尽管使用了较好的雷达模拟器，但雷达感知领域的Sim-to-Real gap极大（多径、杂波、噪声在真实世界中远比模拟复杂），缺乏真实数据验证极大地降低了结论的可信度。
3. 验证不足：论文主要通过生成文本描述的准确性（自定义的定位感知指标）来评估模型，而没有在标准的下游任务（如3D目标检测）上进行Fine-tuning并与主流方法（如CenterPoint, RadarNet等）对比mAP等指标，难以证明该预训练特征在实际自动驾驶任务中的优越性。
4. 贡献点：开源了大规模的模拟雷达-文本对数据集，对学术界探索多模态雷达感知有一定参考价值，但工业应用潜力目前受限于纯模拟环境。
## Abstract: 
Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
