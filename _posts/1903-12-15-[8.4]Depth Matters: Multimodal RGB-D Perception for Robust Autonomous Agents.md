---
layout: default
title: "[8.4]Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents"
---

# [8.4] Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents

- Authors: Mihaela-Larisa Clement, M\'onika Farsang, Felix Resch, Mihai-Teodor Stanusoiu, Radu Grosu...
- [arXiv Link](https://arxiv.org/abs/2503.16711)
- [PDF Link](https://arxiv.org/pdf/2503.16711.pdf)

## Subfields
 端到端自动驾驶 / 多模态感知融合 / 鲁棒性与实时部署
## Reason for Interest

该论文在自动驾驶领域具有很高的实践价值和研究深度。其核心贡献在于对多模态RGB-D感知融合技术在资源受限小型自动驾驶平台（Roboracer）上的鲁棒性、实时性和泛化能力进行了全面而严谨的实验与分析。

**创新性 (7.5/10):** 虽然RGB-D融合和循环神经网络并非全新概念，但论文系统地比较了多种融合策略（Early, Late, DCN, ZACN）与轻量级循环控制器（LSTM, LTC, CfC, LRC）的组合，并在真实物理平台上进行了端到端驾驶控制的部署和严格测试，特别关注了噪声、帧丢失和Out-of-Distribution (OOD)场景下的鲁棒性。这种针对真实世界挑战的深度实证研究，以及结合注意力图的解释性分析，具有较强的创新性，尤其对于小型化和资源受限自动驾驶系统的设计具有指导意义。

**实验完整性 (9.0/10):** 实验设计非常全面和严谨。论文构建了高质量的自定义RGB-D数据集，并对比了20种不同的模型架构。评估不仅包含开环的预测精度，更重要的是在物理Roboracer平台上的闭环测试，涵盖了多种复杂和OOD场景（反射表面、动态障碍物、交叉路口）。此外，还对模型在噪声干扰和帧丢失情况下的鲁棒性进行了量化分析（如SSIM），并与纯RGB模型、基于LiDAR的模型(TinyLidarNet)以及人类专家驾驶员进行了对比。 Attention map的可视化也增强了结果的可解释性。

**可信度 (9.0/10):** 论文详细描述了硬件设置、数据采集、标注和训练流程，包括数据预处理、模型架构、超参数和优化器设置等，具有很高的透明度。结果通过多次随机种子进行平均，并辅以丰富的定性（如注意力图、车辆轨迹）和定量（MSE、SSIM、成功率）分析，增强了结论的可靠性。对限制（如人类驾驶风格未能完全复制）的坦诚讨论也提升了可信度。

**行业潜力 (8.5/10):** 尽管研究平台是小型Roboracer，但其解决的“Sim-to-Real Gap”、对实时性、鲁棒性和OOD泛化能力的关注，是自动驾驶领域的核心挑战。论文发现早期融合RGB-D结合LSTM在噪声和OOD条件下表现出卓越的鲁棒性，以及RGB-D在某些场景（如反射面）下优于LiDAR的洞察，对实际车端自动驾驶系统的传感器配置、融合策略和控制器设计具有重要的借鉴意义。轻量级控制器的研究也符合未来自动驾驶系统对计算效率的要求。
## Abstract: 
Autonomous agents that rely purely on perception to make real-time control decisions require efficient and robust architectures. In this work, we demonstrate that augmenting RGB input with depth information significantly enhances our agents' ability to predict steering commands compared to using RGB alone. We benchmark lightweight recurrent controllers that leverage the fused RGB-D features for sequential decision-making. To train our models, we collect high-quality data using a small-scale autonomous car controlled by an expert driver via a physical steering wheel, capturing varying levels of steering difficulty. Our models were successfully deployed on real hardware and inherently avoided dynamic and static obstacles, under out-of-distribution conditions. Specifically, our findings reveal that the early fusion of depth data results in a highly robust controller, which remains effective even with frame drops and increased noise levels, without compromising the network's focus on the task.
