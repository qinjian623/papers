---
layout: default
title: "[9.0]LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving"
---

# [9.0] LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving

- Authors: Lingdong Kong, Xiang Xu, Youquan Liu, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai ...
- [arXiv Link](https://arxiv.org/abs/2501.04005)
- [PDF Link](https://arxiv.org/pdf/2501.04005.pdf)

## Subfields
 3D感知 / 跨模态预训练
## Reason for Interest

该论文提出了一种基于视觉基础模型（VFM）的大规模点云预训练框架LargeAD，核心创新在于利用SAM、SEEM等大模型生成的高质量语义超像素替代传统的SLIC算法进行2D-3D蒸馏，解决了传统对比学习中的语义冲突问题。此外，论文还设计了多源数据预训练策略和时序一致性模块，显著提升了模型在不同LiDAR配置下的泛化能力。实验规模庞大，覆盖11个数据集和多种感知任务（分割、检测、全景分割），特别是在少样本（Few-shot）和跨域迁移场景下表现优异，对解决自动驾驶3D数据标注成本高的问题具有极高的实用价值和学术意义。
## Abstract: 
Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.
