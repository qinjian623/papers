---
layout: default
title: "[8.8]dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning"
---

# [8.8] dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning

- Authors: Yingzi Ma, Yulong Cao, Wenhao Ding, Shuibai Zhang, Yan Wang, Boris Ivanovic, Ming Jiang, ...
- [arXiv Link](https://arxiv.org/abs/2512.04459v1)
- [PDF Link](https://arxiv.org/pdf/2512.04459v1.pdf)

## Subfields
 端到端自动驾驶 / 视觉语言模型 (VLM-based E2E Driving)
## Reason for Interest

1. **创新性强**：论文敏锐地指出了主流自回归 VLM 在自动驾驶中的固有缺陷（单向注意力导致无法利用后续信息修正前期推理，导致推理与轨迹不匹配），并开创性地将离散扩散模型（Discrete Diffusion）引入该领域。通过双向注意力和迭代去噪机制，实现了全局上下文感知。 
2. **方法论扎实**：提出的‘动态去噪策略’（Dynamic Denoise Strategy）和‘结构化填空模版’（Structured CoT Template），有效解决了生成任务中的长度偏差问题，并在不损失灵活性的前提下强制模型遵守安全约束，这对于实车部署至关重要。
3. **实验充分且可信**：在 Waymo 和 nuScenes 两大主流数据集上进行了详尽的对比。虽然在纯轨迹误差（ADE）上与最强基线互有胜负，但在更能反映类人驾驶行为的 RFS 指标上取得了 SOTA，且在抗干扰实验（Prompt Perturbation）中展现了远超 AR 模型的鲁棒性。
4. **行业价值**：为大模型在自动驾驶中的‘可控性’和‘安全性’提供了一条非常有前景的新技术路线（从 AR 转向 Diffusion），具有较高的启发意义。
## Abstract: 

