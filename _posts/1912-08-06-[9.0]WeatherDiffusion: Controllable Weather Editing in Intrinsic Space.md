---
layout: default
title: "[9.0]WeatherDiffusion: Controllable Weather Editing in Intrinsic Space"
---

# [9.0] WeatherDiffusion: Controllable Weather Editing in Intrinsic Space

- Authors: Yixin Zhu, Zuoliang Zhu, Jian Yang, Milo\v{s} Ha\v{s}an, Jin Xie, Beibei Wang
- [arXiv Link](https://arxiv.org/abs/2508.06982)
- [PDF Link](https://arxiv.org/pdf/2508.06982.pdf)

## Subfields
 AIGC数据生成 / 鲁棒感知
## Reason for Interest

该论文针对自动驾驶场景中的恶劣天气长尾问题，提出了一种基于本征空间（Intrinsic Space）的Diffusion编辑框架。与传统的像素级风格迁移不同，该方法通过解耦材质、几何（法向）和光照，实现了符合物理规律的天气编辑，极大减少了生成数据中的几何畸变，这对自动驾驶感知数据至关重要。论文不仅构建了含有本征真值的合成数据集WeatherSynthetic，还在真实数据集（ACDC）上证明了该方法作为前处理步骤能显著提升检测和分割性能（AP提升约87%）。尽管基于Diffusion的推理速度限制了其车端实时应用，但作为离线数据增强和感知模型鲁棒性训练工具，具有极高的行业价值。
## Abstract: 
We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches.We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
