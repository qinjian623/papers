---
layout: default
title: "[8.8]A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection"
---

# [8.8] A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection

- Authors: Qifeng Liu, Dawei Zhao, Yabo Dong, Linzhi Shang, Liang Xiao, Juan Wang, Kunkong Zhao, Don...
- [arXiv Link](https://arxiv.org/abs/2508.16069)
- [PDF Link](https://arxiv.org/pdf/2508.16069.pdf)

## Subfields
 LiDAR感知 / 3D目标检测
## Reason for Interest

论文针对基于序列化（Transformer/SSM）的3D检测模型缺乏空间特征扩散的问题，提出了一种通用的体素扩散模块（VDM）。该模块通过稀疏3D卷积在序列化前增加前景体素密度并聚合细粒度特征，显著提升了模型对小目标的检测能力。创新性在于巧妙结合了卷积的局部扩散优势与序列模型的长程建模能力。实验工作非常扎实，在四个主流数据集上全面验证并取得SOTA，具有较高的落地潜力和通用性。
## Abstract: 
Recent advances in point cloud object detection have increasingly adopted Transformer-based and State Space Models (SSMs), demonstrating strong performance. However, voxelbased representations in these models require strict consistency in input and output dimensions due to their serialized processing, which limits the spatial diffusion capability typically offered by convolutional operations. This limitation significantly affects detection accuracy. Inspired by CNN-based object detection architectures, we propose a novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and diffusion in point cloud data. VDM is composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. To ensure computational efficiency, the output feature maps are downsampled to one-fourth of the original input resolution. VDM serves two primary functions: (1) diffusing foreground voxel features through sparse 3D convolutions to enrich spatial context, and (2) aggregating fine-grained spatial information to strengthen voxelwise feature representation. The enhanced voxel features produced by VDM can be seamlessly integrated into mainstream Transformer- or SSM-based detection models for accurate object classification and localization, highlighting the generalizability of our method. We evaluate VDM on several benchmark datasets by embedding it into both Transformerbased and SSM-based models. Experimental results show that our approach consistently improves detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new stateof-the-art performance across all datasets. Our code will be made publicly available.
