---
layout: default
title: "[8.8]OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving"
---

# [8.8] OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving

- Authors: Pei Liu, Hongliang Lu, Haichao Liu, Haipeng Liu, Xin Liu, Ruoyu Yao, Shengbo Eben Li, Jun...
- [arXiv Link](https://arxiv.org/abs/2509.19973v2)
- [PDF Link](https://arxiv.org/pdf/2509.19973v2.pdf)

## Subfields
 端到端自动驾驶 (End-to-End AD) / 视觉语言模型 (VLM) 应用
## Reason for Interest

该论文提出了一种利用视觉语言模型（VLM）增强端到端自动驾驶系统的方法 OmniScene。其核心创新在于通过“教师-学生”蒸馏架构，将 VLM 的高层语义理解和类人注意力机制注入到稀疏 3D 几何特征中，有效解决了传统端到端模型缺乏语义推理能力的问题。实验设计非常完整，覆盖了感知、预测、规划及 VQA 多个任务。虽然在 3D 检测指标（mAP/NDS）上提升幅度相对较小，但在下游的预测和规划任务（特别是降低 L2 误差和碰撞率）以及解释性（VQA）上表现优异。论文提供了详尽的消融实验和速度分析，且代码开源，具有较高的行业参考价值。
## Abstract: 

