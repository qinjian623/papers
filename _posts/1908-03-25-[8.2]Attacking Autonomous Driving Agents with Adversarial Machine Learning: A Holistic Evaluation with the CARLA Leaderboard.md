---
layout: default
title: "[8.2]Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard"
---

# [8.2] Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard

- Authors: Henry Wong, Clement Fung, Weiran Lin, Karen Li, Stanley Chen, Lujo Bauer
- [arXiv Link](https://arxiv.org/abs/2511.14876)
- [PDF Link](https://arxiv.org/pdf/2511.14876.pdf)

## Subfields
 自动驾驶安全性 / 对抗性攻击与鲁棒性评估
## Reason for Interest

该论文具有较高的行业价值，主要体现在它纠正了自动驾驶对抗攻击研究中的一个常见误区：即认为欺骗了感知模型就等于欺骗了车辆。作者通过在CARLA中对完整驾驶管线（端到端模型+规则+控制器）进行攻击测试，发现下游的PID控制器和GPS规则往往能纠正感知层面的对抗扰动。这种'系统级'的安全性评估比单纯的'模型级'评估更具现实意义。实验设计严谨，利用CARLA Leaderboard API进行标准化测试，结论对提升自动驾驶系统的鲁棒性设计有直接指导作用。
## Abstract: 
To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.
