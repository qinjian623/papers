---
layout: default
title: "[9.2]World Simulation with Video Foundation Models for Physical AI"
---

# [9.2] World Simulation with Video Foundation Models for Physical AI

- Authors: NVIDIA,  :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany C...
- [arXiv Link](https://arxiv.org/abs/2511.00062)
- [PDF Link](https://arxiv.org/pdf/2511.00062.pdf)

## Subfields
 自动驾驶仿真 / 合成数据生成 / 世界模型
## Reason for Interest

该论文提出了 NVIDIA Cosmos 世界基础模型的最新一代，[Cosmos-Predict2.5] 和 [Cosmos-Transfer2.5]，专注于物理 AI 领域的视频生成和世界仿真。其核心创新性在于将文本、图像、视频到世界的生成统一到单个流匹配模型中，并专门针对物理世界（包括自动驾驶、机器人等）的物理合理性和可控性进行优化。

**自动驾驶相关性及创新性**：论文将自动驾驶作为核心应用之一，并详细介绍了为自动驾驶仿真定制的多视角视频生成方法。通过引入基于“世界场景图”（包含高清地图元素和动态 3D 边界框）的控制信号，实现了高保真、多视角、条件控制的自动驾驶场景视频生成。这在自动驾驶合成数据生成和闭环仿真方面具有极高的创新性和实用价值。

**实验完整性**：论文在超大规模（2亿视频片段）数据集上进行预训练，并针对特定领域（包括自动驾驶）进行监督微调和强化学习后训练。在评估方面，除了通用的 PAI-Bench 基准测试，还专门针对自动驾驶场景的视觉质量和下游感知任务（例如 3D 障碍物和车道线检测）进行了量化评估，并与前代模型及真实视频进行对比。消融研究也展示了关键设计选择的有效性。开放源代码、预训练模型和基准测试也极大地提升了实验的可信度和可复现性。

**可信度**：作为 NVIDIA 的工作，该研究投入了大量资源，且模型和基准的开放性进一步增强了其可信度。实验结果显示，新模型在多个指标上显著优于前代，并且在自动驾驶仿真任务中表现出接近真实视频的感知性能，有效解决了幻觉和错误累积问题，这对于自动驾驶至关重要。

**行业潜力**：该工作在自动驾驶领域具有巨大的行业潜力。高保真、可控的多视角合成数据生成能力，能够大幅降低自动驾驶系统开发对真实世界数据采集的依赖，提高训练效率和安全性。其在 Sim2Real 和 Real2Real 转换方面的能力，对于弥合仿真与现实之间的鸿沟至关重要，为自动驾驶的规划、控制和验证提供了强大的工具。
## Abstract: 
We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.
