---
layout: default
title: "[8.2]Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection"
---

# [8.2] Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection

- Authors: Xiangzhong Liu, Jiajie Zhang, Hao Shen
- [arXiv Link](https://arxiv.org/abs/2512.12884v1)
- [PDF Link](https://arxiv.org/pdf/2512.12884v1.pdf)

## Subfields
 3D Perception / Multi-Sensor Fusion (Camera + Object Lists)
## Reason for Interest

The paper introduces a novel 'Cross-Level Fusion' framework that integrates high-level Object Lists (simulating V2X or smart sensor outputs) with raw camera images using Transformer query denoising. 

Strengths:
1. **Innovation**: Cleverly adapts DN-DETR to treat object lists as noisy queries and uses Deformable Gaussian Masks to guide attention, addressing the practical industry problem where raw data from all sensors is unavailable (e.g., due to bandwidth or IP).
2. **Performance**: Demonstrates massive improvement over the vision-only baseline (PETRv2), boosting NDS from 0.40 to 0.56.
3. **Validation**: Thorough ablation studies and validation with both simulated noise and real detector outputs (CenterPoint, PointPillars).

Weaknesses:
1. **Fusion Upper Bound**: The method functions as a 'vision enhancement'; experiments show that if the input object list is of very high quality (e.g., CenterPoint with NDS 0.65), the fused output (NDS 0.53) may actually degrade the prior, indicating the system is bottlenecked by image capabilities.
2. **Dependency**: Relies on the availability of object lists during inference, though this is the intended use case.
## Abstract: 

