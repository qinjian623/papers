---
layout: default
title: "[8.8]Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection"
---

# [8.8] Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection

- Authors: Shenao Zhao, Pengpeng Liang, Zhoufan Yang
- [arXiv Link](https://arxiv.org/abs/2511.07966v1)
- [PDF Link](https://arxiv.org/pdf/2511.07966v1.pdf)

## Subfields
 3D Object Detection / Unsupervised Domain Adaptation (UDA)
## Reason for Interest

The paper introduces a novel 'MMAssist' framework that leverages Multi-Modal (Image & Text) features as a bridge to align LiDAR features across domains, utilizing LVLMs (LLaVA) and GroundingDINO. This integration of Vision-Language models into the 3D UDA pipeline is innovative and timely. 

Strengths:
1. **Innovation**: Effectively uses text descriptions generated by LVLMs to bridge the domain gap, based on the insight that semantic descriptions are more domain-invariant than raw sensor data.
2. **Performance**: Achieves SOTA on the majority of tested adaptation scenarios (e.g., Waymo→nuScenes, nuScenes→KITTI) and significantly closes the domain gap (up to 104.8% closed gap reported in specific configs).
3. **Rigor**: Validated across three popular detectors (PV-RCNN, SECOND-IoU, PointPillars) and three dataset pairs. Detailed ablation studies confirm the contribution of each module (Image Alignment, Text Alignment, Pseudo Labeling).
4. **Practicality**: The multi-modal heavy lifting is done during training; inference remains LiDAR-only, ensuring no runtime latency penalty.

Weaknesses:
1. In the Waymo→KITTI task, it slightly lags behind competitors (CMT, GroupEXP-DA) in AP_3D for some detectors, though it leads in AP_BEV. The paper honestly discusses these trade-offs.
## Abstract: 

