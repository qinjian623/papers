---
layout: default
title: "[9.2]Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving"
---

# [9.2] Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving

- Authors: Dapeng Zhang, Zhenlong Yuan, Zhangquan Chen, Chih-Ting Liao, Yinda Chen, Fei Shen, Qinggu...
- [arXiv Link](https://arxiv.org/abs/2511.19912)
- [PDF Link](https://arxiv.org/pdf/2511.19912.pdf)

## Subfields
 端到端自动驾驶 / Vision-Language-Action (VLA) 模型
## Reason for Interest

1. 创新性高：提出了并行可学习动作查询（Learnable Action Queries）机制，解决了传统VLA模型自回归生成动作速度慢的问题（推理速度提升约60倍），使其具备上车实时性的潜力。引入GRPO强化学习算法优化轨迹生成，紧跟大模型训练技术前沿（类DeepSeek-R1思路）。
2. 实验扎实：构建了包含8个主流数据集的统一推理数据集（Unified CoT Dataset），验证了模型在跨数据域（如从nuScenes到Waymo/NAVSIM）的强大泛化能力。
3. 结果惊人：在nuScenes上的开环指标大幅超越现有SOTA（UniAD, VAD, SparseDrive），虽然提升幅度之大（如L2误差减少数倍）需要社区复现验证，但代码已开源，且提供了闭环测试（NeuroNCAP）佐证，可信度较高。
4. 行业价值：解决了VLA模型在自动驾驶中推理延迟高和泛化差的两大痛点，指明了通用端到端大模型落地的可行路径。
## Abstract: 
Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.
