---
layout: default
title: "[8.1]Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving"
---

# [8.1] Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving

- Authors: Jiawei Yang, Ziyu Chen, Yurong You, Yan Wang, Yiming Li, Yuxiao Chen, Boyi Li, Boris Ivan...
- [arXiv Link](https://arxiv.org/abs/2512.10947)
- [PDF Link](https://arxiv.org/pdf/2512.10947.pdf)

## Subfields
 端到端自动驾驶 (End-to-End AD) / 视觉表征学习
## Reason for Interest

1. 创新性（8.5/10）：论文挑战了当前端到端自动驾驶中普遍依赖BEV、Occupancy等显式3D几何先验的主流范式，提出了一种几何无关（Geometry-Agnostic）的纯Transformer编码方案（Flex），通过隐式学习Scene Tokens实现了高效的时空压缩，且观察到了涌现出的场景分解能力（如自动关注车道线、目的地），极具启发性。
2. 实验完整性（7.5/10）：在极大规模（2万小时）数据上进行了充分的训练和消融实验，证明了方法的有效性和效率。引入的Interleaved Prediction训练策略对提升VLA模型的数据效率有显著帮助。
3. 可信度与局限（扣分项）：核心评估完全依赖私有数据集，未在nuScenes或Waymo Open Dataset等公开基准上提供对比结果，导致其“SOTA”声明缺乏全行业的横向可比性，结果的可复现性受限。尽管如此，作为工业界（NVIDIA）的大规模验证，其工程价值和方向指引意义重大。
## Abstract: 
We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.
