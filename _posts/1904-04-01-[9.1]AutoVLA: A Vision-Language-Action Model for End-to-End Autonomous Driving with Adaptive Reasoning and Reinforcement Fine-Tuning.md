---
layout: default
title: "[9.1]AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning"
---

# [9.1] AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning

- Authors: Zewei Zhou, Tianhui Cai, Seth Z. Zhao, Yun Zhang, Zhiyu Huang, Bolei Zhou, Jiaqi Ma
- [arXiv Link](https://arxiv.org/abs/2506.13757)
- [PDF Link](https://arxiv.org/pdf/2506.13757.pdf)

## Subfields
 端到端自动驾驶 / 视觉-语言-动作模型 (VLA)
## Reason for Interest

该论文提出了AutoVLA，一个新颖的端到端视觉-语言-动作 (VLA) 模型，用于自动驾驶，旨在克服现有VLA模型在动作物理可行性、模型复杂度和推理效率方面的限制。

**创新性：**
1.  **统一推理与动作生成：** 将语义推理和轨迹规划整合到一个单一的自回归生成模型中，直接从原始视觉输入和语言指令生成可行的物理动作，而非依赖中间表征或下游规划器。
2.  **物理动作Token化：** 将连续轨迹离散化为物理动作Token并直接集成到语言模型中，有效弥合了VLM的高级推理与低级物理控制之间的鸿沟。
3.  **自适应双重思维模式：** 通过监督微调 (SFT) 训练模型具备快速思维（仅轨迹生成）和慢速思维（带有思维链CoT推理）两种模式，根据场景复杂性自适应切换，提高了效率和鲁棒性。
4.  **强化学习微调 (RFT) 与GRPO：** 引入基于Group Relative Policy Optimization (GRPO) 的强化学习微调方法，进一步提升规划性能和效率，并通过惩罚不必要的推理来减少冗余。
5.  **自动化推理数据标注：** 提出了一种使用大型VLM (Qwen2.5-VL-72B) 自动生成高质量CoT推理数据的流水线，克服了现有推理数据集的局限性，并经过人工质量检查证实了其可靠性。

**实验完整性：**
实验非常全面，在多个真实世界数据集（nuPlan、Waymo、nuScenes）和模拟器（CARLA）上进行了广泛评估，涵盖了开环和闭环两种设置。消融研究充分验证了关键设计（如物理动作Token化、RFT和双重思维模式）的有效性。定量结果和定性分析都清晰地展示了模型的性能和自适应推理能力。

**可信度：**
方法描述清晰，实验设置详细，并与多个主流基线进行了比较。模型在复杂场景下的自适应推理和准确规划能力得到了有效验证。虽然并非在所有指标上都达到绝对SOTA，但在特定挑战性指标（如Waymo RFS Spotlight）和闭环仿真（CARLA的驾驶分数和成功率）中取得了领先，证明了其方法的有效性和竞争力。对模型当前局限性（如对GPU和内存的高依赖）的讨论也增加了论文的坦诚性。

**行业潜力：**
该研究方向对自动驾驶领域具有巨大的潜力，通过将感知、推理和规划控制统一到单一模型中，能够提升系统在复杂和长尾场景下的决策能力和泛化性。自适应推理能力对于车辆实时部署的效率至关重要，而端到端的可解释性也为安全性和信任度提供了新的途径。未来在模型量化和运行时优化方面的探索将进一步推动其实际应用。
## Abstract: 
Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities. However, current VLA models often struggle with physically infeasible action outputs, complex model structures, or unnecessarily long reasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies reasoning and action generation within a single autoregressive generation model for end-to-end autonomous driving. AutoVLA performs semantic reasoning and trajectory planning directly from raw visual inputs and language instructions. We tokenize continuous trajectories into discrete, feasible actions, enabling direct integration into the language model. For training, we employ supervised fine-tuning to equip the model with dual thinking modes: fast thinking (trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning). To further enhance planning performance and efficiency, we introduce a reinforcement fine-tuning method based on Group Relative Policy Optimization (GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive experiments across real-world and simulated datasets and benchmarks, including nuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of AutoVLA in both open-loop and closed-loop settings. Qualitative results showcase the adaptive reasoning and accurate planning capabilities of AutoVLA in diverse scenarios.
