---
layout: default
title: "[7.2]VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes"
---

# [7.2] VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes

- Authors: Zhengyu Zou, Jingfeng Li, Hao Li, Xiaolei Hou, Jinwen Hu, Jingkun Chen, Lechao Cheng, Din...
- [arXiv Link](https://arxiv.org/abs/2511.06408v1)
- [PDF Link](https://arxiv.org/pdf/2511.06408v1.pdf)

## Subfields
 自动驾驶仿真 / 三维场景重建 / 视觉定位
## Reason for Interest

论文提出了VDNeRF框架，针对自动驾驶动态场景在无位姿（Pose-free）条件下的重建难题做出了有效贡献。1. 创新性：采用静态/动态双NeRF解耦设计，结合光流/深度监督和渐进式优化策略，有效解决了动态物体干扰下的相机位姿估计问题，这对于利用大规模低成本行车视频进行仿真场景构建具有重要意义。2. 实验结果：在NOTR和Pandaset数据集上展现了优越的性能，大幅领先现有的无位姿NeRF方法，甚至在动态物体细节上优于依赖位姿的EmerNeRF，结果可信度高。3. 局限性与扣分项：作为一篇标记为2025年的论文，仍完全基于隐式NeRF进行构建，单场景训练耗时12小时（RTX3090），相比于3D Gaussian Splatting等新兴实时渲染技术，在训练效率和推理速度上缺乏竞争力，限制了其在大规模工业化仿真中的直接落地潜力；且虽然服务于自动驾驶仿真，但本质属于离线工具链，非车端实时算法。
## Abstract: 

