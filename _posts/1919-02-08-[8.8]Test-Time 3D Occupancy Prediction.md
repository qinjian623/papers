---
layout: default
title: "[8.8]Test-Time 3D Occupancy Prediction"
---

# [8.8] Test-Time 3D Occupancy Prediction

- Authors: Fengyi Zhang, Xiangyu Sun, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo
- [arXiv Link](https://arxiv.org/abs/2503.08485)
- [PDF Link](https://arxiv.org/pdf/2503.08485.pdf)

## Subfields
 3D感知 / 占据栅格预测 (Occupancy Prediction)
## Reason for Interest

1. 创新性：提出了一种无需训练（Training-free）的测试时（Test-time）3D 占据预测框架，巧妙结合了视觉基础模型（VFM）的通用语义/深度先验与 3D 高斯溅射（3DGS）的几何一致性，打破了传统方法对大量 3D 标注数据的依赖。
2. 实用价值：支持开放词汇（Open-vocabulary）感知和任意分辨率体素化，能够识别训练集中未见过的长尾物体，这对处理自动驾驶中的 Corner Case 极具价值。
3. 实验与性能：实验设计完善，在两个主流基准上均取得了显著优于同类自监督方法的效果，证明了“Lift-Track-Voxelize”流程的有效性。
4. 行业潜力：虽然目前的推理速度（视觉版 ~0.7 FPS，LiDAR 版 ~1.9 FPS）尚不足以支撑车端实时闭环控制，但该方法作为自动驾驶数据闭环中的“自动化标注工具（Auto-labeling）”或“离线场景理解引擎”具有极高的应用前景，能够大幅降低数据生产成本。
## Abstract: 
Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VFMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VFMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VFMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VFM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions.
