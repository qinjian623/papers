---
layout: default
title: "[9.6]UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction"
---

# [9.6] UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction

- Authors: Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang
- [arXiv Link](https://arxiv.org/abs/2511.04595)
- [PDF Link](https://arxiv.org/pdf/2511.04595.pdf)

## Subfields
 3D重建 / 动态场景感知 / 新视角合成
## Reason for Interest

该论文提出了UniSplat，一个新颖的端到端前向框架，用于从多相机视频中进行动态驾驶场景重建和新视角合成。其核心创新在于构建统一的3D潜在支架，通过集成预训练的基础模型（几何和视觉）来捕捉几何和语义上下文，并在此支架内实现高效的时空融合。该方法采用双分支高斯解码器，结合点锚定细化和体素生成，以确保重建的精细度和完整性，并维护静态高斯的持久记忆，从而实现超出当前相机覆盖范围的场景补全和动态物体处理。

**创新性：** 论文提出的统一3D潜在支架及其内部时空融合机制，以及动态感知的双分支高斯生成和记忆机制，都具有很强的创新性，有效解决了稀疏视图、非重叠相机和复杂动态场景下的重建挑战。

**实验完整性：** 实验在Waymo Open Dataset和nuScenes这两个大规模自动驾驶基准数据集上进行了全面的评估，涵盖了输入视角重建和新视角合成两个任务。与多个SOTA前向重建方法进行了定量和定性比较，并提供了详细的消融研究，充分验证了各模块的有效性。

**可信度：** 方法设计合理，实验结果清晰且一致地展示了其优越性。尽管在nuScenes上的LPIPS指标略逊于一个基线，但整体PSNR和SSIM的显著提升足以支持其SOTA声明，并且对自动驾驶的实际需求（如场景理解和规划）更具意义。

**行业潜力：** UniSplat作为一种前向（实时）、可泛化的动态场景重建方法，直接面向自动驾驶的核心需求。其能够鲁棒地处理动态场景、完成超出相机覆盖范围的场景，并支持高质量的新视角合成，这对于自动驾驶的仿真、数据生成、规划和感知都具有巨大的应用潜力，是车端自动驾驶系统不可或缺的基础技术。
## Abstract: 
Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.
