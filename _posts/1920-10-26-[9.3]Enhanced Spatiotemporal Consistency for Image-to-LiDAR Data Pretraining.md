---
layout: default
title: "[9.3]Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining"
---

# [9.3] Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining

- Authors: Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Qingsha...
- [arXiv Link](https://arxiv.org/abs/2503.19912)
- [PDF Link](https://arxiv.org/pdf/2503.19912.pdf)

## Subfields
 3D感知 / 多模态预训练 (Image-to-LiDAR)
## Reason for Interest

1. 创新性：针对现有 Image-to-LiDAR 预训练忽略时间维度的问题，提出了基于光流的对比学习（Flow-Based Contrastive Learning）和稠密到稀疏（Dense-to-Sparse）的正则化机制，有效利用了自动驾驶数据的时序特性。
2. 实验完整性：在 11 个异构 LiDAR 数据集上进行了极为详尽的测试，涵盖了语义分割、目标检测、域泛化（Domain Generalization）和鲁棒性评估，实验设计非常扎实。
3. 行业价值：该方法显著提升了小样本（Data-Efficient）场景下的感知性能，对于降低自动驾驶数据标注成本具有极高的实际应用价值。
4. 结果可信度：代码开源，消融实验清晰，多项指标显著优于基线（SLidR, Seal 等）。
## Abstract: 
LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow
