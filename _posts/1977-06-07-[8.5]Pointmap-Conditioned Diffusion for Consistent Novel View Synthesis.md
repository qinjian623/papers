---
layout: default
title: "[8.5]Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis"
---

# [8.5] Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis

- Authors: Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Rold\~ao, Moussab Bennehar, Dzmitry Tsishkou, ...
- [arXiv Link](https://arxiv.org/abs/2501.02913)
- [PDF Link](https://arxiv.org/pdf/2501.02913.pdf)

## Subfields
 自动驾驶仿真 / 神经渲染与重建 (Simulation / Neural Rendering)
## Reason for Interest

1. **创新性**：提出PointmapDiff，利用稀疏LiDAR点云投影生成的Point Map作为Diffusion模型的几何条件（ControlNet），有效解决了自动驾驶场景中稀疏视角下的几何一致性问题，相比单纯依赖RGB或深度估计的方法有显著改进。
2. **实用性**：该方法专注于‘视角外推’（View Extrapolation），即生成训练轨迹之外的视角，这对自动驾驶仿真（如变道、转向模拟）和数据增强具有极高的应用价值。
3. **实验完整性**：在两个主流自动驾驶数据集（KITTI-360, Waymo）上进行了验证，并展示了辅助3D Gaussian Splatting (3DGS) 优化的能力，证明了其作为基础生成模型的潜力。
4. **综合评价**：论文切中自动驾驶重建中的痛点（动态场景、稀疏几何），方法设计合理且从物理传感器出发，具备较高的行业落地潜力。
## Abstract: 
Synthesizing extrapolated views remains a difficult task, especially in urban driving scenes, where the only reliable sources of data are limited RGB captures and sparse LiDAR points. To address this problem, we present PointmapDiff, a framework for novel view synthesis that utilizes pre-trained 2D diffusion models. Our method leverages point maps (i.e., rasterized 3D scene coordinates) as a conditioning signal, capturing geometric and photometric priors from the reference images to guide the image generation process. With the proposed reference attention layers and ControlNet for point map features, PointmapDiff can generate accurate and consistent results across varying viewpoints while respecting geometric fidelity. Experiments on real-life driving data demonstrate that our method achieves high-quality generation with flexibility over point map conditioning signals (e.g., dense depth map or even sparse LiDAR points) and can be used to distill to 3D representations such as 3D Gaussian Splatting for improving view extrapolation.
