---
layout: default
title: "[9.2]Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving"
---

# [9.2] Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving

- Authors: Dapeng Zhang, Zhenlong Yuan, Zhangquan Chen, Chih-Ting Liao, Yinda Chen, Fei Shen, Qinggu...
- [arXiv Link](https://arxiv.org/abs/2511.19912v1)
- [PDF Link](https://arxiv.org/pdf/2511.19912v1.pdf)

## Subfields
 端到端自动驾驶 / VLA (Vision-Language-Action) 规划控制
## Reason for Interest

1. **高价值创新 (Speed & Architecture)**：针对 VLM/VLA 在自动驾驶中推理速度慢的痛点，提出了基于 Learnable Action Queries 的并行解码架构，替代了传统的 Autoregressive 生成，将推理速度从不可用的低频提升至 11Hz，具有极高的工程落地价值。
2. **强大的泛化性验证 (Generalization)**：整合了 8 个主流数据集 (nuScenes, Waymo, NAVSIM 等) 构建统一的 Chain-of-Thought 数据集，并通过 Zero-shot 实验证明了跨传感器、跨场景的泛化能力。
3. **先进的训练策略**：引入了 DeepSeek-R1 同款的 GRPO 强化学习算法，配合物理动力学 Reward (转向、加速度约束)，有效解决了 VLM 输出不符合车辆动力学的问题。
4. **性能指标显著**：实验结果显示在各项指标上大幅超越 UniAD、VAD 等经典端到端模型，虽然 L2 误差下降幅度惊人（需关注评估集对齐细节），但闭环测试结果佐证了其实力。
## Abstract: 

