---
layout: default
title: "[6.5]Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts"
---

# [6.5] Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts

- Authors: Seungjun Yu, Junsung Park, Youngsun Lim, Hyunjung Shim
- [arXiv Link](https://arxiv.org/abs/2510.19001v1)
- [PDF Link](https://arxiv.org/pdf/2510.19001v1.pdf)

## Subfields
 自动驾驶 VQA / 视觉语言模型规划解释 (Vision-Language Planning)
## Reason for Interest

该论文展示了基于 Qwen2.5-VL 的自动驾驶问答系统设计。其核心价值在于 Phase-1 的工程实践：证明了通过引入时序历史（History Frames）和思维链（CoT）及 Self-Consistency 策略，可将纯视觉 VLM 的性能从基线大幅提升（从 ~52% 提至 ~66%）。此外，论文诚实地报告了“视觉提示（Visual Prompts，如消失点）”在大模型上失效的负面结果，具有科学参考价值。

然而，扣分点主要在于 Phase-2 的方法论缺陷：该阶段在推理过程中使用了 Ground Truth 元数据（如 nuScenes 的 3D 对象标注和自身车辆状态），这在真实自动驾驶感知中是无法获取的“上帝视角”信息。论文声称的“抗视觉腐蚀鲁棒性（Corruption Robustness 达到 100%）”主要源于直接读取了 GT 文本标签而非通过视觉感知，这种设置虽符合特定竞赛规则，但对实际车端部署的参考价值较低，属于 Oracle 实验。综合来看，是一篇有扎实工程实验但落地逻辑存在硬伤的技术报告。
## Abstract: 

