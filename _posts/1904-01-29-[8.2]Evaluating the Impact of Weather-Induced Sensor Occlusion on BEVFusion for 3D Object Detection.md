---
layout: default
title: "[8.2]Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection"
---

# [8.2] Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection

- Authors: Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eis...
- [arXiv Link](https://arxiv.org/abs/2511.04347)
- [PDF Link](https://arxiv.org/pdf/2511.04347.pdf)

## Subfields
 多传感器融合 / 3D目标检测 / 鲁棒性评估
## Reason for Interest

本论文的核心价值在于其对现有自动驾驶BEV多传感器融合模型（BEVFusion）在传感器遮挡条件下的鲁棒性进行了系统且深入的评估。创新性体现在研究设计和分析的深度上，而非提出新模型。作者模拟了相机（使用Woodscape污渍掩模）和LiDAR（随机点云丢弃）的多种遮挡情况，并在nuScenes数据集上进行了详尽的定量和定性实验。实验设计严谨，涵盖了纯相机、纯LiDAR以及相机与LiDAR融合的不同遮挡场景，并详细分析了各模态在遮挡下的性能表现及相互补偿作用。结果可信度高，清晰地量化了BEVFusion在不同遮挡程度下的性能下降，并揭示了LiDAR在融合感知中的关键作用。行业潜力巨大，因为传感器遮挡是实际自动驾驶部署中不可避免的挑战，本研究直接指出了现有SOTA模型的局限性，并为未来开发更具鲁棒性的融合策略提供了重要的洞察和研究方向。虽然论文没有提出新的SOTA算法，但其对核心问题的深入分析和量化评估对于推动自动驾驶技术的实际落地具有非常高的参考价值和实践意义。
## Abstract: 
Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird's Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.
