---
layout: default
title: "[8.8]OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence"
---

# [8.8] OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence

- Authors: Yu Zheng, Jie Hu, Kailun Yang, Jiaming Zhang
- [arXiv Link](https://arxiv.org/abs/2512.15621)
- [PDF Link](https://arxiv.org/pdf/2512.15621.pdf)

## Subfields
 4D Occupancy World Model / Perception & Prediction (4D 占据栅格世界模型 / 感知预测)
## Reason for Interest

1. 创新性强：提出了无 Tokenizer（Tokenizer-free）的 4D 占据预测架构，利用 Mamba（SSM）的线性复杂度处理高分辨率体素，避免了传统 VQ-VAE 带来的几何量化损失，这在精细结构重建上具有显著优势。
2. 关注长尾鲁棒性：定义了 OccSTeP 基准，包含丢帧（Discontinuous）、视角缺失（Fragmentary）、语义噪声（Reductive）等 4 种挑战性场景，填补了当前世界模型在传感器失效与时空持久性（Persistence）评估上的空白。
3. 实验扎实：在 Occ3D 及新提出的鲁棒性基准上均取得了 SOTA 结果。不仅在标准预测任务上领先，还在下游规划任务（Planning L2/L1）中证明了更好的轨迹一致性。
4. 行业价值：针对真实自动驾驶中常见的传感器波动问题提出了有效的解决方案（增量式状态融合），且代码开源，具备较高的应用参考价值。
## Abstract: 
Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.
