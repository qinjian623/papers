---
layout: default
title: "[9.2]WPT: World-to-Policy Transfer via Online World Model Distillation"
---

# [9.2] WPT: World-to-Policy Transfer via Online World Model Distillation

- Authors: Guangfeng Jiang, Yueru Luo, Jun Liu, Yi Huang, Yiyao Zhu, Zhan Qu, Dave Zhenyu Chen, Bing...
- [arXiv Link](https://arxiv.org/abs/2511.20095)
- [PDF Link](https://arxiv.org/pdf/2511.20095.pdf)

## Subfields
 端到端自动驾驶 / 世界模型 (World Models)
## Reason for Interest

该论文提出了一种极具实用价值的训练范式 WPT，解决了世界模型（World Model）在车端部署时推理延迟高、算力消耗大的核心痛点。核心创新在于利用世界模型仅在训练阶段作为‘导师’，通过奖励模型（Reward Model）和双重蒸馏机制（策略蒸馏+奖励蒸馏），将对未来环境演变的预测能力转移给轻量级的学生网络（Student Policy）。实验设计非常完善，涵盖了主流的开环评测（nuScenes）和新兴的闭环评测（Bench2Drive），且在闭环指标上取得了显著的性能提升（Driving Score 提升约 15 分）。这种方法既保留了世界模型对长时序规划的指导优势，又实现了 4.9 倍的推理加速，具有极高的行业应用潜力。
## Abstract: 
Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.
