---
layout: default
title: "[7.8]Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving"
---

# [7.8] Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving

- Authors: Guizhe Jin, Zhuoren Li, Bo Leng, Ran Yu, Lu Xiong, Chen Sun
- [arXiv Link](https://arxiv.org/abs/2506.23771)
- [PDF Link](https://arxiv.org/pdf/2506.23771.pdf)

## Subfields
 规划控制 / 强化学习 (Planning & Control / RL)
## Reason for Interest

论文针对自动驾驶强化学习（RL）中高层决策与底层控制时标不匹配导致的行为抖动问题，提出了一种扎实的多时标分层RL框架。

1. 创新性较强：
- 提出了混合动作空间（离散车道决策+连续目标点生成）作为高层运动引导，更加贴合结构化道路约束。
- 设计了“增量更新机制”，解决了高层低频指令与底层高频执行之间的状态不一致问题，这是分层RL在连续控制中的关键痛点。
- 引入基于人工势场（APF）的分层安全机制，显著提升了RL策略的安全性。

2. 实验验证充分：
- 除了标准的Highway-Env仿真，还利用HighD真实数据集构建交通流进行验证，增强了结果的可信度。
- 对比了包括PPO、Gen-H-RL、RL-PTA等多种基线，并在效率、舒适性（动作平滑度）和安全性三个维度进行了全面评估。

3. 行业价值与局限：
- 该方法有效缓解了端到端RL“控制不稳”的顽疾，分层结构使其比纯端到端方法更具可解释性，符合工业界“规划+控制”的解耦习惯。
- 主要局限在于目前仅在高速公路场景验证，且RL直接输出底层控制信号（油门/转向）在实车部署中仍面临安全认证挑战，相比MPC等传统控制方法的优势在于处理复杂交互的能力。
## Abstract: 
Reinforcement Learning (RL) is increasingly used in autonomous driving (AD) and shows clear advantages. However, most RL-based AD methods overlook policy structure design. An RL policy that only outputs short-timescale vehicle control commands results in fluctuating driving behavior due to fluctuations in network outputs, while one that only outputs long-timescale driving goals cannot achieve unified optimality of driving behavior and control. Therefore, we propose a multi-timescale hierarchical reinforcement learning approach. Our approach adopts a hierarchical policy structure, where high- and low-level RL policies are unified-trained to produce long-timescale motion guidance and short-timescale control commands, respectively. Therein, motion guidance is explicitly represented by hybrid actions to capture multimodal driving behaviors on structured road and support incremental low-level extend-state updates. Additionally, a hierarchical safety mechanism is designed to ensure multi-timescale safety. Evaluation in simulator-based and HighD dataset-based highway multi-lane scenarios demonstrates that our approach significantly improves AD performance, effectively increasing driving efficiency, action consistency and safety.
