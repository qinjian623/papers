---
layout: default
title: "[4.8]Dropping the D: RGB-D SLAM Without the Depth Sensor"
---

# [4.8] Dropping the D: RGB-D SLAM Without the Depth Sensor

- Authors: Mert Kiray, Alican Karaomer, Benjamin Busam
- [arXiv Link](https://arxiv.org/abs/2510.06216)
- [PDF Link](https://arxiv.org/pdf/2510.06216.pdf)

## Subfields
 单目SLAM / 基于预训练视觉模型的RGB-D SLAM
## Reason for Interest

1.  **创新性**: 论文提出了一种新颖的模块化方法，通过集成先进的预训练单目深度估计器、学习到的关键点检测器和实例分割网络，在无需主动深度传感器的情况下实现了度量尺度的鲁棒单目SLAM。这种“即插即用”的设计允许将现代强大的视觉模型整合到未修改的经典RGB-D SLAM后端中，展示了当前视觉模型的潜力。论文还强调了时间一致性对于SLAM的重要性，而非单纯的单帧精度，这是一个有价值的见解。
2.  **实验完整性与可信度**: 实验部分非常详尽，包含了对各模块贡献、不同深度尺度策略、特征预算以及时间一致性关键作用的全面消融研究。系统也实现了实时性能。然而，论文摘要和正文中关于单目变体的SOTA主张存在部分夸大，尤其是在静态序列和与所有单目动态SLAM基线进行比较时，这略微影响了其可信度。
3.  **自动驾驶相关性（严格评分依据）**: 该研究主要集中在室内环境（TUM RGB-D数据集，涉及静态和动态人体运动）的通用SLAM鲁棒性提升。尽管SLAM是自动驾驶的核心组件，但这项工作并未直接解决车载自动驾驶所面临的独特挑战，如大规模室外环境、多样化的驾驶条件或恶劣天气，也未使用自动驾驶领域的特定数据集（如Waymo, nuScenes, KITTI）。因此，其对车端自动驾驶研究的直接价值是间接的。根据严格的评审标准，如果论文不直接和车端自动驾驶相关，最高评分为5分。鉴于其在通用SLAM领域的优秀质量但与车载自动驾驶的直接相关性有限，综合评分为4.8分。
## Abstract: 
We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network. Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features. These are processed by an unmodified RGB-D SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS on a single GPU. These results suggest that modern pretrained vision models can replace active depth sensors as reliable, real-time sources of metric scale, marking a step toward simpler and more cost-effective SLAM systems.
