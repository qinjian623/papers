---
layout: default
title: "[9.2]GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation"
---

# [9.2] GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation

- Authors: Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo ...
- [arXiv Link](https://arxiv.org/abs/2512.23180)
- [PDF Link](https://arxiv.org/pdf/2512.23180.pdf)

## Subfields
 自动驾驶世界模型 (Driving World Model) / 3D场景理解与生成
## Reason for Interest

该论文提出了首个基于3D高斯泼溅（3DGS）的统一驾驶世界模型框架（GaussianDWM），具有极高的创新性。核心贡献在于：1. 将语言特征显式嵌入3D高斯原语，实现了精细的3D场景理解与文本对齐；2. 设计了任务感知的混合采样策略（Task-aware Hybrid Sampling），有效解决了3D高斯在大模型中Token冗余的痛点；3. 提出了双条件多模态生成模块，结合高层语言知识与低层视觉特征，显著提升了生成的一致性和可控性。实验部分非常完整，涵盖了场景描述、2D/3D定位、规划问答以及空间/时间维度的视频生成，且在多个权威数据集上均达成SOTA。该工作对于自动驾驶的闭环仿真、长尾数据生成及端到端解释性具有重要的行业价值。
## Abstract: 
Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.
