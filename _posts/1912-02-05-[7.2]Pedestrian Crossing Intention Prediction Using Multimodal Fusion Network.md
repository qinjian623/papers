---
layout: default
title: "[7.2]Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network"
---

# [7.2] Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network

- Authors: Yuanzhe Li, Steffen M\"uller
- [arXiv Link](https://arxiv.org/abs/2511.20008)
- [PDF Link](https://arxiv.org/pdf/2511.20008.pdf)

## Subfields
 行为预测 / 行人意图预测
## Reason for Interest

论文提出了一种利用Transformer和深度引导注意力机制的多模态融合网络（P-MFNet），有效整合了RGB、深度、语义分割、姿态和车辆运动信息，在JAAD数据集上取得了SOTA性能。然而，该方法存在显著的工程落地短板：预处理依赖OpenPose、SegFormer、Depth Anything v2等多个繁重的预训练模型，计算开销巨大，难以满足车端实时性要求。此外，实验仅在JAAD单一数据集上进行，缺乏在PIE或TITAN等其他主流数据集上的验证，泛化性证明略显不足。
## Abstract: 
Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.
