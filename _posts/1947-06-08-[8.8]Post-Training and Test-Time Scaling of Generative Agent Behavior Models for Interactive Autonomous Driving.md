---
layout: default
title: "[8.8]Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving"
---

# [8.8] Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving

- Authors: Hyunki Seong, Jeong-Kyun Lee, Heesoo Myeong, Yongho Shin, Hyun-Mook Cho, Duck Hoon Kim, P...
- [arXiv Link](https://arxiv.org/abs/2512.13262)
- [PDF Link](https://arxiv.org/pdf/2512.13262.pdf)

## Subfields
 生成式智能体仿真 / 行为预测 (Generative Agent Simulation / Behavior Prediction)
## Reason for Interest

1. 创新性强：论文敏锐地将大语言模型（LLM）领域的先进后训练技术——DeepSeekMath提出的GRPO（Group Relative Policy Optimization）——迁移至自动驾驶行为生成领域（提出GRBO），有效解决了模仿学习在长尾安全关键场景下的鲁棒性问题。同时提出了Warm-K采样策略，巧妙解决了自回归模型在闭环执行时的时序一致性问题。
2. 实验扎实：在行业主流的Waymo数据集和Waymax模拟器上进行了详尽的闭环评估，对比了SMART、CAT-K等最新SOTA方法，结果显著（碰撞率大幅下降）。
3. 行业价值高：生成式Sim Agent是当前解决自动驾驶数据稀缺和仿真评估难题的关键技术方向。该论文提出的Test-Time Scaling（测试时计算扩展）概念符合当前GenAI在端侧应用的前沿趋势，不仅对构建高保真世界模型有重要意义，对未来的生成式规划（Generative Planning）也有直接借鉴价值。
## Abstract: 
Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.
