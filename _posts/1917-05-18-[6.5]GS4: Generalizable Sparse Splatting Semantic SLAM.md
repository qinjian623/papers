---
layout: default
title: "[6.5]GS4: Generalizable Sparse Splatting Semantic SLAM"
---

# [6.5] GS4: Generalizable Sparse Splatting Semantic SLAM

- Authors: Mingqi Jiang, Chanho Kim, Chen Ziwen, Li Fuxin
- [arXiv Link](https://arxiv.org/abs/2506.06517)
- [PDF Link](https://arxiv.org/pdf/2506.06517.pdf)

## Subfields
 语义SLAM / 3D重建
## Reason for Interest

论文提出了GS4，一种基于泛化神经网络（Feed-forward）的语义高斯Splatting SLAM系统。其核心创新在于利用预训练Transformer直接从RGB-D图像预测高斯参数，取代了传统GS-SLAM昂贵的逐场景优化过程，实现了10倍的速度提升和存储效率优化，并具备零样本泛化能力。尽管实验局限于室内RGB-D数据集（ScanNet），且严重依赖稠密深度图，限制了其在室外稀疏传感器配置下的直接应用（车端通常为LiDAR或稀疏视觉），但该方法为高效、可泛化的在线3D语义建图提供了极具价值的新思路，对自动驾驶领域的仿真场景构建和众包建图有重要借鉴意义。
## Abstract: 
Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, when significant pose changes are detected, we perform only 1-5 iterations of joint Gaussian-pose optimization to correct drift, remove floaters, and further improve tracking accuracy. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.
