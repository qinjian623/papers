---
layout: default
title: "[9.2]AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models"
---

# [9.2] AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models

- Authors: Tianyi Yan, Tao Tang, Xingtai Gui, Yongkang Li, Jiasen Zhesng, Weiyao Huang, Lingdong Kon...
- [arXiv Link](https://arxiv.org/abs/2511.20325)
- [PDF Link](https://arxiv.org/pdf/2511.20325.pdf)

## Subfields
 End-to-End Autonomous Driving / World Models / Model-Based RL
## Reason for Interest

论文切中端到端自动驾驶研究的一个核心痛点：世界模型（World Models）通常存在“乐观偏差”（Optimistic Bias），即在接收到危险指令时倾向于幻想安全的未来，导致无法作为RL的可靠模拟器。文章提出的“反事实合成”（Counterfactual Synthesis）数据管线极具创新性，通过生成合理的碰撞数据训练“公正世界模型”（Impartial World Model），使其能真实预测风险。基于此构建的闭环RL框架（AD-R1）在NavSim标准榜单上取得了SOTA性能，证明了该方法能有效提升端到端策略的安全性和鲁棒性。逻辑严密，实验充分，对行业从纯模仿学习向基于世界模型的强化学习转型具有重要参考价值。
## Abstract: 
End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.
