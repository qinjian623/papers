---
layout: default
title: "[8.5]OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning"
---

# [8.5] OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning

- Authors: Xusheng Guo, Wanfa Zhang, Shijia Zhao, Qiming Xia, Xiaolong Xie, Mingming Wang, Hai Wu, C...
- [arXiv Link](https://arxiv.org/abs/2512.05698)
- [PDF Link](https://arxiv.org/pdf/2512.05698.pdf)

## Subfields
 无监督3D目标检测 / LiDAR感知
## Reason for Interest

该论文在无监督3D目标检测领域取得了显著突破。主要创新点在于：1. 引入占据栅格（Occupancy）预测任务进行网络热身（Warm-up），有效解决了无监督学习初期特征提取能力弱导致的伪标签噪声累积问题；2. 利用大语言模型（LLM）的常识推理能力（基于尺寸、运动等线索）对伪标签进行精细化过滤和修正，这是一种利用基础模型先验知识降低标注成本的有效范式。实验表明，该方法在Waymo和KITTI数据集上大幅超越了现有的SOTA无监督方法（如CPD、OYSTER），虽然距离全监督方法仍有差距，但在降低数据标注成本方面展现了极高的潜力和实用价值。
## Abstract: 
Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.
