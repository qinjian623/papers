---
layout: default
title: "[9.2]Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles"
---

# [9.2] Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles

- Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi...
- [arXiv Link](https://arxiv.org/abs/2512.03454v2)
- [PDF Link](https://arxiv.org/pdf/2512.03454v2.pdf)

## Subfields
 Visual Grounding / Multimodal Perception / World Model
## Reason for Interest

The paper presents a high-value contribution by introducing World Models into the Visual Grounding (VG) task for autonomous driving, addressing the critical challenge of reasoning about future states for ambiguous commands. 

1. **Innovation**: It proposes 'ThinkDeeper', a novel framework using a Spatial-Aware World Model (SA-WM) to 'imagine' future scene states before grounding, which is a significant conceptual advance over existing one-stage/two-stage methods.
2. **Dataset**: It releases 'DrivePilot', a large-scale, high-quality VG dataset based on nuScenes, annotated using an advanced pipeline (Qwen2-VL + RAG + CoT), filling a gap in high-quality semantic annotations for AD.
3. **Performance**: The method claims the #1 spot on the established Talk2Car leaderboard and SOTA on multiple other benchmarks, validating its effectiveness. The robustness experiments (low-data regime) further strengthen the credibility.
4. **Relevance**: The focus on human-vehicle interaction (natural language commands) and handling dynamic, complex urban scenarios is highly relevant for L4/L5 autonomous driving user experience and safety.
## Abstract: 

