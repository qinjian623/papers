---
layout: default
title: "[4.5]Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios"
---

# [4.5] Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios

- Authors: Siyue Yao, Mingjie Sun, Eng Gee Lim, Ran Yi, Baojiang Zhong, Moncef Gabbouj
- [arXiv Link](https://arxiv.org/abs/2507.09915)
- [PDF Link](https://arxiv.org/pdf/2507.09915.pdf)

## Subfields
 合成数据生成 / 弱点感知样本生成（泛领域，对自动驾驶有潜在应用）
## Reason for Interest

论文提出了一种名为Crucial-Diff的领域无关扩散模型，用于在数据稀缺场景下合成“关键”图像和标注。其核心创新在于“弱点感知样本挖掘器 (WASM)”模块，该模块通过结合下游模型的反馈来生成难以检测的样本，从而克服了现有生成模型生成“简单”或重复数据导致下游模型性能饱和的局限。此外，“场景无关特征提取器 (SAFE)”模块确保了模型在不同领域间的泛化能力，无需针对特定领域进行重新训练。论文在工业异常检测 (MVTec) 和医学图像分割 (polyp dataset) 两个领域进行了广泛且深入的实验，结果显示相较于现有SOTA方法有显著提升，且进行了详细的消融研究，证明了方法的有效性和鲁棒性。方法的创新性、实验的完整性（在其所选领域内）和可信度都非常高。

然而，尽管论文在摘要和引言中将“自动驾驶”列为数据稀缺场景的一个重要应用方向，但其所有实验验证和结果展示均集中在工业检测和医疗诊断领域，并未涉及任何自动驾驶数据集或任务的直接应用。根据评审要求，如果论文不直接与车端自动驾驶相关，则最高得分不得超过5分。鉴于此，尽管该方法在概念上对自动驾驶领域（如生成稀有或高难度感知场景样本）具有巨大潜力，但缺乏直接的车端自动驾驶实验验证，因此无法获得更高的分数。
## Abstract: 
The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide "crucial information" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code is publicly available at https://github.com/JJessicaYao/Crucial-diff.
