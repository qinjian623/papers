---
layout: default
title: "[8.8]Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection"
---

# [8.8] Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection

- Authors: Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daiz...
- [arXiv Link](https://arxiv.org/abs/2512.22972)
- [PDF Link](https://arxiv.org/pdf/2512.22972.pdf)

## Subfields
 3D Perception / Multi-Sensor Fusion (4D Radar & Camera)
## Reason for Interest

The paper addresses a critical bottleneck in all-weather perception by fusing Camera with 4D Radar Tensors (preserving raw signal info) rather than sparse point clouds. 

Strengths:
1. **Innovation**: The proposed 'Wavelet Attention Mixture-of-Experts (WA-MoE)' is physically grounded, leveraging wavelet transforms to handle the frequency-domain nature of radar signals and suppress noise effectively. The 'Geometry-guided Progressive Fusion (GPF)' provides a sensible coarse-to-fine alignment strategy.
2. **Performance**: Demonstrates clear SOTA performance on the K-Radar benchmark, significantly outperforming competitors in adverse weather conditions (Fog, Rain, Snow) where cameras fail.
3. **Efficiency**: Despite processing heavy tensor data, the projection strategy (Range-Azimuth / Elevation-Azimuth) achieves a viable inference speed of ~15 FPS on an RTX 3090.

Weaknesses:
- Evaluation is limited to the K-Radar dataset. While this is currently the primary dataset for 4D Radar Tensors, testing on proprietary or other emerging datasets would strengthen the generalization claims.
## Abstract: 
4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.
