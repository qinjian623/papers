---
layout: default
title: "[9.0]Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds"
---

# [9.0] Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds

- Authors: Leon Schwarzer, Matthias Zeller, Daniel Casado Herraez, Simon Dierl, Michael Heidingsfeld...
- [arXiv Link](https://arxiv.org/abs/2511.02395)
- [PDF Link](https://arxiv.org/pdf/2511.02395.pdf)

## Subfields
 3D感知 / 雷达感知 / 运动物体分割 / 自监督学习
## Reason for Interest

这篇论文解决了自动驾驶领域一个关键且具有挑战性的问题：在稀疏且噪声大的雷达点云上进行运动物体分割 (MOS)，并侧重于自监督学习以提高标注效率。其主要创新点在于提出了一种新颖的基于聚类的对比损失函数，该函数专门为雷达数据的运动特性和点云特性（稀疏性、噪声）设计。通过结合 HDBSCAN 聚类和动态点去除 (DPR) 滤波，生成运动感知的伪标签，用于在 student-teacher 框架中进行表示学习，从而有效地应对了雷达数据的特殊挑战。

实验部分非常完整和严谨。论文在两个主流的雷达数据集 (View-of-Delft 和 RadarScenes) 上进行了全面的评估，并与多个基线进行了比较：包括从头训练的修改版 RIT* 架构（其已是当前最先进的雷达运动物体分割方法之一）、一个自监督基线 RaFlow [10] 以及另一个监督学习方法 RVT [2]。论文特别关注了标注效率问题，通过使用不同比例的标注数据（如 1%、10%、100%）进行微调，清晰地展示了其方法在数据稀缺情况下的显著优势，尤其是在极低标注（1%）下对 RaFlow 的大幅超越。

消融研究也充分验证了所提出损失函数中各个组件（聚类和 DPR 细化）的有效性，增强了结果的可信度。该方法能够仅依赖单帧雷达数据进行分割，从而降低了自动驾驶系统所需的感知延迟。

从行业潜力来看，减少对昂贵人工标注数据的依赖，对于自动驾驶的规模化部署至关重要。雷达传感器在恶劣天气下的鲁棒性使其成为自动驾驶不可或缺的一部分，而该方法在雷达 MOS 方面的性能提升和标注效率改进，具有巨大的应用价值。尽管在 100% 数据集上的绝对性能提升相对于 SOTA 有时不是非常巨大，但论文的核心价值在于其在有限标注数据下表现出的卓越性能和对自监督方法的显著推进，这正是自动驾驶研发的关键痛点。
## Abstract: 
Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning. While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context. Radar sensors overcome this problem with their ability to provide a direct measurement of a point's Doppler velocity, which can be exploited for single-scan moving object segmentation. However, radar point clouds are often sparse and noisy, making data annotation for use in supervised learning very tedious, time-consuming, and cost-intensive. To overcome this problem, we address the task of self-supervised moving object segmentation of sparse and noisy radar point clouds. We follow a two-step approach of contrastive self-supervised representation learning with subsequent supervised fine-tuning using limited amounts of annotated data. We propose a novel clustering-based contrastive loss function with cluster refinement based on dynamic points removal to pretrain the network to produce motion-aware representations of the radar data. Our method improves label efficiency after fine-tuning, effectively boosting state-of-the-art performance by self-supervised pretraining.
