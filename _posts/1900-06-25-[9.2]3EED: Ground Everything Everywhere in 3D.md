---
layout: default
title: "[9.2]3EED: Ground Everything Everywhere in 3D"
---

# [9.2] 3EED: Ground Everything Everywhere in 3D

- Authors: Rong Li, Yuhao Dong, Tianshuai Hu, Ao Liang, Youquan Liu, Dongyue Lu, Liang Pan, Lingdong...
- [arXiv Link](https://arxiv.org/abs/2511.01755)
- [PDF Link](https://arxiv.org/pdf/2511.01755.pdf)

## Subfields
 3D视觉定位 / 多平台多模态感知
## Reason for Interest

该论文提出了3EED，一个开创性的、大规模、多平台、多模态的户外3D视觉定位基准数据集。其核心价值在于解决了现有数据集在室内环境、单一平台和规模受限等方面的局限性，为自动驾驶和更广泛的具身智能领域提供了极具挑战性和真实性的研究平台。

**创新性 (9.5/10)**:
1.  **数据集创新性极高**：3EED是首个涵盖车辆、无人机和四足机器人三种异构平台的数据集，提供了RGB和LiDAR数据。其规模（超过128,000个对象，22,000个人工验证的指代表达）比现有户外数据集大10倍，并且涵盖了更广阔的场景范围和高度变化，这是当前3D视觉定位领域急需的。
2.  **标注流程创新**：结合了视觉-语言模型（VLM）提示与人工验证的标注流程，实现了高质量、可扩展的多模态数据标注，这对于构建如此规模和复杂性的数据集至关重要。
3.  **方法创新**：论文提出的基线方法融合了平台感知归一化 (CPA)、多尺度采样 (MSS) 和尺度感知融合 (SAF) 等技术。CPA通过对重力轴的对齐和高度归一化，有效减少了平台间的几何差异；MSS和SAF则能应对LiDAR稀疏性、物体尺度巨大变化以及跨平台视角/密度变化带来的挑战，这些都是自动驾驶感知中真实存在的问题。

**实验完整性 (9.5/10)**:
1.  **全面评估**：设计了四种基准评估协议：单平台单物体定位、跨平台迁移、多物体定位、多平台联合训练，全面评估了模型的泛化能力和鲁棒性。
2.  **详细消融研究**：对CPA、MSS、SAF等核心模块进行了详尽的消融实验，清晰地展示了每个组件对性能提升的贡献，证明了方法的有效性和稳健性。
3.  **多角度分析**：对数据集进行了深入的统计分析，包括视点几何、物体密度、点云几何分布等，并讨论了这些因素对定位性能的影响，为后续研究提供了宝贵的见解。
4.  **强大基线对比**：与多个SOTA的3D视觉定位模型（如BUTD-DETR、EDA、WildRefer）进行了充分对比，并展示了其基线方法在3EED上的显著优势。

**可信度 (9.0/10)**:
1.  **数据来源可靠**：数据集构建在Waymo Open Dataset和M3ED等公认的高质量自动驾驶和机器人数据集之上，确保了原始数据的可靠性。
2.  **方法阐述清晰**：对任务定义、数据处理、模型结构和评估指标的描述详细而透明。
3.  **结果支持度高**：定性和定量实验结果一致地支持了论文的核心主张，即3EED能够揭示现有方法的局限性，并推动更具泛化能力的3D视觉定位技术发展。
4.  **资源开放性**：数据集和工具包的发布，确保了研究的可复现性和社区协作的可能性。

**行业潜力 (9.5/10)**:
1.  **直接相关自动驾驶**：论文的Vehicle平台数据直接来源于Waymo，并且提出的3D视觉定位任务是自动驾驶车辆理解高层人类指令、实现精确感知的核心能力之一。
2.  **拓展具身智能边界**：多平台（无人机、四足机器人）的引入，将3D视觉定位任务扩展到更广泛的具身智能领域，对于未来智能物流、巡检、救援等场景具有巨大潜力，这些都与自动驾驶生态系统紧密相关。
3.  **挑战前沿问题**：明确指出了现有方法在稀疏点云、尺度变化和跨平台泛化等方面的挑战，这些都是自动驾驶感知领域亟待解决的关键难题。
4.  **促进通用模型发展**：该基准测试将极大地推动开发能够跨不同传感器配置、视点和环境条件进行鲁棒泛化的语言驱动3D感知系统，这对于实现L4/L5级自动驾驶至关重要。

综合来看，这篇论文在自动驾驶及具身智能领域做出了非常扎实和具有前瞻性的工作。其发布的大规模多平台多模态数据集以及在此数据集上验证的强大基线，将对未来的研究产生深远影响。鉴于其极高的创新性、详尽的实验以及对行业未来发展的巨大潜力，给予高分。
## Abstract: 
Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes -- 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.
