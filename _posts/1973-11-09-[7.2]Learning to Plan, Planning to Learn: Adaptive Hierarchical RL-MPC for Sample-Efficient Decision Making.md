---
layout: default
title: "[7.2]Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making"
---

# [7.2] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making

- Authors: Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman
- [arXiv Link](https://arxiv.org/abs/2512.17091)
- [PDF Link](https://arxiv.org/pdf/2512.17091.pdf)

## Subfields
 规划控制 / 混合RL-MPC
## Reason for Interest

该论文由丰田研究院（TRI）提出，方法论质量很高。创新点在于构建了RL与MPPI的双向信息流，利用MPPI的规划轨迹作为虚拟样本加速RL训练，并通过不确定性估计自适应调整数据权重，有效解决了基于学习的控制方法中的样本效率和安全性问题。虽然实验包含CARLA仿真，但主要侧重于赛车/动力学控制场景，相比于复杂的城市道路自动驾驶规划任务，场景复杂度有限；且对比基线主要是通用RL算法而非行业最先进的专用规划器。总体而言，是一篇理论扎实且具有良好行业应用前景的控制类论文。
## Abstract: 
We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.
