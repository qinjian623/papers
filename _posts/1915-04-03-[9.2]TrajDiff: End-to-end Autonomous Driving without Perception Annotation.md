---
layout: default
title: "[9.2]TrajDiff: End-to-end Autonomous Driving without Perception Annotation"
---

# [9.2] TrajDiff: End-to-end Autonomous Driving without Perception Annotation

- Authors: Xingtai Gui, Jianbo Zhao, Wencheng Han, Jikai Wang, Jiahao Gong, Feiyang Tan, Cheng-zhong...
- [arXiv Link](https://arxiv.org/abs/2512.00723)
- [PDF Link](https://arxiv.org/pdf/2512.00723.pdf)

## Subfields
 端到端自动驾驶 / 轨迹规划
## Reason for Interest

论文针对端到端自动驾驶中感知标注成本高昂的痛点，提出了一种极具价值的无标注训练框架TrajDiff。核心创新在于设计了'Gaussian BEV Heatmap'作为自监督目标，利用未来轨迹隐式地学习环境特征，替代了传统的物体检测和地图分割任务。结合专门设计的Trajectory-oriented BEV Diffusion Transformer (TB-DiT)，实现了无Anchor的高质量轨迹生成。实验不仅在权威榜单NAVSIM上证明了其超越有监督方法的性能，还展示了清晰的数据扩展性(Data Scaling)，表明该方法能随数据量增加持续提升，具有极高的行业落地潜力和学术启发性。
## Abstract: 
End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.
