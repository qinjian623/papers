---
layout: default
title: "[7.8]Vehicle-centric Perception via Multimodal Structured Pre-training"
---

# [7.8] Vehicle-centric Perception via Multimodal Structured Pre-training

- Authors: Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo
- [arXiv Link](https://arxiv.org/abs/2512.19934)
- [PDF Link](https://arxiv.org/pdf/2512.19934.pdf)

## Subfields
 2D感知 / 视觉基础模型 (Vision Foundation Model)
## Reason for Interest

论文提出了一种针对车辆感知的预训练大模型VehicleMAE-V2，结合了对称性、轮廓和语义先验，构建了含有400万图像的Autobot4M数据集。1. 创新性：针对车辆特性（刚体对称、轮廓、属性文本）改进MAE掩码和重建策略，思路清晰且有效。2. 实验完整性：在属性识别、重识别、检测、分割等5个下游任务上进行了广泛验证，且消融实验充分。3. 行业价值：虽然构建了大规模车辆预训练底座，对提升2D感知长尾问题有帮助，但目前自动驾驶核心感知已转向3D BEV和多模态融合，纯2D图像感知的应用场景在车端相对受限（更多用于路侧感知或车端辅助），因此在方向相关性上略有扣分。
## Abstract: 
Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.
