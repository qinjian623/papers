---
layout: default
title: "[8.0]Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching"
---

# [8.0] Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching

- Authors: Uday Bhaskar, Rishabh Bhattacharya, Avinash Patel, Sarthak Khoche, Praveen Anil Kulkarni,...
- [arXiv Link](https://arxiv.org/abs/2511.09955v1)
- [PDF Link](https://arxiv.org/pdf/2511.09955v1.pdf)

## Subfields
 2D Perception / Weakly Supervised Object Detection (VLM Distillation)
## Reason for Interest

The paper addresses a critical challenge in autonomous driving: the high cost of data annotation. 

Strengths:
1. **Practical Innovation**: Proposes a 'Per-Object Co-teaching' strategy that filters noisy pseudo-labels at the anchor/box level. This is a clever adaptation of co-teaching (usually image-level) to object detection, effectively mitigating the hallucinations common in Vision-Language Models (VLMs).
2. **Exceptional Robustness Results**: On the ACDC dataset (adverse weather conditions like fog/rain), the method achieves 27.31% mAP, which is remarkably close to the fully supervised ground-truth upper bound (29.57% mAP). This suggests high value for domain adaptation in difficult environments without manual labeling.
3. **Real-time Viability**: Successfully distills a heavy VLM (OWLv2) into a deployable real-time detector (YOLOv5), bridging the gap between foundation models and on-vehicle inference.

Weaknesses:
1. **Performance Gap on Clean Data**: On standard datasets like KITTI, there remains a massive gap between this method (46.61% mAP) and fully supervised approaches (>90% mAP). This limits its immediate standalone use for primary safety-critical perception in standard conditions.
2. **Base Model**: Experiments use YOLOv5, which is slightly dated, though the methodology is likely model-agnostic.

Overall, the paper is highly relevant for scalable, data-efficient AD perception.
## Abstract: 

