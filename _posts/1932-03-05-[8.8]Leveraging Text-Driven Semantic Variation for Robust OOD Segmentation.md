---
layout: default
title: "[8.8]Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation"
---

# [8.8] Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation

- Authors: Seungheon Song, Jaekoo Lee
- [arXiv Link](https://arxiv.org/abs/2511.07238v1)
- [PDF Link](https://arxiv.org/pdf/2511.07238v1.pdf)

## Subfields
 感知 / 语义分割 / OOD检测 (Perception / Semantic Segmentation / OOD Detection)
## Reason for Interest

该论文针对自动驾驶长尾问题中的未知物体检测提出了有效的视觉-语言融合方案。利用WordNet构建语义距离Prompt的思路新颖，能有效利用语言模型的泛化能力。实验覆盖了主流Benchmark，且在像素级和对象级指标上均取得了显著的SOTA性能，对提升自动驾驶安全性有较高研究价值。
## Abstract: 
  To this end, we present a novel approach that trains a Text-Driven OOD Segmentation model to learn a semantically diverse set of objects in the vision-language space. Concretely, our approach combines a vision-language model's encoder with a transformer decoder, employs Distance-Based OOD prompts located at varying semantic distances from in-distribution (ID) classes, and utilizes OOD Semantic Augmentation for OOD representations. By aligning visual and textual information, our approach effectively generalizes to unseen objects and provides robust OOD segmentation in diverse driving environments.
  We conduct extensive experiments on publicly available OOD segmentation datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets, demonstrating that our approach achieves state-of-the-art performance across both pixel-level and object-level evaluations. This result underscores the potential of vision-language-based OOD segmentation to bolster the safety and reliability of future autonomous driving systems.
