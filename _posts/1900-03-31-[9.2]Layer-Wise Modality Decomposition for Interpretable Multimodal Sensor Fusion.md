---
layout: default
title: "[9.2]Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion"
---

# [9.2] Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion

- Authors: Jaehyun Park, Konyul Park, Daehun Kim, Junseo Park, Jun Won Choi
- [arXiv Link](https://arxiv.org/abs/2511.00859)
- [PDF Link](https://arxiv.org/pdf/2511.00859.pdf)

## Subfields
 可解释AI (XAI) / 多传感器融合感知解释
## Reason for Interest

该论文提出了一种名为LMD（Layer-Wise Modality Decomposition）的后验、模型无关的可解释性方法，旨在分解自动驾驶多传感器融合模型中各层级的模态特定信息。这项工作在自动驾驶领域具有极高的价值和重要性，直接解决了黑盒多模态感知模型在安全性和可信度方面的核心挑战。

**创新性 (9.5/10):** 论文提出的LMD是首次尝试在自动驾驶多传感器融合系统中将感知模型预测归因于单个输入模态的方法，具有高度创新性。通过对神经网络操作进行局部线性化，并基于Layer-Wise Relevance Propagation (LRP)和Deep Taylor Decomposition (DTD)框架，实现了层级模态分解。此外，论文还引入了一套新颖的基于扰动的指标（Pearson Correlation Coefficient和Mean Squared Error）来量化评估模态分离性，填补了该领域评估方法的空白。

**实验完整性 (9.0/10):** 实验在nuScenes数据集上进行，验证了LMD在相机-雷达、相机-LiDAR以及相机-雷达-LiDAR等多种传感器融合配置下的有效性，并使用了SimpleBEV和CRN等具体融合模型。论文不仅提供了定性分析（通过可视化展示模态贡献），还进行了详尽的定量评估，包括对不同偏差拆分策略（如Ratio Rule、Identity Rule等）的消融研究，证明了所提方法的稳健性和最优配置。计算效率分析也比较了LMD与LRP和Shapley-based方法的复杂度。

**可信度 (9.0/10):** 方法建立在扎实的数学和XAI理论基础之上（Taylor展开、LRP、DTD），附录提供了详细的数学推导。实验结果一致且支持论文的核心主张，即LMD能够有效实现模态分离。开放源代码的承诺进一步增强了结果的可复现性和可信度。对潜在风险（如误导性保证、偏差放大）的讨论也显示了研究团队的严谨性。

**行业潜力 (10/10):** 可解释性是自动驾驶系统实现L3及以上级别部署的关键障碍之一。该方法直接有助于理解复杂多模态融合模型的决策过程，识别单传感器故障，加速系统审计和认证，对于提高自动驾驶系统的安全性、可靠性和公众信任至关重要。其模型无关性也使其能够广泛应用于其他多模态感知任务。
## Abstract: 
In autonomous driving, transparency in the decision-making of perception models is critical, as even a single misperception can be catastrophic. Yet with multi-sensor inputs, it is difficult to determine how each modality contributes to a prediction because sensor information becomes entangled within the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. To our knowledge, LMD is the first approach to attribute the predictions of a perception model to individual input modalities in a sensor-fusion system for autonomous driving. We evaluate LMD on pretrained fusion models under camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous driving. Its effectiveness is validated using structured perturbation-based metrics and modality-wise visual decompositions, demonstrating practical applicability to interpreting high-capacity multimodal architectures. Code is available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.
