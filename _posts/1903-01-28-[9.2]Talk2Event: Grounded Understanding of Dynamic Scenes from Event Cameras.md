---
layout: default
title: "[9.2]Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras"
---

# [9.2] Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras

- Authors: Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei ...
- [arXiv Link](https://arxiv.org/abs/2507.17664)
- [PDF Link](https://arxiv.org/pdf/2507.17664.pdf)

## Subfields
 事件相机感知 / 视觉语言接地 / 多模态融合
## Reason for Interest

本论文在自动驾驶领域具有重要价值和方向性。其核心贡献在于：
1.  **开创性数据集**：首次推出了大型事件相机视觉语言接地基准数据集Talk2Event，填补了该领域的空白。该数据集基于真实世界驾驶数据，包含丰富的语言描述和创新的四种属性标注（外观、状态、与观察者关系、与其他物体关系），极大地促进了对动态场景的细粒度、可解释和时间感知的理解。
2.  **创新方法**：提出了EventRefer属性感知接地框架，通过MoEE（Mixture of Event-Attribute Experts）动态融合多属性表示。这种机制使得模型能够自适应地侧重于当前场景中最具信息量的属性（例如在夜间侧重运动线索，在白天侧重外观线索），增强了模型的鲁棒性和可解释性。
3.  **全面且严格的实验**：在Frame-Only、Event-Only和Event-Frame Fusion三种模态设置下，与大量最先进的基线方法进行了广泛而全面的比较，并展示了在各类物体和不同场景复杂性下的显著性能提升。消融实验充分验证了PWM、MAF和MoEE等核心组件以及四种属性的有效性，证明了设计选择的合理性。
4.  **高度相关性和潜力**：事件相机因其低延迟、高动态范围和对运动模糊的鲁棒性，在自动驾驶等高动态、低光照场景中具有巨大潜力。将事件数据与自然语言理解相结合，为自动驾驶系统提供了更丰富、更具语义的感知能力，有助于实现更安全、更智能的人机交互、意图理解和复杂指令执行。

尽管论文承认存在一些局限性（如数据集地理区域和相机设置的潜在偏差、LLM生成标注的偏差、目前未扩展到3D世界坐标等），但其对事件相机视觉语言接地这一新兴且关键领域的推动作用，以及提出的高质量数据集和有效方法，使其在自动驾驶研究中具有非常高的价值。
## Abstract: 
Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, we provide over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy.
