---
layout: default
title: "[8.8]OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving"
---

# [8.8] OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving

- Authors: Zhenguo Zhang, Haohan Zhen, Yishen Wang, Le Xu, Tianchen Deng, Xuefeng Chen, Qu Chen, Bo ...
- [arXiv Link](https://arxiv.org/abs/2512.14044)
- [PDF Link](https://arxiv.org/pdf/2512.14044.pdf)

## Subfields
 端到端自动驾驶 / VLM 驾驶智能体 (Vision-Language Autonomous Driving)
## Reason for Interest

该论文在自动驾驶视觉语言模型（VLM）领域做出了具有高度创新性和实用价值的工作。

1. 强创新性（RL + 主动感知）：针对 VLM 在驾驶场景中的幻觉和感知定位问题，提出了 'Interleaved Multi-modal CoT' (iMCoT) 机制。核心亮点是设计了 'Zoom-in' 工具，允许模型通过强化学习（RL）自主决定是否对图像特定区域进行“聚焦”细看。此外，提出的 Clip-GRPO 算法利用 CLIP 的跨模态一致性作为无标注奖励（Reward），巧妙解决了训练数据缺乏密集定位标注的难题。

2. 实验扎实且结果惊艳：在 DriveLMM-o1 基准测试中，模型相比基座 Qwen2.5VL-7B 取得了巨大的性能飞跃（推理分提升 +28.58%，MCQ 提升 +35.81%），并且击败了包括 GPT-4o 和 InternVL-2.5 在内的强力竞争对手。消融实验清晰地证明了 RL 训练策略和 Clip-GRPO 的有效性。

3. 行业价值：该方法为解决端到端大模型“看不清、瞎推理”的通病提供了一条极具潜力的技术路线（即通过 RL 训练模型的主动感知能力），且无需依赖昂贵的人工细粒度标注，具备较高的落地参考价值。

唯一不足是目前的评估主要基于开环的驾驶问答（Reasoning/VQA），尚未展示在闭环仿真环境中的规划控制性能，但作为驾驶推理模型，其表现已十分出色。
## Abstract: 
The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.
