---
layout: default
title: "[8.8]TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References"
---

# [8.8] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References

- Authors: Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
- [arXiv Link](https://arxiv.org/abs/2512.21641)
- [PDF Link](https://arxiv.org/pdf/2512.21641.pdf)

## Subfields
 3D Visual Grounding / Multimodal Tracking
## Reason for Interest

该论文提出了一个针对自动驾驶动态场景的时序多模态3D定位框架TrackTeller，核心创新在于解决了基于'行为'（如运动状态、短期交互）的自然语言目标定位问题。论文设计了LiDAR-图像融合的UniScene表达，并结合历史记忆与未来预测的时序推理模块，显著增强了对动态描述的理解能力。实验在NuPrompt大规模数据集上进行，结果显示其在保持高召回率的同时大幅降低了误报率，证明了方法在复杂动态交通场景下的鲁棒性，对人车交互和端到端自动驾驶具有重要的研究价值。
## Abstract: 
Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.
