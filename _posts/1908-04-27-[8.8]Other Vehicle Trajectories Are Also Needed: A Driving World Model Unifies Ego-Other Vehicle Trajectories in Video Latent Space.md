---
layout: default
title: "[8.8]Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space"
---

# [8.8] Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space

- Authors: Jian Zhu, Zhengyu Jia, Tian Gao, Jiaxin Deng, Shidi Li, Lang Zhang, Fu Liu, Peng Jia, Xia...
- [arXiv Link](https://arxiv.org/abs/2503.09215)
- [PDF Link](https://arxiv.org/pdf/2503.09215.pdf)

## Subfields
 自动驾驶仿真 / 世界模型 (World Model)
## Reason for Interest

论文针对当前驾驶世界模型（World Model）仅能控制自车（Ego）轨迹而无法精确控制他车（Other Vehicles）交互的痛点，提出了一种基于视频潜空间统一表征的方法（EOT-WM）。

创新点：
1. 提出将BEV轨迹投影到图像空间并编码为“轨迹视频”，解决了BEV空间特征与视频生成潜空间（Latent Space）不对齐的问题。
2. 设计了TiDiT（Trajectory-injected Diffusion Transformer），实现了对多车交互场景的细粒度控制。

价值与潜力：
1. 实验数据显著优于Vista、Drive-WM等现有SOTA，生成的视频在多车交互场景下不仅画质更高（FID），且时序一致性（FVD）大幅提升。
2. 对于端到端自动驾驶系统的闭环仿真测试具有极高价值，允许开发者通过编辑他车轨迹生成Corner Case（如切入、急停），是迈向高保真数据驱动仿真的重要一步。
3. 作者来自理想汽车（Li Auto），具备明确的量产落地背景。
## Abstract: 
Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In this paper, we propose a driving World Model named EOT-WM, unifying Ego-Other vehicle Trajectories in videos for driving simulation. Specifically, it remains a challenge to match multiple trajectories in the BEV space with each vehicle in the video to control the video generation. We first project ego-other vehicle trajectories in the BEV space into the image coordinate for vehicle-trajectory match via pixel positions. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30% in FID and 55% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.
