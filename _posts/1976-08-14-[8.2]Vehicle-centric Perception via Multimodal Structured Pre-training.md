---
layout: default
title: "[8.2]Vehicle-centric Perception via Multimodal Structured Pre-training"
---

# [8.2] Vehicle-centric Perception via Multimodal Structured Pre-training

- Authors: Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo
- [arXiv Link](https://arxiv.org/abs/2512.19934v1)
- [PDF Link](https://arxiv.org/pdf/2512.19934v1.pdf)

## Subfields
 2D Perception / Visual Foundation Models
## Reason for Interest

The paper addresses a foundational gap in autonomous driving perception: the lack of vehicle-specific priors in general vision pre-training. 

1. **Innovation**: It proposes 'VehicleMAE-V2', a novel Masked Auto-Encoder framework that integrates vehicle symmetry, contour structure, and semantic (text) alignment. This domain-specific inductive bias is a logical and effective improvement over vanilla MAE.
2. **Contribution**: The release of 'Autobot4M' (4 million vehicle images) is a significant contribution to the community, aggregating diverse data sources (surveillance, driving scenes like SODA10M, internet).
3. **Performance**: The method demonstrates consistent SOTA performance across five downstream tasks (Detection, Re-ID, Attributes, Fine-grained recognition, Segmentation). The ablation studies clearly justify the new modules (SMM, CRM, SRM).
4. **Relevance**: While the tasks are primarily 2D and heavily overlap with intelligent transportation systems (surveillance) rather than purely ego-centric 3D AD, the improved representation learning for vehicles is highly transferable to on-board perception stacks. The approach provides a strong backbone for any camera-based vehicle understanding system.
## Abstract: 

