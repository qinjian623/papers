---
layout: default
title: "[8.6]Language-Guided Open-World Anomaly Segmentation"
---

# [8.6] Language-Guided Open-World Anomaly Segmentation

- Authors: Klara Reichard, Nikolas Brasch, Nassir Navab, Federico Tombari
- [arXiv Link](https://arxiv.org/abs/2512.01427)
- [PDF Link](https://arxiv.org/pdf/2512.01427.pdf)

## Subfields
 感知 / 开放世界语义分割与异常检测
## Reason for Interest

论文提出了一种名为Clipomaly的基于CLIP的零样本（Zero-shot）开放世界异常分割方法，直接针对自动驾驶长尾问题。1. 创新性强：不同于传统仅输出二值掩码的异常检测，该方法利用视觉-语言模型（VLM）不仅定位未知物体，还能为其分配语义标签（如'岩石'、'轮胎'），这对下游规控极其重要。2. 实验扎实：在RoadAnomaly上刷新了SOTA，且无需训练阶段见过异常数据（Zero-shot）；对比了RAM（大模型打标）和字典匹配两种策略，分析透彻。3. 实用潜力：由TUM和宝马联合发布，方法设计考虑了车端部署的词汇动态扩展需求，解决了封闭集语义分割的痛点，具有很高的行业应用价值。
## Abstract: 
Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.
