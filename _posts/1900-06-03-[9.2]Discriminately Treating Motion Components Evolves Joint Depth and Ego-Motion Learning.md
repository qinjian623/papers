---
layout: default
title: "[9.2]Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning"
---

# [9.2] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning

- Authors: Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli W...
- [arXiv Link](https://arxiv.org/abs/2511.01502)
- [PDF Link](https://arxiv.org/pdf/2511.01502.pdf)

## Subfields
 3D感知 / 单目深度估计 / 视觉里程计
## Reason for Interest

该论文提出了一种名为DiMoDE的无监督联合深度和自运动学习框架，核心创新点在于对运动分量进行区分性处理。论文通过深入分析旋转、切向平移和径向平移对刚性光流的影响，发现现有方法混淆或忽略了这些分量的独特几何特性。DiMoDE引入了显式几何约束，通过光学轴和成像平面对齐来分别引导和优化不同的运动分量，并将其重新表述为同轴和共面形式，从而通过封闭形式的几何关系相互推导深度和翻译分量，这在理论和实践上都具有很强的创新性。

实验部分非常完整和严谨。作者在KITTI Odometry、MIAS-Odom（新收集的挑战性数据集）、DDAD、nuScenes等多个公共数据集上进行了广泛评估，并与多种SOTA方法（包括几何、学习和混合方法）进行了全面比较。特别是在MIAS-Odom和nuScenes等挑战性数据集上，DiMoDE在视觉里程计和深度估计任务中均展现了卓越的性能。消融研究设计周密，清晰地验证了所提出约束（包括对PoseNet和DepthNet的约束）的有效性、渐进式训练策略的必要性以及模型对超参数的鲁棒性，极大地增强了结果的可信度。

该研究直接解决了自动驾驶中3D感知（特别是复杂环境中鲁棒的深度和姿态估计）的关键挑战。无监督学习范式降低了对昂贵标注数据的依赖，而轻量级网络架构则有望实现实时推理。DiMoDE在恶劣条件（如大旋转、相机抖动、曝光不足、运动模糊）下的出色性能，使其在自动驾驶领域具有巨大的行业应用潜力。论文严谨的分析、详尽的实验以及对SOTA声明的谨慎措辞（例如，承认在KITTI深度上仅具竞争力而非绝对SOTA），都体现了研究的高度可信度。
## Abstract: 
Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.
