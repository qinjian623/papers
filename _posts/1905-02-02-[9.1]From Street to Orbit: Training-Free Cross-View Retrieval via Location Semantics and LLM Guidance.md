---
layout: default
title: "[9.1]From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance"
---

# [9.1] From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance

- Authors: Jeongho Min, Dongyoung Kim, Jaehyup Lee
- [arXiv Link](https://arxiv.org/abs/2511.09820)
- [PDF Link](https://arxiv.org/pdf/2511.09820.pdf)

## Subfields
 定位与感知 / 跨视角图像检索 / GPS拒止环境下的定位
## Reason for Interest

这篇论文提出了一种创新且高度实用的训练无关、LLM引导的跨视角图像检索框架，旨在解决自动驾驶和自主导航在GPS拒止环境中定位的关键难题。其核心创新点包括：

1.  **高度创新性**：首次将LLM与预训练视觉编码器结合，用于从单目街景图像的文本语境中推断地理语义，进而生成卫星查询图像，绕过了传统方法在街景与卫星图之间巨大的域差异和视角差异带来的视觉匹配挑战。这种训练无关的范式极大地降低了对大规模标注数据集和昂贵训练的需求。
2.  **强大的零样本能力**：在没有额外训练或成对监督的情况下，方法在University-1652等基准测试中达到了SOTA性能，展现了其卓越的泛化能力和鲁棒性。
3.  **实用且可扩展**：该方法仅需单目街景图像作为输入，这与车载自动驾驶系统常用的感知设备高度吻合，且能够利用现有的地图API进行地理编码和卫星图像生成。PCA白化等轻量级特征优化在推理时应用，进一步提升了检索鲁棒性。此外，其框架能够自动构建语义对齐的街景-卫星图像对，为未来学习型方法提供了可扩展、低成本的数据集生成途径。
4.  **实验完整性与可信度**：论文在University-1652和ILIAS两个主流数据集上进行了充分的实验验证，并与多种SOTA基线方法进行了严格对比。消融研究清晰地展示了PCA白化以及不同预训练编码器的有效性，增强了结果的可信度。
5.  **巨大的行业潜力**：对于自动驾驶而言，在城市峡谷、隧道、远程区域等GPS信号弱或缺失的环境中实现精确、鲁棒的定位至关重要。本文提出的方法为解决这一关键挑战提供了一个极具前景的解决方案，其训练无关、低成本、高可扩展性的特点使其在实际部署中具有很高的价值。尽管对外部API的依赖可能带来一些潜在的限制，但整体的框架和效果仍令人印象深刻。鉴于其直接与车端自动驾驶的核心功能（定位）相关联，且在多个维度上表现出色，因此给予高分。
## Abstract: 
Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.
