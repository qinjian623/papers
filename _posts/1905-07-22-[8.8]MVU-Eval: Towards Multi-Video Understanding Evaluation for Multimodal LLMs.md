---
layout: default
title: "[8.8]MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs"
---

# [8.8] MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs

- Authors: Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang...
- [arXiv Link](https://arxiv.org/abs/2511.07250)
- [PDF Link](https://arxiv.org/pdf/2511.07250.pdf)

## Subfields
 多模态大模型评估 / 多视频理解 / 自动驾驶多传感器融合与感知
## Reason for Interest

该论文提出了MVU-Eval，这是首个全面评估多模态大模型（MLLMs）多视频理解能力的基准测试。其创新性极强，填补了现有视频理解基准主要限于单视频输入的空白。研究将多视频理解能力分解为八项核心任务（包括感知和高阶推理），并设计了1,824个精心策划的问题-答案对，涵盖4,959个视频。数据构建过程严谨，包含自动化生成、信息泄露移除、难度过滤和大量人工验证，确保了数据的高质量和可信度。

从自动驾驶研究的角度来看，该工作具有重要的价值和潜力。论文明确指出，该基准测试与“自动驾驶中的多传感器融合”（multi-sensor synthesis in autonomous systems）以及“需要多摄像头信息的自动驾驶”（autonomous driving requiring information from multiple cameras）等实际应用紧密对齐。特别地，其“空间理解”（Spatial Understanding）任务旨在评估模型从互补相机角度建模空间布局的能力，这直接模拟了自动驾驶多相机融合感知中的关键挑战。此外，“异步多视频场景中的时间推理”（Temporal Reasoning in Asynchronous Multi-Video Scenarios）也对自动驾驶不同传感器时间未对齐的数据融合至关重要。数据集中明确包含“自动驾驶”（Autonomous Driving）类别的视频，进一步强化了其直接相关性。

尽管论文本身不提出自动驾驶算法，但它提供了一个评估基础模型（MLLMs）关键能力的工具，这些基础模型将是未来自动驾驶系统多传感器融合和复杂环境理解的核心。通过揭示当前MLLMs在多视频理解方面的局限性，该基准为开发更强大、更鲁棒的自动驾驶感知系统指明了方向。实验部分对多个主流开源和闭源MLLMs进行了详尽评估，并进行了消融实验分析输入帧数、分辨率和输入格式对性能的影响，分析全面。

综合来看，MVU-Eval是一个高质量、创新性强的基准测试，其对自动驾驶多传感器融合和复杂环境感知领域具有显著的直接和间接贡献，为未来智能驾驶感知大模型的发展提供了重要的评估工具。
## Abstract: 
The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.
