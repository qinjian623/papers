---
layout: default
title: "[8.9]DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning"
---

# [8.9] DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning

- Authors: Zhe Liu, Runhui Huang, Rui Yang, Siming Yan, Zining Wang, Lu Hou, Di Lin, Xiang Bai, Heng...
- [arXiv Link](https://arxiv.org/abs/2512.12799)
- [PDF Link](https://arxiv.org/pdf/2512.12799.pdf)

## Subfields
 端到端自动驾驶 / VLA (Vision-Language-Action) 模型 / 3D 占据感知
## Reason for Interest

论文提出了 DrivePI，一种创新的端到端自动驾驶 VLA 框架。其核心贡献在于有效地将 MLLM 的语义理解能力与 LiDAR/多视角相机的精确 3D 空间感知（占据栅格、占据流）相结合，解决了传统 VLA 模型空间感知弱和传统 VA 模型缺乏可解释性的痛点。

主要亮点：
1. **架构创新**：提出了并行的粗粒度语言理解与细粒度 3D 感知/预测头（Occupancy & Flow），并通过 LiDAR 增强几何信息，方案设计合理且切中行业痛点。
2. **效果显著**：仅使用 0.5B 参数的轻量级 Backbone，在感知、预测、规划和问答四大任务上均全面超越了现有的更大参数模型（如 7B 的 OpenDriveVLA）和专用端到端模型（如 VAD），展现了极高的效率和性能潜力。
3. **实验详实**：在多个权威数据集（nuScenes, OpenOcc）上进行了全面验证，Ablation Study 充分证明了多模态融合与多任务协同的有效性。
4. **行业价值**：该方法为低算力平台部署高性能端到端大模型提供了可行路径，且显著提升了规划的安全性和可解释性，具有很高的落地参考价值。
## Abstract: 
Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI
