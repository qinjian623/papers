---
layout: default
title: "[5.0]VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving"
---

# [5.0] VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving

- Authors: Hyunki Seong, Seongwoo Moon, Hojin Ahn, Jehun Kang, David Hyunchul Shim
- [arXiv Link](https://arxiv.org/abs/2511.12405v1)
- [PDF Link](https://arxiv.org/pdf/2511.12405v1.pdf)

## Subfields
 端到端自动驾驶 / 开放世界野外导航 / 具身智能
## Reason for Interest

1. 场景与平台局限（主要扣分点）：论文实验基于Clearpath Jackal移动机器人平台，在野外非结构化环境（草地、树林）进行低速测试，属于‘移动机器人/具身智能’范畴，而非典型的‘车端’自动驾驶（如城市道路、高速场景）。且未包含动态交通参与者，不符合车端自动驾驶的复杂性要求，依据评分标准上限为5分。
2. 数据规模极小：仅使用约2小时（3.6万帧）的自建数据进行训练，远低于自动驾驶领域通常所需的数据量级，泛化性结论的说服力有限。
3. 方法创新性：提出‘视觉-语言动作检索（Retrieval）’范式代替传统的回归或分类，利用冻结的YOLOE进行开放世界特征提取，思路新颖，具备一定的可解释性。
4. 总结：这是一篇不错的机器人野外导航论文，但作为‘自动驾驶’研究，缺乏在标准车端数据集（如nuScenes, Waymo）或真车平台上的验证，行业参考价值受限。
## Abstract: 

