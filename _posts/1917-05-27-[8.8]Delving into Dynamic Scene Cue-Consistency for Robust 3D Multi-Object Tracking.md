---
layout: default
title: "[8.8]Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking"
---

# [8.8] Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking

- Authors: Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, Zheng Yang
- [arXiv Link](https://arxiv.org/abs/2508.11323)
- [PDF Link](https://arxiv.org/pdf/2508.11323.pdf)

## Subfields
 3D Multi-Object Tracking (3D MOT)
## Reason for Interest

论文提出了一种基于动态场景线索一致性（DSC-Track）的 3D MOT 方法，创新地引入了点对特征（PPF）和几何一致性 Transformer 来解决复杂场景下的关联模糊问题。这种方法不再单纯依赖单一对象的运动预测（如卡尔曼滤波），而是利用物体间的相对几何结构进行关联，极大地提高了跟踪的稳定性，显著降低了 ID 切换（IDS）。实验在 nuScenes 和 Waymo 上均取得了 SOTA 成绩，且推理速度达到 26.77 FPS，具备极高的车端落地潜力和学术价值。
## Abstract: 
3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
