---
layout: default
title: "[8.2]Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard"
---

# [8.2] Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard

- Authors: Henry Wong, Clement Fung, Weiran Lin, Karen Li, Stanley Chen, Lujo Bauer
- [arXiv Link](https://arxiv.org/abs/2511.14876v1)
- [PDF Link](https://arxiv.org/pdf/2511.14876v1.pdf)

## Subfields
 自动驾驶安全 / 对抗性鲁棒性评估 (Safety & Adversarial Robustness)
## Reason for Interest

论文极具批判性思维，指出了现有对抗攻击研究往往脱离自动驾驶系统全链路（如PID控制和规则约束）的盲点。通过在标准化的CARLA Leaderboard上进行全系统攻击测试，作者有力地证明了'模型级脆弱性'并不等同于'系统级危险'，并量化了规则模块对攻击的防御作用。这种'Holistic Evaluation'的方法论对自动驾驶安全验证具有重要的纠偏和指导意义，尽管仅限于仿真环境，但逻辑严密，实验设计科学。
## Abstract: 
  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.
  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.
