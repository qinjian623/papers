---
layout: default
title: "[9.2]FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection"
---

# [9.2] FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection

- Authors: Jiangyong Yu, Changyong Shu, Sifan Zhou, Zichen Yu, Xing Hu, Yan Chen, Dawei Yang
- [arXiv Link](https://arxiv.org/abs/2502.15488)
- [PDF Link](https://arxiv.org/pdf/2502.15488.pdf)

## Subfields
 3D感知 / 模型量化与部署
## Reason for Interest

该论文针对自动驾驶中高性能Transformer模型（PETR系列）难以在车端低算力平台部署的痛点，提出了极具实战价值的解决方案。其核心贡献在于深刻剖析了PETR量化失败的根本原因（位置编码与图像特征的量级失衡），并通过算法-硬件协同设计（Co-design）的方式，提出了量化友好的位置编码（QFPE）和双查找表（DULUT）非线性算子近似方法。实验结果表明，该方法能让PETR系列模型在INT8精度下近乎无损运行，解决了从SOTA算法到工程落地的关键“最后一公里”问题，对于自动驾驶量产落地具有极高的参考价值。
## Abstract: 
Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.
