---
layout: default
title: "[8.5]BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining"
---

# [8.5] BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining

- Authors: Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt
- [arXiv Link](https://arxiv.org/abs/2510.18244v1)
- [PDF Link](https://arxiv.org/pdf/2510.18244v1.pdf)

## Subfields
 3D Perception / Zero-Shot Object Classification
## Reason for Interest

论文针对自动驾驶中长尾物体识别难题，提出了一种基于课程学习（Curriculum Learning）的多模态数据混合策略（BlendCLIP）。该方法创新性地解决了合成数据与真实LiDAR数据之间的域差异问题，具有极高的行业应用价值：
1. **效果显著**：在真实自动驾驶数据集（nuScenes, TruckScenes）上取得了大幅领先的 Zero-shot 分类精度，尤其在跨传感器/跨域（TruckScenes）测试中表现出强大的泛化能力。
2. **标签效率高**：仅需引入 1.5% 的真实标注数据即可带来 27% 的性能提升，极大降低了对大规模3D标注的依赖。
3. **实验完整**：涵盖了合成域、真实域及跨域评估，并提供了详细的消融实验（如混合比例、遮挡增强等）。
4. **局限性明确**：虽然在稀疏物体上的表现略逊于纯合成数据训练的模型（这也是扣分点之一），但在高可见度区域（对安全最关键）的提升具有决定性意义。
## Abstract: 
  We introduce BlendCLIP, a multimodal pretraining framework that bridges this synthetic-to-real gap by strategically combining the strengths of both domains. We first propose a pipeline to generate a large-scale dataset of object-level triplets -- consisting of a point cloud, image, and text description -- mined directly from real-world driving data and human annotated 3D boxes. Our core contribution is a curriculum-based data mixing strategy that first grounds the model in the semantically rich synthetic CAD data before progressively adapting it to the specific characteristics of real-world scans.
  Our experiments show that our approach is highly label-efficient: introducing as few as 1.5\% real-world samples per batch into training boosts zero-shot accuracy on the nuScenes benchmark by 27\%. Consequently, our final model achieves state-of-the-art performance on challenging outdoor datasets like nuScenes and TruckScenes, improving over the best prior method by 19.3\% on nuScenes, while maintaining strong generalization on diverse synthetic benchmarks. Our findings demonstrate that effective domain adaptation, not full-scale real-world annotation, is the key to unlocking robust open-vocabulary 3D perception. Our code and dataset will be released upon acceptance on https://github.com/kesu1/BlendCLIP.
