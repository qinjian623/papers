---
layout: default
title: "[9.3]V2X-Radar: A Multi-modal Dataset with 4D Radar for Cooperative Perception"
---

# [9.3] V2X-Radar: A Multi-modal Dataset with 4D Radar for Cooperative Perception

- Authors: Lei Yang, Xinyu Zhang, Jun Li, Chen Wang, Jiaqi Ma, Zhiying Song, Tong Zhao, Ziying Song,...
- [arXiv Link](https://arxiv.org/abs/2411.10962)
- [PDF Link](https://arxiv.org/pdf/2411.10962.pdf)

## Subfields
 合作感知 / 多模态3D目标检测 / 数据集
## Reason for Interest

该论文提出了V2X-Radar，这是首个将4D毫米波雷达引入协同感知领域的大规模真实世界多模态数据集。这填补了现有协同感知数据集主要关注相机和激光雷达而忽视4D雷达的空白，而4D雷达在恶劣天气下具有卓越的感知鲁棒性，对自动驾驶的实际落地至关重要，因此创新性极强。

数据集的收集过程非常严谨，通过网联车辆平台和智能路侧单元，配备了4D雷达、激光雷达和多视角相机。数据涵盖了晴天、雨天、雾天、雪天等多种天气条件，以及白天、黄昏、夜晚等不同时间段，场景多样性高，能够支持对鲁棒感知算法的全面评估。传感器同步和标定工作描述详细且精确，保证了数据质量。

实验部分不仅提供了详细的数据分析，还为单车端（路侧和车载）以及协同感知任务的3D目标检测建立了全面的基准。这些基准涵盖了多种主流的感知算法，并进行了关键的消融研究：例如，分析了通信延迟对协同感知性能的显著影响，以及4D雷达（特别是多普勒信息）在恶劣天气下对感知的提升作用，证明了其与激光雷达和相机的互补性。这些发现对未来协同感知算法的设计具有重要的指导意义。

从行业潜力来看，4D雷达在量产车上的应用日益广泛，而协同感知是突破单车感知局限性的关键。V2X-Radar数据集将这两者结合，直接服务于开发更安全、更鲁棒的自动驾驶系统，具有巨大的工业价值。论文承诺发布数据集和代码库，将极大促进相关研究的进展。综合来看，这是一项高质量、具有深远影响力的工作。
## Abstract: 
Modern autonomous vehicle perception systems often struggle with occlusions and limited perception range. Previous studies have demonstrated the effectiveness of cooperative perception in extending the perception range and overcoming occlusions, thereby enhancing the safety of autonomous driving. In recent years, a series of cooperative perception datasets have emerged; however, these datasets primarily focus on cameras and LiDAR, neglecting 4D Radar, a sensor used in single-vehicle autonomous driving to provide robust perception in adverse weather conditions. In this paper, to bridge the gap created by the absence of 4D Radar datasets in cooperative perception, we present V2X-Radar, the first large-scale, real-world multi-modal dataset featuring 4D Radar. V2X-Radar dataset is collected using a connected vehicle platform and an intelligent roadside unit equipped with 4D Radar, LiDAR, and multi-view cameras. The collected data encompasses sunny and rainy weather conditions, spanning daytime, dusk, and nighttime, as well as various typical challenging scenarios. The dataset consists of 20K LiDAR frames, 40K camera images, and 20K 4D Radar data, including 350K annotated boxes across five categories. To support various research domains, we have established V2X-Radar-C for cooperative perception, V2X-Radar-I for roadside perception, and V2X-Radar-V for single-vehicle perception. Furthermore, we provide comprehensive benchmarks across these three sub-datasets. We will release all datasets and benchmark codebase at https://huggingface.co/datasets/yanglei18/V2X-Radar and https://github.com/yanglei18/V2X-Radar.
