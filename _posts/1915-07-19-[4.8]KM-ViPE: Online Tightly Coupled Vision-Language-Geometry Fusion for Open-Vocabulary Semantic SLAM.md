---
layout: default
title: "[4.8]KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM"
---

# [4.8] KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM

- Authors: Zaid Nasser, Mikhail Iumanov, Tianhao Li, Maxim Popov, Jaafar Mahmoud, Malik Mohrat, Ilya...
- [arXiv Link](https://arxiv.org/abs/2512.01889)
- [PDF Link](https://arxiv.org/pdf/2512.01889.pdf)

## Subfields
 vSLAM / 语义建图 (Semantic Mapping)
## Reason for Interest

论文提出了一种利用视觉大模型（DINO）特征紧耦合到后端优化（Bundle Adjustment）中的SLAM方法，通过自适应鲁棒核函数（ARK）有效处理动态场景中的外点，创新性较强。然而，该研究明确针对‘以人为中心的（Ego-centric）’AR/VR及室内机器人应用场景，主要实验均基于室内数据集（TUM-RGBD, Replica, ARIA），且使用的是无标定单目相机设置，这与车端自动驾驶（通常为标定多传感器、室外大场景、高动态）的直接应用场景存在较大差异。尽管动态场景处理技术对自动驾驶有参考价值，但根据评分规则‘如果不直接和车端自动驾驶相关，最多5分’，给出4.8分。
## Abstract: 
We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.
