---
layout: default
title: "[3.5]Intelligent Image Search Algorithms Fusing Visual Large Models"
---

# [3.5] Intelligent Image Search Algorithms Fusing Visual Large Models

- Authors: Kehan Wang, Tingqiong Cui, Yang Zhang, Yu Chen, Shifeng Wu, Zhenzhang Li
- [arXiv Link](https://arxiv.org/abs/2511.19920)
- [PDF Link](https://arxiv.org/pdf/2511.19920.pdf)

## Subfields
 智能交通监控 / 细粒度图像检索 (Traffic Surveillance / Fine-grained Image Retrieval)
## Reason for Interest

1. **方向偏差**：论文核心任务是基于监控视频的‘图像检索’（如查找特定状态的车辆），属于安防或智能交通管理（V2X基础设施端）应用，而非车端自动驾驶的实时感知或规划控制，符合‘非直接车端相关’的扣分标准。
2. **创新性有限**：提出的‘检测器(YOLO) + VLM(Qwen-VL)’级联架构属于目前工业界常见的工程组合模式。虽引入了动态提示词优化（Dynamic Prompt Optimization），但本质是基于规则的启发式调整，缺乏深度的算法创新。
3. **实验局限**：实验完全基于非公开的自建数据集，缺乏公信力。对比基线仅选择了闭集训练的YOLO检测器，未对比当前主流的开放词汇检测（如Grounding DINO）或多模态检索模型（如CLIP及其变体），导致‘SOTA’声明的说服力极低。
4. **零样本评估问题**：在零样本测试（如检测戴口罩）中，将具备零样本能力的VLM与未在对应类别训练的YOLO进行对比，这种比较方式在学术上是不公平且显而易见的。
综上，该论文更偏向于安防领域的工程应用报告，对自动驾驶核心研究价值较低。
## Abstract: 
Fine-grained image retrieval, which aims to find images containing specific object components and assess their detailed states, is critical in fields like security and industrial inspection. However, conventional methods face significant limitations: manual features (e.g., SIFT) lack robustness; deep learning-based detectors (e.g., YOLO) can identify component presence but cannot perform state-specific retrieval or zero-shot search; Visual Large Models (VLMs) offer semantic and zero-shot capabilities but suffer from poor spatial grounding and high computational cost, making them inefficient for direct retrieval. To bridge these gaps, this paper proposes DetVLM, a novel intelligent image search framework that synergistically fuses object detection with VLMs. The framework pioneers a search-enhancement paradigm via a two-stage pipeline: a YOLO detector first conducts efficient, high-recall component-level screening to determine component presence; then, a VLM acts as a recall-enhancement unit, performing secondary verification for components missed by the detector. This architecture directly enables two advanced capabilities: 1) State Search: Guided by task-specific prompts, the VLM refines results by verifying component existence and executing sophisticated state judgments (e.g., "sun visor lowered"), allowing retrieval based on component state. 2) Zero-shot Search: The framework leverages the VLM's inherent zero-shot capability to recognize and retrieve images containing unseen components or attributes (e.g., "driver wearing a mask") without any task-specific training. Experiments on a vehicle component dataset show DetVLM achieves a state-of-the-art overall retrieval accuracy of 94.82\%, significantly outperforming detection-only baselines. It also attains 94.95\% accuracy in zero-shot search for driver mask-wearing and over 90\% average accuracy in state search tasks.
