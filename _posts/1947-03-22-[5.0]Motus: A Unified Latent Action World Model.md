---
layout: default
title: "[5.0]Motus: A Unified Latent Action World Model"
---

# [5.0] Motus: A Unified Latent Action World Model

- Authors: Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao...
- [arXiv Link](https://arxiv.org/abs/2512.13030)
- [PDF Link](https://arxiv.org/pdf/2512.13030.pdf)

## Subfields
 具身智能 / 世界模型 (机器人操控)
## Reason for Interest

论文提出了Motus，一种基于混合Transformer（MoT）架构的统一潜在动作世界模型，能够整合视觉语言理解、视频生成、逆动力学等多种能力。其核心创新在于利用光流提取‘潜在动作’，从而有效利用大规模无标签视频数据进行预训练。虽然该方法在具身智能和机器人操控领域表现出卓越的性能（SOTA）且技术路线（统一生成式世界模型）对自动驾驶极具启发性，但论文实验完全基于机械臂操控（如RoboTwin、Aloha），未涉及车端自动驾驶场景或数据集。根据评分规则，非车端自动驾驶相关论文最高不超过5分。
## Abstract: 
While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.
