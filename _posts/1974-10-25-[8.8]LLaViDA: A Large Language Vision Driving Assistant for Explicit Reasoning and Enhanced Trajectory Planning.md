---
layout: default
title: "[8.8]LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning"
---

# [8.8] LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning

- Authors: Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, J...
- [arXiv Link](https://arxiv.org/abs/2512.18211)
- [PDF Link](https://arxiv.org/pdf/2512.18211.pdf)

## Subfields
 End-to-End Autonomous Driving / VLM-based Trajectory Planning
## Reason for Interest

论文提出 LLaViDA 框架，核心创新在于引入轨迹偏好优化 (TPO)，巧妙利用 DPO 思想将 VLM 的离散 Token 生成与连续轨迹回归对齐，解决了 VLM 在数值规划上的精度痛点。实验设计严谨，在 ST-P3 和 UniAD 两种主流评估协议下均取得了显著优于现有 SOTA（包括 UniAD 和 GPT-Driver）的成绩。同时开源了 NuScenes-TP 数据集和推理链标注，对社区有较大贡献。主要扣分点在于仅进行了开环评估（Open-loop），缺乏闭环（Closed-loop）仿真验证，且推理延迟（~776ms）对于实车部署仍具挑战。
## Abstract: 
Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful "VLM Trajectory Planner for Autonomous Driving." On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.
