---
layout: default
title: "[4.5]Zoo3D: Zero-Shot 3D Object Detection at Scene Level"
---

# [4.5] Zoo3D: Zero-Shot 3D Object Detection at Scene Level

- Authors: Andrey Lemeshko, Bulat Gabdullin, Nikita Drozdov, Anton Konushin, Danila Rukhovich, Maksi...
- [arXiv Link](https://arxiv.org/abs/2511.20253)
- [PDF Link](https://arxiv.org/pdf/2511.20253.pdf)

## Subfields
 Open-Vocabulary 3D Object Detection (Indoor)
## Reason for Interest

论文提出了一种基于基础模型（CLIP, SAM, DUSt3R）的零样本和自监督3D目标检测框架，利用2D分割和图聚类生成3D包围框，并实现了从无位姿图像到3D检测的端到端能力。尽管该方法在ScanNet等室内数据集上取得了SOTA效果，且开放词汇（Open-Vocabulary）感知是自动驾驶的前沿方向，但该研究完全聚焦于室内静态场景（Indoor Scenes），依赖于密集重建和多视角一致性，未在nuScenes或Waymo等室外自动驾驶数据集上进行验证。其技术路线（基于Mesh/密集点云的MaskClustering）与主流车端LiDAR/BEV感知方案存在较大的域差异（Domain Gap），因此对车端自动驾驶的直接应用价值有限，根据评分规则给予最高非相关类评分。
## Abstract: 
3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .
