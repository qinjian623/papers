---
layout: default
title: "[8.5]Improving Partially Observed Trajectories Forecasting by Target-driven Self-Distillation"
---

# [8.5] Improving Partially Observed Trajectories Forecasting by Target-driven Self-Distillation

- Authors: Peng Shu, Pengfei Zhu, Mengshi Qi, Liang Liu
- [arXiv Link](https://arxiv.org/abs/2501.16767)
- [PDF Link](https://arxiv.org/pdf/2501.16767.pdf)

## Subfields
 轨迹预测 (Motion Forecasting)
## Reason for Interest

该论文针对自动驾驶感知中常见的上游跟踪丢失（Partial Observation）问题，提出了一种目标驱动的自蒸馏（TSD）框架。创新点在于摒弃了传统昂贵的Teacher-Student两阶段训练，利用自蒸馏和MMD损失在单阶段内实现特征对齐，并结合无锚点目标生成器引导预测。实验在三个主流大数据集上验证了该方法不仅大幅提升了模型在丢帧、遮挡情况下的鲁棒性，甚至反向提升了全观测场景下的预测精度。工作具有很高的工程落地价值和学术参考意义。
## Abstract: 
Accurate prediction of future trajectories of traffic agents is essential for ensuring safe autonomous driving. However, partially observed trajectories can significantly degrade the performance of even state-of-the-art models. Previous approaches often rely on knowledge distillation to transfer features from fully observed trajectories to partially observed ones. This involves firstly training a fully observed model and then using a distillation process to create the final model. While effective, they require multi-stage training, making the training process very expensive. Moreover, knowledge distillation can lead to a performance degradation of the model. In this paper, we introduce a Target-drivenSelf-Distillation method (TSD) for motion forecasting. Our method leverages predicted accurate targets to guide the model in making predictions under partial observation conditions. By employing self-distillation, the model learns from the feature distributions of both fully observed and partially observed trajectories during a single end-to-end training process. This enhances the model's ability to predict motion accurately in both fully observed and partially observed scenarios. We evaluate our method on multiple datasets and state-of-the-art motion forecasting models. Extensive experimental results demonstrate that our approach achieves significant performance improvements in both settings. To facilitate further research, we will release our code and model checkpoints.
