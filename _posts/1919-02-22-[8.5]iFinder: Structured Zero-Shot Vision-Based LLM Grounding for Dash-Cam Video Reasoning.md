---
layout: default
title: "[8.5]iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning"
---

# [8.5] iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning

- Authors: Manyi Yao, Bingbing Zhuang, Sparsh Garg, Amit Roy-Chowdhury, Christian Shelton, Manmohan ...
- [arXiv Link](https://arxiv.org/abs/2509.19552)
- [PDF Link](https://arxiv.org/pdf/2509.19552.pdf)

## Subfields
 自动驾驶视频理解与问答 / 驾驶场景认知 (Driving Video Understanding & VQA)
## Reason for Interest

1. 创新性：针对端到端多模态大模型（V-VLM）在驾驶场景中空间推理弱、易幻觉的问题，提出了一种基于显式感知模块（3D检测、车道线、自车运动估计等）生成结构化JSON描述的框架（iFinder），实现了感知与推理的解耦，思路清晰且实用。
2. 实验完整性：在四个主流数据集（MM-AU, SUTD, LingoQA, Nexar）上进行了广泛的测试，包含消融实验（如天气影响、模块贡献度），证明了结构化输入对提升推理可靠性的关键作用。
3. 结果可信度：在零样本设定下大幅超越现有的通用及驾驶专用VLM（如DriveMM），特别是在需要精细时空推理的事故分析任务中提升显著（最高提升39%）。
4. 行业价值：虽然推理延迟较高（非实时），但该方法为离线数据挖掘、长尾场景分析和事故责任归因提供了高可解释性、高精度的解决方案，具有重要的工程应用潜力。
## Abstract: 
Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.
