---
layout: default
title: "[8.2]QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents"
---

# [8.2] QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents

- Authors: Jordan Abi Nader, David Lee, Nathaniel Dennler, Andreea Bobu
- [arXiv Link](https://arxiv.org/abs/2511.17855)
- [PDF Link](https://arxiv.org/pdf/2511.17855.pdf)

## Subfields
 自动驾驶人机交互 / 偏好学习 (HRI / Preference Learning)
## Reason for Interest

论文针对自动驾驶中‘物理接管意图模糊’与‘语言指令缺乏物理落地’的痛点，提出了一种新颖的贝叶斯框架（QuickLAP）。创新性地利用大语言模型（LLM）解析自然语言反馈，生成注意力掩码和偏好偏移量，将其作为概率观测与物理接管数据融合，实现实时的奖励函数更新。方法论逻辑严密，数学推导清晰。实验部分不仅包含充分的仿真消融实验，还进行了15人的真人用户研究，证明了该方法在理解用户意图和提升协作体验方面的有效性。虽然实验基于简化的线性奖励函数假设，且主要适用于L2+/L3级辅助驾驶或个性化驾驶场景，但其多模态交互学习的思路对行业具有较高的启发价值。
## Abstract: 
Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP.
