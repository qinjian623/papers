---
layout: default
title: "[4.5]CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation"
---

# [4.5] CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation

- Authors: Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie
- [arXiv Link](https://arxiv.org/abs/2511.03992)
- [PDF Link](https://arxiv.org/pdf/2511.03992.pdf)

## Subfields
 3D场景理解 / 3D高斯辐射场指代分割 (Referring 3D Gaussian Splatting Segmentation)
## Reason for Interest

论文提出CaRF框架，旨在解决3D高斯辐射场指代分割(R3DGS)中跨视角一致性不足的挑战。核心创新在于引入了高斯场相机编码(GFCE)和训练中配对视角监督(ITPVS)机制。GFCE通过将相机参数嵌入高斯特征空间，实现了视角感知的高斯-文本交互，增强了几何推理。ITPVS则通过在单个训练迭代中将相同的高斯投影到两个校准过的视角并施加联合损失，有效地解决了单视角过拟合问题并强制实现了跨视角的一致性。

**优点：**
1. **创新性强：** 针对R3DGS领域的核心痛点——跨视角一致性，提出了新颖且有效的解决方案。GFCE和ITPVS的设计巧妙，能够直接在3D高斯空间进行操作，避免了对2D伪监督和视角特定特征学习的依赖。
2. **实验完整且可信：** 在Ref-LERF、LERF-OVS和3D-OVS三个代表性基准数据集上进行了广泛实验，并声称超越了现有SOTA方法ReferSplat，取得了显著的性能提升。消融实验充分验证了GFCE和ITPVS各自以及协同工作的有效性，并对相机编码融合方式和训练视角数量进行了详细分析。
3. **效率较高：** CaRF在推理阶段几乎没有额外的计算开销，仅通过轻量级MLP增加了少量参数，在保持高效推理的同时实现了性能提升。

**局限性与自动驾驶相关性（严格评分依据）：**
论文多次提及“自动驾驶感知”是潜在应用之一，并引用了一些与开放词汇3D场景图和多模态3D映射相关的文献。然而，该研究的核心任务“指代3D高斯辐射场分割”——即根据自由形式的语言描述在静态的3DGS场景中定位对应的3D区域——与车端自动驾驶的**核心、实时感知任务（如车辆、行人、车道线、可行驶区域的动态检测与跟踪）存在较大差异**。目前自动驾驶主要依赖实时传感器数据处理，而3DGS通常用于静态场景的离线重建和理解，且论文中的方法存在“per-scene optimization”的限制，不直接适用于动态、大范围、实时变化的驾驶环境。因此，其对自动驾驶的直接价值主要体现在更高层次的场景理解、人机交互或辅助规划，而非直接的车端实时感知决策。根据评分标准“如果不直接和车端自动驾驶相关，最多5分”，本论文尽管在其专业领域内具有极高的技术价值和创新性，但在自动驾驶领域的直接应用上有所欠缺，故给出此分数。
## Abstract: 
Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.
