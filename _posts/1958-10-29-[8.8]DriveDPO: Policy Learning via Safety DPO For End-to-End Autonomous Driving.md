---
layout: default
title: "[8.8]DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving"
---

# [8.8] DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving

- Authors: Shuyao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, Zhaoxiang Zhang
- [arXiv Link](https://arxiv.org/abs/2509.17940v1)
- [PDF Link](https://arxiv.org/pdf/2509.17940v1.pdf)

## Subfields
 端到端自动驾驶 / 规划控制 (End-to-End Autonomous Driving / Planning)
## Reason for Interest

论文针对端到端模仿学习中‘像人但不安全’的问题，提出 DriveDPO 框架。核心创新在于结合规则安全分数的统一策略蒸馏（Unified Policy Distillation）和迭代式安全直接偏好优化（Safety DPO）。

1. 创新性：将 DPO 引入自动驾驶策略学习，并设计了基于模仿和距离的拒绝采样策略（Rejected Trajectory Selection）来构建偏好对，有效提升了模型对安全边界的理解，方法逻辑严密且符合大模型/RLHF 的技术趋势。
2. 实验完整性：在 NAVSIM（大规模非交互式仿真）和 Bench2Drive（闭环评估）两个主流基准上进行了详尽实验。对比了 DiffusionDrive、UniAD、VADv2 等强基准，且消融实验清晰地证明了 DPO 及不同采样策略的贡献。
3. 结果可信度：在 NAVSIM 上 PDMS 达到 90.0，不仅超过了所有端到端基线，还击败了基于真值感知的 PDM-closed 规则基线，这在端到端领域是一个显著的突破，证明了学习型策略在安全性上可以超越传统规则。
4. 行业价值：解决了端到端模型在长尾场景下安全性不足的痛点，通过结合规则监督和数据驱动优化，为实车部署提供了高价值的技术路径。
## Abstract: 

