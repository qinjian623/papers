---
layout: default
title: "[9.2]Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving"
---

# [9.2] Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving

- Authors: Sheng Yang, Tong Zhan, Guancheng Chen, Yanfeng Lu, Jian Wang
- [arXiv Link](https://arxiv.org/abs/2510.00060v2)
- [PDF Link](https://arxiv.org/pdf/2510.00060v2.pdf)

## Subfields
 端到端自动驾驶 / VLM 轨迹规划
## Reason for Interest

1. **创新性强且切中痛点**：论文指出了当前 VLM 应用于自动驾驶控制的核心矛盾——离散文本 Token 与连续物理轨迹之间的模态失配。通过引入特殊 Token 进行回归预测（Regression on Special Tokens）并结合物理距离损失（L2 Loss），取代了传统的交叉熵损失，这一简洁的设计（Less is More）在理论上更符合驾驶任务本质。
2. **SOTA 提升显著**：在 nuScenes 开环规划指标上取得了碾压式的性能提升。相比 UniAD、VAD 等专用架构以及 OpenDriveVLA 等 VLM 方法，误差大幅降低（例如 L2_avg 降低约 36%-68%），证明了纯 VLM 架构在端到端规划中的巨大潜力。
3. **实验充分与泛化性验证**：除了标准的 nuScenes 评估，论文还进行了严格的跨域零样本（Zero-shot）测试（View-of-Delft 和 Oxford RobotCar），验证了模型在不同国家、不同驾驶侧向（左/右舵）、不同传感器配置下的鲁棒性，这一点极大地增强了结果的可信度和行业价值。
4. **潜力巨大**：该方法证明了通用 VLM 经过特定适配（回归头设计）可以成为极其强大的驾驶策略模型，为大模型上车提供了高效、可行的技术路线。
## Abstract: 

