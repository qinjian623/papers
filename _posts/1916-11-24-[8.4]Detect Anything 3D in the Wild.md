---
layout: default
title: "[8.4]Detect Anything 3D in the Wild"
---

# [8.4] Detect Anything 3D in the Wild

- Authors: Hanxue Zhang, Haoran Jiang, Qingsong Yao, Yanan Sun, Renrui Zhang, Hao Zhao, Hongyang Li,...
- [arXiv Link](https://arxiv.org/abs/2504.07958)
- [PDF Link](https://arxiv.org/pdf/2504.07958.pdf)

## Subfields
 3D感知 / 单目3D目标检测 (Open-Vocabulary Detection)
## Reason for Interest

该论文提出了首个基于Prompt的通用3D检测基础模型DetAny3D，针对自动驾驶中的长尾物体检测（Open-World）和多传感器参数迁移（Cross-Camera）两大痛点提出了有效的解决方案。创新性在于结合SAM和DINO两大2D基础模型，并通过Zero-Embedding Mapping (ZEM) 机制实现了稳定的2D到3D知识迁移。实验表明其在零样本设置下具有极强的泛化能力。尽管当前推理速度（1.5 FPS）无法满足车端实时控制需求，但该方法在自动驾驶离线数据挖掘、自动标注（Auto-labeling）及仿真场景生成方面具有极高的研究价值和行业潜力。
## Abstract: 
Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which stabilizes early training in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at our code repository.
