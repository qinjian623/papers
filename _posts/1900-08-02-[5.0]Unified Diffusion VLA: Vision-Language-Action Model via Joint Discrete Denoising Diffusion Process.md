---
layout: default
title: "[5.0]Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process"
---

# [5.0] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process

- Authors: Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao, Feilong Tang, Donglin Wa...
- [arXiv Link](https://arxiv.org/abs/2511.01718)
- [PDF Link](https://arxiv.org/pdf/2511.01718.pdf)

## Subfields
 VLA (Vision-Language-Action) 模型 / 机器人操作 (Robotic Manipulation) / Embodied AI
## Reason for Interest

本论文提出了一种统一扩散VLA（UD-VLA）模型，并通过联合离散去噪扩散过程（JD3P）实现图像生成和动作预测的协同优化。JD3P在单次去噪轨迹中整合了多种模态，使理解、生成和行动能够内在协同。论文在统一的多模态token空间和混合注意力机制的基础上构建模型，并设计了两阶段训练流程和多种推理时技术以优化性能和效率。

**自动驾驶关联性判断：** 本研究的核心方向是面向具身智能体（Embodied Agent）的机器人操作，通过视觉-语言-动作模型来理解自然语言指令并执行相应动作。其应用场景主要是机械臂操作，而非直接的车端自动驾驶。虽然VLA模型、多模态融合、预测、规划等概念与自动驾驶有高层次的共通之处，但具体任务、数据、评估基准和目标场景与自动驾驶领域存在显著差异。根据评审要求，如果不直接和车端自动驾驶相关，最高评分为5分。

**基于机器人操作领域的评价（但分数受限于自动驾驶相关性）：**
*   **创新性（高）：** 论文提出的JD3P通过同步去噪过程联合优化视觉生成和动作预测，以及在统一token空间和混合注意力机制下的设计，具有很强的创新性。这有效解决了现有VLA模型中模态分离或生成与行动解耦的局限性。未来图像作为动作预测的指导，以及离散扩散模型带来的并行解码能力，都是重要的亮点。
*   **实验完整性（高）：** 论文在CALVIN、LIBERO、SimplerEnv三个主流机器人操作基准上进行了全面评估，并与大量现有SOTA方法进行了详细比较。消融实验充分验证了混合注意力机制、未来图像生成、以及JD3P相对于其他解码机制的有效性。此外，还进行了真实世界机器人平台的实验，增强了结果的说服力。
*   **可信度（高）：** SOTA声称在多个基准测试上得到数据支持，性能提升显著，尤其是在推理速度上实现了4倍加速。实验结果和分析清晰明了，且对方法本身的局限性也有所讨论（例如图像生成保真度）。
*   **行业潜力（高，针对机器人操作领域）：** UD-VLA在统一理解、生成和行动方面展现的强大能力，以及显著提升的推理效率，对于开发更通用、更鲁棒的具身智能体和机器人操作策略具有巨大潜力。通过预测未来视觉状态来指导动作规划，有助于解决复杂、长时程的机器人任务。

综合来看，若在机器人操作领域进行评审，这会是一篇高质量且具有高创新性的论文。但由于其与车端自动驾驶的直接关联性较低，因此严格按照要求给出5分的最高分。
## Abstract: 
Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.
