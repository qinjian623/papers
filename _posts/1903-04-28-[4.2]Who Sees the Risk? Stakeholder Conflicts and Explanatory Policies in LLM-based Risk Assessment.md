---
layout: default
title: "[4.2]Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment"
---

# [4.2] Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment

- Authors: Srishti Yadav, Jasmina Gajcin, Erik Miehling, Elizabeth Daly
- [arXiv Link](https://arxiv.org/abs/2511.03152)
- [PDF Link](https://arxiv.org/pdf/2511.03152.pdf)

## Subfields
 AI治理 / LLM风险评估
## Reason for Interest

这篇论文的核心贡献在于AI治理和LLM风险评估领域，旨在理解不同利益相关者如何感知AI系统的风险，并解释这些风险感知中的冲突。论文采用LLM作为判断器，结合Risk Atlas Nexus和GloVE解释框架，生成利益相关者特定的可解释风险政策。虽然论文将“自动驾驶系统”作为其三个案例研究之一（其他为医疗AI和欺诈检测），但其关注点在于利用LLM生成利益相关者视角下的高层级风险评估，而非自动驾驶系统内部的3D感知、多传感器融合、轨迹预测、规划控制或仿真评估等技术细节。论文未涉及任何车端自动驾驶数据、模型或仿真。鉴于任务要求对非直接车端自动驾驶相关的研究最高评分不得超过5分，该论文的创新性、实验完整性和可信度在其所属领域（AI治理和可解释AI）内表现良好，但与自动驾驶核心技术研发的直接相关性较低，因此给出4.2分。
## Abstract: 
Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.
