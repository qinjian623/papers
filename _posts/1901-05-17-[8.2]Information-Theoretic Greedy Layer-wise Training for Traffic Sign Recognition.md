---
layout: default
title: "[8.2]Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition"
---

# [8.2] Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition

- Authors: Shuyan Lyu, Zhanzimo Wu, Junliang Du
- [arXiv Link](https://arxiv.org/abs/2510.27651)
- [PDF Link](https://arxiv.org/pdf/2510.27651.pdf)

## Subfields
 神经网络训练算法 / 2D 感知 (交通标志识别)
## Reason for Interest

论文提出了一种基于信息论确定性信息瓶颈（DIB）和矩阵Rényi α阶熵函数的新型贪婪分层训练方法（Greedy-DIB）。该方法从信息论角度对深度神经网络的端到端训练动态进行了系统分析，揭示了即使在E2EBP训练中，网络也倾向于逐层收敛并遵循马尔可夫信息瓶颈原理。其创新性体现在：1) 深入的理论分析和实证观察，为分层训练提供了坚实的基础；2) 提出了一种新颖的局部学习目标，通过结合交叉熵损失和信息压缩项（基于DIB和Rényi熵）来学习最小且充分的任务相关表示；3) 实验在多个通用图像分类数据集和主流CNN架构上验证了方法的有效性，显示其性能优于所有现有分层训练基线，并能与传统SGD方法性能媲美。更重要的是，该方法被成功应用于自动驾驶领域的关键2D感知任务——交通标志识别（包括检测和分类），并在CTSRD和GTSRB等数据集上取得了超越传统SGD训练的性能，这一结果具有显著的实践价值和行业潜力。分层训练在减少内存消耗和缓解梯度问题方面的优势，对于资源受限的车载自动驾驶系统尤为重要。虽然论文的核心贡献在于训练算法而非提出新的自动驾驶感知模型架构，但其在自动驾驶实际任务中展现出的优越性，使其成为自动驾驶研究中一个高价值的方向。
## Abstract: 
Modern deep neural networks (DNNs) are typically trained with a global cross-entropy loss in a supervised end-to-end manner: neurons need to store their outgoing weights; training alternates between a forward pass (computation) and a top-down backward pass (learning) which is biologically implausible. Alternatively, greedy layer-wise training eliminates the need for cross-entropy loss and backpropagation. By avoiding the computation of intermediate gradients and the storage of intermediate outputs, it reduces memory usage and helps mitigate issues such as vanishing or exploding gradients. However, most existing layer-wise training approaches have been evaluated only on relatively small datasets with simple deep architectures. In this paper, we first systematically analyze the training dynamics of popular convolutional neural networks (CNNs) trained by stochastic gradient descent (SGD) through an information-theoretic lens. Our findings reveal that networks converge layer-by-layer from bottom to top and that the flow of information adheres to a Markov information bottleneck principle. Building on these observations, we propose a novel layer-wise training approach based on the recently developed deterministic information bottleneck (DIB) and the matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each layer is trained jointly with an auxiliary classifier that connects directly to the output layer, enabling the learning of minimal sufficient task-relevant representations. We empirically validate the effectiveness of our training procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further demonstrate its applicability to a practical task involving traffic sign recognition. Our approach not only outperforms existing layer-wise training baselines but also achieves performance comparable to SGD.
