---
layout: default
title: "[7.8]RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera"
---

# [7.8] RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera

- Authors: Hafeez Husain Cholakkal, Stefano Arrigoni, Francesco Braghin
- [arXiv Link](https://arxiv.org/abs/2512.08262v1)
- [PDF Link](https://arxiv.org/pdf/2512.08262v1.pdf)

## Subfields
 Sensor Calibration / Multi-Sensor Fusion (LiDAR-RADAR-Camera)
## Reason for Interest

The paper addresses a critical and practical problem in autonomous driving: simultaneous online calibration of three sensor modalities (LiDAR, Radar, Camera). 

Strengths:
1. **Methodological Innovation**: The introduction of a Message Passing Network (MPN) to enforce loop closure consistency is a logical and effective addition to standard deep calibration pipelines. The soft-mask feature sharing strategy is well-motivated.
2. **Practicality**: The inclusion of an online calibration framework (temporal filtering, outlier rejection) makes it highly relevant for real-world deployment. Inference speed (54ms) supports real-time use.
3. **Comprehensive Ablation**: The ablation studies on feature sharing and MPN iterations clearly justify the architectural choices.

Weaknesses:
1. **Evaluation Scope**: While the evaluation on the View of Delft (VoD) dataset uses a reasonable test set (2247 frames), the comparison on the NuScenes dataset appears to be limited to a single scene (Scene 343). This significantly weakens the generalizability of the SOTA claim against the Peršić et al. baseline.
2. **Baseline limitations**: The baselines compared against are somewhat limited, and deep learning-based calibration methods often struggle with generalization across different sensor setups compared to geometric methods, which isn't fully explored here.

Overall, a solid engineering paper with high industry value, slightly held back by the limited scope of the comparative evaluation on public benchmarks.
## Abstract: 

