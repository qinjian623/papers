---
layout: default
title: "[8.8]AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models"
---

# [8.8] AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models

- Authors: Tianyi Yan, Tao Tang, Xingtai Gui, Yongkang Li, Jiasen Zhesng, Weiyao Huang, Lingdong Kon...
- [arXiv Link](https://arxiv.org/abs/2511.20325v1)
- [PDF Link](https://arxiv.org/pdf/2511.20325v1.pdf)

## Subfields
 端到端自动驾驶 / 世界模型 (World Models) / 强化学习 (RL)
## Reason for Interest

1. 创新性强：论文精准指出了当前世界模型存在的“乐观偏差”（Optimistic Bias）痛点，即无法正确预测非安全动作的后果。提出的“反事实合成”（Counterfactual Synthesis）数据生成流水线是一个极具价值的解决方案，使得世界模型能从“被动补全”转变为“公正判决”。
2. 方法论严谨：将公正世界模型（Impartial World Model）作为内部 Critic，结合 GRPO（Group Relative Policy Optimization）进行闭环策略优化的逻辑自洽且紧跟前沿（如 DeepSeek-R1 的强化学习思路）。
3. 实验扎实：在权威榜单 NavSim 上取得了 SOTA 成绩（91.9 PDMS），并专门构建了 Risk Foreseeing Benchmark (RFB) 来量化模型对危险场景的预测能力，消融实验充分证明了合成数据和模型修正模块的有效性。
4. 行业价值：该工作为解决自动驾驶中 Sim-to-Real gap 和长尾数据稀缺问题提供了新的生成式仿真路径，具有较高的学术和应用价值。
## Abstract: 

