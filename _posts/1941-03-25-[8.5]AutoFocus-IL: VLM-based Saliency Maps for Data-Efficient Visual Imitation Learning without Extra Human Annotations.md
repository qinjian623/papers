---
layout: default
title: "[8.5]AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations"
---

# [8.5] AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations

- Authors: Litian Gong, Fatemeh Bahrani, Yutai Zhou, Amin Banayeeanzade, Jiachen Li, Erdem Bıyık
- [arXiv Link](https://arxiv.org/abs/2511.18617v2)
- [PDF Link](https://arxiv.org/pdf/2511.18617v2.pdf)

## Subfields
 端到端自动驾驶 / 模仿学习 (End-to-End AD / Imitation Learning)
## Reason for Interest

1. 创新性强：论文提出利用视觉语言模型（Qwen2.5-VL）自动生成时序显著性图来监督模仿学习，巧妙地解决了传统‘视线引导模仿学习’需要昂贵人工眼动数据的问题，是一种低成本、高扩展性的数据增强方案。
2. 解决核心痛点：针对端到端学习中常见的‘因果混淆’（Causal Confusion）问题，通过 VLM 过滤背景干扰并聚焦关键物体（如红绿灯、车辆），显著提升了模型的泛化能力。
3. 实验扎实：在 CARLA 最新基准 Bench2Drive 上进行了严格评估，并在‘有干扰’（Confounded）设置下证明了其鲁棒性；同时辅以真机机械臂实验验证了方法的通用性。
4. 行业价值：为自动驾驶数据闭环提供了一种无需人工标注即可挖掘‘难例’和‘关键特征’的自动化路径，具有较高的落地潜力。扣分点在于仅限于仿真驾驶验证，缺乏实车路测数据。
## Abstract: 

