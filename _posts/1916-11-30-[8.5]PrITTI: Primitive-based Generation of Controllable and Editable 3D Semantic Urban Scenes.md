---
layout: default
title: "[8.5]PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Urban Scenes"
---

# [8.5] PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Urban Scenes

- Authors: Christina Ourania Tze, Daniel Dauner, Yiyi Liao, Dzmitry Tsishkou, Andreas Geiger
- [arXiv Link](https://arxiv.org/abs/2506.19117)
- [PDF Link](https://arxiv.org/pdf/2506.19117.pdf)

## Subfields
 World Modeling / 3D Scene Generation (Simulation)
## Reason for Interest

该论文提出了一种基于图元（Primitive-based）的3D城市场景生成方法，利用矢量化物体和光栅化地面的混合表示，替代了传统的体素网格。该方法不仅在生成质量（FID）和显存效率上显著优于体素基线，核心创新在于其带来的可编辑性：支持在潜在空间进行对象级操作（如移动车辆、调整植被密度）以及场景的Inpainting/Outpainting。这对自动驾驶仿真场景的大规模构建和边缘场景生成具有重要价值，解决了体素方法难以精细编辑的痛点。
## Abstract: 
Existing approaches to 3D semantic urban scene generation predominantly rely on voxel-based representations, which are bound by fixed resolution, challenging to edit, and memory-intensive in their dense form. In contrast, we advocate for a primitive-based paradigm where urban scenes are represented using compact, semantically meaningful 3D elements that are easy to manipulate and compose. To this end, we introduce PrITTI, a latent diffusion model that leverages vectorized object primitives and rasterized ground surfaces for generating diverse, controllable, and editable 3D semantic urban scenes. This hybrid representation yields a structured latent space that facilitates object- and ground-level manipulation. Experiments on KITTI-360 show that primitive-based representations unlock the full capabilities of diffusion transformers, achieving state-of-the-art 3D scene generation quality with lower memory requirements, faster inference, and greater editability than voxel-based methods. Beyond generation, PrITTI supports a range of downstream applications, including scene editing, inpainting, outpainting, and photo-realistic street-view synthesis. Code and models are publicly available at $\href{https://raniatze.github.io/pritti/}{https://raniatze.github.io/pritti}$.
