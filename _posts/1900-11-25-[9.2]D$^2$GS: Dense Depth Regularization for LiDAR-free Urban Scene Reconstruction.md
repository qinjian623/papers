---
layout: default
title: "[9.2]D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction"
---

# [9.2] D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction

- Authors: Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang
- [arXiv Link](https://arxiv.org/abs/2510.25173)
- [PDF Link](https://arxiv.org/pdf/2510.25173.pdf)

## Subfields
 LiDAR-free 3D 城市场景重建 / Gaussian Splatting for Autonomous Driving
## Reason for Interest

该论文提出了 D2GS，一个针对自动驾驶领域 LiDAR-free 城市动态场景重建的创新框架。它解决了传统 3DGS 方法对 LiDAR 依赖的痛点（数据采集成本高、标定困难、重投影误差、深度稀疏性），具有极高的行业应用潜力。

**创新性：**
1.  **LiDAR-free 密集初始化与渐进式剪枝：** 首次利用预训练的多视图深度估计模型生成密集点云作为初始几何先验，并通过基于高斯不透明度的渐进式剪枝策略有效管理计算成本和内存，同时提高重建质量。
2.  **扩散模型引导的深度增强器：** 引入了一个新颖的、基于扩散模型的深度增强器（Depth Enhancer），通过迭代优化高斯几何与预测的密集度量深度，并利用深度基础模型的扩散先验和多视图几何一致性（通过置信度加权的 Lref、Lw、Lsmooth 损失）来增强深度图。这种将扩散模型与 3DGS 深度精炼结合的方式具有很强的创新性。
3.  **道路节点几何先验：** 针对自动驾驶场景的特点，引入了专门的道路节点（Road Node），通过对高斯的位置、形状和法线属性进行约束，有效提升了地面几何的重建精度，增强了场景的结构化信息。

**实验完整性与可信度：**
1.  **数据集选择：** 在自动驾驶领域广泛使用的 Waymo NOTR Dynamic32 数据集上进行了全面实验，验证了方法在动态城市环境下的有效性。
2.  **基线对比：** 与包括使用 LiDAR 的 S3GS, PVG, OmniRe 以及多个 LiDAR-free 基线（如 OmniRe w/o LiDAR, OmniRe+DS）进行了充分比较，展示了 D2GS 的优越性。
3.  **指标评估：** 使用图像重建的标准指标 (PSNR, SSIM, LPIPS) 和深度估计的严格指标 (L1, Abs. Rel., RMSE, δ < 1.25) 进行评估，结果可靠。
4.  **消融研究：** 详细的消融实验清晰地展示了渐进式剪枝、深度增强器及其内部各个损失项、以及道路节点对整体性能的贡献，证明了每个组件的有效性。
5.  **定性分析：** 提供了丰富的定性结果图，直观地展示了 D2GS 在图像重建和深度估计方面的卓越细节和几何一致性，尤其在 LiDAR 缺失或稀疏区域的表现。

**行业潜力：**
该方法直接解决了自动驾驶中高保真 3D 场景重建对昂贵且难以标定 LiDAR 的依赖问题。LiDAR-free 的特性将极大地降低数据采集成本，加速高精地图构建、仿真环境生成和在线感知系统的发展。对于端到端自动驾驶系统，能够仅凭相机输入构建高质量的 3D 场景模型，具有巨大的实际应用价值和商业潜力。该工作直接服务于车端自动驾驶的关键需求，有望推动相关技术的大规模落地。
## Abstract: 
Recently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.
