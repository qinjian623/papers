---
layout: default
title: "[4.5]EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation"
---

# [4.5] EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation

- Authors: Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield
- [arXiv Link](https://arxiv.org/abs/2511.02953)
- [PDF Link](https://arxiv.org/pdf/2511.02953.pdf)

## Subfields
 事件相机感知 / 单目深度估计 / 数据集
## Reason for Interest

该论文提出了EvtSlowTV，一个据称是目前最大且多样化的事件相机数据集，通过对公开YouTube视频进行事件仿真生成。其核心贡献在于尝试解决事件相机领域大规模标注数据集稀缺的问题，并提出了一个自监督的深度估计框架来利用这一数据集。

创新性方面，从大量多样化的YouTube视频中合成事件数据以构建大规模数据集，是一个有创意的扩展数据规模的方法，也符合自监督学习的趋势。提出的自监督深度估计框架，结合了对比最大化损失（Contrast Maximization Loss）和师生学习策略，旨在利用事件数据的异步特性并避免对外部传感器的依赖，这些方法方向上是合理的。

然而，论文在实验验证和结果可信度方面存在显著问题。首先，EvtSlowTV数据集是通过模拟器（ESIM）从传统帧视频合成的，并非真实事件相机捕获的数据。尽管场景内容多样化，但合成的事件数据可能无法完全捕获真实事件相机特有的噪声模式和传感器特性，这在一定程度上削弱了其“自然主义”的声明。更严重的是，在与现有方法的对比实验中（表2），虽然该方法在Absolute mean depth error (Abs mean)指标上表现出优越性，但在log-mean-square error (rms_log)指标上，其误差值比所有基线方法高出数倍（3-4倍）。论文作者也承认：“然而，虽然我们的方法在估计绝对误差方面表现出色，但在维持比例深度估计方面却很吃力。”对于自动驾驶而言，准确的相对深度估计（rms_log衡量）至关重要，因为这直接关系到碰撞避免、安全距离判断等任务。如此高的rms_log误差，使得该方法的实用价值大打折扣，甚至可能在实际应用中带来安全隐患。论文在摘要和结论中仍强调“显著优于现有方法”或“性能与SOTA相当”，这与实际数据存在明显矛盾，严重影响了结果的可信度。

鉴于上述关键问题，特别是对自动驾驶至关重要的相对深度估计性能不佳，尽管数据集的规模和自监督学习的方向有一定价值，但整体研究结果的有效性和可靠性不足以支撑其在自动驾驶领域产生高影响力。因此，评分必须十分严格。
## Abstract: 
Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.
